- abstract: This paper proposes a framework to address the issue of data scarcity
    in Document-Grounded Dialogue Systems(DGDS). Our model leverages high-resource
    languages to enhance the capability of dialogue generation in low-resource languages.
    Specifically, We present a novel pipeline CLEM (Cross-Lingual Enhanced Model)
    including adversarial training retrieval (Retriever and Re-ranker), and Fid (fusion-in-decoder)
    generator. To further leverage high-resource language, we also propose an innovative
    architecture to conduct alignment across different languages with translated training.
    Extensive experiment results demonstrate the effectiveness of our model and we
    achieved 4th place in the DialDoc 2023 Competition. Therefore, CLEM can serve
    as a solution to resource scarcity in DGDS and provide useful guidance for multi-lingual
    alignment tasks.
  authors:
  - Qi Gou
  - Zehua Xia
  - Wenzhe Du
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - DialDoc
  forum: ''
  id: DialDoc_2
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Cross-lingual Data Augmentation for Document-grounded Dialog Systems in Low
    Resource Languages
  tldr: This paper proposes a framework to address the issue of data scarcity in Document-Grounded
    Dialogue Systems(DGDS). Our model leverages high-resource languages to enhance
    the capability of dialogue generation in low-resource languages. Specifically,
    We present a novel pipeline CLEM (Cross-Lingual Enh
  track: Proceedings of the Third DialDoc Workshop on Document-grounded Dialogue and
    Conversational Question Answering
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Previous research on open-domain question answering (QA) mainly focuses
    on questions with short answers. However, information-seeking QA often requires
    various formats of answers depending on the nature of the questions, e.g., why/how
    questions typically require a long answer. In this paper, we present MoQA, a benchmark
    for open-domain QA that requires building one system that can provide short, medium,
    long, and yes/no answers to different questions accordingly. MoQA builds upon
    Natural Questions with multiple types of questions and additional crowdsourcing
    efforts to ensure high query quality. We adapt state-of-the-art models, and reveal
    unique findings in multi-type open-domain QA: (1) For retriever-reader models,
    training one retriever on all types achieves the overall best performance, but
    it is challenging to train one reader model to output answers of different formats,
    or to train a question classifier to distinguish between types; (2) An end-to-end
    closed-book QA model trained on multiple types struggles with the task across
    the board; (3) State-of-the-art large language models such as the largest GPT-3
    models (Brown et al., 2020; Ouyang et al., 2022) also lag behind open-book QA
    models. Our benchmark and analysis call for more effort into building versatile
    open-domain QA models in the future. '
  authors:
  - Howard Yen
  - Tianyu Gao
  - Jinhyuk Lee
  - Danqi Chen
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - DialDoc
  forum: ''
  id: DialDoc_3
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long -
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'MoQA: Benchmarking Multi-Type Open-Domain Question Answering'
  tldr: Previous research on open-domain question answering (QA) mainly focuses on
    questions with short answers. However, information-seeking QA often requires various
    formats of answers depending on the nature of the questions, e.g., why/how questions
    typically require a long answer. In this paper, we pres
  track: Proceedings of the Third DialDoc Workshop on Document-grounded Dialogue and
    Conversational Question Answering
  underline_id: null
  underline_url: null
  video_url: null
- abstract: "Transferring DGD models from high-resource languages to low-resource\
    \ languages is a meaningful but challenging task. Being able to provide multilingual\
    \ responses to multilingual documents further complicates the task. This paper\
    \ describes our method at DialDoc23 Shared Task (Document-Grounded Dialogue and\
    \ Conversational Question Answering) for generate responses based on the most\
    \ relevant passage retrieved. We divide it into three steps of retrieval, re-ranking\
    \ and generation. Our methods include negative sample augmentation, prompt learning,\
    \ pseudo-labeling and ensemble. On the submission page, we rank 2nd based on the\
    \ sum\_of token-level F1, SacreBleu\_and Rouge-L scores used for the final evaluation,\
    \ and get the total score of 210.25."
  authors:
  - Xiaocheng Zhang
  - Huang Qing
  - Fu Lin
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - DialDoc
  forum: ''
  id: DialDoc_4
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short -
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Exploration of multilingual prompts in document-grounded dialogue
  tldr: Transferring DGD models from high-resource languages to low-resource languages
    is a meaningful but challenging task. Being able to provide multilingual responses
    to multilingual documents further complicates the task. This paper describes our
    method at DialDoc23 Shared Task (Document-Grounded Dialog
  track: Proceedings of the Third DialDoc Workshop on Document-grounded Dialogue and
    Conversational Question Answering
  underline_id: null
  underline_url: null
  video_url: null
- abstract: "With the power of large pretrained language models, various research\
    \ works have integrated knowledge into dialogue systems. The traditional techniques\
    \ treat knowledge as part of the input sequence for the dialogue system, prepending\
    \ a set of knowledge statements in front of dialogue history.\nHowever, such a\
    \ mechanism forces knowledge sets to be concatenated in an ordered manner, making\
    \ models implicitly pay imbalanced attention to the sets during training.\nIn\
    \ this paper, we first investigate how the order of the knowledge set can influence\
    \ autoregressive dialogue systems' responses. \nWe conduct experiments on two\
    \ commonly used dialogue datasets with two types of transformer-based models and\
    \ find that models view the input knowledge unequally. \nTo this end, we propose\
    \ a simple and novel technique to alleviate the order effect by modifying the\
    \ position embeddings of knowledge input in these models. With the proposed position\
    \ embedding method, the experimental results show that each knowledge statement\
    \ is uniformly considered to generate responses."
  authors:
  - Hsuan Su
  - Shachi H. Kumar
  - Sahisnu Mazumder
  - Wenda Chen
  - Ramesh Manuvinakurike
  - Eda Okur
  - Saurav Sahay
  - Lama Nachman
  - Shang-Tse Chen
  - Hung-yi Lee
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - DialDoc
  forum: ''
  id: DialDoc_5
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short -
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Position Matters! Empirical Study of Order Effect in Knowledge-grounded Dialogue
  tldr: 'With the power of large pretrained language models, various research works
    have integrated knowledge into dialogue systems. The traditional techniques treat
    knowledge as part of the input sequence for the dialogue system, prepending a
    set of knowledge statements in front of dialogue history.

    However'
  track: Proceedings of the Third DialDoc Workshop on Document-grounded Dialogue and
    Conversational Question Answering
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'The Dialdoc23 shared task presents a Multilingual Document-Grounded Dialogue
    Systems (MDGDS) challenge, where system responses are generated in multiple languages
    using user''s queries, historical dialogue records and relevant passages. A major
    challenge for this task is the limited training data available in low-resource
    languages such as French and Vietnamese. In this paper, we propose Cascaded Prompt-based
    Post-training Models, dividing the task into three subtasks: Retrieval, Reranking
    and Generation. We conduct post-training on high-resource language such as English
    and Chinese to enhance performance of low-resource languages by using the similarities
    of languages. Additionally, we utilize the prompt method to activate model''s
    ability on diverse languages within the dialogue domain and explore which prompt
    is a good prompt. Our comprehensive experiments demonstrate the effectiveness
    of our proposed methods, which achieved the first place on the leaderboard with
    a total score of 215.40 in token-level F1, SacreBleu, and Rouge-L metrics.'
  authors:
  - Jun Liu
  - Shuang Cheng
  - Zineng Zhou
  - Yang Gu
  - Jian Ye
  - Haiyong Luo
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - DialDoc
  forum: ''
  id: DialDoc_6
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long -
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Enhancing Multilingual Document-Grounded Dialogue Using Cascaded Prompt-Based
    Post-Training Models
  tldr: The Dialdoc23 shared task presents a Multilingual Document-Grounded Dialogue
    Systems (MDGDS) challenge, where system responses are generated in multiple languages
    using user's queries, historical dialogue records and relevant passages. A major
    challenge for this task is the limited training data ava
  track: Proceedings of the Third DialDoc Workshop on Document-grounded Dialogue and
    Conversational Question Answering
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Document-grounded dialogue generation based on multilingual is a challenging
    and realistic task. Unlike previous tasks, it need to tackle with multiple high-resource
    languages facilitating low-resource languages. This paper summarizes our research
    based on a three-stage pipeline that includes retrieval, re-rank and generation
    where each component is individually optimized.  In different languages with limited
    data scenarios, we mainly improve the robustness of the pipeline through data
    augmentation and embedding perturbation with purpose of improving the performance
    designing three training methods: cross-language enhancement training, weighted
    training with neighborhood distribution augmentation, and ensemble adversarial
    training, all of that can be used as plug and play modules. Through experiments
    with different settings, it has been shown that our methods can effectively improve
    the generalization performance of pipeline  with score ranking 6th  among the
    public submissions on leaderboards.'
  authors:
  - Hai Li
  - Yang Li
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - DialDoc
  forum: ''
  id: DialDoc_7
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short -
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Enhanced Training Methods for Multiple Languages
  tldr: Document-grounded dialogue generation based on multilingual is a challenging
    and realistic task. Unlike previous tasks, it need to tackle with multiple high-resource
    languages facilitating low-resource languages. This paper summarizes our research
    based on a three-stage pipeline that includes retrie
  track: Proceedings of the Third DialDoc Workshop on Document-grounded Dialogue and
    Conversational Question Answering
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Multilingual  document-grounded dialogue, where the system is required
    to generate responses based on both the conversation Multilingual  context and
    external knowledge sources.

    Traditional pipeline methods for knowledge identification and response generation,
    while effective in certain scenarios, suffer from error propagation issues and
    fail to capture the interdependence between these two sub-tasks. To overcome these
    challenges, we propose the application of the SLDT method, which treats passage-knowledge
    selection as a sequential decision process rather than a single-step decision
    process.

    We achieved winner 3rd in dialdoc 2023 and we also validated the effectiveness
    of our method on other datasets. The ablation experiment also shows that our method
    significantly improves the basic model compared to other methods.'
  authors:
  - Zhanyu Ma
  - Zeming Liu
  - Jian Ye
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - DialDoc
  forum: ''
  id: DialDoc_10
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long -
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'SLDT: Sequential Latent Document Transformer for Multilingual Document-based
    Dialogue'
  tldr: 'Multilingual  document-grounded dialogue, where the system is required to
    generate responses based on both the conversation Multilingual  context and external
    knowledge sources.

    Traditional pipeline methods for knowledge identification and response generation,
    while effective in certain scenarios, s'
  track: Proceedings of the Third DialDoc Workshop on Document-grounded Dialogue and
    Conversational Question Answering
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In healthcare, the ability to care for oneself is reflected in the "Activities
    of Daily Living (ADL)," which serve as a measure of functional ability (functioning).
    A lack of functioning may lead to poor living conditions requiring personal care
    and assistance. To accurately identify those in need of support, assistance programs
    continuously evaluate participants' functioning across various domains. However,
    the assessment process may encounter consistency issues when multiple assessors
    with varying levels of expertise are involved. Novice assessors, in particular,
    may lack the necessary preparation for real-world interactions with participants.
    To address this issue, we developed a dialogue system that simulates interactions
    between assessors and individuals of varying functioning in a natural and reproducible
    way. The dialogue system consists of two major modules, one for natural language
    understanding (NLU) and one for natural language generation (NLG), respectively.
    In order to generate responses consistent with the underlying knowledge base,
    the dialogue system requires both an understanding of the user's query and of
    biographical details of an individual being simulated. To fulfill this requirement,
    we experimented with query classification and generated responses based on those
    biographical details using some recently released InstructGPT-like models.
  authors:
  - Zhecheng Sheng
  - Raymond Finzel
  - lucke096@umn.edu lucke096@umn.edu
  - gahmx008@umn.edu gahmx008@umn.edu
  - Maria Gini
  - Serguei Pakhomov
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - DialDoc
  forum: ''
  id: DialDoc_12
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long -
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'A Dialogue System for Assessing Activities of Daily Living: Improving Consistency
    with Grounded Knowledge'
  tldr: In healthcare, the ability to care for oneself is reflected in the "Activities
    of Daily Living (ADL)," which serve as a measure of functional ability (functioning).
    A lack of functioning may lead to poor living conditions requiring personal care
    and assistance. To accurately identify those in need o
  track: Proceedings of the Third DialDoc Workshop on Document-grounded Dialogue and
    Conversational Question Answering
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Collecting and constructing human-annotated corpora for training conversational
    question-answering (CQA) models has recently been shown to be inefficient and
    costly. To solve this problem, previous works have proposed training QA models
    with automatically generated QA data. In this work, we extend earlier studies
    on QA synthesis, and propose an efficient QA data generation algorithm under conversational
    settings. Our model recognizes potential dialogue topics, generates corresponding
    questions, and extracts answers from grounding passages. To improve the quality
    of generated QAs and downstream self-training of CQA models, we propose dropout
    and agreement-based QA selection methods. We conduct experiments on both data
    augmentation and domain adaptation settings. Experiments on the QuAC and Doc2Dial
    tasks show that the proposed method can significantly improve the quality of generated
    QA data, and also improves the accuracy of self-trained CQA models based on the
    constructed training corpora.
  authors:
  - Tianhua Zhang
  - Liping Tang
  - Wei Fang
  - Hongyin Luo
  - Xixin Wu
  - Helen Meng
  - James Glass
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - DialDoc
  forum: ''
  id: DialDoc_14
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long -
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'ConvRGX: Recognition, Generation, and Extraction for Self-trained Conversational
    Question Answering'
  tldr: Collecting and constructing human-annotated corpora for training conversational
    question-answering (CQA) models has recently been shown to be inefficient and
    costly. To solve this problem, previous works have proposed training QA models
    with automatically generated QA data. In this work, we extend e
  track: Proceedings of the Third DialDoc Workshop on Document-grounded Dialogue and
    Conversational Question Answering
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The DialDoc 2023 shared task has expanded the document-grounded dialogue
    task to encompass multiple languages, despite having limited annotated data. This
    paper assesses the effectiveness of both language-agnostic and language-aware
    paradigms for multilingual pre-trained transformer models in a bi-encoder-based
    dense passage retriever (DPR), concluding that the language-agnostic approach
    is superior. Additionally, the study investigates the impact of query rewriting
    techniques using large language models, such as ChatGPT, on multilingual, document-grounded
    question-answering systems. The experiments conducted demonstrate that, for the
    examples examined, query rewriting does not enhance performance compared to the
    original queries. This failure is due to topic switching in final dialogue turns
    and irrelevant topics being considered for query rewriting.
  authors:
  - Srinivas Gowriraj
  - Soham Dinesh Tiwari
  - Mitali Potnis
  - Srijan Bansal
  - Teruko Mitamura
  - Eric Nyberg
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - DialDoc
  forum: ''
  id: DialDoc_15
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short -
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Language-Agnostic Transformers and Assessing ChatGPT-Based Query Rewriting
    for Multilingual Document-Grounded QA
  tldr: The DialDoc 2023 shared task has expanded the document-grounded dialogue task
    to encompass multiple languages, despite having limited annotated data. This paper
    assesses the effectiveness of both language-agnostic and language-aware paradigms
    for multilingual pre-trained transformer models in a bi-e
  track: Proceedings of the Third DialDoc Workshop on Document-grounded Dialogue and
    Conversational Question Answering
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Crowd-sourcing has been one of the primary ways to curate conversational
    data, specially for certain scenarios like grounding in knowledge. In this setting,
    using online platforms like AMT, non-expert participants are hired to converse
    with each other, following  instructions which try to guide the outcome towards
    the desired format. The resulting data then is used for different parts of dialog
    modelling like knowledge selection and response selection/generation.

    In this work, we take a closer look into two of the most popular knowledge grounded
    dialog (KGD) datasets. Investigating potential biases and artefacts in knowledge
    selection labels, we observe that in many cases the `knowledge selection flow''
    simply follows the order of presented knowledge pieces. In Wizard of Wikipedia
    (the most popular KGD dataset) we use simple content-agnostic models based on
    this bias to get significant knowledge selection performance. In Topical-Chat
    we see a similar correlation between the knowledge selection sequence and the
    order of entities and their segments, as provided to crowd-source workers. We
    believe that the observed results, question the significance and origin of the
    presumed dialog-level attributes like `knowledge flow'' in these crowd-sourced
    datasets. '
  authors:
  - Ehsan Lotfi
  - Maxime De Bruyn
  - jeska.buhmann@uantwerpen.be jeska.buhmann@uantwerpen.be
  - Walter Daelemans
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - DialDoc
  forum: ''
  id: DialDoc_18
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long -
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Follow the Knowledge: Structural Biases and Artefacts in Knowledge Grounded
    Dialog Datasets'
  tldr: 'Crowd-sourcing has been one of the primary ways to curate conversational
    data, specially for certain scenarios like grounding in knowledge. In this setting,
    using online platforms like AMT, non-expert participants are hired to converse
    with each other, following  instructions which try to guide the '
  track: Proceedings of the Third DialDoc Workshop on Document-grounded Dialogue and
    Conversational Question Answering
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In contrast to large text corpora, knowledge graphs (KG) provide dense
    and structured representations of factual information. This makes them attractive
    for systems that supplement or ground the knowledge found in pre-trained language
    models with an external knowledge source. This has especially been the case for
    classification tasks, where recent work has focused on creating pipeline models
    that retrieve information from KGs like ConceptNet as additional context. Many
    of these models consist of multiple components, and although they differ in the
    number and nature of these parts, they all have in common that for some given
    text query, they attempt to identify and retrieve a relevant subgraph from the
    KG. Due to the noise and idiosyncrasies often found in KGs, it is not known how
    current methods compare to a scenario where the aligned subgraph is completely
    relevant to the query. In this work, we try to bridge this knowledge gap by reviewing
    current approaches to text-to-KG alignment and evaluating them on two datasets
    where manually created graphs are available, providing insights into the effectiveness
    of current methods. We release our code for reproducibility.
  authors:
  - Sondre Wold
  - "Lilja \xD8vrelid"
  - Erik Velldal
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - MATCHING
  forum: ''
  id: MATCHING_M1
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Text-To-KG Alignment: Comparing Current Methods on Classification Tasks'
  tldr: In contrast to large text corpora, knowledge graphs (KG) provide dense and
    structured representations of factual information. This makes them attractive
    for systems that supplement or ground the knowledge found in pre-trained language
    models with an external knowledge source. This has especially bee
  track: The First Workshop on Matching From Unstructured and Structured Data (MATCHING
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Humans often describe complex quantitative data using trend-based patterns.
    Trend-based patterns can be interpreted as higher order functions and relations
    over numerical data such as extreme values, rates of change, or cyclical repetition.
    One application where trends abound are descriptions of numerical tabular data.
    Therefore, the alignment of numerical tables and textual description of trends
    enables easier interpretations of tables. Most existing approaches can align quantities
    in text with tabular data but are unable to detect and align trend-based patterns
    about data. In this paper, we introduce the initial steps for aligning trend-based
    patterns about the data, i.e. the detection of textual description of trends and
    the alignment of trends with a relevant table. We introduce the problem of identifying
    quantifiably verifiable statements (QVS) in the text and aligning them with tables
    and datasets. We define the structure of these statements and implement a structured
    based detection. In our experiments, we demonstrate our method can detect and
    align these statements from several domains and compare favorably with traditional
    sequence labeling methods.
  authors:
  - Pegah Jandaghi
  - Jay Pujara
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - MATCHING
  forum: ''
  id: MATCHING_M6
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Identifying Quantifiably Verifiable Statements from Text
  tldr: Humans often describe complex quantitative data using trend-based patterns.
    Trend-based patterns can be interpreted as higher order functions and relations
    over numerical data such as extreme values, rates of change, or cyclical repetition.
    One application where trends abound are descriptions of num
  track: The First Workshop on Matching From Unstructured and Structured Data (MATCHING
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Event-event temporal relation extraction aims to extract the temporal
    order between a pair of event mentions, which is usually used to construct temporal
    event graphs. However, event graphs generated by existing methods are usually
    globally inconsistent (event graphs containing cycles), semantically irrelevant
    (two unrelated events having temporal links), and context unaware (neglecting
    neighborhood information of an event node). In this paper, we propose a novel
    event-event temporal relation extraction method to address these limitations.
    Our model combines a pretrained language model and a graph neural network to output
    event embeddings, which captures the contextual information of event graphs. Moreover,
    to achieve global consistency and semantic relevance, (1) event temporal order
    should be in accordance with the norm of their embeddings, and (2) two events
    have temporal relation only if their embeddings are close enough. Experimental
    results on a real-world event dataset demonstrate that our method achieves state-of-the-art
    performance and generates high-quality event graphs.
  authors:
  - Xiaomeng Jin
  - Haoyang Wen
  - Xinya Du
  - Heng Ji
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - MATCHING
  forum: ''
  id: MATCHING_M8
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Toward Consistent and Informative Event-Event Temporal Relation Extraction
  tldr: Event-event temporal relation extraction aims to extract the temporal order
    between a pair of event mentions, which is usually used to construct temporal
    event graphs. However, event graphs generated by existing methods are usually
    globally inconsistent (event graphs containing cycles), semantically
  track: The First Workshop on Matching From Unstructured and Structured Data (MATCHING
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Event extraction is a complex task that involves extracting events from
    unstructured text. Prior classification-based methods require comprehensive entity
    annotations for joint training, while newer generation-based methods rely on heuristic
    templates containing oracle information such as event type, which is often unavailable
    in real-world scenarios. In this study, we consider a more realistic task setting,
    namely the Oracle-Free Event Extraction (OFEE) task, where only the input context
    is given, without any oracle information including event type, event ontology,
    or trigger word. To address this task, we propose a new framework, COFFEE. This
    framework extracts events solely based on the document context, without referring
    to any oracle information. In particular, COFFEE introduces a contrastive selection
    model to refine the generated triggers and handle multi-event instances. Our proposed
    COFFEE outperforms state-of-the-art approaches in the oracle-free setting of the
    event extraction task, as evaluated on two public variants of the ACE05 benchmark.
    The code used in our study has been made publicly available.
  authors:
  - Meiru Zhang
  - Yixuan Su
  - Zaiqiao Meng
  - Zihao Fu
  - Nigel Collier
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - MATCHING
  forum: ''
  id: MATCHING_M9
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'COFFEE: A Contrastive Oracle-Free Framework for Event Extraction'
  tldr: Event extraction is a complex task that involves extracting events from unstructured
    text. Prior classification-based methods require comprehensive entity annotations
    for joint training, while newer generation-based methods rely on heuristic templates
    containing oracle information such as event type
  track: The First Workshop on Matching From Unstructured and Structured Data (MATCHING
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Relation extraction is a crucial language processing task for various
    downstream applications, including knowledge base completion, question answering,
    and summarization. Traditional relation-extraction techniques, however, rely on
    a predefined set of relations and model the extraction as a classification task.
    Consequently, such closed-world extraction methods are insufficient for inducing
    novel relations from a corpus. Unsupervised techniques like OpenIE, which extract
    <head, relation, tail> triples, generate relations that are too general for practical
    information extraction applications. In this work, we contribute the following:
    1) We motivate and introduce a new task, corpus-based task-specific relation discovery.
    2) We adapt existing data sources to create Wiki-Art, a novel dataset for task-specific
    relation discovery. 3) We develop a novel framework for relation discovery using
    zero-shot entity linking, prompting, and type-specific clustering. Our approach
    effectively connects unstructured text spans to their shared underlying relations,
    bridging the data-representation gap and significantly outperforming baselines
    on both quantitative and qualitative metrics. Our code and data are available
    in our GitHub repository.'
  authors:
  - Karthik Ramanan
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - MATCHING
  forum: ''
  id: MATCHING_M11
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Corpus-Based Task-Specific Relation Discovery
  tldr: Relation extraction is a crucial language processing task for various downstream
    applications, including knowledge base completion, question answering, and summarization.
    Traditional relation-extraction techniques, however, rely on a predefined set
    of relations and model the extraction as a classifi
  track: The First Workshop on Matching From Unstructured and Structured Data (MATCHING
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Fifteen years of work on entity linking has established the importance
    of different information sources in making linking decisions: mention and entity
    name similarity, contextual relevance, and features of the knowledge base. Modern
    state-of-the-art systems build on these features, including through neural representations
    (Wu et al., 2020). In contrast to this trend, the autoregressive language model
    GENRE (De Cao et al., 2021) generates normalized entity names for mentions and
    beats many other entity linking systems, despite making no use of knowledge base
    (KB) information. How is this possible? We analyze the behavior of GENRE on several
    entity linking datasets and demonstrate that its performance stems from memorization
    of name patterns. In contrast, it fails in cases that might benefit from using
    the KB. We experiment with a modification to the model to enable it to utilize
    KB information, highlighting challenges to incorporating traditional entity linking
    information sources into autoregressive models.'
  authors:
  - Elliot Schumacher
  - James Mayfield
  - Mark Dredze
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - MATCHING
  forum: ''
  id: MATCHING_M12
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: On the Surprising Effectiveness of Name Matching Alone in Autoregressive
    Entity Linking
  tldr: 'Fifteen years of work on entity linking has established the importance of
    different information sources in making linking decisions: mention and entity
    name similarity, contextual relevance, and features of the knowledge base. Modern
    state-of-the-art systems build on these features, including throug'
  track: The First Workshop on Matching From Unstructured and Structured Data (MATCHING
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: "Large Language Models (LLMs) are capable of performing zero-shot closed-book\
    \ question answering tasks, based on their internal knowledge stored in parameters\
    \ during pre-training. However, such internalized knowledge might be insufficient\
    \ and incorrect, which could lead LLMs to generate factually wrong answers. Furthermore,\
    \ fine-tuning LLMs to update their knowledge is expensive. To this end, we propose\
    \ to augment the knowledge directly in the input of LLMs. Specifically, we first\
    \ retrieve the relevant facts to the input question from the knowledge graph based\
    \ on semantic similarities between the question and its associated facts. After\
    \ that, we prepend the retrieved facts to the input question in the form of the\
    \ prompt, which is then forwarded to LLMs to generate the answer. Our framework,\
    \ Knowledge-Augmented language model PromptING (KAPING), requires no model training,\
    \ thus completely zero-shot. We validate the performance of our KAPING framework\
    \ on the knowledge graph question answering task, that aims to answer the user\u2019\
    s question based on facts over a knowledge graph, on which ours outperforms relevant\
    \ zero-shot baselines by up to 48% in average, across multiple LLMs of various\
    \ sizes."
  authors:
  - Jinheon Baek
  - Alham Aji
  - Amir Saffari
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - MATCHING
  forum: ''
  id: MATCHING_M14
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph
    Question Answering
  tldr: Large Language Models (LLMs) are capable of performing zero-shot closed-book
    question answering tasks, based on their internal knowledge stored in parameters
    during pre-training. However, such internalized knowledge might be insufficient
    and incorrect, which could lead LLMs to generate factually wro
  track: The First Workshop on Matching From Unstructured and Structured Data (MATCHING
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Lihu Chen
  - Simon Razniewski
  - Gerhard Weikum
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - MATCHING
  forum: ''
  id: MATCHING_M15
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Knowledge Base Completion for Long-Tail Entities
  tldr: ''
  track: The First Workshop on Matching From Unstructured and Structured Data (MATCHING
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Entity standardization maps noisy mentions from free-form text to standard
    entities in a knowledge base. The unique challenge of this task relative to other
    entity-related tasks is the lack of surrounding context and numerous variations
    in the surface form of the mentions, especially when it comes to generalization
    across domains where labeled data is scarce. Previous research mostly focuses
    on developing models either heavily relying on context, or dedicated solely to
    a specific domain. In contrast, we propose CoSiNES, a generic and adaptable framework
    with Contrastive Siamese Network for Entity Standardization that effectively adapts
    a pretrained language model to capture the syntax and semantics of the entities
    in a new domain. We construct a new dataset in the technology domain, which contains
    640 technical stack entities and 6,412 mentions collected from industrial content
    management systems. We demonstrate that CoSiNES yields higher accuracy and faster
    runtime than baselines derived from leading methods in this domain. CoSiNES also
    achieves competitive performance in four standard datasets from the chemistry,
    medicine, and biomedical domains, demonstrating its cross-domain applicability.
    Code and data is available at https://github.com/konveyor/tackle-container-advisor/tree/main/entity_standardizer/cosines
  authors:
  - Jiaqing Yuan
  - Michele Merler
  - Mihir Choudhury
  - Raju Pavuluri
  - Munindar Singh
  - Maja Vukovic
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - MATCHING
  forum: ''
  id: MATCHING_M16
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'CoSiNES: Contrastive Siamese Network for Entity Standardization'
  tldr: Entity standardization maps noisy mentions from free-form text to standard
    entities in a knowledge base. The unique challenge of this task relative to other
    entity-related tasks is the lack of surrounding context and numerous variations
    in the surface form of the mentions, especially when it comes t
  track: The First Workshop on Matching From Unstructured and Structured Data (MATCHING
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Since conventional knowledge embedding models cannot take full advantage
    of the abundant textual information, there have been extensive research efforts
    in enhancing knowledge embedding using texts. However, existing enhancement approaches
    cannot apply to temporal knowledge graphs (tKGs), which contain time-dependent
    event knowledge with complex temporal dynamics. Specifically, existing enhancement
    approaches often assume knowledge embedding is time-independent. In contrast,
    the entity embedding in tKG models usually evolves, which poses the challenge
    of aligning temporally relevant texts with entities. To this end, we propose to
    study enhancing temporal knowledge embedding with textual data in this paper.
    As an approach to this task, we propose Enhanced Temporal Knowledge Embeddings
    with Contextualized Language Representations (ECOLA), which takes the temporal
    aspect into account and injects textual information into temporal knowledge embedding.
    To evaluate ECOLA, we introduce three new datasets for training and evaluating
    ECOLA. Extensive experiments show that ECOLA significantly enhances temporal KG
    embedding models with up to 287% relative improvements regarding Hits@1 on the
    link prediction task. The code and models are publicly available.
  authors:
  - Zhen Han
  - Ruotong Liao
  - Jindong Gu
  - Yao Zhang
  - Zifeng Ding
  - Yujia Gu
  - Heinz Koeppl
  - "Hinrich Sch\xFCtze"
  - Volker Tresp
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - MATCHING
  forum: ''
  id: MATCHING_F1
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'ECOLA: Enhancing Temporal Knowledge Embeddings with Contextualized Language
    Representations'
  tldr: Since conventional knowledge embedding models cannot take full advantage of
    the abundant textual information, there have been extensive research efforts in
    enhancing knowledge embedding using texts. However, existing enhancement approaches
    cannot apply to temporal knowledge graphs (tKGs), which cont
  track: The First Workshop on Matching From Unstructured and Structured Data (MATCHING
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Information extraction (IE) systems aim to automatically extract structured
    information, such as named entities, relations between entities, and events, from
    unstructured texts. While most existing work addresses a particular IE task, universally
    modeling various IE tasks with one model has achieved great success recently.
    Despite their success, they employ a one-stage learning strategy, i.e., directly
    learning to extract the target structure given the input text, which contradicts
    the human learning process. In this paper, we propose a unified easy-to-hard learning
    framework consisting of three stages, i.e., the easy stage, the hard stage, and
    the main stage, for IE by mimicking the human learning process. By breaking down
    the learning process into multiple stages, our framework facilitates the model
    to acquire general IE task knowledge and improve its generalization ability. Extensive
    experiments across four IE tasks demonstrate the effectiveness of our framework.
    We achieve new state-of-the-art results on 13 out of 17 datasets. Our code is
    available at https://github.com/DAMO-NLP-SG/IE-E2H.
  authors:
  - Chang Gao
  - Wenxuan Zhang
  - Wai Lam
  - Lidong Bing
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - MATCHING
  forum: ''
  id: MATCHING_F3
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Easy-to-Hard Learning for Information Extraction
  tldr: Information extraction (IE) systems aim to automatically extract structured
    information, such as named entities, relations between entities, and events, from
    unstructured texts. While most existing work addresses a particular IE task, universally
    modeling various IE tasks with one model has achieved
  track: The First Workshop on Matching From Unstructured and Structured Data (MATCHING
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - "Elisa Bassignana\u263C"
  - "Filip Ginter\xDA"
  - "Sampo Pyysalo\xDA"
  - Rob van der Goot
  - Barbara Plank
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - MATCHING
  forum: ''
  id: MATCHING_F4
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Silver Syntax Pre-training for Cross-Domain Relation Extraction
  tldr: ''
  track: The First Workshop on Matching From Unstructured and Structured Data (MATCHING
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: "Information Synchronization of semi-structured data across languages\
    \ is challenging. For instance, Wikipedia tables in one language should be synchronized\
    \ across languages. To address this problem, we introduce a new dataset INFOSYNC\
    \ and a two-step method for tabular synchronization. INFOSYNC contains 100K entity-centric\
    \ tables (Wikipedia Infoboxes) across 14 languages, of which a subset (\u223C\
    3.5K pairs) are manually annotated. The proposed method includes 1) Information\
    \ Alignment to map rows and 2) Information Update for updating missing/outdated\
    \ information for aligned tables across multilingual tables. When evaluated on\
    \ INFOSYNC, information alignment achieves an F1 score of 87.91 (en \u2194 non-en).\
    \ To evaluate information updation, we perform human-assisted Wikipedia edits\
    \ on Infoboxes for 603 table pairs. Our approach obtains an acceptance rate of\
    \ 77.28% on Wikipedia, showing the effectiveness of the proposed method."
  authors:
  - Siddharth Khincha
  - Chelsi Jain
  - Vivek Gupta
  - Tushar Kataria
  - Shuo Zhang
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - MATCHING
  forum: ''
  id: MATCHING_F5
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'INFOSYNC: Information Synchronization across Multilingual Semi-structured
    Tables'
  tldr: Information Synchronization of semi-structured data across languages is challenging.
    For instance, Wikipedia tables in one language should be synchronized across languages.
    To address this problem, we introduce a new dataset INFOSYNC and a two-step method
    for tabular synchronization. INFOSYNC contai
  track: The First Workshop on Matching From Unstructured and Structured Data (MATCHING
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: "Recent work has shown that fine-tuning large language models (LLMs) on\
    \ large-scale instruction-following datasets substantially improves their performance\
    \ on a wide range of NLP tasks, especially in the zero-shot setting. However,\
    \ even advanced instruction-tuned LLMs still fail to outperform small LMs on relation\
    \ extraction (RE), a fundamental information extraction task. We hypothesize that\
    \ instruction-tuning has been unable to elicit strong RE capabilities in LLMs\
    \ due to RE\u2019s low incidence in instruction-tuning datasets, making up less\
    \ than 1% of all tasks (Wang et al., 2022). To address this limitation, we propose\
    \ QA4RE, a framework that aligns RE with question answering (QA), a predominant\
    \ task in instruction-tuning datasets. Comprehensive zero-shot RE experiments\
    \ over four datasets with two series of instruction-tuned LLMs (six LLMs in total)\
    \ demonstrate that our QA4RE framework consistently improves LLM performance,\
    \ strongly verifying our hypothesis and enabling LLMs to outperform strong zero-shot\
    \ baselines by a large margin. Additionally, we provide thorough experiments and\
    \ discussions to show the robustness, few-shot effectiveness, and strong transferability\
    \ of our QA4RE framework. This work illustrates a promising way of adapting LLMs\
    \ to challenging and underrepresented tasks by aligning these tasks with more\
    \ common instruction-tuning tasks like QA."
  authors:
  - Kai Zhang
  - "Bernal Guti\xE9rrez"
  - Yu Su
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - MATCHING
  forum: ''
  id: MATCHING_F6
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Aligning Instruction Tasks Unlocks Large Language Models as Zero-Shot Relation
    Extractors
  tldr: Recent work has shown that fine-tuning large language models (LLMs) on large-scale
    instruction-following datasets substantially improves their performance on a wide
    range of NLP tasks, especially in the zero-shot setting. However, even advanced
    instruction-tuned LLMs still fail to outperform small L
  track: The First Workshop on Matching From Unstructured and Structured Data (MATCHING
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We present a cross-domain approach for automated measurement and context
    extraction based on pre-trained language models. We construct a multi-source,
    multi-domain corpus and train an end-to-end extraction pipeline. We then apply
    multi-source task-adaptive pre-training and fine-tuning to benchmark the cross-domain
    generalization capability of our model. Further, we conceptualize and apply a
    task-specific error analysis and derive insights for future work. Our results
    suggest that multi-source training leads to the best overall results, while single-source
    training yields the best results for the respective individual domain. While our
    setup is successful at extracting quantity values and units, more research is
    needed to improve the extraction of contextual entities. We make the cross-domain
    corpus used in this work available online.
  authors:
  - Yueling Li
  - Sebastian Martschat
  - Simone Paolo Ponzetto
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_1
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Multi-Source (Pre-)Training for Cross-Domain Measurement, Unit and Context
    Extraction
  tldr: We present a cross-domain approach for automated measurement and context extraction
    based on pre-trained language models. We construct a multi-source, multi-domain
    corpus and train an end-to-end extraction pipeline. We then apply multi-source
    task-adaptive pre-training and fine-tuning to benchmark t
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Automatically identifying genetic mutations in the cancer literature using
    text mining technology has been an important way to study the vast amount of cancer
    medical literature. However, novel knowledge regarding the genetic variants proliferates
    rapidly, though current supervised learning models struggle with discovering these
    unknown entity types. Few-shot learning allows a model to perform effectively
    with great generalization on new entity types, which has not been explored in
    recognizing cancer mutation detection. This paper addresses cancer mutation detection
    tasks with few-shot learning paradigms. We propose GDPN framework, which models
    the label dependency from the training examples in the support set and approximates
    the transition scores via Gaussian distribution. The experiments on three benchmark
    cancer mutation datasets show the effectiveness of our proposed model.
  authors:
  - Jiarun Cao
  - Niels Peek
  - Andrew Renehan
  - Sophia Ananiadou
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_2
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Gaussian Distributed Prototypical Network for Few-shot Genomic Variant Detection
  tldr: Automatically identifying genetic mutations in the cancer literature using
    text mining technology has been an important way to study the vast amount of cancer
    medical literature. However, novel knowledge regarding the genetic variants proliferates
    rapidly, though current supervised learning models s
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Biomedical entity linking (EL) consists of named entity recognition (NER)
    and named entity disambiguation (NED). EL models are trained on corpora labeled
    by a predefined KB. However, it is a common scenario that only entities within
    a subset of the KB are precious to stakeholders. We name this scenario partial
    knowledge base inference; training an EL model with one KB and inferring on the
    part of it without further training. In this work, we give a detailed definition
    and evaluation procedures for this practically valuable but significantly understudied
    scenario and evaluate methods from three representative EL paradigms. We construct
    partial KB inference benchmarks and witness a catastrophic degradation in EL performance
    due to dramatically precision drop.Our findings reveal these EL paradigms can
    not correctly handle unlinkable mentions (NIL), so they are not robust to partial
    KB inference. We also propose two simple-and-effective redemption methods to combat
    the NIL issue with little computational overhead.
  authors:
  - Hongyi Yuan
  - Keming Lu
  - Zheng Yuan
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_5
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Exploring Partial Knowledge Base Inference in Biomedical Entity Linking
  tldr: Biomedical entity linking (EL) consists of named entity recognition (NER)
    and named entity disambiguation (NED). EL models are trained on corpora labeled
    by a predefined KB. However, it is a common scenario that only entities within
    a subset of the KB are precious to stakeholders. We name this scena
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Recent transformer-based models have made significant strides in generating
    radiology reports from chest X-ray images. However, a prominent challenge remains;
    these models often lack prior knowledge, resulting in the generation of synthetic
    reports that mistakenly reference non-existent prior exams. This discrepancy can
    be attributed to a knowledge gap between radiologists and the generation models.
    While radiologists possess patient-specific prior information, the models solely
    receive X-ray images at a specific time point. To tackle this issue, we propose
    a novel approach that leverages a rule-based labeler to extract comparison prior
    information from radiology reports. This extracted comparison prior is then seamlessly
    integrated into state-of-the-art transformer-based models, enabling them to produce
    more realistic and comprehensive reports. Our method is evaluated on English report
    datasets, such as IU X-ray and MIMIC-CXR. The results demonstrate that our approach
    surpasses baseline models in terms of natural language generation metrics. Notably,
    our model generates reports that are free from false references to non-existent
    prior exams, setting it apart from previous models. By addressing this limitation,
    our approach represents a significant step towards bridging the gap between radiologists
    and generation models in the domain of medical report generation.
  authors:
  - Sanghwan Kim
  - Farhad Nooralahzadeh
  - Morteza Rohanian
  - Koji Fujimoto
  - Mizuho Nishio
  - Ryo Sakamoto
  - Fabio Rinaldi
  - Michael Krauthammer
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_7
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Boosting Radiology Report Generation by Infusing Comparison Prior
  tldr: Recent transformer-based models have made significant strides in generating
    radiology reports from chest X-ray images. However, a prominent challenge remains;
    these models often lack prior knowledge, resulting in the generation of synthetic
    reports that mistakenly reference non-existent prior exams.
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Processing information locked within clinical health records is a challenging
    task that remains an active area of research in biomedical NLP.  In this work,
    we evaluate a broad set of machine learning techniques ranging from simple RNNs
    to specialised transformers such as BioBERT on a dataset containing clinical notes
    along with a set of annotations indicating whether a sample is cancer-related
    or not. Furthermore, we specifically employ efficient fine-tuning methods from
    NLP, namely, bottleneck adapters and prompt tuning, to adapt the models to our
    specialised task. Our evaluations suggest that fine-tuning a frozen BERT model
    pre-trained on natural language and with bottleneck adapters outperforms all other
    strategies, including full fine-tuning of the specialised BioBERT model. Based
    on our findings, we suggest that using bottleneck adapters in low-resource situations
    with limited access to labelled data or processing capacity could be a viable
    strategy in biomedical text mining.
  authors:
  - Omid Rohanian
  - Hannah Jauncey
  - Mohammadmahdi Nouriborji
  - Vinod Kumar
  - Bronner P. Gonalves
  - Christiana Kartsonaki
  - ISARIC Clinical Characterisation Group
  - Laura Merson
  - David Clifton
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_9
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Using Bottleneck Adapters to Identify Cancer in Clinical Notes under Low-Resource
    Constraints
  tldr: Processing information locked within clinical health records is a challenging
    task that remains an active area of research in biomedical NLP.  In this work,
    we evaluate a broad set of machine learning techniques ranging from simple RNNs
    to specialised transformers such as BioBERT on a dataset contai
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: A common metric for evaluating Automatic Speech Recognition (ASR) is Word
    Error Rate (WER) which solely takes into account discrepancies at the word-level.
    Although useful, WER is not guaranteed to correlate well with human judgment or
    performance on downstream tasks that use ASR. Meaningful assessment of ASR mistakes
    becomes even more important in high-stake scenarios such as health-care. We propose
    2 general measures to evaluate the severity of mistakes made by ASR systems, one
    based on sentiment analysis and another based on text embeddings. We evaluate
    these measures on simulated patient-doctor conversations using 5 ASR systems.
    Results show that these measures capture characteristics of ASR errors that WER
    does not. Furthermore, we train an ASR system incorporating severity and demonstrate
    the potential for using severity not only in the evaluation, but in the development
    of ASR. Advantages and limitations of this methodology are analyzed and discussed.
  authors:
  - Ryan Whetten
  - Casey Kennington
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_11
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Evaluating and Improving Automatic Speech Recognition using Severity
  tldr: A common metric for evaluating Automatic Speech Recognition (ASR) is Word
    Error Rate (WER) which solely takes into account discrepancies at the word-level.
    Although useful, WER is not guaranteed to correlate well with human judgment or
    performance on downstream tasks that use ASR. Meaningful assessm
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The goal of temporal relation extraction is to infer the temporal relation
    between two events in the document. Supervised models are dominant in this task.
    In this work, we investigate ChatGPT's ability on zero-shot temporal relation
    extraction. We designed three different prompt techniques to break down the task
    and evaluate ChatGPT. Our experiments show that ChatGPT's performance has a large
    gap with that of supervised methods and can heavily rely on the design of prompts.
    We further demonstrate that ChatGPT can infer more small relation classes correctly
    than supervised methods. The current shortcomings of ChatGPT on temporal relation
    extraction are also discussed in this paper. We found that ChatGPT cannot keep
    consistency during temporal inference and it fails in actively long-dependency
    temporal inference.
  authors:
  - Chenhan Yuan
  - Qianqian Xie
  - Sophia Ananiadou
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_12
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Zero-shot Temporal Relation Extraction with ChatGPT
  tldr: The goal of temporal relation extraction is to infer the temporal relation
    between two events in the document. Supervised models are dominant in this task.
    In this work, we investigate ChatGPT's ability on zero-shot temporal relation
    extraction. We designed three different prompt techniques to break
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The rapid growth of scientific publications, particularly during the COVID-19
    pandemic, emphasizes the need for tools to help researchers efficiently comprehend
    the latest advancements. One essential part of understanding scientific literature
    is research aspect classification, which categorizes sentences in abstracts to
    Background, Purpose, Method, and Finding. In this study, we investigate the impact
    of different datasets on model performance for the crowd-annotated CODA-19 research
    aspect classification task. Specifically, we explore the potential benefits of
    using the large, automatically curated PubMed 200K RCT dataset and evaluate the
    effectiveness of large language models (LLMs), such as LLaMA, GPT-3, ChatGPT,
    and GPT-4. Our results indicate that using the PubMed 200K RCT dataset does not
    improve performance for the CODA-19 task. We also observe that while GPT-4 performs
    well, it does not outperform the SciBERT model fine-tuned on the CODA-19 dataset,
    emphasizing the importance of a dedicated and task-aligned datasets dataset for
    the target task.
  authors:
  - Shreya Chandrasekhar
  - Chieh-Yang Huang
  - Ting-Hao Huang
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_13
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Short paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Good Data, Large Data, or No Data? Comparing Three Approaches in Developing
    Research Aspect Classifiers for Biomedical Papers
  tldr: The rapid growth of scientific publications, particularly during the COVID-19
    pandemic, emphasizes the need for tools to help researchers efficiently comprehend
    the latest advancements. One essential part of understanding scientific literature
    is research aspect classification, which categorizes sen
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Early identification of depression is beneficial to public health surveillance
    and disease treatment. There are many models that mainly treat the detection as
    a binary classification task, such as detecting whether a user is depressed. However,
    identifying users' depression severity levels from posts on social media is more
    clinically useful for future prevention and treatment. Existing severity detection
    methods mainly model the semantic information of posts while ignoring the relevant
    sentiment information, which can reflect the user's state of mind and could be
    helpful for severity detection. In addition, they treat all severity levels equally,
    making the model difficult to distinguish between closely-labeled categories.
    We propose a sentiment-guided Transformer model, which efficiently fuses social
    media posts' semantic information with sentiment information. Furthermore, we
    also utilize a supervised severity-aware contrastive learning framework to enable
    the model to better distinguish between different severity levels. The experimental
    results show that our model achieves superior performance on two public datasets,
    while further analysis proves the effectiveness of all proposed modules.
  authors:
  - Tianlin Zhang
  - Kailai Yang
  - Sophia Ananiadou
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_14
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Sentiment-guided Transformer with Severity-aware Contrastive Learning for
    Depression Detection on Social Media
  tldr: Early identification of depression is beneficial to public health surveillance
    and disease treatment. There are many models that mainly treat the detection as
    a binary classification task, such as detecting whether a user is depressed. However,
    identifying users' depression severity levels from post
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Social media (SM) can provide valuable information about patients' experiences
    with multiple drugs during treatments. Although information extraction from SM
    has been well-studied, drug switches detection and reasons behind these switches
    from SM have not been studied yet. Therefore, in this paper, we present a new
    SM listening approach for analyzing online patient conversations that contain
    information about drug switching, drug effectiveness, side effects, and adverse
    drug reactions. We describe a deep learning-based approach for identifying instances
    of drug switching in SM posts, as well as a method for extracting the reasons
    behind these switches. To train and test our models, we used annotated SM data
    from internal dataset which is automatically created using a rule-based method.
    We evaluated our models using Text-to-Text Transfer Transformer (T5) and found
    that our SM listening approach can extract medication change information and reasons
    with high accuracy, achieving  an F1-score of 98% and a ROUGE-1 score of 93%,
    respectively. Overall, our results suggest that our SM listening approach has
    the potential to provide valuable insights into patients' experiences with drug
    treatments, which can be used to improve patient outcomes and the effectiveness
    of drug treatments.
  authors:
  - Mourad Sarrouti
  - Carson Tao
  - Yoann Mamy Randriamihaja
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_15
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Short paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Exploring Drug Switching in Patients: A Deep Learning-based Approach to
    Extract Drug Changes and Reasons from Social Media'
  tldr: 'Social media (SM) can provide valuable information about patients'' experiences
    with multiple drugs during treatments. Although information extraction from SM
    has been well-studied, drug switches detection and reasons behind these switches
    from SM have not been studied yet. Therefore, in this paper, '
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'The use of seed articles in information retrieval provides many advantages,
    such as a longercontext and more details about the topic being searched for. Given
    a seed article (i.e., a PMID), PubMed provides a pre-compiled list of similar
    articles to support the user in finding equivalent papers in the biomedical literature.
    We aimed at performing a quantitative evaluation of the PubMed Similar Articles
    based on three existing biomedical text similarity datasets, namely, RELISH, TREC-COVID,
    and SMAFIRA-c. Further, we carried out a survey and an evaluation of various text
    similarity methods on these three datasets. Our experiments considered the original
    title and abstract from PubMed as well as automatically detected sections and
    manually annotated relevant sentences. We provide an overview about which methods
    better performfor each dataset and compare them to the ranking in PubMed similar
    articles. While resultsvaried considerably among the datasets, we were able to
    obtain a better performance thanPubMed for all of them. Datasets and source codes
    are available at: https://github.com/mariananeves/reranking'
  authors:
  - Mariana Neves
  - Ines Schadock
  - Beryl Eusemann
  - Gilbert Schnfelder
  - Bettina Bert
  - Daniel Butzke
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_16
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Is the ranking of PubMed similar articles good enough? An evaluation of text
    similarity methods for three datasets
  tldr: The use of seed articles in information retrieval provides many advantages,
    such as a longercontext and more details about the topic being searched for. Given
    a seed article (i.e., a PMID), PubMed provides a pre-compiled list of similar
    articles to support the user in finding equivalent papers in th
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Biomedical event extraction can be divided into three main subtasks; (1)
    biomedical event trigger detection, (2) biomedical argument identification and
    (3) event construction. This work focuses in the two first subtasks. For the first
    subtask we analyze a set of transformer language models that are commonly used
    in the biomedical domain to evaluate and compare their capacity for event trigger
    detection. We fine-tune the models using seven manually annotated corpora to assess
    their performance in different biomedical subdomains. SciBERT emerged as the highest
    performing model, presenting a slight improvement compared to baseline models.
    Then, for the second subtask we construct a knowledge graph (KG) from the biomedical
    corpora and integrate its KG embeddings to SciBERT to enrich its semantic information.
    We demonstrate that adding the KG embeddings to the model improves the argument
    identification performance by around 20 \%, and by around 15 \% compared to two
    baseline models. Our results suggest that fine-tuning a transformer model that
    is pretrained from scratch with biomedical and general data allows to detect event
    triggers and identify arguments covering different biomedical subdomains, and
    therefore improving its generalization. Furthermore, the integration of KG embeddings
    into the model can significantly improve the performance of biomedical event argument
    identification, outperforming the results of baseline models.
  authors:
  - Laura Zanella
  - Yannick Toussaint
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_17
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: How Much do Knowledge Graphs Impact Transformer Models for Extracting Biomedical
    Events?
  tldr: 'Biomedical event extraction can be divided into three main subtasks; (1)
    biomedical event trigger detection, (2) biomedical argument identification and
    (3) event construction. This work focuses in the two first subtasks. For the first
    subtask we analyze a set of transformer language models that are '
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We consider the task of automatically extracting various overlapping frames,
    i.e, structured entities composed of multiple labels and mentions, from long clinical
    breast radiology documents. While many methods exist for related topics such as
    event extraction, slot filling, or discontinuous entity recognition, a challenge
    in our study resides in the fact that clinical reports typically contain overlapping
    frames that span multiple sentences or paragraphs.We propose a new method that
    addresses these difficulties and evaluate it on a new annotated corpus.Despite
    the small number of documents, we show that the hybridization between knowledge
    injection and a learning-based system allows us to quickly obtain proper results.We
    will also introduce the concept of scope relations and show that it both improves
    the performance of our system, and provides a visual explanation of the predictions.
  authors:
  - Perceval Wajsburt
  - Xavier Tannier
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_18
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: An end-to-end neural model based on cliques and scopes for frame extraction
    in long breast radiology reports
  tldr: We consider the task of automatically extracting various overlapping frames,
    i.e, structured entities composed of multiple labels and mentions, from long clinical
    breast radiology documents. While many methods exist for related topics such as
    event extraction, slot filling, or discontinuous entity r
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We propose a distantly supervised pipeline NER which executes entity span
    detection and entity classification in sequence named DISTANT (DIstantly Supervised
    enTity spAN deTection and classification).The former entity span detector extracts
    possible entity mention spans by the distant supervision. Then the later entity
    classifier assigns each entity span to one of the positive entity types or none
    by employing a positive and unlabeled (PU) learning framework. Two models were
    built based on the pre-trained SciBERT model and fine-tuned with the silver corpus
    generated by the distant supervision.Experimental results on BC5CDR and NCBI-Disease
    datasets show that our method outperforms the end-to-end NER baselines without
    PU learning by a large margin. In particular, it increases the recall score effectively.
  authors:
  - Ken Yano
  - Makoto Miwa
  - Sophia Ananiadou
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_19
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Short paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'DISTANT: Distantly Supervised Entity Span Detection and Classification'
  tldr: We propose a distantly supervised pipeline NER which executes entity span
    detection and entity classification in sequence named DISTANT (DIstantly Supervised
    enTity spAN deTection and classification).The former entity span detector extracts
    possible entity mention spans by the distant supervision. T
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In clinical and other specialized domains, data are scarce due to their
    confidential nature. This lack of data is a major problem when fine-tuning language
    models.Nevertheless, very large language models (LLMs) are promising for the medical
    domain but cannot be used directly in healthcare facilities due to data confidentiality
    issues. We explore an approach of annotating training data with LLMs to train
    smaller models more adapted to our problem. We show that this method yields promising
    results for information extraction tasks.
  authors:
  - Simon Meoni
  - Eric De la Clergerie
  - Theo Ryffel
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_20
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Short paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Large Language Models as Instructors: A Study on Multilingual Clinical Entity
    Extraction'
  tldr: In clinical and other specialized domains, data are scarce due to their confidential
    nature. This lack of data is a major problem when fine-tuning language models.Nevertheless,
    very large language models (LLMs) are promising for the medical domain but cannot
    be used directly in healthcare facilities
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Extracting temporal relations usually entails identifying and classifying
    the relation between two mentions. However, the definition of temporal mentions
    strongly depends on the text type and the application domain. Clinical text in
    particular is complex. It may describe events that occurred at different times,
    contain redundant information and a variety of domain-specific temporal expressions.
    In this paper, we propose a novel event-independent representation of temporal
    relations that is task-independent and, therefore, domain-independent. We are
    interested in identifying homogeneous text portions from a temporal standpoint
    and classifying the relation between each text portion and the document creation
    time. Temporal relation extraction is cast as a sequence labeling task and evaluated
    on oncology notes. We further evaluate our temporal representation by the temporal
    positioning of toxicity events of chemotherapy administrated to colon and lung
    cancer patients described in French clinical reports. An overall macro F-measure
    of 0.86 is obtained for temporal relation extraction by a neural token classification
    model trained on clinical texts written in French. Our results suggest that the
    toxicity event extraction task can be performed successfully by automatically
    identifying toxicity events and placing them within the patient timeline (F-measure
    .62). The proposed system has the potential to assist clinicians in the preparation
    of tumor board meetings.
  authors:
  - Nesrine Bannour
  - Bastien Rance
  - Xavier Tannier
  - Aurelie Neveol
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_22
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Event-independent temporal positioning: application to French clinical text'
  tldr: Extracting temporal relations usually entails identifying and classifying
    the relation between two mentions. However, the definition of temporal mentions
    strongly depends on the text type and the application domain. Clinical text in
    particular is complex. It may describe events that occurred at diff
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Early identification of Adverse Drug Events (ADE) is critical for taking
    prompt actions while introducing new drugs into the market. These ADEs information
    are available through various unstructured data sources like clinical study reports,
    patient health records, social media posts, etc. Extracting ADEs and the related
    suspect drugs using machine learning is a challenging task due to the complex
    linguistic relations between drug  ADE pairs in textual data and unavailability
    of large corpus of labelled datasets. This paper introduces ADEQA, a question-
    answer(QA) based approach using quasi supervised labelled data and sequence-to-sequence
    transformers to extract ADEs, drug suspects and the relationships between them.
    Unlike traditional QA models, natural language generation (NLG) based models don't
    require extensive token level labelling and thereby reduces the adoption barrier
    significantly. On a public ADE corpus, we were able to achieve state-of-the-art
    results with an F1 score of 94% on establishing the relationships between ADEs
    and the respective suspects.
  authors:
  - Vinayak Arannil
  - Tomal Deb
  - Atanu Roy
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_24
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'ADEQA: A Question Answer based approach for joint ADE-Suspect Extraction
    using Sequence-To-Sequence Transformers'
  tldr: Early identification of Adverse Drug Events (ADE) is critical for taking prompt
    actions while introducing new drugs into the market. These ADEs information are
    available through various unstructured data sources like clinical study reports,
    patient health records, social media posts, etc. Extracting
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Social media platforms have enabled individuals suffering from mental
    illnesses to share their lived experiences and find the online support necessary
    to cope. However, many users fail to receive genuine clinical support, thus exacerbating
    their symptoms. Screening users based on what they post online can aid providers
    in administering targeted healthcare and minimize false positives. Pre-trained
    Language Models (LMs) can assess users' social media data and classify them in
    terms of their mental health risk. We propose a Question-Answering (QA) approach
    to assess mental health risk using the Unified-QA model on two large mental health
    datasets. To protect user data, we extend Unified-QA by anonymizing the model
    training process using differential privacy. Our results demonstrate the effectiveness
    of modeling risk assessment as a QA task, specifically for mental health use cases.
    Furthermore, the model's performance decreases by less than 1% with the inclusion
    of differential privacy. The proposed system's performance is indicative of a
    promising research direction that will lead to the development of privacy-aware
    diagnostic systems.
  authors:
  - Prateek Chhikara
  - Ujjwal Pasupulety
  - John Marshall
  - Dhiraj Chaurasia
  - Shweta Kumari
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_25
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Short paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Privacy Aware Question-Answering System for Online Mental Health Risk Assessment
  tldr: Social media platforms have enabled individuals suffering from mental illnesses
    to share their lived experiences and find the online support necessary to cope.
    However, many users fail to receive genuine clinical support, thus exacerbating
    their symptoms. Screening users based on what they post onli
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Over the past few years, domain specific pretrained language models have
    been investigated and have shown remarkable achievements in different downstream
    tasks, especially in biomedical domain. These achievements stem on the well known
    BERT architecture which uses an attention based self-supervision for context learning
    of textual documents. However, these domain specific biomedical pretrained language
    models mainly use English corpora. Therefore, non-English, domain-specific pretrained
    models remain quite rare, both of these requirements being hard to achieve. In
    this work, we proposed AliBERT, a biomedical pretrained language model for French
    and investigated different learning strategies. AliBERT is trained using regularized
    Unigram based tokenizer trained for this purpose. AliBERT has achieved state of
    the art F1 and accuracy scores in different down-stream biomedical tasks. Our
    pretrained model manages to outperform some French non domain-specific models
    such as CamemBERT and FlauBERT on diverse down-stream tasks, with less pretraining
    and training time and with much smaller corpora.
  authors:
  - Aman Berhe
  - Guillaume Draznieks
  - Vincent Martenot
  - Valentin Masdeu
  - Lucas Davy
  - Jean-Daniel Zucker
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_28
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'AliBERT: A Pre-trained Language Model for French Biomedical Text'
  tldr: Over the past few years, domain specific pretrained language models have been
    investigated and have shown remarkable achievements in different downstream tasks,
    especially in biomedical domain. These achievements stem on the well known BERT
    architecture which uses an attention based self-supervision
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Fact-checking of health-related claims has become necessary in this digital
    age, where any information posted online is easily available to everyone. The
    most effective way to verify such claims is by using evidences obtained from reliable
    sources of medical knowledge, such as PubMed. Recent advances in the field of
    NLP have helped automate such fact-checking tasks. In this work, we propose a
    domain-specific BERT-based model using a transfer learning approach for the task
    of predicting the veracity of claim-evidence pairs for the verification of health-related
    facts. We also improvise on a method to combine multiple evidences retrieved for
    a single claim, taking into consideration conflicting evidences as well. We also
    show how our model can be exploited when labelled data is available and how back-translation
    can be used to augment data when there is data scarcity.
  authors:
  - Pritam Deka
  - Anna Jurek-Loughrey
  - Deepak P
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_29
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Multiple Evidence Combination for Fact-Checking of Health-Related Information
  tldr: Fact-checking of health-related claims has become necessary in this digital
    age, where any information posted online is easily available to everyone. The
    most effective way to verify such claims is by using evidences obtained from reliable
    sources of medical knowledge, such as PubMed. Recent advance
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We present a manually annotated new corpus, Species-Species Interaction
    (SSI), for extracting meaningful binary relations between species, in biomedical
    texts, at sentence level, with a focus on the gut microbiota. The corpus leverages
    PubTator to annotate species in full-text articles after evaluating different
    NER species taggers. Our first results are promising for extracting relations
    between species using BERT and its biomedical variants.
  authors:
  - Oumaima El Khettari
  - Solen Quiniou
  - Samuel Chaffron
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_31
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Short paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Building a Corpus for Biomedical Relation Extraction of Species Mentions
  tldr: We present a manually annotated new corpus, Species-Species Interaction (SSI),
    for extracting meaningful binary relations between species, in biomedical texts,
    at sentence level, with a focus on the gut microbiota. The corpus leverages PubTator
    to annotate species in full-text articles after evaluat
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Understanding protein interactions and pathway knowledge is essential
    for comprehending living systems and investigating the mechanisms underlying various
    biological functions and complex diseases. While numerous databases curate such
    biological data obtained from literature and other sources, they are not comprehensive
    and require considerable effort to maintain. One mitigation strategies can be
    utilizing large language models to automatically extract biological information
    and explore their potential in life science research. This study presents an initial
    investigation of the efficacy of utilizing a large language model, Galactica in
    life science research by assessing its performance on tasks involving protein
    interactions, pathways, and gene regulatory relation recognition. The paper details
    the results obtained from the model evaluation, highlights the findings, and discusses
    the opportunities and challenges.
  authors:
  - Gilchan Park
  - Byung-Jun Yoon
  - Xihaier Luo
  - Vanessa Lpez-Marrero
  - Patrick Johnstone
  - Shinjae Yoo
  - Francis Alexander
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_32
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Short paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Automated Extraction of Molecular Interactions and Pathway Knowledge using
    Large Language Model, Galactica: Opportunities and Challenges'
  tldr: 'Understanding protein interactions and pathway knowledge is essential for
    comprehending living systems and investigating the mechanisms underlying various
    biological functions and complex diseases. While numerous databases curate such
    biological data obtained from literature and other sources, they '
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Background: More than 400.000 biomedical concepts and some of their relationships
    are contained in SnomedCT, a comprehensive biomedical ontology. However, their
    concept names are not always readily interpretable by non-experts, or patients
    looking at their own electronic health records (EHR). Clear definitions or descriptions
    in understandable language or often not available. Therefore, generating human-readable
    definitions for biomedical concepts might help make the information they encode
    more accessible and understandable to a wider public.Objective: In this article,
    we introduce the Automatic Glossary of Clinical Terminology (AGCT), a large-scale
    biomedical dictionary of clinical concepts generated using high-quality information
    extracted from the biomedical knowledge contained in SnomedCT.Methods: We generate
    a novel definition for every SnomedCT concept, after prompting the OpenAI Turbo
    model, a variant of GPT 3.5, using a high-quality verbalization of the SnomedCT
    relationships of the to-be-defined concept. A significant subset of the generated
    definitions was subsequently evaluated by NLP researchers with biomedical expertise
    on 5-point scales along the following three axes: factuality, insight, and fluency.Results:
    AGCT contains 422,070 computer-generated definitions for SnomedCT concepts, covering
    various domains such as diseases, procedures, drugs, and anatomy. The average
    length of the definitions is 49 words. The definitions were assigned average scores
    of over 4.5 out of 5 on all three axes, indicating a majority of factual, insightful,
    and fluent definitions.Conclusion: AGCT is a novel and valuable resource for biomedical
    tasks that require human-readable definitions for SnomedCT concepts. It can also
    serve as a base for developing robust biomedical retrieval models or other applications
    that leverage natural language understanding of biomedical knowledge.'
  authors:
  - Franois Remy
  - Kris Demuynck
  - Thomas Demeester
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_34
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Short paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Automatic Glossary of Clinical Terminology: a Large-Scale Dictionary of
    Biomedical Definitions Generated from Ontological Knowledge'
  tldr: 'Background: More than 400.000 biomedical concepts and some of their relationships
    are contained in SnomedCT, a comprehensive biomedical ontology. However, their
    concept names are not always readily interpretable by non-experts, or patients
    looking at their own electronic health records (EHR). Clear '
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'We compare three simple and popular approaches for NER: 1) SEQ (sequence
    labeling with a linear token classifier) 2) SeqCRF (sequence labeling with Conditional
    Random Fields), and 3) SpanPred (span prediction with boundary token embeddings).
    We compare the approaches on 4 biomedical NER tasks: GENIA, NCBI-Disease, LivingNER
    (Spanish), and SocialDisNER (Spanish). The SpanPred model demonstrates state-of-the-art
    performance on LivingNER and SocialDisNER, improving F1 by 1.3 and 0.6 F1 respectively.
    The SeqCRF model also demonstrates state-of-the-art performance on LivingNER and
    SocialDisNER, improving F1 by 0.2 F1 and 0.7 respectively. The SEQ model is competitive
    with the state-of-the-art on LivingNER dataset. We explore some simple ways of
    combining the three approaches. We find that majority voting consistently gives
    high precision and high F1 across all 4 datasets.Lastly, we implement a system
    that learns to combine SEQ''s and SpanPred''s predictions, generating systems
    that give high recall and high F1 across all 4 datasets. On the GENIA dataset,
    we find that our learned combiner system significantly boosts F1(+1.2) and recall(+2.1)
    over the systems being combined.'
  authors:
  - Harsh Verma
  - Sabine Bergler
  - Narjesossadat Tahaei
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_35
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Short paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Comparing and combining some popular NER approaches on Biomedical tasks
  tldr: 'We compare three simple and popular approaches for NER: 1) SEQ (sequence
    labeling with a linear token classifier) 2) SeqCRF (sequence labeling with Conditional
    Random Fields), and 3) SpanPred (span prediction with boundary token embeddings).
    We compare the approaches on 4 biomedical NER tasks: GENIA'
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Understanding biological mechanisms requires determining mutual protein-protein
    interactions (PPI). Obtaining drug-drug interactions (DDI) from scientific articles
    provides important information about drugs. Extracting such medical entity interactions
    from biomedical articles is challenging due to complex sentence structures. To
    address this issue, our proposed model utilizes tree-transformers to generate
    the sentence representation first, and then a sentence-to-word update step to
    fine-tune the word embeddings which are again used by the tree-transformers to
    generate enriched sentence representations. Using the tree-transformers helps
    the model preserve syntactical information and provide semantic information. The
    fine-tuning provided by the continuous update step adds improved semantics to
    the representation of each sentence. Our model outperforms other prominent models
    with a significant performance boost on the five standard PPI corpora and a performance
    boost on the one benchmark DDI corpus that are used in our experiments.
  authors:
  - Sudipta Singha Roy
  - Robert E. Mercer
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_40
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Extracting Drug-Drug and Protein-Protein Interactions from Text using a Continuous
    Update of Tree-Transformers
  tldr: Understanding biological mechanisms requires determining mutual protein-protein
    interactions (PPI). Obtaining drug-drug interactions (DDI) from scientific articles
    provides important information about drugs. Extracting such medical entity interactions
    from biomedical articles is challenging due to c
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Elliptical coordinated compound noun phrases (ECCNPs), a special kind
    of coordination ellipsis, are a common phenomenon in German medical texts. As
    their presence is known to affect the performance in downstream tasks such as
    entity extraction and disambiguation, their resolution can be a useful preprocessing
    step in information extraction pipelines. In this work, we present a new comprehensive
    dataset of more than 4,000 manually annotated ECCNPs in German medical text, along
    with the respective ground truth resolutions. Based on this data, we propose a
    generative encoder-decoder Transformer model, allowing for a simple end-to-end
    resolution of ECCNPs from raw input strings with very high accuracy (90.5% exact
    match score). We compare our approach to an elaborate rule-based baseline, which
    the generative model outperforms by a large margin. We further investigate different
    scenarios for prompting large language models (LLM) to resolve ECCNPs. In a zero-shot
    setting, performance is remarkably poor (21.6% exact matches), as the LLM tends
    to apply complex changes to the inputs unrelated to our specific task. We also
    find no improvement over the generative model when using the LLM for post-filtering
    of generated candidate resolutions.
  authors:
  - Niklas Kammer
  - Florian Borchert
  - Silvia Winkler
  - Gerard de Melo
  - Matthieu-P. Schapranow
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_41
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Resolving Elliptical Compounds in German Medical Text
  tldr: Elliptical coordinated compound noun phrases (ECCNPs), a special kind of coordination
    ellipsis, are a common phenomenon in German medical texts. As their presence is
    known to affect the performance in downstream tasks such as entity extraction
    and disambiguation, their resolution can be a useful pre
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Amid ongoing health crisis, there is a growing necessity to discern possible
    signs of Wellness Dimensions (WD) manifested in self-narrated text. As the distribution
    of WD on social media data is intrinsically imbalanced, we experiment the generative
    AI techniques for data augmentation to enable further improvement in the pre-screening
    task of classifying WD. To this end, we propose a simple yet effective data augmentation
    approach through prompt-based Generative AI models, and evaluate the ROUGE scores
    and syntactic/ semantic similarity among existing interpretations and augmented
    data. Our approach with ChatGPT model surpasses all the other methods and achieves
    improvement over baselines such as Easy-Data Augmentation (EDA) and Backtranslation
    (BT).
  authors:
  - Chandreen Liyanage
  - Muskan Garg
  - Vijay Mago
  - Sunghwan Sohn
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_42
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Short paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Augmenting Reddit Posts to Determine Wellness Dimensions impacting Mental
    Health
  tldr: Amid ongoing health crisis, there is a growing necessity to discern possible
    signs of Wellness Dimensions (WD) manifested in self-narrated text. As the distribution
    of WD on social media data is intrinsically imbalanced, we experiment the generative
    AI techniques for data augmentation to enable furt
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Understanding temporal relationships in text from electronic health records
    can be valuable for many important downstream clinical applications. Since Clinical
    TempEval 2017, there has been little work on end-to-end systems for temporal relation
    extraction, with most work focused on the setting where gold standard events and
    time expressions are given. In this work, we make use of a novel multi-headed
    attention mechanism on top of a pre-trained transformer encoder to allow the learning
    process to attend to multiple aspects of the contextualized embeddings. Our system
    achieves state of the art results on the THYME corpus by a wide margin, in both
    the in-domain and cross-domain settings.
  authors:
  - Timothy Miller
  - Steven Bethard
  - Dmitriy Dligach
  - Guergana Savova
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_43
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Short paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: End-to-end clinical temporal information extraction with multi-head attention
  tldr: Understanding temporal relationships in text from electronic health records
    can be valuable for many important downstream clinical applications. Since Clinical
    TempEval 2017, there has been little work on end-to-end systems for temporal relation
    extraction, with most work focused on the setting wher
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Accurate human-annotated data for real-worlduse cases can be scarce and
    expensive to obtain.In the clinical domain, obtaining such data is evenmore difficult
    due to privacy concerns which notonly restrict open access to quality data but
    also require that the annotation be done by domain experts.In this paper, we propose
    a novel framework - InterDAPT - that leverages Intermediate Domain Finetuning
    to allow language models to adapt to narrow domains with small, noisy datasets.
    By making use of peripherally-related, unlabeled datasets,this framework circumvents
    domain-specific datascarcity issues. Our results show that this weaklysupervised
    framework provides performance improvements in downstream clinical named entityrecognition
    tasks.
  authors:
  - Shilpa Suresh
  - Nazgol Tavabi
  - Shahriar Golchin
  - Leah Gilreath
  - Rafael Garcia-Andujar
  - Alexander Kim
  - Joseph Murray
  - Blake Bacevich
  - Ata Kiapour
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_44
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Short paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Intermediate Domain Finetuning for Weakly Supervised Domain-adaptive Clinical
    NER
  tldr: Accurate human-annotated data for real-worlduse cases can be scarce and expensive
    to obtain.In the clinical domain, obtaining such data is evenmore difficult due
    to privacy concerns which notonly restrict open access to quality data but also
    require that the annotation be done by domain experts.In t
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ChatGPT is a large language model developed by OpenAI. Despite its impressive
    performance across various tasks, no prior work has investigated its capability
    in the biomedical domain yet. To this end, this paper aims to evaluate the performance
    of ChatGPT on various benchmark biomedical tasks, such as relation extraction,
    document classification, question answering, and summarization. To the best of
    our knowledge, this is the first work that conducts an extensive evaluation of
    ChatGPT in the biomedical domain. Interestingly, we find based on our evaluation
    that in biomedical datasets that have smaller training sets, zero-shot ChatGPT
    even outperforms the state-of-the-art fine-tuned generative transformer models,
    such as BioGPT and BioBART. This suggests that ChatGPT's pre-training on large
    text corpora makes it quite specialized even in the biomedical domain. Our findings
    demonstrate that ChatGPT has the potential to be a valuable tool for various tasks
    in the biomedical domain that lack large annotated data.
  authors:
  - Israt Jahan
  - Md Tahmid Rahman Laskar
  - Chun Peng
  - Jimmy Huang
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_46
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Short paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Evaluation of ChatGPT on Biomedical Tasks: A Zero-Shot Comparison with Fine-Tuned
    Generative Transformers'
  tldr: 'ChatGPT is a large language model developed by OpenAI. Despite its impressive
    performance across various tasks, no prior work has investigated its capability
    in the biomedical domain yet. To this end, this paper aims to evaluate the performance
    of ChatGPT on various benchmark biomedical tasks, such '
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Using language models (LMs) pre-trained in a self-supervised setting on
    large corpora and then fine-tuning for a downstream task has helped to deal with
    the problem of limited label data for supervised learning tasks such as Named
    Entity Recognition (NER). Recent research in biomedical language processing has
    offered a number of biomedical LMs pre-trained using different methods and techniques
    that advance results on many BioNLP tasks, including NER. However, there is still
    a lack of a comprehensive comparison of pre-training approaches that would work
    more optimally in the biomedical domain. This paper aims to investigate different
    pre-training methods, such as pre-training the biomedical LM from scratch and
    pre-training it in a continued fashion. We compare existing methods with our proposed
    pre-training method of initializing weights for new tokens by distilling existing
    weights from the BERT model inside the context where the tokens were found. The
    method helps to speed up the pre-training stage and improve performance on NER.
    In addition, we compare how masking rate, corruption strategy, and masking strategies
    impact the performance of the biomedical LM. Finally, using the insights from
    our experiments, we introduce a new biomedical LM (BIOptimus), which is pre-trained
    using Curriculum Learning (CL) and contextualized weight distillation method.
    Our model sets new states of the art on several biomedical Named Entity Recognition
    (NER) tasks. We release our code and all pre-trained models.
  authors:
  - Vera Pavlova
  - Mohammed Makhlouf
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_47
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'BIOptimus: Pre-training an Optimal Biomedical Language Model with Curriculum
    Learning for Named Entity Recognition'
  tldr: Using language models (LMs) pre-trained in a self-supervised setting on large
    corpora and then fine-tuning for a downstream task has helped to deal with the
    problem of limited label data for supervised learning tasks such as Named Entity
    Recognition (NER). Recent research in biomedical language proc
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: As opposed to general English, many concepts in biomedical terminology
    have been designed in recent history by biomedical professionals with the goal
    of being precise and concise. This is often achieved by concatenating meaningful
    biomedical morphemes to create new semantic units. Nevertheless, most modern biomedical
    language models (LMs) are pre-trained using standard domain-specific tokenizers
    derived from large scale biomedical corpus statistics without explicitly leveraging
    the agglutinating nature of biomedical language. In this work, we first find that
    standard open-domain and biomedical tokenizers are largely unable to segment biomedical
    terms into meaningful components. Therefore, we hypothesize that using a tokenizer
    which segments biomedical terminology more accurately would enable biomedical
    LMs to improve their performance on downstream biomedical NLP tasks, especially
    ones which involve biomedical terms directly such as named entity recognition
    (NER) and entity linking. Surprisingly, we find that pre-training a biomedical
    LM using a more accurate biomedical tokenizer does not improve the entity representation
    quality of a language model as measured by several intrinsic and extrinsic measures
    such as masked language modeling prediction (MLM) accuracy as well as NER and
    entity linking performance. These quantitative findings, along with a case study
    which explores entity representation quality more directly, suggest that the biomedical
    pre-training process is quite robust to instances of sub-optimal tokenization.
  authors:
  - Bernal Jimenez Gutierrez
  - Huan Sun
  - Yu Su
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_48
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Biomedical Language Models are Robust to Sub-optimal Tokenization
  tldr: As opposed to general English, many concepts in biomedical terminology have
    been designed in recent history by biomedical professionals with the goal of being
    precise and concise. This is often achieved by concatenating meaningful biomedical
    morphemes to create new semantic units. Nevertheless, most
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We propose a novel distantly supervised document-level biomedical relation
    extraction model that uses partial knowledge graphs that include the graph neighborhood
    of the entities appearing in each input document. Most conventional distantly
    supervised relation extraction methods use only the entity relations automatically
    annotated by using knowledge base entries. They do not fully utilize the rich
    information in the knowledge base, such as entities other than the target entities
    and the network of heterogeneous entities defined in the knowledge base. To address
    this issue, our model integrates the representations of the entities acquired
    from the neighborhood knowledge graphs with the representations of the input document.
    We conducted experiments on the ChemDisGene dataset using Comparative Toxicogenomics
    Database (CTD) for document-level relation extraction with respect to interactions
    between drugs, diseases, and genes. Experimental results confirmed the performance
    improvement by integrating entities and their neighborhood biochemical information
    from the knowledge base.
  authors:
  - Takuma Matsubara
  - Makoto Miwa
  - Yutaka Sasaki
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_49
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Short paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Distantly Supervised Document-Level Biomedical Relation Extraction with Neighborhood
    Knowledge Graphs
  tldr: 'We propose a novel distantly supervised document-level biomedical relation
    extraction model that uses partial knowledge graphs that include the graph neighborhood
    of the entities appearing in each input document. Most conventional distantly
    supervised relation extraction methods use only the entity '
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'We propose a novel Biomedical domain-specific Non-AutoRegressive Transformer
    model for natural language generation: BioNART. Our BioNART is based on an encoder-decoder
    model, and both encoder and decoder are compatible with widely used BERT architecture,
    which allows benefiting from publicly available pre-trained biomedical language
    model checkpoints. We performed additional pre-training and fine-tuned BioNART
    on biomedical summarization and doctor-patient dialogue tasks. Experimental results
    show that our BioNART achieves about 94% of the ROUGE score to the pre-trained
    autoregressive model while realizing an 18 times faster inference speed on the
    iCliniq dataset.'
  authors:
  - Masaki Asada
  - Makoto Miwa
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_50
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Short paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'BioNART: A Biomedical Non-AutoRegressive Transformer for Natural Language
    Generation'
  tldr: 'We propose a novel Biomedical domain-specific Non-AutoRegressive Transformer
    model for natural language generation: BioNART. Our BioNART is based on an encoder-decoder
    model, and both encoder and decoder are compatible with widely used BERT architecture,
    which allows benefiting from publicly availab'
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Recently, several methods have tackled the relation extraction task with
    QA and have shown successful results. However, the effectiveness of existing methods
    in specific domains, such as the biomedical domain, is yet to be verified. When
    there are multiple entity pairs that share an entity in a sentence, a QA-based
    relation extraction model that outputs only one single answer to a given question
    may not extract desired relations. In addition, these methods employ QA models
    that are not tuned for relation extraction. To address these issues, we first
    extend and apply a span QA-based relation extraction method to the drug-protein
    relation extraction by creating question templates and incorporating entity type
    markers. We further propose a binary QA-based method that directly uses the entity
    information available in the relation extraction task. The experimental results
    on the DrugProt dataset show that our QA-based methods, especially the proposed
    binary QA method, are effective for drug-protein relation extraction.
  authors:
  - Koshi Yamada
  - Makoto Miwa
  - Yutaka Sasaki
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_51
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Short paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Biomedical Relation Extraction with Entity Type Markers and Relation-specific
    Question Answering
  tldr: Recently, several methods have tackled the relation extraction task with QA
    and have shown successful results. However, the effectiveness of existing methods
    in specific domains, such as the biomedical domain, is yet to be verified. When
    there are multiple entity pairs that share an entity in a sent
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper proposes a new document classification method that incorporates
    the representations of a literature graph created from bibliographic and entity
    information.Recently, document classification performance has been significantly
    improved with large pre-trained language models; however, there still remain documents
    that are difficult to classify. External information, such as bibliographic information,
    citation links, descriptions of entities, and medical taxonomies, has been considered
    one of the keys to dealing with such documents in document classification. Although
    several document classification methods using external information have been proposed,
    they only consider limited relationships, e.g., word co-occurrence and citation
    relationships. However, there are multiple types of external information.To overcome
    the limitation of the conventional use of external information, we propose a document
    classification model that simultaneously considers bibliographic and entity information
    to deeply model the relationships among documents using the representations of
    the literature graph.The experimental results show that our proposed method outperforms
    existing methods on two document classification datasets in the biomedical domain
    with the help of the literature graph.
  authors:
  - Ryuki Ida
  - Makoto Miwa
  - Yutaka Sasaki
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_53
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Biomedical Document Classification with Literature Graph Representations
    of Bibliographies and Entities
  tldr: 'This paper proposes a new document classification method that incorporates
    the representations of a literature graph created from bibliographic and entity
    information.Recently, document classification performance has been significantly
    improved with large pre-trained language models; however, there '
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Meta-analysis of randomized clinical trials (RCTs) plays a crucial role
    in evidence-based medicine but can be labor-intensive and error-prone. This study
    explores the use of large language models to enhance the efficiency of aggregating
    results from randomized clinical trials (RCTs) at scale. We perform a detailed
    comparison of the performance of these models in zero-shot prompt-based information
    extraction from a diverse set of RCTs to traditional manual annotation methods.
    We analyze the results for two different meta-analyses aimed at drug repurposing
    in cancer therapy pharmacovigilience in chronic myeloid leukemia. Our findings
    reveal that the best model for the two demonstrated tasks, ChatGPT can generally
    extract correct information and identify when the desired information is missing
    from an article. We additionally conduct a systematic error analysis, documenting
    the prevalence of diverse error types encountered during the process of prompt-based
    information extraction.
  authors:
  - David Kartchner
  - Selvi Ramalingam
  - Irfan Al-Hussaini
  - Olivia Kronick
  - Cassie Mitchell
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_55
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Zero-Shot Information Extraction for Clinical Meta-Analysis using Large Language
    Models
  tldr: Meta-analysis of randomized clinical trials (RCTs) plays a crucial role in
    evidence-based medicine but can be labor-intensive and error-prone. This study
    explores the use of large language models to enhance the efficiency of aggregating
    results from randomized clinical trials (RCTs) at scale. We per
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Social media offers an accessible avenue for individuals of diverse backgrounds
    and circumstances to share their unique perspectives and experiences. Our study
    focuses on the experience of low carbohydrate diets, motivated by recent research
    and clinical trials that elucidates the diet's promising health benefits. Given
    the lack of any suitable annotated dataset in this domain, we first define an
    annotation schema that reflects the interests of healthcare professionals and
    then manually annotate data from the Reddit social network.Finally, we benchmark
    the effectiveness of several classification approaches that are based on statistical
    Support Vector Machines (SVM) classifier, pre-train-then-finetune RoBERTa classifier,
    and, off-the-shelf ChatGPT API, on our annotated dataset.Our annotations and scripts
    that are used to download the Reddit posts are publicly available at https://data.csiro.au/collection/csiro:59208.
  authors:
  - Skyler Zou
  - Xiang Dai
  - Grant Brinkworth
  - Pennie Taylor
  - Sarvnaz Karimi
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_56
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Short paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Can Social Media Inform Dietary Approaches for Health Management? A Dataset
    and Benchmark for Low-Carb Diet
  tldr: Social media offers an accessible avenue for individuals of diverse backgrounds
    and circumstances to share their unique perspectives and experiences. Our study
    focuses on the experience of low carbohydrate diets, motivated by recent research
    and clinical trials that elucidates the diet's promising h
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Automatically rating the quality of published research is a critical step
    in medical evidence synthesis. While several methods have been proposed, their
    algorithmic fairness has been   overlooked even though significant risks may follow
    when such systems are deployed in biomedical contexts. In this work, we study
    fairness on two systems along two sensitive attributes, participant sex and medical
    area. In some cases, we find important inequalities, leading us to apply various
    debiasing methods. Upon examining an interplay of systems' predictive performance,
    fairness, as well as medically critical selective classification capabilities
    and calibration performance, we find that fairness can sometimes improve through
    debiasing, but at a cost in other performance measures.
  authors:
  - Simon Suster
  - Timothy Baldwin
  - Karin Verspoor
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_57
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Promoting Fairness in Classification of Quality of Medical Evidence
  tldr: 'Automatically rating the quality of published research is a critical step
    in medical evidence synthesis. While several methods have been proposed, their
    algorithmic fairness has been   overlooked even though significant risks may follow
    when such systems are deployed in biomedical contexts. In this '
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Fine-tuning biomedical pre-trained language models (BioPLMs) such as BioBERT
    has become a common practice dominating leaderboards across various natural language
    processing tasks. Despite their success and wide adoption, prevailing fine-tuning
    approaches for named entity recognition (NER)  naively train BioPLMs on targeted
    datasets without considering class distributions. This is problematic especially
    when dealing with imbalanced biomedical gold-standard datasets for NER in which
    most biomedical entities are underrepresented.In this paper, we address the class
    imbalance problem and propose WeLT, a cost-sensitive fine-tuning approach based
    on new re-scaled class weights for the task of biomedical NER. We evaluate WeLT's
    fine-tuning performance on mixed-domain and domain-specific BioPLMs using eight
    biomedical gold-standard datasets. We compare our approach against vanilla fine-tuning
    and three other existing re-weighting schemes. Our results show the positive impact
    of handling the class imbalance problem. WeLT outperforms all the vanilla fine-tuned
    models. Furthermore, our method demonstrates advantages over other existing weighting
    schemes in most experiments.
  authors:
  - Ghadeer Mobasher
  - Wolfgang Mller
  - Olga Krebs
  - Michael Gertz
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_58
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'WeLT: Improving Biomedical Fine-tuned Pre-trained Language Models with Cost-sensitive
    Learning'
  tldr: Fine-tuning biomedical pre-trained language models (BioPLMs) such as BioBERT
    has become a common practice dominating leaderboards across various natural language
    processing tasks. Despite their success and wide adoption, prevailing fine-tuning
    approaches for named entity recognition (NER)  naively t
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Summarization of medical notes has been studied for decades with hospital
    discharge summaries garnering recent interest in the research community. While
    methods for summarizing these notes have been the focus, there has been little
    work in understanding the feasibility of this task. We believe this effort is
    warranted given the notes' length and complexity, and that they are often riddled
    with poorly formatted structured data and redundancy in copy and pasted text.
    In this work, we investigate the feasibility of the summarization task by finding
    the origin, or data provenance, of the discharge summary's source text. As a motivation
    to understanding the data challenges of the summarization task, we present DSProv,
    a new dataset of 51 hospital admissions annotated by clinical informatics physicians.
    The dataset is analyzed for semantics and the extent of copied text from human
    authored electronic health record (EHR) notes. We also present a novel unsupervised
    method of matching notes used in discharge summaries, and release our annotation
    dataset1 and source code to the community.
  authors:
  - Paul Landes
  - Aaron Chaise
  - Kunal Patel
  - Sean Huang
  - Barbara Di Eugenio
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_59
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Hospital Discharge Summarization Data Provenance
  tldr: 'Summarization of medical notes has been studied for decades with hospital
    discharge summaries garnering recent interest in the research community. While
    methods for summarizing these notes have been the focus, there has been little
    work in understanding the feasibility of this task. We believe this '
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We systematically investigate lightweight strategies to adapt large language
    models (LLMs) for the task of radiology report summarization (RRS). Specifically,
    we focus on domain adaptation via pretraining (on natural language, biomedical
    text, or clinical text) and via discrete prompting or parameter-efficient fine-tuning.
    Our results consistently achieve best performance by maximally adapting to the
    task via pretraining on clinical text and fine-tuning on RRS examples. Importantly,
    this method fine-tunes a mere 0.32% of parameters throughout the model, in contrast
    to end-to-end fine-tuning (100% of parameters). Additionally, we study the effect
    of in-context examples and out-of-distribution (OOD) training before concluding
    with a radiologist reader study and qualitative analysis. Our findings highlight
    the importance of domain adaptation in RRS and provide valuable insights toward
    developing effective natural language processing solutions for clinical tasks.
  authors:
  - Dave Van Veen
  - Cara Van Uden
  - Maayane Attias
  - Anuj Pareek
  - Christian Bluethgen
  - Malgorzata Polacin
  - Wah Chiu
  - Jean-Benoit Delbrouck
  - Juan Zambrano Chaves
  - Curtis Langlotz
  - Akshay Chaudhari
  - John Pauly
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_61
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'RadAdapt: Radiology Report Summarization via Lightweight Domain Adaptation
    of Large Language Models'
  tldr: We systematically investigate lightweight strategies to adapt large language
    models (LLMs) for the task of radiology report summarization (RRS). Specifically,
    we focus on domain adaptation via pretraining (on natural language, biomedical
    text, or clinical text) and via discrete prompting or paramete
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The BioNLP Workshop 2023 initiated the launch of a shared task on Problem
    List Summarization (ProbSum) in January 2023. The aim of this shared task is to
    attract future research efforts in building NLP models for real-world diagnostic
    decision support applications, where a system generating relevant and accurate
    diagnoses will augment the healthcare providers' decision-making process and improve
    the quality of care for patients. The goal for participants is to develop models
    that generated a list of diagnoses and problems using input from the daily care
    notes collected from the hospitalization of critically ill patients. Eight teams
    submitted their final systems to the shared task leaderboard. In this paper, we
    describe the tasks, datasets, evaluation metrics, and baseline systems. Additionally,
    the techniques and results of the evaluation of the different approaches tried
    by the participating teams are summarized.
  authors:
  - Yanjun Gao
  - Dmitriy Dligach
  - Timothy Miller
  - Majid Afshar
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_62
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Short paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Overview of the Problem List Summarization (ProbSum) 2023 Shared Task on
    Summarizing Patients' Active Diagnoses and Problems from Electronic Health Record
    Progress Notes
  tldr: The BioNLP Workshop 2023 initiated the launch of a shared task on Problem
    List Summarization (ProbSum) in January 2023. The aim of this shared task is to
    attract future research efforts in building NLP models for real-world diagnostic
    decision support applications, where a system generating relevant
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper presents the results of the shared task on Lay Summarisation
    of Biomedical Research Articles (BioLaySumm), hosted at the BioNLP Workshop at
    ACL 2023.  The goal of this shared task is to develop abstractive summarisation
    models capable of generating ``lay summaries'''' (i.e., summaries that are comprehensible
    to non-technical audiences) in both a controllable and non-controllable setting.There
    are two subtasks: 1) Lay Summarisation, where the goal is for participants to
    build models for lay summary generation only, given the full article text and
    the corresponding abstract as input; and2) Readability-controlled Summarisation,
    where the goal is for participants to train models to generate both the technical
    abstract and the lay summary, given an article''s main text as input.In addition
    to overall results, we report on the setup and insights from the BioLaySumm shared
    task, which attracted a total of 20 participating teams across both subtasks.'
  authors:
  - Tomas Goldsack
  - Zheheng Luo
  - Qianqian Xie
  - Carolina Scarton
  - Matthew Shardlow
  - Sophia Ananiadou
  - Chenghua Lin
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_63
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'BioLaySumm 2023 Shared Task: Lay Summarisation of Biomedical Research Articles'
  tldr: 'This paper presents the results of the shared task on Lay Summarisation of
    Biomedical Research Articles (BioLaySumm), hosted at the BioNLP Workshop at ACL
    2023.  The goal of this shared task is to develop abstractive summarisation models
    capable of generating ``lay summaries'''' (i.e., summaries that '
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Radiology report summarization is a growing area of research. Given the
    Findings and/or Background sections of a radiology report, the goal is to generate
    a summary (called an Impression section) that highlights the key observations
    and conclusions of the radiology study. Recent efforts have released systems that
    achieve promising performance as measured by widely used summarization metrics
    such as BLEU and ROUGE. However, the research area of radiology report summarization
    currently faces two important limitations. First, most of the results are reported
    on private datasets. This limitation prevents the ability to reproduce results
    and fairly compare different systems and solutions. Secondly, to the best of our
    knowledge, most research is carried out on chest X-rays. To palliate these two
    limitations, we propose a radiology report summarization (RadSum) challenge on
    i) a new dataset of eleven different modalities and anatomies pairs based on the
    MIMIC-III database ii) a multimodal report summarization dataset based on MIMIC-CXR
    enhanced with a brand-new test-set from Stanford Hospital. In total, we received
    112 submissions across 11 teams.
  authors:
  - Jean-Benoit Delbrouck
  - Maya Varma
  - Pierre Chambon
  - Curtis Langlotz
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_64
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Short paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Overview of the RadSum23 Shared Task on Multi-modal and Multi-anatomical
    Radiology Report Summarization
  tldr: Radiology report summarization is a growing area of research. Given the Findings
    and/or Background sections of a radiology report, the goal is to generate a summary
    (called an Impression section) that highlights the key observations and conclusions
    of the radiology study. Recent efforts have release
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Communication of scientific findings to the public is important for keeping
    non-experts informed of developments such as life-saving medical treatments. However,
    generating readable lay summaries from scientific documents is challenging, and
    currently, these summaries suffer from critical factual errors. One popular intervention
    for improving factuality is using additional external knowledge to provide factual
    grounding. However, it is unclear how these grounding sources should be retrieved,
    selected, or integrated, and how supplementary grounding documents might affect
    the readability or relevance of the generated summaries. We develop a simple method
    for selecting grounding sources and integrating them with source documents. We
    then use the BioLaySum summarization dataset to evaluate the effects of different
    grounding sources on summary quality. We found that grounding source documents
    improves the relevance and readability of lay summaries but does not improve factuality
    of lay summaries. This continues to be true in zero-shot summarization settings
    where we hypothesized that grounding might be even more important for factual
    lay summaries.
  authors:
  - Domenic Rosati
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_101
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'GRASUM at BioLaySumm Task 1: Background Knowledge Grounding for Readable,
    Relevant, and Factual Biomedical Lay Summaries'
  tldr: Communication of scientific findings to the public is important for keeping
    non-experts informed of developments such as life-saving medical treatments. However,
    generating readable lay summaries from scientific documents is challenging, and
    currently, these summaries suffer from critical factual er
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper summarizes two approaches developed for BioNLP2023 workshop
    task 1A: clinical problem list summarization. We develop two types of methods
    with either rules or pre-trained language models. In the rule-based summarization
    model, we leverage UMLS (Unified Medical Language System) and a negation detector
    to extract text spans to represent the summary. We also fine tune three pre-trained
    language models (BART, T5 and GPT2) to generate the summaries.  Experiment results
    show the rule based system returns extractive summaries but lower ROUGE-L score
    (0.043), while the fine tuned T5 returns a higher ROUGE-L score (0.208).'
  authors:
  - Ming Liu
  - Dan Zhang
  - Weicong Tan
  - He Zhang
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_102
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'DeakinNLP at ProbSum 2023: Clinical Progress Note Summarization with Rules
    and Language ModelsClinical Progress Note Summarization with Rules and Languague
    Models'
  tldr: 'This paper summarizes two approaches developed for BioNLP2023 workshop task
    1A: clinical problem list summarization. We develop two types of methods with
    either rules or pre-trained language models. In the rule-based summarization model,
    we leverage UMLS (Unified Medical Language System) and a negat'
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes the submission of the TALP-UPC team to the Problem
    List Summarization task from the BioNLP 2023 workshop. This task consists of automatically
    extracting a list of health issues from the e-health medical record of a given
    patient. Our submission combines additional steps of data annotationwith finetuning
    of BERT pre-trained language models. Our experiments focus on the impact of finetuning
    on different datasets as well as the addition of data augmentation techniques
    to delay overfitting.
  authors:
  - Neil Torrero
  - Gerard Sant
  - Carlos Escolano
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_104
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'TALP-UPC at ProbSum 2023: Fine-tuning and Data Augmentation Strategies for
    NER'
  tldr: 'This paper describes the submission of the TALP-UPC team to the Problem List
    Summarization task from the BioNLP 2023 workshop. This task consists of automatically
    extracting a list of health issues from the e-health medical record of a given
    patient. Our submission combines additional steps of data '
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Medical progress notes play a crucial role in documenting a patient's
    hospital journey, including his or her condition, treatment plan, and any updates
    for healthcare providers. Automatic summarisation of a patient's problems in the
    form of a ``problem list'' can aid stakeholders in understanding a patient's condition,
    reducing workload and cognitive bias. BioNLP 2023 Shared Task 1A focusses on generating
    a list of diagnoses and problems from the provider's progress notes during hospitalisation.
    In this paper, we introduce our proposed approach to this task, which integrates
    two complementary components. One component employs large language models (LLMs)
    for data augmentation; the other is an abstractive summarisation LLM with a novel
    pre-training objective for generating the patients' problems summarised as a list.
    Our approach was ranked second among all submissions to the shared task.  The
    performance of our model on the development and test datasets shows that our approach
    is more robust on unknown data, with an improvement of up to 3.1 points over the
    same size of the larger model.
  authors:
  - Hao Li
  - Yuping Wu
  - Viktor Schlegel
  - Riza Batista-Navarro
  - Thanh-Tung Nguyen
  - Abhinav Ramesh Kashyap
  - Xiao-Jun Zeng
  - Daniel Beck
  - Stefan Winkler
  - Goran Nenadic
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_106
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Team:PULSAR at ProbSum 2023:PULSAR: Pre-training with Extracted Healthcare
    Terms for Summarising Patients'' Problems and Data Augmentation with Black-box
    Large Language Models'
  tldr: 'Medical progress notes play a crucial role in documenting a patient''s hospital
    journey, including his or her condition, treatment plan, and any updates for healthcare
    providers. Automatic summarisation of a patient''s problems in the form of a ``problem
    list'''' can aid stakeholders in understanding a '
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In this paper, we elaborate on our approach for the shared task 1A issued
    by BioNLP Workshop 2023 titled Problem List Summarization. With an increase in
    the digitization of health records, a need arises for quick and precise summarization
    of large amounts of records. With the help of summarization, medical professionals
    can sieve through multiple records in a short span of time without overlooking
    any crucial point. We use abstractive text summarization for this task and experiment
    with multiple state-of-the-art models like Pegasus, BART, and T5, along with various
    pre-processing and data augmentation techniques to generate summaries from patients'
    progress notes. For this task, the metric used was the ROUGE-L score. From our
    experiments, we conclude that Pegasus is the best-performing model on the dataset,
    achieving a ROUGE-L F1 score of 0.2744 on the test dataset (3rd rank on the leaderboard).
  authors:
  - Gaurav Kolhatkar
  - Aditya Paranjape
  - Omkar Gokhale
  - Dipali Kadam
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_107
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Team Converge at ProbSum 2023: Abstractive Text Summarization of Patient
    Progress Notes'
  tldr: 'In this paper, we elaborate on our approach for the shared task 1A issued
    by BioNLP Workshop 2023 titled Problem List Summarization. With an increase in
    the digitization of health records, a need arises for quick and precise summarization
    of large amounts of records. With the help of summarization, '
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In this paper, we consider the challenge of summarizing patients medical
    progress notes in a limited data setting. For the Problem List Summarization (shared
    task 1A) at the BioNLP Workshop 2023, we demonstrate that ClinicalT5 fine-tuned
    to 765 medical clinic notes outperforms other extractive, abstractive and zero-shot
    baselines, yielding reasonable baseline systems for medical note summarization.
    Further, we introduce Hierarchical Ensemble of Summarization Models (HESM), consisting
    of token-level ensembles of diverse fine-tuned ClinicalT5 models, followed by
    Minimum Bayes Risk (MBR) decoding. Our HESM approach lead to a considerable summarization
    performance boost, and when evaluated on held-out challenge data achieved a ROUGE-L
    of 32.77, which was the best-performing system at the top of the shared task leaderboard.
  authors:
  - Potsawee Manakul
  - Yassir Fathullah
  - Adian Liusie
  - Vyas Raina
  - Vatsal Raina
  - Mark Gales
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_108
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'CUED at ProbSum 2023: Hierarchical Ensemble of Summarization Models'
  tldr: In this paper, we consider the challenge of summarizing patients medical progress
    notes in a limited data setting. For the Problem List Summarization (shared task
    1A) at the BioNLP Workshop 2023, we demonstrate that ClinicalT5 fine-tuned to
    765 medical clinic notes outperforms other extractive, abst
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper presents our system at the Radiology Report Summarization Shared
    Task-1B of the 22nd BioNLP Workshop 2023. Inspired by the work of the BioBART
    model, we continuously pre-trained a general domain BART model with biomedical
    data to adapt it to this specific domain. In the pre-training phase, several pre-training
    tasks are aggregated to inject linguistic knowledge and increase the abstractivity
    of the generated summaries. We present the results of our models, and also, we
    have carried out an additional study on the lengths of the generated summaries,
    which has provided us with interesting information.
  authors:
  - Vicent Ahuir Esteve
  - Encarna Segarra
  - Lluis Hurtado
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_110
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'ELiRF-VRAIN at BioNLP Task 1B: Radiology Report Summarization'
  tldr: This paper presents our system at the Radiology Report Summarization Shared
    Task-1B of the 22nd BioNLP Workshop 2023. Inspired by the work of the BioBART
    model, we continuously pre-trained a general domain BART model with biomedical
    data to adapt it to this specific domain. In the pre-training phase
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper covers participation of the SINAI team in the shared task
    1B: Radiology Report Summarization at the BioNLP workshop held on ACL 2023. Our
    proposal follows a sequence-to-sequence approach which leverages pre-trained multilingual
    general domain and monolingual biomedical domain pre-trained language models.
    The best performing system based on domain-specific model reached 33.96 F1RadGraph
    score which is the fourth best result among the challenge participants. This model
    was made publicly available on HuggingFace. We also describe an attempt of Proximal
    Policy Optimization Reinforcement Learning that was made in order to improve the
    factual correctness measured with F1RadGraph but did not lead to satisfactory
    results.'
  authors:
  - Mariia Chizhikova
  - Manuel Diaz-Galiano
  - L. Alfonso Urena-Lopez
  - M. Teresa Martin-Valdivia
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_111
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'SINAI at RadSum23: Radiology Report Summarization Based on Domain-Specific
    Sequence-To-Sequence Transformer Model'
  tldr: 'This paper covers participation of the SINAI team in the shared task 1B:
    Radiology Report Summarization at the BioNLP workshop held on ACL 2023. Our proposal
    follows a sequence-to-sequence approach which leverages pre-trained multilingual
    general domain and monolingual biomedical domain pre-trained '
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper presents our contribution to the RadSum23 shared task organized
    as part of the BioNLP 2023. We compared state-of-the-art generative language models
    in generating high-quality summaries from radiology reports. A two-stage fine-tuning
    approach was introduced for utilizing knowledge learnt from different datasets.
    We evaluated the performance of our method using a variety of metrics, including
    BLEU, ROUGE, bertscore, CheXbert, and RadGraph. Our results revealed the potentials
    of different models in summarizing radiology reports and demonstrated the effectiveness
    of the two-stage fine-tuning approach. We also discussed the limitations and future
    directions of our work, highlighting the need for better understanding the architecture
    design's effect and optimal way of fine-tuning accordingly in automatic clinical
    summarizations.
  authors:
  - Jinge Wu
  - Daqian Shi
  - Abul Hasan
  - Honghan Wu
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_112
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'KnowLab at RadSum23: comparing pre-trained language models in radiology
    report summarization'
  tldr: This paper presents our contribution to the RadSum23 shared task organized
    as part of the BioNLP 2023. We compared state-of-the-art generative language models
    in generating high-quality summaries from radiology reports. A two-stage fine-tuning
    approach was introduced for utilizing knowledge learnt f
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper describes the experiments undertaken and their results as
    part of the BioNLP 2023 workshop. We took part in Task 1B: Radiology Report Summarization.
    Multiple runs were submitted for evaluation from solutions utilizing transfer
    learning from pre-trained transformer models, which were then fine-tuned on MIMIC-III
    dataset, for abstractive report summarization.'
  authors:
  - Sri Macharla
  - Ashok Madamanchi
  - Nikhilesh Kancharla
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_113
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'nav-nlp at RadSum23: Abstractive Summarization of Radiology Reports using
    BART Finetuning'
  tldr: 'This paper describes the experiments undertaken and their results as part
    of the BioNLP 2023 workshop. We took part in Task 1B: Radiology Report Summarization.
    Multiple runs were submitted for evaluation from solutions utilizing transfer
    learning from pre-trained transformer models, which were then '
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We describe the participation of team e-Health CSIRO in the BioNLP RadSum
    task of 2023. This task aims to develop automatic summarisation methods for radiology.
    The subtask that we participated in was multimodal; the impression section of
    a report was to be summarised from a given findings section and set of Chest X-rays
    (CXRs) of a subject's study. For our method, we adapted an encoder-to-decoder
    model for CXR report generation to the subtask. e-Health CSIRO placed seventh
    amongst the participating teams with a RadGraph ER F1 score of 23.9.
  authors:
  - Aaron Nicolson
  - Jason Dowling
  - Bevan Koopman
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_114
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'e-Health CSIRO at RadSum23: Adapting a Chest X-Ray Report Generator to Multimodal
    Radiology Report Summarisation'
  tldr: We describe the participation of team e-Health CSIRO in the BioNLP RadSum
    task of 2023. This task aims to develop automatic summarisation methods for radiology.
    The subtask that we participated in was multimodal; the impression section of
    a report was to be summarised from a given findings section a
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Instruction-tuned generative large language models (LLMs), such as ChatGPT
    and Bloomz, possess excellent generalization abilities. However, they face limitations
    in understanding radiology reports, particularly when generating the IMPRESSIONS
    section from the FINDINGS section. These models tend to produce either verbose
    or incomplete IMPRESSIONS, mainly due to insufficient exposure to medical text
    data during training. We present a system that leverages large-scale medical text
    data for domain-adaptive pre-training of instruction-tuned LLMs, enhancing their
    medical knowledge and performance on specific medical tasks. We demonstrate that
    this system performs better in a zero-shot setting compared to several pretrain-and-finetune
    adaptation methods on the IMPRESSIONS generation task. Furthermore, it ranks 1st
    among participating systems in Task 1B: Radiology Report Summarization.'
  authors:
  - Sanjeev Kumar Karn
  - Rikhiya Ghosh
  - Kusuma P
  - Oladimeji Farri
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_116
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'shs-nlp at RadSum23: Domain-Adaptive Pre-training of Instruction-tuned LLMs
    for Radiology Report Impression Generation'
  tldr: Instruction-tuned generative large language models (LLMs), such as ChatGPT
    and Bloomz, possess excellent generalization abilities. However, they face limitations
    in understanding radiology reports, particularly when generating the IMPRESSIONS
    section from the FINDINGS section. These models tend to p
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Radiology report summarization aims to automatically provide concise summaries
    of radiology findings, reducing time and errors in manual summaries. However,
    current methods solely summarize the text, which overlooks critical details in
    the images. Unfortunately, directly using the images in a multimodal model is
    difficult. Multimodal models are susceptible to overfitting due to their increased
    capacity, and modalities tend to overfit and generalize at different rates. Thus,
    we propose a novel retrieval-based approach that uses image similarities to generate
    additional text features. We further employ few-shot with chain-of-thought and
    ensemble techniques to boost performance. Overall, our method achieves state-of-the-art
    performance in the F1RadGraph score, which measures the factual correctness of
    summaries. We rank second place in both MIMIC-CXR and MIMIC-III hidden tests among
    11 teams.
  authors:
  - Tongnian Wang
  - Xingmeng Zhao
  - Anthony Rios
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_117
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'UTSA-NLP at RadSum23: Multi-modal Retrieval-Based Chest X-Ray Report Summarization'
  tldr: Radiology report summarization aims to automatically provide concise summaries
    of radiology findings, reducing time and errors in manual summaries. However,
    current methods solely summarize the text, which overlooks critical details in
    the images. Unfortunately, directly using the images in a multim
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In this paper, we introduce CheXOFA, a new pre-trained vision-language
    model (VLM) for the chest X-ray domain. Our model is initially pre-trained on
    various multimodal datasets within the general domain before being transferred
    to the chest X-ray domain. Following a prominent VLM, we unify various domain-specific
    tasks into a simple sequence-to-sequence schema.It enables the model to effectively
    learn the required knowledge and skills from limited resources in the domain.Demonstrating
    superior performance on the benchmark datasets provided by the BioNLP shared task
    (Delbrouck et al., 2023), our model benefits from its training across multiple
    tasks and domains.With subtle techniques including ensemble and factual calibration,
    our system achieves first place on the RadSum23 leaderboard for the hidden test
    set.
  authors:
  - Gangwoo Kim
  - Hajung Kim
  - Lei Ji
  - Seongsu Bae
  - chanhwi kim
  - Mujeen Sung
  - Hyunjae Kim
  - Kun Yan
  - Eric Chang
  - Jaewoo Kang
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_118
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'KU-DMIS-MSRA at RadSum23: Pre-trained Vision-Language Model for Radiology
    Report Summarization'
  tldr: In this paper, we introduce CheXOFA, a new pre-trained vision-language model
    (VLM) for the chest X-ray domain. Our model is initially pre-trained on various
    multimodal datasets within the general domain before being transferred to the
    chest X-ray domain. Following a prominent VLM, we unify various d
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We describe our systems participated in the BioLaySumm 2023 Task 1, which
    aims at automatically generating lay summaries of scientific articles in a simplified
    way so that its content becomes easier to comprehend for non-expert readers. Our
    approaches are based on selecting key information by both explicit and implicit
    strategies. For explicit selection strategies, we conduct extractive summarization
    based on selecting key sentences for training abstractive summarization models.
    For implicit selection strategies, we utilize a method based on a factorized energy-based
    model, which is able to extract important information from long documents to generate
    summaries and achieve promising results. We build our systems using sequence-to-sequence
    models, which enable us to leverage powerful and biomedical domain pre-trained
    language models and apply different strategies to generate lay summaries from
    long documents. We conducted various experiments to carefully investigate the
    effects of different aspects of this long-document summarization task such as
    extracting different document lengths and utilizing different pre-trained language
    models. We achieve the third rank in the shared task (and the second rank excluding
    the baseline submission of the organizers).
  authors:
  - Phuc Phan
  - Tri Tran
  - Hai-Long Trieu
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_120
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'VBD-NLP at BioLaySumm Task 1: Explicit and Implicit Key Information Selection
    for Lay Summarization on Biomedical Long Documents'
  tldr: We describe our systems participated in the BioLaySumm 2023 Task 1, which
    aims at automatically generating lay summaries of scientific articles in a simplified
    way so that its content becomes easier to comprehend for non-expert readers. Our
    approaches are based on selecting key information by both e
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In this paper we tackle a lay summarization task which aims to produce
    lay-summary of biomedical articles. BioLaySumm in the BioNLP Workshop at ACL 2023
    (Goldsack et al., 2023), has presented us with this lay summarization task for
    biomedical articles. Our proposed models provide a three-step abstractive approach
    for summarizing biomedical articles. Our methodology involves breaking down the
    original document into distinct sections, generating candidate summaries for each
    subsection, then finally re-ranking and selecting the top performing paragraph
    for each section. We run ablation studies to establish that each step in our pipeline
    is critical for improvement in the quality of lay summary. This model achieved
    the second-highest rank in terms of readability scores (Luo et al., 2022). Our
    work distinguishes itself from previous studies by not only considering the content
    of the paper but also its structure, resulting in more coherent and comprehensible
    lay summaries. We hope that our model for generating lay summaries of biomedical
    articles will be a useful resource for individuals across various domains, including
    academia, industry, and healthcare, who require rapid comprehension of key scientific
    research.
  authors:
  - A.S. Poornash
  - Atharva Deshmukh
  - Archit Sharma
  - Sriparna Saha
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_121
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'APTSumm at BioLaySumm Task 1: Biomedical Breakdown, Improving Readability
    by Relevancy Based Selection'
  tldr: In this paper we tackle a lay summarization task which aims to produce lay-summary
    of biomedical articles. BioLaySumm in the BioNLP Workshop at ACL 2023 (Goldsack
    et al., 2023), has presented us with this lay summarization task for biomedical
    articles. Our proposed models provide a three-step abstra
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This study describes the model design of the NCUEE-NLP system for BioLaySumm
    Task 2 at the BioNLP 2023 workshop. We separately fine-tune pretrained PRIMERA
    models to independently generate technical abstracts and lay summaries of biomedical
    articles. A total of seven evaluation metrics across three criteria were used
    to compare system performance. Our best submission was ranked first for relevance,
    second for readability, and fourth for factuality, tying first for overall performance.
  authors:
  - Chao-Yi Chen
  - Jen-Hao Yang
  - Lung-Hao Lee
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_122
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'NCUEE-NLP at BioLaySumm Task 2: Readability-Controlled Summarization of
    Biomedical Articles Using the PRIMERA Models'
  tldr: 'This study describes the model design of the NCUEE-NLP system for BioLaySumm
    Task 2 at the BioNLP 2023 workshop. We separately fine-tune pretrained PRIMERA
    models to independently generate technical abstracts and lay summaries of biomedical
    articles. A total of seven evaluation metrics across three '
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Lay summarization aims to simplify complex scientific information for
    non-expert audiences. This paper investigates the trade-off between readability
    and relevance in the lay summarization of long biomedical documents. We introduce
    a two-stage framework that attains the best readability metrics in the first subtask
    of BioLaySumm 2023, with 8.924 FleschKincaid Grade Level and 9.188 DaleChall Readability
    Score. However, this comes at the cost of reduced relevance and factuality, emphasizing
    the inherent challenges of balancing readability and content preservation in lay
    summarization. The first stage generates summaries using a large language model,
    such as BART with LSG attention. The second stage uses a zero-shot sentence simplification
    method to improve the readability of the summaries. In the second subtask, a hybrid
    dataset is employed to train a model capable of generating both lay summaries
    and abstracts. This approach achieves the best readability score and shares the
    top overall rank with other leading methods. Our study underscores the importance
    of developing effective methods for creating accessible lay summaries while maintaining
    information integrity. Future work will integrate simplification and summary generation
    within a joint optimization framework that generates high-quality lay summaries
    that effectively communicate scientific content to a broader audience.
  authors:
  - Irfan Al-Hussaini
  - Austin Wu
  - Cassie Mitchell
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_123
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Pathology Dynamics at BioLaySumm: the trade-off between Readability, Relevance,
    and Factuality in Lay Summarization'
  tldr: Lay summarization aims to simplify complex scientific information for non-expert
    audiences. This paper investigates the trade-off between readability and relevance
    in the lay summarization of long biomedical documents. We introduce a two-stage
    framework that attains the best readability metrics in t
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes the entry by the Intelligent Knowledge Management
    (IKM) Laboratory in the BioLaySumm 2023 task1. We aim to transform lengthy biomedical
    articles into concise, reader-friendly summaries that can be easily comprehended
    by the general public. We utilized a long-text abstractive summarization longformer
    model and experimented with several prompt methods for this task. Our entry placed
    10th overall, but we were particularly proud to achieve a 3rd place score in the
    readability evaluation metric.
  authors:
  - Yu-Hsuan Wu
  - Ying-Jia Lin
  - Hung-Yu Kao
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_124
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'IKM_Lab at BioLaySumm Task 1: Longformer-based Prompt Tuning for Biomedical
    Lay Summary Generation'
  tldr: This paper describes the entry by the Intelligent Knowledge Management (IKM)
    Laboratory in the BioLaySumm 2023 task1. We aim to transform lengthy biomedical
    articles into concise, reader-friendly summaries that can be easily comprehended
    by the general public. We utilized a long-text abstractive sum
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper presents our approach to the BioLaySumm Task 1 shared task,
    held at the BioNLP 2023 Workshop. The effective communication of scientific knowledge
    to the general public is often limited by the technical language used in research,
    making it difficult for non-experts to comprehend. To address this issue, lay
    summaries can be used to explain research findings to non-experts in an accessible
    form. We conduct an evaluation of autoregressive language models, both general
    and specialized for the biomedical domain, to generate lay summaries from biomedical
    research article abstracts. Our findings demonstrate that a GPT-3.5 model combined
    with a straightforward few-shot prompt produces lay summaries that achieve significantly
    relevance and factuality compared to those generated by a fine-tuned BioGPT model.
    However, the summaries generated by the BioGPT model exhibit better readability.
    Notably, our submission for the shared task achieved 1st place in the competition.
  authors:
  - Oisn Turbitt
  - Robert Bevan
  - Mouhamad Aboshokor
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_125
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'MDC at BioLaySumm Task 1: Evaluating GPT Models for Biomedical Lay Summarization'
  tldr: This paper presents our approach to the BioLaySumm Task 1 shared task, held
    at the BioNLP 2023 Workshop. The effective communication of scientific knowledge
    to the general public is often limited by the technical language used in research,
    making it difficult for non-experts to comprehend. To addres
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: As part of our participation in BioLaySumm 2023, we explored the use of
    large language models (LLMs) to automatically generate concise and readable summaries
    of biomedical research articles. We utilized pre-trained LLMs to fine-tune our
    summarization models on two provided datasets, and adapt them to the shared task
    within the constraints of training time and computational power. Our final models
    achieved very high relevance and factuality scores on the test set, and ranked
    among the top five models in the overall performance.
  authors:
  - Quancheng Liu
  - Xiheng Ren
  - V.G.Vinod Vydiswaran
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_126
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'LHS712EE at BioLaySumm 2023: Using BART and LED to summarize biomedical
    research articles'
  tldr: As part of our participation in BioLaySumm 2023, we explored the use of large
    language models (LLMs) to automatically generate concise and readable summaries
    of biomedical research articles. We utilized pre-trained LLMs to fine-tune our
    summarization models on two provided datasets, and adapt them t
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Initially, we analyzed the datasets in a statistical way so as to learn
    about various sections' contributions to the final summary in both the pros and
    life datasets. We found that both the datasets have an Introduction and Abstract
    along with some initial parts of the results contributing to the summary. We considered
    only these sections in the next stage of analysis. We found the optimal length
    or no of sentences of each of the Introduction, abstract, and result which contributes
    best to the summary. After this statistical analysis, we took the pre-trained
    model Facebook/bart-base and fine-tuned it with both the datasets PLOS and eLife.
    While fine-tuning and testing the results we have used chunking because the text
    lengths are huge. So to not lose information due to the number of token constraints
    of the model, we used chunking. Finally, we saw the eLife model giving more accurate
    results than PLOS in terms of readability aspect, probably because the PLOS summary
    is closer to its abstract, we have considered the eLife model as our final model
    and tuned the hyperparameters. We are ranked 7th overall and 1st  in readability
  authors:
  - Venkat praneeth Reddy
  - Pinnapu Reddy Harshavardhan Reddy
  - Karanam Sai Sumedh
  - Raksha Sharma
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_127
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: IITR at BioLaySumm Task 1:Lay Summarization of BioMedical articles using
    Transformers
  tldr: Initially, we analyzed the datasets in a statistical way so as to learn about
    various sections' contributions to the final summary in both the pros and life
    datasets. We found that both the datasets have an Introduction and Abstract along
    with some initial parts of the results contributing to the su
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Lay summarisation aims at generating a summary for non-expert audience
    which allows them to keep updated with latest research in a specific field. Despite
    the significant advancements made in the field of text summarisation, lay summarisation
    remains relatively under-explored. We present a comprehensive set of experiments
    and analysis to investigate the effectiveness of existing pre-trained language
    models in generating lay summaries. When evaluate our models using a BioNLP Shared
    Task, BioLaySumm, our submission ranked second for the relevance criteria and
    third overall among 21 competing teams.
  authors:
  - Mong Yuan Sim
  - Xiang Dai
  - Maciej Rybinski
  - Sarvnaz Karimi
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_128
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'CSIRO Data61 Team at BioLaySumm Task 1: Lay Summarisation of Biomedical
    Research Articles Using Generative Models'
  tldr: Lay summarisation aims at generating a summary for non-expert audience which
    allows them to keep updated with latest research in a specific field. Despite
    the significant advancements made in the field of text summarisation, lay summarisation
    remains relatively under-explored. We present a comprehen
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Communicating scientific research to the general public is an essential
    yet challenging task.Lay summaries, which provide a simplified version of research
    findings, can bridge the gapbetween scientific knowledge and public understanding.
    The BioLaySumm task (Goldsack et al., 2023) is a shared task that seeks to automate
    this process by generating lay summaries from biomedical articles. Two different
    datasets that have been created from curating two biomedical journals (PLOS and
    eLife) are provided by the task organizers. As a participant in this shared task,
    we developed a system to generate a lay summary from an article's abstract and
    main text.
  authors:
  - Cagla Colak
  - lknur Karadeniz
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BioNLP-ST
  forum: ''
  id: BioNLP_130
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'ISIKSumm at BioLaySumm Task 1: BART-based Summarization System Enhanced
    with Bio-Entity Labels'
  tldr: Communicating scientific research to the general public is an essential yet
    challenging task.Lay summaries, which provide a simplified version of research
    findings, can bridge the gapbetween scientific knowledge and public understanding.
    The BioLaySumm task (Goldsack et al., 2023) is a shared task t
  track: BioNLP and BioNLP-ST 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Reasoning, as an essential ability for complex problem-solving, can provide
    back-end support for various real-world applications, such as medical diagnosis,
    negotiation, etc. This paper provides a comprehensive survey of cutting-edge research
    on reasoning with language model prompting. We introduce research works with comparisons
    and summaries and provide systematic resources to help beginners. We also discuss
    the potential reasons for emerging such reasoning abilities and highlight future
    research directions.
  authors:
  - Shuofei Qiao
  - Yixin Ou
  - Ningyu Zhang
  - Xiang Chen
  - Yunzhi Yao
  - Shumin Deng
  - Chuanqi Tan
  - Fei Huang
  - Huajun Chen
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLRSE
  forum: ''
  id: ACL_36
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Reasoning with Language Model Prompting: A Survey'
  tldr: 'Reasoning, as an essential ability for complex problem-solving, can provide
    back-end support for various real-world applications, such as medical diagnosis,
    negotiation, etc. This paper provides a comprehensive survey of cutting-edge research
    on reasoning with language model prompting. We introduce '
  track: 1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'While Chain-of-Thought (CoT) prompting boosts Language Models'' (LM)
    performance on a gamut of complex reasoning tasks, the generated reasoning chain
    does not necessarily reflect how the model arrives at the answer (aka. *faithfulness*).
    We propose **Faithful CoT**, a faithful-by-construction framework that decomposes
    a reasoning task into two stages: **Translation** (Natural Language query  symbolic
    reasoning chain) and **Problem Solving** (reasoning chain  answer), using an LM
    and a deterministic solver respectively. We demonstrate the efficacy of our approach
    on 10 reasoning datasets from 4 diverse domains. It outperforms traditional CoT
    prompting on 9 out of the 10 datasets, with an average accuracy gain of 4.4 on
    Math Word Problems, 1.9 on Planning, 4.0 on Multi-hop Question Answering (QA),
    and 18.1 on Logical Inference, under greedy decoding. Together with self-consistency
    decoding, we achieve new state-of-the-art few-shot performance on 7 out of the
    10 datasets, showing a strong synergy between faithfulness and accuracy.'
  authors:
  - Qing Lyu
  - Shreya Havaldar
  - Adam Stein
  - Li Zhang
  - Delip Rao
  - Eric Wong
  - Marianna Apidianaki
  - Chris Callison-burch
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLRSE
  forum: ''
  id: ACL_38
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Faithful Chain-of-Thought Reasoning
  tldr: While Chain-of-Thought (CoT) prompting boosts Language Models' (LM) performance
    on a gamut of complex reasoning tasks, the generated reasoning chain does not
    necessarily reflect how the model arrives at the answer (aka. *faithfulness*).
    We propose **Faithful CoT**, a faithful-by-construction framewo
  track: 1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Logical reasoning is central to human cognition and intelligence. Past
    research of logical reasoning within AI uses formal language as knowledge representation\textasciitilde{}(and
    symbolic reasoners). However, reasoning with formal language has proved challenging\textasciitilde{}(e.g.,
    brittleness and knowledge-acquisition bottleneck). This paper provides a comprehensive
    overview on a new paradigm of logical reasoning, which uses natural language as
    knowledge representation\textasciitilde{}(and pretrained language models as reasoners),
    including philosophical definition and categorization of logical reasoning, advantages
    of the new paradigm, benchmarks and methods, challenges of the new paradigm, desirable
    tasks \textbackslash{}\& methods in the future, and relation to related NLP fields.
    This new paradigm is promising since it not only alleviates many challenges of
    formal representation but also has advantages over end-to-end neural methods.
  authors:
  - Zonglin Yang
  - Xinya Du
  - Rui Mao
  - Jinjie Ni
  - Erik Cambria
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLRSE
  forum: ''
  id: ACL_41
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Logical Reasoning over Natural Language as Knowledge Representation: A Survey'
  tldr: 'Logical reasoning is central to human cognition and intelligence. Past research
    of logical reasoning within AI uses formal language as knowledge representation\textasciitilde{}(and
    symbolic reasoners). However, reasoning with formal language has proved challenging\textasciitilde{}(e.g.,
    brittleness '
  track: 1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Large language models have shown impressive abilities to reason over input
    text, however, they are prone to hallucinations. On the other hand, end-to-end
    knowledge graph question answering (KGQA) models output responses grounded in
    facts, but they still struggle with complex reasoning, such as comparison or ordinal
    questions. In this paper, we propose a new method for complex question answering
    where we combine a knowledge graph retriever based on an end-to-end KGQA model
    with a language model that reasons over the retrieved facts to return an answer.
    We observe that augmenting language model prompts with retrieved KG facts improves
    performance over using a language model alone by an average of 83\%. In particular,
    we see improvements on complex questions requiring count, intersection, or multi-hop
    reasoning operations.
  authors:
  - Priyanka Sen
  - Sandeep Mavadia
  - Amir Saffari
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLRSE
  forum: ''
  id: ACL_42
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Knowledge Graph-augmented Language Models for Complex Question Answering
  tldr: Large language models have shown impressive abilities to reason over input
    text, however, they are prone to hallucinations. On the other hand, end-to-end
    knowledge graph question answering (KGQA) models output responses grounded in
    facts, but they still struggle with complex reasoning, such as compa
  track: 1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Natural language is among the most accessible tools for explaining decisions
    to humans, and large pretrained language models (PLMs) have demonstrated impressive
    abilities to generate coherent natural language explanations (NLE). The existing
    NLE research perspectives do not take the audience into account. An NLE can have
    high textual quality, but it might not accommodate audiences' needs and preference.
    To address this limitation, we propose an alternative perspective, \_situated
    NLE\_, including a situated generation framework and a situated evaluation framework.
    On the generation side, we propose simple prompt engineering methods that adapt
    the NLEs to situations. In human studies, the annotators preferred the situated
    NLEs. On the evaluation side, we set up automated evaluation scores in lexical,
    semantic, and pragmatic categories. The scores can be used to select the most
    suitable prompts to generate NLEs. Situated NLE provides a perspective to conduct
    further research on automatic NLE generations.
  authors:
  - Zining Zhu
  - Haoming Jiang
  - Jingfeng Yang
  - Sreyashi Nag
  - Chao Zhang
  - Jie Huang
  - Yifan Gao
  - Frank Rudzicz
  - Bing Yin
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLRSE
  forum: ''
  id: ACL_43
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Situated Natural Language Explanations
  tldr: Natural language is among the most accessible tools for explaining decisions
    to humans, and large pretrained language models (PLMs) have demonstrated impressive
    abilities to generate coherent natural language explanations (NLE). The existing
    NLE research perspectives do not take the audience into ac
  track: 1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'How to usefully encode compositional task structure has long been a core
    challenge in AI. Recent work in chain of thought prompting has shown that for
    very large neural language models (LMs), explicitly demonstrating the inferential
    steps involved in a target task may improve performance over end-to-end learning
    that focuses on the target task alone. However, chain of thought prompting has
    significant limitations due to its dependency on huge pretrained LMs. In this
    work, we present compositional fine-tuning (CFT): an approach based on explicitly
    decomposing a target task into component tasks, and then fine-tuning smaller LMs
    on a curriculum of such component tasks. We apply CFT to recommendation tasks
    in two domains, world travel and local dining, as well as a previously studied
    inferential task (sports understanding). We show that CFT outperforms end-to-end
    learning even with equal amounts of data, and gets consistently better as more
    component tasks are modeled via fine-tuning. Compared with chain of thought prompting,
    CFT performs at least as well using LMs only 7.4\% of the size, and is moreover
    applicable to task domains for which data are not available during pretraining.'
  authors:
  - Victor Bursztyn
  - David Demeter
  - Doug Downey
  - Larry Birnbaum
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLRSE
  forum: ''
  id: ACL_45
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Learning to Perform Complex Tasks through Compositional Fine-Tuning of Language
    Models
  tldr: How to usefully encode compositional task structure has long been a core challenge
    in AI. Recent work in chain of thought prompting has shown that for very large
    neural language models (LMs), explicitly demonstrating the inferential steps involved
    in a target task may improve performance over end-to
  track: 1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Recent work has shown that prompting language models with code-like representations
    of natural language leads to performance improvements on structured reasoning
    tasks. However, such tasks comprise only a small subset of all natural language
    tasks. In our work, we seek to answer whether or not code-prompting is the preferred
    way of interacting with language models in general. We compare code and text prompts
    across three popular GPT models (davinci, code-davinci-002, and text-davinci-002)
    on a broader selection of tasks (e.g., QA, sentiment, summarization) and find
    that with few exceptions, code prompts do not consistently outperform text prompts.
    Furthermore, we show that the style of code prompt has a large effect on performance
    for some (but not all) tasks and that fine-tuning on text instructions leads to
    better relative performance of code prompts.
  authors:
  - Li Zhang
  - Liam Dugan
  - Hainiu Xu
  - Chris Callison-burch
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLRSE
  forum: ''
  id: ACL_47
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Exploring the Curious Case of Code Prompts
  tldr: Recent work has shown that prompting language models with code-like representations
    of natural language leads to performance improvements on structured reasoning
    tasks. However, such tasks comprise only a small subset of all natural language
    tasks. In our work, we seek to answer whether or not code-
  track: 1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We assume that providing explanations is a process to elicit implicit
    knowledge in human communication, and propose a general methodology to generate
    commonsense explanations from pairs of semantically related sentences. We take
    advantage of both prompting applied to large, encoder-decoder pre-trained language
    models, and few-shot learning techniques, such as pattern-exploiting training.
    Experiments run on the e-SNLI dataset show that the proposed method achieves state-of-the-art
    results on the explanation generation task, with a substantial reduction of labelled
    data. The obtained results open new perspective on a number of tasks involving
    the elicitation of implicit knowledge.
  authors:
  - Andrea Zaninello
  - Bernardo Magnini
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLRSE
  forum: ''
  id: ACL_48
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'A smashed glass cannot be full: Generation of Commonsense Explanations through
    Prompt-based Few-shot Learning'
  tldr: We assume that providing explanations is a process to elicit implicit knowledge
    in human communication, and propose a general methodology to generate commonsense
    explanations from pairs of semantically related sentences. We take advantage of
    both prompting applied to large, encoder-decoder pre-train
  track: 1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We conduct a preliminary inquiry into the ability of generative transformer
    models to deductively reason from premises provided.We observe notable differences
    in the performance of models coming from different training setups, and find that
    the deductive reasoning ability increases with scale.Further, we discover that
    the performance generally does not decrease with the length of the deductive chain
    needed to reach the conclusion, with the exception of OpenAI GPT-3 and GPT-3.5
    models.Our study considers a wide variety of transformer-decoder models, ranging
    from 117 million to 175 billion parameters in size.
  authors:
  - Peter Belcak
  - Luca Lanzendrfer
  - Roger Wattenhofer
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLRSE
  forum: ''
  id: ACL_49
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Examining the Emergence of Deductive Reasoning in Generative Language Models
  tldr: We conduct a preliminary inquiry into the ability of generative transformer
    models to deductively reason from premises provided.We observe notable differences
    in the performance of models coming from different training setups, and find that
    the deductive reasoning ability increases with scale.Furthe
  track: 1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Saliency maps can explain a neural model's predictions by identifying
    important input features. They are difficult to interpret for laypeople, especially
    for instances with many features. In order to make them more accessible, we formalize
    the underexplored task of translating saliency maps into natural language and
    compare methods that address two key challenges of this approach -- what and how
    to verbalize. In both automatic and human evaluation setups, using token-level
    attributions from text classification tasks, we compare two novel methods (search-based
    and instruction-based verbalizations) against conventional feature importance
    representations (heatmap visualizations and extractive rationales), measuring
    simulatability, faithfulness, helpfulness and ease of understanding. Instructing
    GPT-3.5 to generate saliency map verbalizations yields plausible explanations
    which include associations, abstractive summarization and commonsense reasoning,
    achieving by far the highest human ratings, but they are not faithfully capturing
    numeric information and are inconsistent in their interpretation of the task.
    In comparison, our search-based, model-free verbalization approach efficiently
    completes templated verbalizations, is faithful by design, but falls short in
    helpfulness and simulatability. Our results suggest that saliency map verbalization
    makes feature attribution explanations more comprehensible and less cognitively
    challenging to humans than conventional representations.
  authors:
  - Nils Feldhus
  - Leonhard Hennig
  - Maximilian Nasert
  - Christopher Ebert
  - Robert Schwarzenberg
  - Sebastian Mller
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLRSE
  forum: ''
  id: ACL_50
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Saliency Map Verbalization: Comparing Feature Importance Representations
    from Model-free and Instruction-based Methods'
  tldr: Saliency maps can explain a neural model's predictions by identifying important
    input features. They are difficult to interpret for laypeople, especially for
    instances with many features. In order to make them more accessible, we formalize
    the underexplored task of translating saliency maps into nat
  track: 1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Modern systems for multi-hop question answering (QA) typically break questions
    into a sequence of reasoning steps, termed chain-of-thought (CoT), before arriving
    at a final answer. Often, multiple chains are sampled and aggregated through a
    voting mechanism over the final answers, but the intermediate steps themselves
    are discarded. While such approaches improve performance, they do not consider
    the relations between intermediate steps across chains and do not provide a unified
    explanation for the predicted answer.We introduce Multi-Chain Reasoning (MCR),
    an approach which prompts large language models to meta-reason over multiple chains
    of thought, rather than aggregating their answers. MCR examines different reasoning
    chains, mixes information between them and selects the most relevant facts in
    generating an explanation and predicting the answer. MCR outperforms strong baselines
    on 7 multi-hop QA datasets.Moreover, our analysis reveals that MCR explanations
    exhibit high quality, enabling humans to verify its answers.
  authors:
  - Ori Yoran
  - Tomer Wolfson
  - Ben Bogin
  - Uri Katz
  - Daniel Deutch
  - Jonathan Berant
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLRSE
  forum: ''
  id: ACL_51
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Answering Questions by Meta-Reasoning over Multiple Chains of Thought
  tldr: Modern systems for multi-hop question answering (QA) typically break questions
    into a sequence of reasoning steps, termed chain-of-thought (CoT), before arriving
    at a final answer. Often, multiple chains are sampled and aggregated through a
    voting mechanism over the final answers, but the intermedia
  track: 1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'We develop a symbolic planning-based decoder to improve the few-shot
    semantic parsing of instructional texts. The system takes long-form instructional
    texts as input and produces sequences of actions in a formal language that enable
    execution of the instructions. This task poses unique challenges since input texts
    may contain long context dependencies and ambiguous and domain-specific language.
    Valid semantic parses also require sequences of steps that constitute an executable
    plan. We build on recent progress in semantic parsing by leveraging large language
    models to learn parsers from small amounts of training data. During decoding,
    our method employs planning methods and domain information to rank and correct
    candidate parses. To validate our method, we evaluate on four domains: two household
    instruction-following domains and two cooking recipe interpretation domains. We
    present results for few-shot semantic parsing using leave-one-out cross-validation.
    We show that utilizing planning domain information improves the quality of generated
    plans. Through ablations we also explore the effects of our decoder design choices.'
  authors:
  - Vanya Cohen
  - Raymond Mooney
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLRSE
  forum: ''
  id: ACL_52
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Using Planning to Improve Semantic Parsing of Instructional Texts
  tldr: We develop a symbolic planning-based decoder to improve the few-shot semantic
    parsing of instructional texts. The system takes long-form instructional texts
    as input and produces sequences of actions in a formal language that enable execution
    of the instructions. This task poses unique challenges si
  track: 1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In this paper, we propose PINTO, an LM pipeline that rationalizes via
    prompt-based learning, and learns to faithfully reason over rationales via counterfactual
    regularization. First, PINTO maps out a suitable reasoning process for the task
    input by prompting a frozen rationalizing LM to generate a free-text rationale.
    Second, PINTO's reasoning LM is fine-tuned to solve the task using the generated
    rationale as context, while regularized to output less confident predictions when
    the rationale is perturbed. Across four datasets, we show that PINTO significantly
    improves the generalization ability of the reasoning LM, yielding higher performance
    on both in-distribution and out-of-distribution test sets. Also, we find that
    PINTO"s rationales are more faithful to its task predictions than those generated
    by competitive baselines.
  authors:
  - Peifeng Wang
  - Aaron Chan
  - Filip Ilievski
  - Muhao Chen
  - Xiang Ren
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLRSE
  forum: ''
  id: ACL_53
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'PINTO: Faithful Language Reasoning Using Prompt-Generated Rationales'
  tldr: In this paper, we propose PINTO, an LM pipeline that rationalizes via prompt-based
    learning, and learns to faithfully reason over rationales via counterfactual regularization.
    First, PINTO maps out a suitable reasoning process for the task input by prompting
    a frozen rationalizing LM to generate a f
  track: 1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The relationship between communicated language and intended meaning is
    often probabilistic and sensitive to context. Numerous strategies attempt to estimate
    such a mapping, often leveraging recursive Bayesian models of communication. In
    parallel, large language models (LLMs) have been increasingly applied to semantic
    parsing applications, tasked with inferring logical representations from natural
    language. While existing LLM explorations have been largely restricted to literal
    language use, in this work, we evaluate the capacity of LLMs to infer the meanings
    of pragmatic utterances. Specifically, we explore the case of threshold estimation
    on the gradable adjective "strong", contextually conditioned on a strength prior,
    then extended to composition with qualification, negation, polarity inversion,
    and class comparison. We find that LLMs can derive context-grounded, human-like
    distributions over the interpretations of several complex pragmatic utterances,
    yet struggle composing with negation. These results inform the inferential capacity
    of statistical language models, and their use in pragmatic and semantic parsing
    applications. All corresponding code is made publicly available.
  authors:
  - Benjamin Lipkin
  - Lionel Wong
  - Gabriel Grand
  - Josh Tenenbaum
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLRSE
  forum: ''
  id: ACL_54
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Evaluating statistical language models as pragmatic reasoners
  tldr: The relationship between communicated language and intended meaning is often
    probabilistic and sensitive to context. Numerous strategies attempt to estimate
    such a mapping, often leveraging recursive Bayesian models of communication. In
    parallel, large language models (LLMs) have been increasingly a
  track: 1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Script learning is the study of how daily events unfold. Previous works
    tend to consider scripts as a linear sequence of events while ignoring the potential
    branches that arise due to people's subjective choices. We proposed Choice-75,
    the first benchmark that challenges intelligent systems to make daily decisions
    given descriptive scenarios. It consists of 75 samples of goal-options and more
    than 600 descriptive scenarios. While SoTA large language models (LLM) demonstrate
    overall decent performances, there is still notable headroom for them in many
    of the hard scenarios.
  authors:
  - Zhaoyi Hou
  - Li Zhang
  - Chris Callison-burch
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLRSE
  forum: ''
  id: ACL_55
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Choice-75: A Dataset on Decision Branching in Script Learning'
  tldr: 'Script learning is the study of how daily events unfold. Previous works tend
    to consider scripts as a linear sequence of events while ignoring the potential
    branches that arise due to people''s subjective choices. We proposed Choice-75,
    the first benchmark that challenges intelligent systems to make '
  track: 1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: A popular approach for improving the correctness of output from large
    language models (LLMs) is Self-Consistency  poll the LLM multiple times and output
    the most frequent solution. Existing Self-Consistency techniques always draw a
    constant number of samples per question, where a better approach will be to non-uniformly
    distribute the available budget, based on the amount of agreement in the samples
    drawn so far. In response, we introduce Adaptive-Consistency, a cost-efficient,
    model-agnostic technique that dynamically adjusts the number of samples per question
    using a lightweight stopping criterion. Our experiments over 13 datasets and two
    LLMs demonstrate that Adaptive-Consistency reduces sample budget by up to 6.0
    times with an average accuracy drop of less than 0.1\%
  authors:
  - Pranjal Aggarwal
  - Aman Madaan
  - Yiming Yang
  - Mausam -
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLRSE
  forum: ''
  id: ACL_56
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Let''s Sample Step-by-Step: Adaptive-Consistency for Efficient Reasoning
    with LLMs'
  tldr: A popular approach for improving the correctness of output from large language
    models (LLMs) is Self-Consistency  poll the LLM multiple times and output the
    most frequent solution. Existing Self-Consistency techniques always draw a constant
    number of samples per question, where a better approach wil
  track: 1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Multi-hop Question Generation is the task of generating questions which
    require the reader to reason over and combine information spread across multiple
    passages employing several reasoning steps. Chain-of-thought rationale generation
    has been shown to improve performance on multi-step reasoning tasks and make model
    predictions more interpretable. However, few-shot performance gains from including
    rationales have been largely observed only in +100B language models, and otherwise
    require large-scale manual rationale annotation. In this paper, we introduce a
    new framework for applying chain-of-thought inspired structured rationale generation
    to multi-hop question generation under a very low supervision regime (8- to 128-shot).
    We propose to annotate a small number of examples following our proposed multi-step
    rationale schema, treating each reasoning step as a separate task to be performed
    by a generative language model. We show that our framework leads to improved control
    over the difficulty of the generated questions and better performance compared
    to baselines trained without rationales, both on automatic evaluation metrics
    and in human evaluation. Importantly, we show that this is achievable with a modest
    model size.
  authors:
  - Saurabh Kulshreshtha
  - Anna Rumshisky
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLRSE
  forum: ''
  id: ACL_57
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Reasoning Circuits: Few-shot Multi-hop Question Generation with Structured
    Rationales'
  tldr: Multi-hop Question Generation is the task of generating questions which require
    the reader to reason over and combine information spread across multiple passages
    employing several reasoning steps. Chain-of-thought rationale generation has been
    shown to improve performance on multi-step reasoning tas
  track: 1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Visual metaphors are powerful rhetorical devices used to persuade or
    communicate creative ideas through images. Similar to linguistic metaphors, they
    convey meaning implicitly through symbolism and juxtaposition of the symbols.
    We propose a new task of generating visual metaphors from linguistic metaphors.
    This is a challenging task for diffusion-based text-to-image models, such as DALL-E-2,
    since it requires the ability to model implicit meaning and compositionality.
    We propose to solve the task through the collaboration between Large Language
    Models and Diffusion Models. We use GPT-3 with Chain-of-Thought prompting to generate
    text that represents a visual elaboration of the linguistic metaphor, containing
    the implicit meaning and relevant objects, which is then used as input to the
    diffusion-based text-to-image models. Using a human-AI collaboration framework,
    where humans interact both with the LLM and the top-performing diffusion model,
    we create a high-quality dataset containing 6,476 visual metaphors. Evaluation
    by professional illustrators show the promise of LLM-Diffusion Model collaboration
    for this task. We also perform an intrinsic and an extrinsic evaluation using
    a downstream task: visual entailment. Fine-tuning a state-of-the-art vision-language
    model on our dataset leads to 23-point improvement in accuracy compared to its
    performance when finetuned on SNLI-VE, a large-scale visual entailment dataset.'
  authors:
  - Tuhin Chakrabarty
  - Arkadiy Saakyan
  - Olivia Winn
  - Artemis Panagopoulou
  - Yue Yang
  - Marianna Apidianaki
  - Smaranda Muresan
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLRSE
  forum: ''
  id: ACL_58
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'I Spy a Metaphor: Large Language Models and Diffusion Models Co-Create Visual
    Metaphors'
  tldr: Visual metaphors are powerful rhetorical devices used to persuade or communicate
    creative ideas through images. Similar to linguistic metaphors, they convey meaning
    implicitly through symbolism and juxtaposition of the symbols. We propose a new
    task of generating visual metaphors from linguistic met
  track: 1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Larger language models, such as GPT-3, have shown to be excellent in
    many tasks. However, we demonstrate that out-of-ordinary questions can throw the
    model off guard. This work focuses on finding answers to negated complementary
    questions in commonsense scenarios. We illustrate how such questions adversely
    affect the model responses. We propose a model-agnostic methodology to improve
    the performance in negated complementary scenarios. Our method outperforms few-shot
    generation from GPT-3 (by more than 11 points) and, more importantly, highlights
    the significance of studying the response of large language models in negated
    complementary questions. The code, data, and experiments are available under:
    https://github.com/navidre/negated\_complementary\_commonsense.'
  authors:
  - Navid Rezaei
  - Marek Reformat
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLRSE
  forum: ''
  id: ACL_59
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Negated Complementary Commonsense using Large Language Models
  tldr: Larger language models, such as GPT-3, have shown to be excellent in many
    tasks. However, we demonstrate that out-of-ordinary questions can throw the model
    off guard. This work focuses on finding answers to negated complementary questions
    in commonsense scenarios. We illustrate how such questions ad
  track: 1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In this paper, we investigate whether symbolic semantic representations,
    extracted from deep semantic parsers, can help to reason over the states of involved
    entities in a procedural text. We consider a deep semantic parser\textasciitilde{}(TRIPS)
    and semantic role labeling as two sources of semantic parsing knowledge. First,
    we propose PROPOLIS, a symbolic parsing-based procedural reasoning framework.Second,
    we integrate semantic parsing information into state-of-the-art neural models
    to conduct procedural reasoning.Our experiments indicate that explicitly incorporating
    such semantic knowledge improves procedural understanding. This paper presents
    new metrics for evaluating procedural reasoning tasks that clarify the challenges
    and identify differences among neural, symbolic, and integrated models.
  authors:
  - Hossein Rajaby Faghihi
  - Parisa Kordjamshidi
  - Choh Man Teng
  - James Allen
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLRSE
  forum: ''
  id: ACL_61
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: The Role of Semantic Parsing in Understanding Procedural Text
  tldr: In this paper, we investigate whether symbolic semantic representations, extracted
    from deep semantic parsers, can help to reason over the states of involved entities
    in a procedural text. We consider a deep semantic parser\textasciitilde{}(TRIPS)
    and semantic role labeling as two sources of semanti
  track: 1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We study the problem of generating coherent and correct intermediate solution
    steps for math word problems (MWPs). Solutions to MWPs with step-by-step explanations
    are valuable, especially in education, to help students better comprehend problem-solving
    strategies. Most existing approaches narrowly focus on obtaining the final correct
    answer. A few recent approaches leverage intermediate solution steps to improve
    final answer correctness but often cannot generate coherent steps with a clear
    solution strategy. Contrary to existing work, we focus on improving the correctness
    and coherence of the intermediate solutions steps. We propose a step-by-step planning
    method for intermediate solution generation, which strategically plans the generation
    of the next solution step based on the MWP and the previous solution steps. Our
    approach first plans the next step by predicting the necessary math operation
    needed to proceed given history steps, then generates the next step, token-by-token,
    by prompting a language model with the predicted math operation. Experiments on
    the GSM8K dataset demonstrate that our method improves the accuracy and interpretability
    of the solution by both automatic metrics and human evaluation.
  authors:
  - Mengxue Zhang
  - Zichao Wang
  - Zhichao Yang
  - Weiqi Feng
  - Andrew Lan
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLRSE
  forum: ''
  id: ACL_62
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Interpretable Math Word Problem Solution Generation Via Step-by-step Planning
  tldr: 'We study the problem of generating coherent and correct intermediate solution
    steps for math word problems (MWPs). Solutions to MWPs with step-by-step explanations
    are valuable, especially in education, to help students better comprehend problem-solving
    strategies. Most existing approaches narrowly '
  track: 1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Large language models (LMs) beyond a certain scale, demonstrate the emergent
    capability of generating free-text rationales for their predictions via chain-of-thought
    (CoT) prompting. While CoT can yield dramatically improved performance, such gains
    are only observed for sufficiently large LMs. Even more concerning, there is little
    guarantee that the generated rationales are consistent with LM's predictions or
    faithfully justify the decisions. In this work, we propose a faithful knowledge
    distillation method to learn a small, self-consistent CoT model from a teacher
    model that is orders of magnitude larger. To form better supervision, we elicit
    rationales supporting the gold answers from a large LM (teacher) by contrastive
    decoding, which encourages the teacher to generate tokens that become more plausible
    only when the answer is considered. To ensure faithful distillation, we use the
    teacher-generated rationales to learn a student LM with a counterfactual reasoning
    objective, which prevents the student from ignoring the rationales to make inconsistent
    predictions. Experiments show that while yielding comparable performance,  our
    method leads to a more faithful model than baselines. Further analysis shows that
    such a model respects the rationales more when making decisions; thus, we can
    improve its performance more by refining its rationales.
  authors:
  - Peifeng Wang
  - Zhengyang Wang
  - Zheng Li
  - Yifan Gao
  - Bing Yin
  - Xiang Ren
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLRSE
  forum: ''
  id: ACL_65
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'SCOTT: Self-Consistent Chain-of-Thought Distillation'
  tldr: 'Large language models (LMs) beyond a certain scale, demonstrate the emergent
    capability of generating free-text rationales for their predictions via chain-of-thought
    (CoT) prompting. While CoT can yield dramatically improved performance, such gains
    are only observed for sufficiently large LMs. Even '
  track: 1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Prompting has been utilized to exploit large language models (LLM) for
    sequential planning tasks within interactive settings. In this paper, we propose
    a novel prompting approach, Actor-Summarizer-Hierarchical prompting, for interactive
    web navigation. Diverging from previous prompting approaches that always put the
    full state (eg a web page) to the prompt, we propose to first construct an action-aware
    state which is more condensed and relevant with a dedicated summarizer prompt.
    The resulting state is concatenated to the summarized history and fed to an actor
    prompt to predict the next action. This hierarchical mechanism is especially useful
    since the full state of a step in web navigation often contains redundant and
    irrelevant information. Our approach outperforms the previous state-of-the-art
    prompting mechanism with the same LLM by 6.2\% on task success rate, demonstrating
    its potential on interactive decision making tasks with long observation traces.
  authors:
  - Chi-fan Lo
  - Abishek Sridhar
  - Hao Zhu
  - Frank F. Xu
  - Shuyan Zhou
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLRSE
  forum: ''
  id: ACL_66
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Hierarchical Prompting Assists Large Language Model on Web Navigation
  tldr: Prompting has been utilized to exploit large language models (LLM) for sequential
    planning tasks within interactive settings. In this paper, we propose a novel
    prompting approach, Actor-Summarizer-Hierarchical prompting, for interactive web
    navigation. Diverging from previous prompting approaches th
  track: 1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Reasoning is a fundamental aspect of human intelligence that plays a crucial
    role in activities such as problem solving, decision making, and critical thinking.
    In recent years, large language models (LLMs) have made significant progress in
    natural language processing, and there is observation that these models may exhibit
    reasoning abilities when they are sufficiently large. However, it is not yet clear
    to what extent LLMs are capable of reasoning. This paper provides a comprehensive
    overview of the current state of knowledge on reasoning in LLMs, including techniques
    for improving and eliciting reasoning in these models, methods and benchmarks
    for evaluating reasoning abilities, findings and implications of previous research
    in this field, and suggestions on future directions. Our aim is to provide a detailed
    and up-to-date review of this topic and stimulate meaningful discussion and future
    work.
  authors:
  - Jie Huang
  - Kevin Chen-chuan Chang
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLRSE
  forum: ''
  id: ACL_67
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Towards Reasoning in Large Language Models: Survey, Implication, and Reflection'
  tldr: 'Reasoning is a fundamental aspect of human intelligence that plays a crucial
    role in activities such as problem solving, decision making, and critical thinking.
    In recent years, large language models (LLMs) have made significant progress in
    natural language processing, and there is observation that '
  track: 1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Users' physical safety is an increasing concern as the market for intelligent
    systems continues to grow, where unconstrained systems may recommend users dangerous
    actions that can lead to serious injury. Covertly unsafe text is an area of particular
    interest, as such texts may arise from everyday scenarios and are challenging
    to detect as harmful. We propose FARM, a novel framework that leverages external
    knowledge for trustworthy rationale generation in the context of safety. In particular,
    FARM foveates on missing knowledge to qualify the information required to reason
    in specific scenarios and retrieves this information with attribution to trustworthy
    sources. It then uses this knowledge to both classify the safety of the original
    text and generate human-interpretable rationales, shedding light on the risk of
    systems to specific user groups and helping both stakeholders manage the risks
    of their systems and policymakers to provide concrete safeguards for consumer
    safety. Our experiments show that FARM obtains state-of-the-art results on the
    SafeText dataset, showing absolute improvement in safety classification accuracy
    by 5.9 points.
  authors:
  - Alex Mei
  - Sharon Levy
  - William Yang Wang
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLRSE
  forum: ''
  id: ACL_68
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Foveate, Attribute, and Rationalize: Towards Physically Safe and Trustworthy
    AI'
  tldr: Users' physical safety is an increasing concern as the market for intelligent
    systems continues to grow, where unconstrained systems may recommend users dangerous
    actions that can lead to serious injury. Covertly unsafe text is an area of particular
    interest, as such texts may arise from everyday sc
  track: 1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Large language models (LLMs) have achieved impressive performance on code
    generation. However, for complex programming tasks, generating the correct solution
    in one go becomes challenging. In this work, we propose self-debugging, which
    teaches a large language model to debug its predicted program via few-shot demonstrations.
    In particular, we demonstrate that self-debugging can teach the large language
    model to perform rubber duck debugging; i.e., without any feedback on the code
    correctness or error messages, the model is able to identify its mistakes by explaining
    the generated code in natural language. Self-debugging achieves the state-of-the-art
    performance on several code generation benchmarks, including the Spider dataset
    for text-to-SQL generation, TransCoder for C++-to-Python translation, and MBPP
    for text-to-Python generation. On the Spider benchmark where there are no unit
    tests to verify the correctness of predictions, self-debugging with code explanation
    consistently improves the baseline by 2-3\%, and improves the prediction accuracy
    on problems of the hardest label by 9\%\$. On TransCoder and MBPP where unit tests
    are available, self-debugging can improve the baseline accuracy by 12\%. Meanwhile,
    by leveraging feedback messages and reusing failed predictions, self-debugging
    notably improves sample efficiency, and can match or outperform baseline models
    that generate more than 10x candidate programs.
  authors:
  - Xinyun Chen
  - Maxwell Lin
  - Nathanael Schaerli
  - Denny Zhou
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLRSE
  forum: ''
  id: ACL_71
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Teaching Large Language Models to Self-Debug
  tldr: Large language models (LLMs) have achieved impressive performance on code
    generation. However, for complex programming tasks, generating the correct solution
    in one go becomes challenging. In this work, we propose self-debugging, which
    teaches a large language model to debug its predicted program vi
  track: 1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Existing benchmarks for open-domain question answering (ODQA) typically
    focus on questions whose answers are all in a single paragraph. By contrast, many
    natural questions, such as "What players were drafted by the Brooklyn Nets? have
    a long list of answers extracted from multiple paragraphs. Answering such questions
    requires retrieving and reading many passages from a large corpus. We introduce
    QAMPARI, an ODQA benchmark, where answers are lists of entities, spread across
    many paragraphs. We created QAMPARI by (a) generating questions with multiple
    answers from Wikipedia's knowledge graph and tables, (b) automatically pairing
    answers with supporting evidence in Wikipedia paragraphs, and (c) manually paraphrasing
    questions and validating each answer. Across a wide range of ODQA models, we find
    that QAMPARI is challenging in terms of both passage retrieval and answer generation,
    with models reaching an F1 score of 32.8 at best. We view QAMPARI as a valuable
    resource for ODQA research, which will aid to develop models that handle a broad
    range of question types, including single and multianswer questions.
  authors:
  - Samuel Amouyal
  - Tomer Wolfson
  - Ohad Rubin
  - Ori Yoran
  - Jonathan Herzig
  - Jonathan Berant
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLRSE
  forum: ''
  id: ACL_72
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'QAMPARI: A Benchmark for Open-domain Questions with Many Answers'
  tldr: Existing benchmarks for open-domain question answering (ODQA) typically focus
    on questions whose answers are all in a single paragraph. By contrast, many natural
    questions, such as "What players were drafted by the Brooklyn Nets? have a long
    list of answers extracted from multiple paragraphs. Answer
  track: 1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Large Language Models (LLMs) are capable of performing zero-shot closed-book
    question answering tasks, based on their internal knowledge stored in parameters
    during pre-training. However, such internalized knowledge might be insufficient
    and incorrect, which could lead LLMs to generate factually wrong answers. Furthermore,
    fine-tuning LLMs to update their knowledge is expensive. To this end, we propose
    to augment the knowledge directly in the input of LLMs. Specifically, we first
    retrieve the relevant facts to the input question from the knowledge graph based
    on semantic similarities between the question and its associated facts. After
    that, we prepend the retrieved facts to the input question in the form of the
    prompt, which is then forwarded to LLMs to generate the answer. Our framework,
    Knowledge-Augmented language model PromptING (KAPING), requires no model training,
    thus completely zero-shot. We validate the performance of our KAPING framework
    on the knowledge graph question answering task, that aims to answer the user's
    question based on facts over a knowledge graph, on which ours outperforms relevant
    zero-shot baselines by up to 48\% in average, across multiple LLMs of various
    sizes.
  authors:
  - Jinheon Baek
  - Alham Fikri Aji
  - Amir Saffari
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLRSE
  forum: ''
  id: ACL_73
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph
    Question Answering
  tldr: Large Language Models (LLMs) are capable of performing zero-shot closed-book
    question answering tasks, based on their internal knowledge stored in parameters
    during pre-training. However, such internalized knowledge might be insufficient
    and incorrect, which could lead LLMs to generate factually wro
  track: 1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: A common practice for text retrieval is to use an encoder to map the documents
    and the query to a common vector space and perform a nearest neighbor search (NNS);
    multi-hop retrieval also often adopts the same paradigm, usually with a modification
    of iteratively reformulating the query vector so that it can retrieve different
    documents at each hop. However, such a bi-encoder approach has limitations in
    multi-hop settings; (1) the reformulated query gets longer as the number of hops
    increases, which further tightens the embedding bottleneck of the query vector,
    and (2) it is prone to error propagation. In this paper, we focus on alleviating
    these limitations in multi-hop settings by formulating the problem in a fully
    generative way. We propose an encoder-decoder model that performs multi-hop retrieval
    by simply generating the entire text sequences of the retrieval targets, which
    means the query and the documents interact in the language model's parametric
    space rather than L2 or inner product space as in the bi-encoder approach. Our
    approach, Generative Multi-hop Retrieval (GMR), consistently achieves comparable
    or higher performance than bi-encoder models in five datasets while demonstrating
    superior GPU memory and storage footprint.
  authors:
  - Hyunji Lee
  - Sohee Yang
  - Hanseok Oh
  - Minjoon Seo
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLRSE
  forum: ''
  id: ACL_74
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Generative Multi-hop Retrieval
  tldr: A common practice for text retrieval is to use an encoder to map the documents
    and the query to a common vector space and perform a nearest neighbor search (NNS);
    multi-hop retrieval also often adopts the same paradigm, usually with a modification
    of iteratively reformulating the query vector so tha
  track: 1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Entities and events are crucial to natural language reasoning and common
    in procedural texts. Existing work has focused either exclusively on entity state
    tracking (e.g., whether a pan is hot) or on event reasoning (e.g., whether one
    would burn themselves by touching the pan), while these two tasks are often causally
    related. We propose CREPE, the first benchmark on causal reasoning of event plausibility
    and entity states. We show that most language models, including GPT-3, perform
    close to chance at .35 F1, lagging far behind human at .87 F1. We boost model
    performance to .59 F1 by creatively representing events as programming languages
    while prompting language models pretrained on code. By injecting the causal relations
    between entities and events as intermediate reasoning steps in our representation,
    we further boost the performance to .67 F1. Our findings indicate not only the
    challenge that CREPE brings for language models, but also the efficacy of code-like
    prompting combined with chain-of-thought prompting for multihop event reasoning.
  authors:
  - Li Zhang
  - Hainiu Xu
  - Yue Yang
  - Shuyan Zhou
  - Weiqiu You
  - Manni Arora
  - Chris Callison-burch
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLRSE
  forum: ''
  id: ACL_75
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Causal Reasoning of Entities and Events in Procedural Texts
  tldr: 'Entities and events are crucial to natural language reasoning and common
    in procedural texts. Existing work has focused either exclusively on entity state
    tracking (e.g., whether a pan is hot) or on event reasoning (e.g., whether one
    would burn themselves by touching the pan), while these two tasks '
  track: 1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Large language models show an emergent ability to learn a new task from
    a small number of input-output demonstrations.However, recent work shows that
    in-context learners largely rely on their pre-trained knowledge, such as the sentiment
    of the labels, instead of finding new associations in the input.However, the commonly-used
    few-shot evaluation settings using a random selection of in-context demonstrations
    can not disentangle models' ability to learn a new skill from demonstrations,
    as most of the randomly-selected demonstrations do not present relations informative
    for prediction beyond exposing the new task distribution.To disentangle models'
    in-context learning ability independent of models' memory, we introduce a Conceptual
    few-shot learning method selecting the demonstrations sharing a possibly-informative
    concept with the predicted sample. We extract a set of such concepts from annotated
    explanations and measure how much can models benefit from presenting these concepts
    in few-shot demonstrations.We find that smaller models are more sensitive to the
    presented concepts. While some of the models are able to benefit from concept-presenting
    demonstrations for each assessed concept, we find that none of the assessed in-context
    learners can benefit from all presented reasoning concepts consistently, leaving
    the in-context concept learning an open challenge.
  authors:
  - Michal Tefnik
  - Marek Kadlcik
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLRSE
  forum: ''
  id: ACL_76
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Can In-context Learners Learn a Reasoning Concept from Demonstrations?
  tldr: Large language models show an emergent ability to learn a new task from a
    small number of input-output demonstrations.However, recent work shows that in-context
    learners largely rely on their pre-trained knowledge, such as the sentiment of
    the labels, instead of finding new associations in the input
  track: 1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: When people answer questions about a specific situation, e.g., "I cheated
    on my mid-term exam last week. Was that wrong?, cognitive science suggests that
    they form a mental picture of that situation before answering. While we do not
    know how language models (LMs) answer such questions, we conjecture that they
    may answer more accurately if they are also provided with additional details about
    the question situation, elaborating the "scene. To test this conjecture, we train
    a new model, DREAM, to answer questions that elaborate the scenes that situated
    questions are about, and then provide those elaborations as additional context
    to a question-answering (QA) model. We find that DREAM is able to create better
    scene elaborations (more accurate, useful, and consistent) than a representative
    state-of-the-art, zero-shot model (Macaw). We also find that using the scene elaborations
    as additional context improves the answer accuracy of a downstream QA system,
    including beyond that obtainable by simply further fine-tuning the QA system on
    DREAM's training data. These results suggest that adding focused elaborations
    about a situation can improve a system's reasoning about it, and may serve as
    an effective way of injecting new scenario-based knowledge into QA models.
  authors:
  - Yuling Gu
  - Bhavana Dalvi Mishra
  - Peter Clark
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLRSE
  forum: ''
  id: ACL_77
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'DREAM: Improving Situational QA by First Elaborating the Situation'
  tldr: When people answer questions about a specific situation, e.g., "I cheated
    on my mid-term exam last week. Was that wrong?, cognitive science suggests that
    they form a mental picture of that situation before answering. While we do not
    know how language models (LMs) answer such questions, we conjecture
  track: 1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Deploying large language models (LLMs) is challenging because they are
    memory inefficient and compute-intensive for practical applications. In reaction,
    researchers train smaller task-specific models by either finetuning with human
    labels or distilling using LLM-generated labels. However, finetuning and distillation
    require large amounts of training data to achieve comparable performance to LLMs.
    We introduce Distilling step-by-step, a new mechanism that (a) trains smaller
    models that outperform LLMs, and (b) achieves so by leveraging less training data
    needed by finetuning or distillation. Our method extracts LLM rationales as additional
    supervision for small models within a multi-task training framework. We present
    three findings across 4 NLP benchmarks: First, compared to both finetuning and
    distillation, our mechanism achieves better performance with much fewer labeled/unlabeled
    training examples. Second, compared to LLMs, we achieve better performance using
    substantially smaller model sizes. Third, we reduce both the model size and the
    amount of data required to outperform LLMs; our 770M T5 model outperforms the
    540B PaLM model using only 80\% of available data on a benchmark task.'
  authors:
  - Cheng-yu Hsieh
  - Chun-liang Li
  - Chih-kuan Yeh
  - Hootan Nakhost
  - Yasuhisa Fujii
  - Alex Ratner
  - Ranjay Krishna
  - Chen-yu Lee
  - Tomas Pfister
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLRSE
  forum: ''
  id: ACL_78
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Distilling Step-by-Step! Outperforming Larger Language Models with Less Training
    Data and Smaller Model Sizes
  tldr: Deploying large language models (LLMs) is challenging because they are memory
    inefficient and compute-intensive for practical applications. In reaction, researchers
    train smaller task-specific models by either finetuning with human labels or distilling
    using LLM-generated labels. However, finetuning
  track: 1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Argumentation is an important means of communication. For describing especially
    arguments about consequences, the notion of effect relations has been introduced
    recently. We propose a method to extract effect relations from large text resources
    and apply it on encyclopedic and argumentative texts. By connecting the extracted
    relations, we generate a knowledge graph which we call effect graph. For evaluating
    the effect graph, we perform crowd and expert annotations and create a novel dataset.
    We demonstrate a possible use case of the effect graph by proposing a method for
    explaining arguments from consequences.
  authors:
  - Jonathan Kobbe
  - Ioana Hulpu
  - Heiner Stuckenschmidt
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLRSE
  forum: ''
  id: ACL_79
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Effect Graph: Effect Relation Extraction for Explanation Generation'
  tldr: Argumentation is an important means of communication. For describing especially
    arguments about consequences, the notion of effect relations has been introduced
    recently. We propose a method to extract effect relations from large text resources
    and apply it on encyclopedic and argumentative texts. B
  track: 1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'In this paper, we approach competitive-level programming problem-solving
    as a composite task of reasoning and code generation. We propose a novel method
    to automatically annotate natural language explanations to the \textless{}problem,
    solution\textgreater{} pairs. We show that despite poor performance in solving
    competitive-level programming problems, state-of-the-art LLMs exhibit a strong
    capacity in describing and explaining their solutions. Our explanation generation
    methodology can generate a structured solution explanation for the problem while
    containing the description and analysis. To evaluate the quality of the annotated
    explanations, we examine their effectiveness in two aspects: 1) satisfying the
    human programming expert who authored the oracle solution, and 2) aiding LLMs
    in solving problems more effectively. The experimental results on the CodeContests
    dataset demonstrate that while LLM GPT3.5''s and GPT-4''s abilities in describing
    the solution are comparable, GPT-4 shows a better understanding of the key idea
    behind the solution.'
  authors:
  - Jierui Li
  - Szymon Tworkowski
  - Yingying Wu
  - Raymond Mooney
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLRSE
  forum: ''
  id: ACL_80
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Explaining Competitive-Level Programming Solutions using LLMs
  tldr: In this paper, we approach competitive-level programming problem-solving as
    a composite task of reasoning and code generation. We propose a novel method to
    automatically annotate natural language explanations to the \textless{}problem,
    solution\textgreater{} pairs. We show that despite poor performa
  track: 1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'We conduct a thorough investigation into the reasoning capabilities of
    Large Language Models (LLMs), focusing specifically on the Open Pretrained Transformers
    (OPT) models as a representative of such models. Our study entails finetuning
    three different sizes of OPT on a carefully curated reasoning corpus, resulting
    in two sets of finetuned models: OPT-R, finetuned without explanations, and OPT-RE,
    finetuned with explanations. We then evaluate all models on 57 out-of-domain tasks
    drawn from the Super-NaturalInstructions benchmark, covering 26 distinct reasoning
    skills, utilizing three prompting techniques. Through a comprehensive grid of
    27 configurations and 6,156 test evaluations, we investigate the dimensions of
    finetuning, prompting, and scale to understand the role of explanations on different
    reasoning skills. Our findings reveal that having explanations in the fewshot
    exemplar has no significant impact on the model''s performance when the model
    is finetuned, while positively affecting the non-finetuned counterpart. Moreover,
    we observe a slight yet consistent increase in classification accuracy as we incorporate
    explanations during prompting and finetuning, respectively. Finally, we offer
    insights on which reasoning skills benefit the most from incorporating explanations
    during finetuning and prompting, such as Numerical (+20.4\%) and Analogical (+13.9\%)
    reasoning, as well as skills that exhibit negligible or negative effects.'
  authors:
  - Badr Alkhamissi
  - Siddharth Verma
  - Ping Yu
  - Zhijing Jin
  - Asli Celikyilmaz
  - Mona Diab
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLRSE
  forum: ''
  id: ACL_82
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'OPT-R: Exploring the Role of Explanations in Finetuning and Prompting for
    Reasoning Skills of Large Language Models'
  tldr: We conduct a thorough investigation into the reasoning capabilities of Large
    Language Models (LLMs), focusing specifically on the Open Pretrained Transformers
    (OPT) models as a representative of such models. Our study entails finetuning
    three different sizes of OPT on a carefully curated reasoning c
  track: 1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Large decoder-only language models (LMs) can be largely improved in terms
    of perplexity by retrieval (e.g., RETRO), but its impact on text generation quality
    and downstream task accuracy is unclear. Thus, it is still an open question: shall
    we pretrain large autoregressive LMs with retrieval? To answer it, we perform
    a comprehensive study on a scalable pretrained retrieval-augmented LM (i.e., RETRO)
    compared with standard GPT and retrieval-augmented GPT incorporated at fine-tuning
    or inference stages. We first provide the recipe to reproduce RETRO up to 9.5B
    parameters while retrieving a text corpus with 330B tokens. Based on that, we
    have the following novel findings: i) RETRO outperforms GPT on text generation
    with much less degeneration (i.e., repetition), moderately higher factual accuracy,
    and slightly lower toxicity with a nontoxic retrieval database. ii) On the LM
    Evaluation Harness benchmark, R ETRO largely outperforms GPT on knowledge-intensive
    tasks, but is on par with GPT on other tasks. Furthermore, we introduce a simple
    variant of the model, RETRO ++, which largely improves the open-domain QA results
    of the original RETRO and significantly outperforms retrieval-augmented GPT across
    different model sizes. Our findings highlight the promising direction of pretraining
    autoregressive LMs with retrieval as future foundation models.'
  authors:
  - Boxin Wang
  - Wei Ping
  - Peng Xu
  - Lawrence Mcafee
  - Zihan Liu
  - Mohammad Shoeybi
  - Yi Dong
  - Oleksii Kuchaiev
  - Bo Li
  - Chaowei Xiao
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLRSE
  forum: ''
  id: ACL_83
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive
    Study
  tldr: 'Large decoder-only language models (LMs) can be largely improved in terms
    of perplexity by retrieval (e.g., RETRO), but its impact on text generation quality
    and downstream task accuracy is unclear. Thus, it is still an open question: shall
    we pretrain large autoregressive LMs with retrieval? To ans'
  track: 1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Human language richly invokes our intuitive physical knowledge. We talk
    about physical objects, scenes, properties, and events; and we can make predictions
    and draw inferences about physical worlds described entirely in language. Understanding
    this everyday language requires inherently probabilistic reasoningover possible
    physical worlds invoked in language and over uncertainty inherent to those physical
    worlds. In this paper, we propose PiLoT, a neurosymbolic generative model that
    translates language into probabilistic programs grounded in a physics engine.
    Our model integrates a large language model to robustly parse language into program
    expressions and uses a probabilistic physics engine to support inferences over
    scenes described in language. We construct a linguistic reasoning benchmark based
    on prior psychophysics experiments that requires reasoning about physical outcomes
    based on linguistic scene descriptions. We show that PiLoT well predicts human
    judgments and outperforms baseline large language models across this battery of
    tasks.
  authors:
  - Cedegao Zhang
  - Lionel Wong
  - Gabriel Grand
  - Josh Tenenbaum
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLRSE
  forum: ''
  id: ACL_87
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Grounded physical language understanding with probabilistic programs and
    simulated worlds
  tldr: Human language richly invokes our intuitive physical knowledge. We talk about
    physical objects, scenes, properties, and events; and we can make predictions
    and draw inferences about physical worlds described entirely in language. Understanding
    this everyday language requires inherently probabilistic
  track: 1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Most benchmarks for question answering on knowledge bases (KBQA) operate
    with the i.i.d. assumption. Recently, the GrailQA dataset was established to evaluate  zero-shot
    generalization capabilities of KBQA models. Reasonable performance of current
    KBQA systems on the zero-shot GrailQA split hints that the field might be moving
    towards more generalizable systems. In this work, we observe a bias in the GrailQA
    dataset towards simpler one or two-hop questions which results in an inaccurate
    assessment of the aforementioned prowess. We propose GrailQA++, a challenging
    zero-shot KBQA test set that contains a larger number of questions relying on
    complex reasoning.  We leverage the concept of reasoning paths to control the
    complexity of the questions and to ensure that our proposed test set has a fair
    distribution of simple and complex questions. Evaluating existing KBQA models
    on this new test set shows that they suffer a substantial drop in performance
    as compared to the GrailQA zero-shot split. This highlights the non-generalizability
    of existing models and the necessity for harder benchmarks. Our analysis reveals
    how reasoning paths can be used to understand complementary strengths of different
    KBQA models, and provide a deeper insight into model mispredictions.
  authors:
  - Ritam Dutt
  - Sopan Khosla
  - Vinayshekhar Bannihatti Kumar
  - Rashmi Gangadharaiah
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLRSE
  forum: ''
  id: ACL_90
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Designing harder benchmarks for evaluating zero-shot generalizability in
    Question Answering over Knowledge Bases
  tldr: Most benchmarks for question answering on knowledge bases (KBQA) operate with
    the i.i.d. assumption. Recently, the GrailQA dataset was established to evaluate  zero-shot
    generalization capabilities of KBQA models. Reasonable performance of current
    KBQA systems on the zero-shot GrailQA split hints th
  track: 1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Current natural language systems designed for multi-step claim validation
    typically operate in two phases: retrieve a set of relevant premise statements
    using heuristics (planning), then generate novel conclusions from those statements
    using a large language model (deduction). The planning step often requires expensive
    Transformer operations and does not scale to arbitrary numbers of premise statements.
    In this paper, we investigate whether efficient planning heuristic is possible
    via embedding spaces compatible with deductive reasoning. Specifically, we evaluate
    whether embedding spaces exhibit a property we call deductive additivity: the
    sum of premise statement embeddings should be close to embeddings of conclusions
    based on those premises. We explore multiple sources of off-the-shelf dense embeddings
    in addition to fine-tuned embeddings from GPT3 and sparse embeddings from BM25.
    We study embedding models both intrinsically, evaluating whether the property
    of deductive additivity holds, and extrinsically, using them to assist planning
    in natural language proof generation. Lastly, we create a dataset, Single-Step
    Reasoning Contrast (SSRC), to further probe performance on various reasoning types.
    Our findings suggest that while standard embedding methods frequently embed conclusions
    near the sums of their premises, they fall short of being effective heuristics
    and lack the ability to model certain categories of reasoning.'
  authors:
  - Zayne Sprague
  - Kaj Bostrom
  - Swarat Chaudhuri
  - Greg Durrett
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLRSE
  forum: ''
  id: ACL_91
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Deductive Additivity for Planning of Natural Language Proofs
  tldr: 'Current natural language systems designed for multi-step claim validation
    typically operate in two phases: retrieve a set of relevant premise statements
    using heuristics (planning), then generate novel conclusions from those statements
    using a large language model (deduction). The planning step ofte'
  track: 1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The ease and speed of spreading misinformation and propaganda on the Web
    motivate the need to develop trustworthy technology for detecting fallacies in
    natural language arguments. However, state-of-the-art language modeling methods
    exhibit a lack of robustness on tasks like logical fallacy classification that
    require complex reasoning. In this paper, we propose a Case-Based Reasoning method
    that classifies new cases of logical fallacy by language-modeling-driven retrieval
    and adaptation of historical cases. We design four complementary strategies to
    enrich input representation for our model, based on external information about
    goals, explanations, counterarguments, and argument structure. Our experiments
    in in-domain and out-of-domain settings indicate that Case-Based Reasoning improves
    the accuracy and generalizability of language models. Our ablation studies suggest
    that representations of similar cases have a strong impact on the model performance,
    that models perform well with fewer retrieved cases, and that the size of the
    case database has a negligible effect on the performance. Finally, we dive deeper
    into the relationship between the properties of the retrieved cases and the model
    performance.
  authors:
  - Zhivar Sourati
  - Filip Ilievski
  - Hng-n Sandlin
  - Alain Mermoud
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLRSE
  forum: ''
  id: ACL_92
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Case-Based Reasoning with Language Models for Classification of Logical Fallacies
  tldr: The ease and speed of spreading misinformation and propaganda on the Web motivate
    the need to develop trustworthy technology for detecting fallacies in natural
    language arguments. However, state-of-the-art language modeling methods exhibit
    a lack of robustness on tasks like logical fallacy classific
  track: 1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Causal reasoning, the ability to identify cause-and-effect relationship,
    is crucial in human thinking. Although large language models (LLMs) succeed in
    many NLP tasks, it is still challenging for them to conduct complex causal reasoning
    like abductive reasoning and counterfactual reasoning. Given the fact that programming
    code may express causal relations more often and explicitly with conditional statements
    like ``if``, we want to explore whether Code-LLMs acquire better causal reasoning
    abilities. Our experiments show that compared to text-only LLMs, Code-LLMs with
    code prompts are better causal reasoners. We further intervene on the prompts
    from different aspects, and discover that the key point is the programming structure.
  authors:
  - Xiao Liu
  - Da Yin
  - Chen Zhang
  - Yansong Feng
  - Dongyan Zhao
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLRSE
  forum: ''
  id: ACL_93
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'The Magic of IF: Investigating Causal Reasoning Abilities in Large Language
    Models of Code'
  tldr: Causal reasoning, the ability to identify cause-and-effect relationship, is
    crucial in human thinking. Although large language models (LLMs) succeed in many
    NLP tasks, it is still challenging for them to conduct complex causal reasoning
    like abductive reasoning and counterfactual reasoning. Given th
  track: 1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: It has been suggested in literature that large pre-trained language models
    (PLMs) are able to suppress human-level performance for natural language inference
    (NLI) tasks. However, the failure of learning the underlying generalizations and
    the inconsistency to small textual perturbations rise doubt about whether models
    rely on adopting shallow heuristics to guess the correct label. To mitigate this
    issue, we propose a neural-symbolic contrastive learning framework inspired by
    Inductive Logic Programming (ILP) to better capture logical relationships from
    data. Unlike the usual methods for NLI tasks, our approach represents data as
    logic programs, sets of logic rules. We aim to learn an embedding space in which  the
    examples share as various as possible textual information with as similar as possible
    underlying logical meanings that are close together, and vice versa. Experimental
    results affirm this approach's ability to enhance the model's transferability
    performance.
  authors:
  - Mingyue Liu
  - Jialin Yu
  - Hao Cui
  - Sara Uckelman
  - Yang Long
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLRSE
  forum: ''
  id: ACL_95
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Neural-symbolic Contrastive Learning for Cross-domain Inference
  tldr: It has been suggested in literature that large pre-trained language models
    (PLMs) are able to suppress human-level performance for natural language inference
    (NLI) tasks. However, the failure of learning the underlying generalizations and
    the inconsistency to small textual perturbations rise doubt a
  track: 1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We introduce a synthetic dataset called Sentences Involving Complex Compositional
    Knowledge (SICCK) and a novel analysis that investigates the performance of Natural
    Language Inference (NLI) models to understand compositionality in logic. We produce
    1,304 sentence pairs by modifying 15 examples from the SICK dataset (Marelli et
    al., 2014). To this end, we modify the original texts using a set of phrases  modifiers
    that correspond to universal quantifiers, existential quantifiers, negation, and
    other concept modifiers in Natural Logic (NL) (MacCartney, 2009). We use these
    phrases to modify the subject, verb, and object parts of the premise and hypothesis.
    Lastly, we annotate these modified texts with the corresponding entailment labels
    following NL rules. We conduct a preliminary verification of how well the change
    in the structural and semantic composition is captured by neural NLI models, in
    both zero-shot and fine-tuned scenarios. We found that the performance of NLI
    models under the zero-shot setting is poor, especially for modified sentences
    with negation and existential quantifiers. After fine-tuning this dataset, we
    observe that models continue to perform poorly over negation, existential and
    universal modifiers.
  authors:
  - Sushma Anand Akoju
  - Robert Vacareanu
  - Eduardo Blanco
  - Haris Riaz
  - Mihai Surdeanu
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLRSE
  forum: ''
  id: ACL_97
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Synthetic Dataset for Evaluating Complex Compositional Knowledge for Natural
    Language Inference
  tldr: We introduce a synthetic dataset called Sentences Involving Complex Compositional
    Knowledge (SICCK) and a novel analysis that investigates the performance of Natural
    Language Inference (NLI) models to understand compositionality in logic. We produce
    1,304 sentence pairs by modifying 15 examples from
  track: 1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We introduce STREET, a unified multi-task and multi-domain natural language
    reasoning and explanation benchmark. Unlike most existing question-answering (QA)
    datasets, we expect models to not only answer questions, but also produce step-by-step
    structured explanations describing how premises in the question are used to produce
    intermediate conclusions that can prove the correctness of a certain answer. We
    perform extensive evaluation with popular language models such as few-shot prompting
    GPT-3 and fine-tuned T5. We find that these models still lag behind human performance
    when producing such structured reasoning steps. We believe this work will provide
    a way for the community to better train and test systems on multi-step reasoning
    and explanations in natural language.
  authors:
  - Danilo Neves Ribeiro
  - Shen Wang
  - Xiaofei Ma
  - Henghui Zhu
  - Rui Dong
  - Deguang Kong
  - Juliette Burger
  - Anjelica Ramos
  - William Yang Wang
  - Zhiheng Huang
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLRSE
  forum: ''
  id: ACL_98
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'STREET: A Multi-Task Structured Reasoning and Explanation Benchmark'
  tldr: 'We introduce STREET, a unified multi-task and multi-domain natural language
    reasoning and explanation benchmark. Unlike most existing question-answering (QA)
    datasets, we expect models to not only answer questions, but also produce step-by-step
    structured explanations describing how premises in the '
  track: 1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Large language models (LLMs) have exhibited remarkable capabilities in
    learning from explanations in prompts, but there has been limited understanding
    of exactly how these explanations function or why they are effective. This work
    aims to better understand the mechanisms by which explanations are used for in-context
    learning. We first study the impact of two different factors on the performance
    of prompts with explanations: the computation trace (the way the solution is decomposed)
    and the natural language used to express the prompt. By perturbing explanations
    on three controlled tasks, we show that both factors contribute to the effectiveness
    of explanations. We further study how to form maximally effective sets of explanations
    for solving a given test query. We find that LLMs can benefit from the complementarity
    of the explanation set: diverse reasoning skills shown by different exemplars
    can lead to better performance. Therefore, we propose a maximal marginal relevance-based
    exemplar selection approach for constructing exemplar sets that are both relevant
    as well as complementary, which successfully improves the in- context learning
    performance across three real- world tasks on multiple LLMs.'
  authors:
  - Xi Ye
  - Srinivasan Iyer
  - Asli Celikyilmaz
  - Veselin Stoyanov
  - Greg Durrett
  - Ramakanth Pasunuru
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLRSE
  forum: ''
  id: ACL_99
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Complementary Explanations for Effective In-Context Learning
  tldr: Large language models (LLMs) have exhibited remarkable capabilities in learning
    from explanations in prompts, but there has been limited understanding of exactly
    how these explanations function or why they are effective. This work aims to better
    understand the mechanisms by which explanations are us
  track: 1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL
    2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The major goal of our study is to test methodsin NLP in the domain of
    health care educationrelated to Covid-19 of vulnerable groups suchas indigenous
    people from Latin America. Inorder to achieve this goal, we asked participantsin
    a survey questionnaire to provide answersabout health related topics. We used
    these answersto measure the health education status ofour participants. In this
    paper, we summarizethe results from our NLP-application on theparticipants' answers.
    In the first experiment,we use embeddings-based tools to measure thesemantic similarity
    between participants' answersand "expert" or "reference" answers. Inthe second
    experiment, we use synonym-basedmethods to classify answers under topics. Wecompare
    the results from both experiments withhuman annotations. Our results show that
    thetested NLP-methods reach a significantly loweraccuracy score than human annotations
    in bothexperiments. We explain this difference by theassumption that human annotators
    are muchbetter in pragmatic inferencing necessary toclassify the semantic similarity
    and topic classificationof answers.
  authors:
  - Olga Kellert
  - Mahmud Zaman
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - AmericasNLP
  forum: ''
  id: ACL_1
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Short Paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Use of NLP in the Context of Belief states of Ethnic Minorities in Latin
    America
  tldr: The major goal of our study is to test methodsin NLP in the domain of health
    care educationrelated to Covid-19 of vulnerable groups suchas indigenous people
    from Latin America. Inorder to achieve this goal, we asked participantsin a survey
    questionnaire to provide answersabout health related topics.
  track: Third Workshop on Natural Language Processing for Indigenous Languages of
    the Americas
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Active learning is an algorithmic approach that strategically selects
    a subset of examples for labeling, with the goal of reducing workload and required
    resources. Previous research has applied active learning to Neural Machine Translation
    (NMT) for high-resource or well-represented languages, achieving significant reductions
    in manual labor. In this study, we explore the application of active learning
    for NMT in the context of Mapudungun, a low-resource language spoken by the Mapuche
    community in South America. Mapudungun was chosen due to the limited number of
    fluent speakers and the pressing need to provide access to content predominantly
    available in widely represented languages. We assess both model-dependent and
    model-agnostic active learning strategies for NMT between Spanish and Mapudungun
    in both directions, demonstrating that we can achieve over 40\% reduction in manual
    translation workload in both cases.
  authors:
  - Begoa Pendas
  - Andres Carvallo
  - Carlos Aspillaga
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - AmericasNLP
  forum: ''
  id: ACL_2
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Short Paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Neural Machine Translation through Active Learning on low-resource languages:
    The case of Spanish to Mapudungun'
  tldr: Active learning is an algorithmic approach that strategically selects a subset
    of examples for labeling, with the goal of reducing workload and required resources.
    Previous research has applied active learning to Neural Machine Translation (NMT)
    for high-resource or well-represented languages, achie
  track: Third Workshop on Natural Language Processing for Indigenous Languages of
    the Americas
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We investigate native language identification (LangID) for Brazilian Indigenous
    Languages (BILs), using the Bible as training data. Our research extends from
    previous work, by presenting two analyses on the generalization of Bible-based
    LangID in non-biblical data. First, with newly collected non-biblical datasets,
    we show that such a LangID can still provide quite reasonable accuracy in languages
    for which there are more established writing standards, such as Guarani Mbya and
    Kaigang, but there can be a quite drastic drop in accuracy depending on the language.
    Then, we applied the LangID on a large set of texts, about 13M sentences from
    the Portuguese Wikipedia, towards understanding the difficulty factors may come
    out of such task in practice. The main outcome is that the lack of handling other
    American indigenous languages can affect considerably the precision for BILs,
    suggesting the need of a joint effort with related languages from the Americas.
  authors:
  - Paulo Cavalin
  - Pedro Domingues
  - Julio Nogima
  - Claudio Pinhanez
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - AmericasNLP
  forum: ''
  id: ACL_22
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Short Paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Understanding Native Language Identification for Brazilian Indigenous Languages
  tldr: We investigate native language identification (LangID) for Brazilian Indigenous
    Languages (BILs), using the Bible as training data. Our research extends from
    previous work, by presenting two analyses on the generalization of Bible-based
    LangID in non-biblical data. First, with newly collected non-bi
  track: Third Workshop on Natural Language Processing for Indigenous Languages of
    the Americas
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes an ongoing effort to create, from the original hand-written
    text, a machine-readable, linguistically-annotated, and easily-searchable corpus
    of the Nahuatl portion of the Florentine Codex, a 16th century Mesoamerican manuscript
    written in Nahuatl and Spanish. The Codex consists of 12 books and over 300,000
    tokens. We describe the process of annotating 3 of these books, the steps of text
    preprocessing undertaken, our approach to efficient manual processing and annotation,
    and some of the challenges faced along the way. We also report on a set of experiments
    evaluating our ability to automate the text processing tasks to aid in the remaining
    annotation effort, and find the results promising despite the relatively low volume
    of training data. Finally, we briefly present a real use case from the humanities
    that would benefit from the searchable, linguistically annotated corpus we describe.
  authors:
  - Francis Tyers
  - Robert Pugh
  - Valery Berthoud F.
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - AmericasNLP
  forum: ''
  id: ACL_23
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long Paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Codex to corpus: Exploring annotation and processing for an open and extensible
    machine-readable edition of the Florentine Codex'
  tldr: 'This paper describes an ongoing effort to create, from the original hand-written
    text, a machine-readable, linguistically-annotated, and easily-searchable corpus
    of the Nahuatl portion of the Florentine Codex, a 16th century Mesoamerican manuscript
    written in Nahuatl and Spanish. The Codex consists '
  track: Third Workshop on Natural Language Processing for Indigenous Languages of
    the Americas
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We describe a suite of finite-state language technologies for Maya, a
    Mayan language spoken in Mexico. At the core is a computational model of Maya
    morphology and phonology using a finite-state transducer. This model results in
    a morphological analyzer and a morphologically-informed spell-checker. All of
    these technologies are designed for use as both a pedagogical reading/writing
    aid for L2 learners and as a general language processing tool capable of supporting
    much of the natural variation in written Maya. We discuss the relevant features
    of Maya morphosyntax and orthography, and then outline the implementation details
    of the analyzer. To conclude, we present a longer-term vision for these tools
    and their use by both native speakers and learners.
  authors:
  - Robert Pugh
  - Francis Tyers
  - Quetzil Castaeda
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - AmericasNLP
  forum: ''
  id: ACL_24
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long Paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Developing finite-state language technology for Maya
  tldr: We describe a suite of finite-state language technologies for Maya, a Mayan
    language spoken in Mexico. At the core is a computational model of Maya morphology
    and phonology using a finite-state transducer. This model results in a morphological
    analyzer and a morphologically-informed spell-checker. A
  track: Third Workshop on Natural Language Processing for Indigenous Languages of
    the Americas
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In this paper, we present an FST based approach for conducting morphological
    analysis, lemmatization and generation of Lushootseed words. Furthermore, we use
    the FST to generate training data for an LSTM based neural model and train this
    model to do morphological analysis. The neural model reaches a 71.9\% accuracy
    on the test data. Furthermore, we discuss reduplication types in the Lushootseed
    language forms. The approach involves the use of both attested instances of reduplication
    and bare stems for applying a variety of reduplications to, as it is unclear just
    how much variation can be attributed to the individual speakers and authors of
    the source materials. That is, there may be areal factors that can be aligned
    with certain types of reduplication and their frequencies.
  authors:
  - Jack Rueter
  - Mika Hmlinen
  - Khalid Alnajjar
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - AmericasNLP
  forum: ''
  id: ACL_26
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Short Paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Modelling the Reduplicating Lushootseed Morphology with an FST and LSTM
  tldr: In this paper, we present an FST based approach for conducting morphological
    analysis, lemmatization and generation of Lushootseed words. Furthermore, we use
    the FST to generate training data for an LSTM based neural model and train this
    model to do morphological analysis. The neural model reaches a
  track: Third Workshop on Natural Language Processing for Indigenous Languages of
    the Americas
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Conventional approaches to learning word embeddings (Mikolov et al., 2013;
    Pennington et al., 2014) are limited to relatively few languages with sufficiently
    large training corpora. To address this limitation, we propose an alternative
    approach to deriving word embeddings for Wolastoqey and Mi'kmaq that leverages
    definitions from a bilingual dictionary. More specifically, following Bear and
    Cook (2022), we experiment with encoding English definitions of Wolastoqey and
    Mi'kmaq words into vector representations using English sequence representation
    models. For this, we consider using and finetuning sentence-RoBERTa models (Reimers
    and Gurevych, 2019). We evaluate our word embeddings using a similar methodology
    to that of Bear and Cook using evaluations based on word classification, clustering
    and reverse dictionary search. We additionally construct word embeddings for higher-resource
    languages  English, German and Spanishusing our methods and evaluate our embeddings
    on existing word-similarity datasets. Our findings indicate that our word embedding
    methods can be used to produce meaningful vector representations for low-resource
    languages such as Wolastoqey and Mi'kmaq and for higher-resource languages.
  authors:
  - Diego Bear
  - Paul Cook
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - AmericasNLP
  forum: ''
  id: ACL_27
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long Paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Fine-tuning Sentence-RoBERTa to Construct Word Embeddings for Low-resource
    Languages from Bilingual Dictionaries
  tldr: 'Conventional approaches to learning word embeddings (Mikolov et al., 2013;
    Pennington et al., 2014) are limited to relatively few languages with sufficiently
    large training corpora. To address this limitation, we propose an alternative
    approach to deriving word embeddings for Wolastoqey and Mi''kmaq '
  track: Third Workshop on Natural Language Processing for Indigenous Languages of
    the Americas
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Natural Language Processing applications, such as Neural Machine Translation,
    typically exhibit substantial biases toward sensitive factors such as gender or
    race. This degrades the performance of machine translation and promotes unfavorable
    preconceptions. The current paper examines the issues and challenges of gender
    bias within Inuktitut, an under- represented Indigenous language of Canada, and
    discusses how to enhance the performance of Inuktitut-English NMT; all with the
    aim of revitalizing the Indigenous language and considering an inclusive NMT.
    Firstly, we per- formed the detection of gender bias in word embeddings in Inuktitut
    and English. Secondly, we compared the debiasing effect with the traditional word
    to vectors and also based on a dictionary. Then, we adopted a strategy, within
    the Inuktitut-English NMT, using the two bilingual debiased word embeddings.This
    work has been presented at the 6th Workshop on Computational Methods for Endangered
    Languages, in collocation with the 8th International Conference on Language Documentation
    \& Conservation 2023 (ICLDC 8).
  authors:
  - Ngoc Tan Le
  - Oussama Hansal
  - Fatiha Sadat
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - AmericasNLP
  forum: ''
  id: ACL_28
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Previously Presented Work
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: An Empirical Study on Gender Bias within an Indigenous Language Revitalization
    Perspective and an Inclusive NMT
  tldr: 'Natural Language Processing applications, such as Neural Machine Translation,
    typically exhibit substantial biases toward sensitive factors such as gender or
    race. This degrades the performance of machine translation and promotes unfavorable
    preconceptions. The current paper examines the issues and '
  track: Third Workshop on Natural Language Processing for Indigenous Languages of
    the Americas
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Colexification refers to linguistic phenomena where multiple concepts
    (meanings) are expressedby the same lexical form, such as polysemy or homophony.
    Semantic typology studies this type of cross-lingual semantic categorization (Evans
    et al.,2010). The term "colexification" was first formalized in semantic typology
    by Franois (2008) to create semantic maps. Colexifications have been found to
    be pervasive across languages and cultures. The investigation of cross-lingual
    colexifications has provided insights across different fields, such as psycholinguistics
    (Jackson et al., 2019), cognitive science (Gibson et al., 2019) and linguistic
    typology (Schapper and Koptjevskaja-Tamm,2022), but remain relatively unexplored
    in NLP (see, e.g., Harvill et al. (2022), Chen et al. (2023)). The Database of
    Cross-Linguistic Colexifications (CLICS3) (Rzymski et al., 2020) was created to
    include 4,228 colexification patterns across 3,156 languages, to facilitate research
    in colexifications.The problem of concreteness/abstractness of concepts is interdisciplinary,
    studied from a cognitivestandpoint in linguistics, psychology, psycholinguistics,
    neurophysiology, etc (Solovyev, 2021). Concrete concepts are those that are perceived
    by the senses, such as CAT and MOUNTAIN, while abstract concepts are not perceived
    by the senses, such as RELATIONSHIP and UNDERSTANDING. Brysbaert et al. (2014)
    curates concreteness ratings for 37,058 English words and 2,896 two-word expressions
    from over 4,000 participants, which has provided insights across various linguistic
    disciplines. The concreteness ratings scales from 1 (abstract) to 5 (concrete).We
    have a markedly differing hypothesis from previous work. Unlike Xu et al. (2020),
    we hypothesize that concepts that are closer in concreteness/abstractness are
    more likely to colexify. We implement preliminary experiment and present the results
    to corroborate the hypothesis, challenging the previous theories and findings
    on the correlation between colexifications and metaphoricity.(We will later make
    our GitHub repository publicly accessible.)
  authors:
  - Yiyi Chen
  - Johannes Bjerva
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - AmericasNLP
  forum: ''
  id: ACL_29
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Non-archival Extended Abstract
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Patterns of Closeness and Abstractness in Colexifications: The Case of Indigineous
    Languages in the Americas'
  tldr: Colexification refers to linguistic phenomena where multiple concepts (meanings)
    are expressedby the same lexical form, such as polysemy or homophony. Semantic
    typology studies this type of cross-lingual semantic categorization (Evans et
    al.,2010). The term "colexification" was first formalized in s
  track: Third Workshop on Natural Language Processing for Indigenous Languages of
    the Americas
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The Ojibwe language has several dialects that vary to some degree in both
    spoken and written form.  We present a method of using support vector machines
    to classify two different dialects (Eastern and Southwestern Ojibwe) using a very
    small corpus of text.  Classification accuracy at the sentence level is 90\% across
    a five-fold cross validation and 72\% when the sentence-trained model is applied
    to a data set of individual words.  Our code and the word level data set are released
    openly on Github at [link to be inserted for final version, working demonstration
    notebook uploaded with paper].
  authors:
  - Kalvin Hartwig
  - Evan Lucas
  - Timothy Havens
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - AmericasNLP
  forum: ''
  id: ACL_32
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Short Paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Identification of Dialect for Eastern and Southwestern Ojibwe Words Using
    a Small Corpus
  tldr: The Ojibwe language has several dialects that vary to some degree in both
    spoken and written form.  We present a method of using support vector machines
    to classify two different dialects (Eastern and Southwestern Ojibwe) using a very
    small corpus of text.  Classification accuracy at the sentence le
  track: Third Workshop on Natural Language Processing for Indigenous Languages of
    the Americas
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We present the first neural machine translation system for the low-resource
    language pair WayunaikiSpanish and explore strategies to inject linguistic knowledge
    into the model to improve translation quality. We explore a wide range of methods
    and combine complementary approaches. Results indicate that incorporating linguistic
    information through linguistically motivated subword segmentation, factored models,
    and pretrained embeddings helps the system to generate improved translations,
    with the segmentation contributing the most.In order to evaluate translation quality
    in a general domain and go beyond the available religious domain data, we gather
    and make publicly available a new test set and supplementary material.Although
    translation quality as measured with automatic metrics is low, we hope these resources
    will facilitate and support further research on Wayunaiki.
  authors:
  - Nora Graichen
  - Josef Van Genabith
  - Cristina Espaa-bonet
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - AmericasNLP
  forum: ''
  id: ACL_33
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long Paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Enriching WayunaikiSpanish Neural Machine Translation with Linguistic Information
  tldr: We present the first neural machine translation system for the low-resource
    language pair WayunaikiSpanish and explore strategies to inject linguistic knowledge
    into the model to improve translation quality. We explore a wide range of methods
    and combine complementary approaches. Results indicate th
  track: Third Workshop on Natural Language Processing for Indigenous Languages of
    the Americas
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Named Entity Recognition is a crucial step to ensure good quality performance
    of several Natural Language Processing applications and tools, including machine
    translation and information retrieval. Moreover, it is considered as a fundamental
    module of many Natural Language Understanding tasks such as question-answering
    systems. This paper presents a first study on NER for an under-represented Indigenous
    Inuit language of Canada, Inuktitut, which lacks linguistic resources and large
    labeled data.Our proposed NER model for Inuktitut is built by transferring linguistic
    characteristics from English to Inuktitut, based on either rules or bilingual
    word embeddings. We provide an empirical study based on a comparison with the
    state of the art models and as well as intrinsic and extrinsic evaluations. In
    terms of Recall, Precision and F-score, the obtained results show the effectiveness
    of the proposed NER methods. Furthermore, it improved the performance of Inuktitut-English
    Neural Machine Translation.
  authors:
  - Ngoc Tan Le
  - Soumia Kasdi
  - Fatiha Sadat
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - AmericasNLP
  forum: ''
  id: ACL_34
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long Paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Towards the First Named Entity Recognition of Inuktitut for an Improved Machine
    Translation
  tldr: Named Entity Recognition is a crucial step to ensure good quality performance
    of several Natural Language Processing applications and tools, including machine
    translation and information retrieval. Moreover, it is considered as a fundamental
    module of many Natural Language Understanding tasks such a
  track: Third Workshop on Natural Language Processing for Indigenous Languages of
    the Americas
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'In this paper, we present a parallel Spanish- Mazatec and Spanish-Mixtec
    corpus for machine translation (MT) tasks, where Mazatec and Mixtec are two indigenous
    Mexican languages. We evaluated the usability of the collected corpus using three
    different approaches: transformer, transfer learning, and fine-tuning pre-trained
    multilingual MT models. Fine-tuning the Facebook m2m100-48 model outperformed
    the other approaches, with BLEU scores of 12.09 and 22.25 for Mazatec-Spanish
    and Spanish-Mazatec translations, respectively, and 16.75 and 22.15 for Mixtec-Spanish
    and Spanish-Mixtec translations, respectively. The results indicate that translation
    performance is influenced by the dataset size (9,799 sentences in Mazatec and
    13,235 sentences in Mixtec) and is more effective when indigenous languages are
    used as target languages. The findings emphasize the importance of creating parallel
    corpora for indigenous languages and fine-tuning models for low-resource translation
    tasks. Future research will investigate zero-shot and few-shot learning approaches
    to further improve translation performance in low-resource settings.'
  authors:
  - Atnafu Lambebo Tonja
  - Christian Maldonado-sifuentes
  - David Alejandro Mendoza Castillo
  - Olga Kolesnikova
  - No Castro-snchez
  - Grigori Sidorov
  - Alexander Gelbukh
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - AmericasNLP
  forum: ''
  id: ACL_36
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long Paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Parallel Corpus for Indigenous Language Translation: Spanish-Mazatec and
    Spanish-Mixtec'
  tldr: 'In this paper, we present a parallel Spanish- Mazatec and Spanish-Mixtec
    corpus for machine translation (MT) tasks, where Mazatec and Mixtec are two indigenous
    Mexican languages. We evaluated the usability of the collected corpus using three
    different approaches: transformer, transfer learning, and '
  track: Third Workshop on Natural Language Processing for Indigenous Languages of
    the Americas
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes the development of a free/open-source finite-state
    morphologicaltransducer for Highland Puebla Nahuatl, a Uto-Aztecan language spoken
    in and around the stateof Puebla in Mexico. The finite-state toolkit used for
    the work is the Helsinki Finite-StateToolkit (HFST); we use the lexc formalism
    for modelling the morphotactics and twol formal-ism for modelling morphophonological
    alternations. An evaluation is presented which showsthat the transducer has a
    reasonable coveragearound 90\%on freely-available corpora of the language, and
    high precisionover 95\%on a manually verified test set
  authors:
  - Robert Pugh
  - Francis Tyers
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - AmericasNLP
  forum: ''
  id: ACL_38
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Short Paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: A finite-state morphological analyser for Highland Puebla Nahuatl
  tldr: This paper describes the development of a free/open-source finite-state morphologicaltransducer
    for Highland Puebla Nahuatl, a Uto-Aztecan language spoken in and around the stateof
    Puebla in Mexico. The finite-state toolkit used for the work is the Helsinki Finite-StateToolkit
    (HFST); we use the lex
  track: Third Workshop on Natural Language Processing for Indigenous Languages of
    the Americas
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Neural models have drastically advanced state of the art for machine translation
    (MT) between high-resource languages. Traditionally, these models rely on large
    amounts of training data, but many language pairs lack these resources. However,
    an important part of the languages in the world do not have this amount of data.
    Most languages from the Americas are among them, having a limited amount of parallel
    and monolingual data, if any.   Here, we present an introduction to the interested
    reader to the basic challenges, concepts, and techniques that involve the creation
    of MT systems for these languages. Finally, we discuss the recent advances and
    findings and open questions, product of an increased interest of the NLP community
    in these languages.
  authors:
  - Manuel Mager
  - Rajat Bhatnagar
  - Graham Neubig
  - Ngoc Thang Vu
  - Katharina Kann
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - AmericasNLP
  forum: ''
  id: ACL_39
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long Paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Neural Machine Translation for the Indigenous Languages of the Americas:
    An Introduction'
  tldr: Neural models have drastically advanced state of the art for machine translation
    (MT) between high-resource languages. Traditionally, these models rely on large
    amounts of training data, but many language pairs lack these resources. However,
    an important part of the languages in the world do not hav
  track: Third Workshop on Natural Language Processing for Indigenous Languages of
    the Americas
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Automatic speech emotion recognition is an important research topic for
    human-computer interaction and affective computing. Over ten million people speak
    the Quechua language throughout South America, and one of the most known variants
    is the Quechua Collao one. However, this language can be considered a low resource
    for machine emotion recognition, creating a barrier for Quechua speakers who want
    to use this technology. Therefore, the contribution of this work is a 15hours
    speech corpus in Quechua Collao, which is made publicly available to the research
    community. The corpus was created from a set of words and sentences explicitly
    collected for this task, divided into nine categorical emotions: happy, sad, bored,
    fear, sleepy, calm, excited, angry, and neutral. The annotation was performed
    on a 5-value discrete scale according to 3 dimensions: valence, arousal, and dominance.
    To demonstrate the usefulness of the corpus, we have performed speech emotion
    recognition using machine learning methods and neural networks.'
  authors:
  - Rosa Paccotacya-yanque
  - Candy Huanca-anquise
  - Judith Escalante Calcina
  - Wilber Ramos-lovn
  - Lvaro Cuno-parari
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - AmericasNLP
  forum: ''
  id: ACL_43
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Previously Presented Work
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: A speech corpus of Quechua Collao for automatic dimensional emotion recognition
  tldr: Automatic speech emotion recognition is an important research topic for human-computer
    interaction and affective computing. Over ten million people speak the Quechua
    language throughout South America, and one of the most known variants is the Quechua
    Collao one. However, this language can be conside
  track: Third Workshop on Natural Language Processing for Indigenous Languages of
    the Americas
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In this paper, we present a new online dictionary of Akuzipik, an Indigenous
    language of St. Lawrence Island (Alaska) and Chukotka (Russia).We discuss community
    desires for strengthening language use in the community and in educational settings,
    and present specific features of an online dictionary designed to serve these
    community goals.
  authors:
  - Benjamin Hunt
  - Lane Schwartz
  - Sylvia Schreiner
  - Emily Chen
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - AmericasNLP
  forum: ''
  id: ACL_44
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long Paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Community consultation and the development of an online Akuzipik-English
    dictionary
  tldr: 'In this paper, we present a new online dictionary of Akuzipik, an Indigenous
    language of St. Lawrence Island (Alaska) and Chukotka (Russia).We discuss community
    desires for strengthening language use in the community and in educational settings,
    and present specific features of an online dictionary '
  track: Third Workshop on Natural Language Processing for Indigenous Languages of
    the Americas
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Modern machine learning techniques have produced many impressive results
    in language technology, but these techniques generally require an amount of training
    data that is many orders of magnitude greater than what exists for low-resource
    languages in general, and endangered ones in particular. However, dictionary definitions
    in a comparatively much more well-resourced majority language can provide a link
    between low-resource languages and machine learning models trained on massive
    amounts of majority-language data. By leveraging a pre-trained English word embedding
    to compute sentence embeddings for definitions in bilingual dictionaries for four
    Indigenous languages spoken in North America, Plains Cree (nhiyawwin), Arapaho
    (Hinno''itit), Northern Haida (Xaad Kl), and Tsuut''ina (Tst''n), we have obtained
    promising results for dictionary search. Not only are the search results in the
    majority language of the definitions more relevant, but they can be semantically
    relevant in ways not achievable with classic information retrieval techniques:
    users can perform successful searches for words that do not occur at all in the
    dictionary. These techniques are directly applicable to any bilingual dictionary
    providing translations between a high- and low-resource language.'
  authors:
  - Antti Arppe
  - Andrew Neitsch
  - Daniel Dacanay
  - Jolene Poulin
  - Daniel Hieber
  - Atticus Harrigan
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - AmericasNLP
  forum: ''
  id: ACL_45
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long Paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Finding words that aren''t there: Using word embeddings to improve dictionary
    search for low-resource languages'
  tldr: Modern machine learning techniques have produced many impressive results in
    language technology, but these techniques generally require an amount of training
    data that is many orders of magnitude greater than what exists for low-resource
    languages in general, and endangered ones in particular. Howev
  track: Third Workshop on Natural Language Processing for Indigenous Languages of
    the Americas
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'We present the LCT-EHU submission to the AmericasNLP 2023 low-resource
    machine translation shared task. We focus on the Spanish-Quechua language pair
    and explore the usage of different approaches: (1) Obtain new parallel corpora
    from the literature and legal domains, (2) Compare a high-resource Spanish-English
    pre-trained MT model with a Spanish-Finnish pre-trained model (with Finnish being
    chosen as a target language due to its morphological similarity to Quechua), and
    (3) Explore additional techniques such as copied corpus and back-translation.
    Overall, we show that the Spanish-Finnish pre-trained model outperforms other
    setups, while low-quality synthetic data reduces the performance.'
  authors:
  - Nouman Ahmed
  - Natalia Flechas Manrique
  - Antonije Petrovi
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - AmericasNLP
  forum: ''
  id: ACL_48
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Shared Task System Description
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Enhancing Spanish-Quechua Machine Translation with Pre-Trained Models and
    Diverse Data Sources: LCT-EHU at AmericasNLP Shared Task'
  tldr: 'We present the LCT-EHU submission to the AmericasNLP 2023 low-resource machine
    translation shared task. We focus on the Spanish-Quechua language pair and explore
    the usage of different approaches: (1) Obtain new parallel corpora from the literature
    and legal domains, (2) Compare a high-resource Span'
  track: Third Workshop on Natural Language Processing for Indigenous Languages of
    the Americas
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We present NusaCrowd, a collaborative initiative to collect and unify
    existing resources for Indonesian languages, including opening access to previously
    non-public resources. Through this initiative, we have brought together 137 datasets
    and 118 standardized data loaders. The quality of the datasets has been assessed
    manually and automatically, and their value is demonstrated through multiple experiments.NusaCrowd's
    data collection enables the creation of the first zero-shot benchmarks for natural
    language understanding and generation in Indonesian and the local languages of
    Indonesia. Furthermore, NusaCrowd brings the creation of the first multilingual
    automatic speech recognition benchmark in Indonesian and the local languages of
    Indonesia. Our work strives to advance natural language processing (NLP) research
    for languages that are under-represented despite being widely spoken.
  authors:
  - Samuel Cahyawijaya
  - Holy Lovenia
  - Alham Fikri Aji
  - Genta Winata
  - Bryan Wilie
  - Fajri Koto
  - Rahmad Mahendra
  - Christian Wibisono
  - Ade Romadhony
  - Karissa Vincentio
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - AmericasNLP
  forum: ''
  id: ACL_49
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Previously Presented Work
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'NusaCrowd: Open Source Initiative for Indonesian NLP Resources'
  tldr: We present NusaCrowd, a collaborative initiative to collect and unify existing
    resources for Indonesian languages, including opening access to previously non-public
    resources. Through this initiative, we have brought together 137 datasets and
    118 standardized data loaders. The quality of the dataset
  track: Third Workshop on Natural Language Processing for Indigenous Languages of
    the Americas
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This report investigates the continuous challenges of Machine Translation
    (MT) systems on indigenous and extremely low-resource language pairs. Despite
    the notable achievements of Large Language Models (LLMs) that excel in various
    tasks, their applicability to low-resource languages remains questionable. In
    this study, we leveraged the AmericasNLP competition to evaluate the translation
    performance of different systems for Spanish to 11 indigenous languages from South
    America. Our team, LTLAmsterdam, submitted a total of four systems including GPT-4,
    a bilingual model, fine-tuned M2M100, and a combination of fine-tuned M2M100 with
    \$k\$NN-MT. We found that even large language models like GPT-4 are not well-suited
    for extremely low-resource languages. Our results suggest that fine-tuning M2M100
    models can offer significantly better performance for extremely low-resource translation.
  authors:
  - David Stap
  - Ali Araabi
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - AmericasNLP
  forum: ''
  id: ACL_50
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Shared Task System Description
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: ChatGPT is not a good indigenous translator
  tldr: This report investigates the continuous challenges of Machine Translation
    (MT) systems on indigenous and extremely low-resource language pairs. Despite
    the notable achievements of Large Language Models (LLMs) that excel in various
    tasks, their applicability to low-resource languages remains question
  track: Third Workshop on Natural Language Processing for Indigenous Languages of
    the Americas
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper presents the experiments to train a Spanish-Aymara machine
    translation model for the AmericasNLP 2023 Machine Translation shared task. We
    included the English-Aymara GlobalVoices corpus and an English-Aymara lexicon
    to train the model and limit our training resources to train the model in a \textbackslash{}textit\{few-shot\}
    manner.
  authors:
  - Liling Tan
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - AmericasNLP
  forum: ''
  id: ACL_51
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Shared Task System Description
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Few-shot Spanish-Aymara Machine Translation Using English-Aymara Lexicon
  tldr: This paper presents the experiments to train a Spanish-Aymara machine translation
    model for the AmericasNLP 2023 Machine Translation shared task. We included the
    English-Aymara GlobalVoices corpus and an English-Aymara lexicon to train the
    model and limit our training resources to train the model in
  track: Third Workshop on Natural Language Processing for Indigenous Languages of
    the Americas
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper presents PlayGround's submission to the AmericasNLP 2023 shared
    task on machine translation (MT) into indigenous languages. We finetuned NLLB-600M,
    a multilingual MT model pre-trained on Flores-200, on 10 low-resource language
    directions and examined the effectiveness of weight averaging and back translation.
    Our experiments showed that weight averaging, on average, led to a 0.0169 improvement
    in the ChrF++ score. Additionally, we found that back translation resulted in
    a 0.008 improvement in the ChrF++ score.
  authors:
  - Tianrui Gu
  - Kaie Chen
  - Siqi Ouyang
  - Lei Li
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - AmericasNLP
  forum: ''
  id: ACL_52
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Shared Task System Description
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: PlayGround Low Resource Machine Translation System for the 2023 AmericasNLP
    Shared Task
  tldr: 'This paper presents PlayGround''s submission to the AmericasNLP 2023 shared
    task on machine translation (MT) into indigenous languages. We finetuned NLLB-600M,
    a multilingual MT model pre-trained on Flores-200, on 10 low-resource language
    directions and examined the effectiveness of weight averaging '
  track: Third Workshop on Natural Language Processing for Indigenous Languages of
    the Americas
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The Helsinki-NLP team participated in the AmericasNLP 2023 Shared Task
    with 6 submissions for all 11 language pairs arising from 4 different multilingual
    systems. We provide a detailed look at the work that went into collecting and
    preprocessing the data that led to our submissions. We explore various setups
    for multilingual Neural Machine Translation (NMT), namely knowledge distillation
    and transfer learning, multilingual NMT including a high-resource language (English),
    language-specific fine-tuning, and multilingual NMT exclusively using low-resource
    data. Our multilingual Model B ranks first in 4 out of the 11 language pairs.
  authors:
  - Ona De Gibert
  - Ral Vzquez
  - Mikko Aulamo
  - Yves Scherrer
  - Sami Virpioja
  - Jrg Tiedemann
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - AmericasNLP
  forum: ''
  id: ACL_53
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Shared Task System Description
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Four Approaches to Low-Resource Multilingual NMT: The Helsinki Submission
    to the AmericasNLP 2023 Shared Task'
  tldr: The Helsinki-NLP team participated in the AmericasNLP 2023 Shared Task with
    6 submissions for all 11 language pairs arising from 4 different multilingual
    systems. We provide a detailed look at the work that went into collecting and
    preprocessing the data that led to our submissions. We explore vario
  track: Third Workshop on Natural Language Processing for Indigenous Languages of
    the Americas
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The University of Sheffield took part in the shared task 2023 AmericasNLP
    for all eleven language pairs. Our models consist of training different variations
    of NLLB-200 model on data provided by the organizers and available data from various
    sources such as constitutions, handbooks and news articles. Our models outperform
    the baseline model on the development set on chrF with substantial improvements
    particularly for Aymara, Guarani and Quechua. On the test set, our best submission
    achieves the highest average chrF of all the submissions, we rank first in four
    of the eleven languages, and at least one of our models ranks in the top 3 for
    all languages.
  authors:
  - Edward Gow-smith
  - Danae Snchez Villegas
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - AmericasNLP
  forum: ''
  id: ACL_54
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Shared Task System Description
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Sheffield's Submission to the AmericasNLP Shared Task on Machine Translation
    into Indigenous Languages
  tldr: The University of Sheffield took part in the shared task 2023 AmericasNLP
    for all eleven language pairs. Our models consist of training different variations
    of NLLB-200 model on data provided by the organizers and available data from various
    sources such as constitutions, handbooks and news articles
  track: Third Workshop on Natural Language Processing for Indigenous Languages of
    the Americas
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes CIC NLP's submission to the AmericasNLP 2023 Shared
    Task on machine translation systems for indigenous languages of the Americas.
    We present the system descriptions for three methods. We used two multilingual
    models, namely M2M-100 and mBART50, and one bilingual (one-to-one) --- Helsinki
    NLP Spanish-English translation model, and experimented with different transfer
    learning setups. We experimented with 11 languages from America and report the
    setups we used as well as the results we achieved. Overall, the mBART setup was
    able to improve upon the baseline for three out of the eleven languages.
  authors:
  - Atnafu Lambebo Tonja
  - Hellina Hailu Nigatu
  - Olga Kolesnikova
  - Grigori Sidorov
  - Alexander Gelbukh
  - Jugal Kalita
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - AmericasNLP
  forum: ''
  id: ACL_55
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Shared Task System Description
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Enhancing Translation for Indigenous Languages: Experiments with Multilingual
    Models'
  tldr: This paper describes CIC NLP's submission to the AmericasNLP 2023 Shared Task
    on machine translation systems for indigenous languages of the Americas. We present
    the system descriptions for three methods. We used two multilingual models, namely
    M2M-100 and mBART50, and one bilingual (one-to-one) ---
  track: Third Workshop on Natural Language Processing for Indigenous Languages of
    the Americas
  underline_id: null
  underline_url: null
  video_url: null
- abstract: "In this work, we present the results of the AmericasNLP 2023 Shared Task\
    \ on Machine Translation into Indigenous Languages of the Americas. This edition\
    \ of the shared task featured eleven language pairs, one of which \u2013 Chatino-Spanish\
    \ \u2013 uses a newly collected evaluation dataset, consisting of professionally\
    \ translated text from the legal domain. Seven teams participated in the shared\
    \ task, with a total of 181 submissions. Additionally, we conduct a human evaluation\
    \ of the best system outputs, and compare them to the best submissions from the\
    \ prior shared task. We find that this analysis agrees with the quantitative measures\
    \ used to rank submissions, which shows further improvements of 9.64 ChrF on average\
    \ across all languages, when compared to the prior winning system."
  authors:
  - Abteen Ebrahimi
  - Manuel Mager
  - Shruti Rijhwani
  - Enora Rice
  - Arturo Oncevay
  - Claudia Baltazar
  - "Mar\xEDa Cort\xE9s"
  - "Cynthia Monta\xF1o"
  - John E. Ortega
  - Rolando Coto-solano
  - Hilaria Cruz
  - Alexis Palmer
  - Katharina Kann
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - AmericasNLP
  forum: ''
  id: ACL_56
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long Paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Findings of the AmericasNLP 2023 Shared Task on Machine Translation into
    Indigenous Languages
  tldr: "In this work, we present the results of the AmericasNLP 2023 Shared Task\
    \ on Machine Translation into Indigenous Languages of the Americas. This edition\
    \ of the shared task featured eleven language pairs, one of which \u2013 Chatino-Spanish\
    \ \u2013 uses a newly collected evaluation dataset, consisting of profess"
  track: Third Workshop on Natural Language Processing for Indigenous Languages of
    the Americas
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Rumour detection on social media is an important topic due to the challenges
    of misinformation propagation and slow verification of misleading information.
    Most previous work focus on the response posts on social media, ignoring the useful
    characteristics of involved users and their relations. In this paper, we propose
    a novel framework, Post-User Fusion Network (PESTO), which models the patterns
    of rumours from both post diffusion and user social networks. Specifically, we
    propose a novel Chronologically-masked Transformer architecture to model both
    temporal sequence and diffusion structure of rumours, and apply a Relational Graph
    Convolutional Network to model the social relations of involved users, with a
    fusion network based on self-attention mechanism to incorporate the two aspects.
    Additionally, two data augmentation techniques are leveraged to improve the robustness
    and accuracy of our models. Empirical results on four datasets of English tweets
    show the superiority of the proposed method.
  authors:
  - Erxue Min
  - Sophia Ananiadou
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_1
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'PESTO: A Post-User Fusion Network for Rumour Detection on Social Media'
  tldr: Rumour detection on social media is an important topic due to the challenges
    of misinformation propagation and slow verification of misleading information.
    Most previous work focus on the response posts on social media, ignoring the useful
    characteristics of involved users and their relations. In th
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Over the years, the task of predicting reader appreciation or literary
    quality has been the object of several studies, but it remains a challenging problem
    in quantitative literary studies and computational linguistics alike, as its definition
    can vary a lot depending on the genre, the adopted features and the annotation
    system. This paper attempts to evaluate the impact of sentiment arc modelling
    versus more classical stylometric features for user-ratings of novels. We run
    our experiments on a corpus of English language narrative literary fiction from
    the 19th and 20th century, showing that syntactic and surface-level features can
    be powerful for the study of literary quality, but can be outperformed by sentiment-characteristics
    of a text. '
  authors:
  - Yuri Bizzoni
  - Pascale Moreira
  - Mads Rosendahl Thomsen
  - Kristoffer Nielbo
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_4
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Sentimental Matters - Predicting Literary Quality by Sentiment Analysis and
    Stylometric Features
  tldr: Over the years, the task of predicting reader appreciation or literary quality
    has been the object of several studies, but it remains a challenging problem in
    quantitative literary studies and computational linguistics alike, as its definition
    can vary a lot depending on the genre, the adopted featu
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: "Aspect-based Sentiment Analysis (ABSA) is a fine-grained sentiment \n\
    analysis task which involves four elements from user-generated texts:\naspect\
    \ term, aspect category, opinion term, and sentiment polarity. \nMost computational\
    \ approaches focus on some of the ABSA sub-tasks\nsuch as tuple (aspect term,\
    \ sentiment polarity) or triplet (aspect term, \nopinion term, sentiment polarity)\
    \ extraction using either pipeline or \njoint modeling approaches. Recently, generative\
    \ approaches have \nbeen proposed to extract all four elements as (one or more)\
    \ quadruplets\nfrom text as a single task. In this work, we take a step further\
    \ and \npropose a unified framework for solving ABSA, and the associated sub-tasks\n\
    to improve the performance in few-shot scenarios. To this end, we fine-tune \n\
    a T5 model with instructional prompts in a multi-task learning fashion covering\
    \ \nall the sub-tasks, as well as the entire quadruple prediction task. In \n\
    experiments with multiple benchmark datasets, we show that the \nproposed multi-task\
    \ prompting approach brings performance boost \n(by absolute 8.29 F1) in the few-shot\
    \ learning setting."
  authors:
  - Siddharth Varia
  - Shuai Wang
  - Kishaloy Halder
  - Robert Vacareanu
  - Miguel Ballesteros
  - Yassine Benajiba
  - Neha Anna John
  - Rishita Anubhai
  - Smaranda Muresan
  - Dan Roth
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_8
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Instruction Tuning for Few-Shot Aspect-Based Sentiment Analysis
  tldr: "Aspect-based Sentiment Analysis (ABSA) is a fine-grained sentiment \nanalysis\
    \ task which involves four elements from user-generated texts:\naspect term, aspect\
    \ category, opinion term, and sentiment polarity. \nMost computational approaches\
    \ focus on some of the ABSA sub-tasks\nsuch as tuple (aspect term,"
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'In this work we use consumed text to infer Big-5 personality inventories
    using data we have collected from the social media platform Reddit. We test our
    model on two datasets, sampled from participants who consumed either fiction content
    ($N = 913$) or news content ($N = 213$). We show that state-of-the-art models
    from a similar task using authored text do not translate well to this task, with
    average correlations of $r=.06$ between the model''s predictions and ground-truth
    personality inventory dimensions. We propose an alternate method of generating
    average personality labels for each piece of text consumed, under which our model
    achieves correlations as high as $r=.34$ when predicting personality from the
    text being read. '
  authors:
  - Adam Sutton
  - Almog Simchon
  - Matthew Edwards
  - Stephan Lewandowsky
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_9
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'You Are What You Read: Inferring Personality From Consumed Textual Content'
  tldr: In this work we use consumed text to infer Big-5 personality inventories using
    data we have collected from the social media platform Reddit. We test our model
    on two datasets, sampled from participants who consumed either fiction content
    ($N = 913$) or news content ($N = 213$). We show that state-of
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Verbal deception has been studied in psychology, forensics, and computational
    linguistics for a variety of reasons, like understanding behaviour patterns, identifying
    false testimonies, and detecting deception in online communication. Varying motivations
    across research fields lead to differences in the domain choices to study and
    in the conceptualization of deception, making it hard to compare models and build
    robust deception detection systems for a given language. With this paper, we improve
    this situation by surveying available English deception datasets which include
    domains like social media reviews, court testimonials, opinion statements on specific
    topics, and deceptive dialogues from online strategy games. We consolidate these
    datasets into a single unified corpus. Based on this resource, we conduct a correlation
    analysis of linguistic cues of deception across datasets to understand the differences
    and perform cross-corpus modeling experiments which show that a cross-domain generalization
    is challenging to achieve. The unified deception corpus (UNIDECOR) can be obtained
    from   https://www.ims.uni-stuttgart.de/data/unidecor.
  authors:
  - Aswathy Velutharambath
  - Roman Klinger
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_10
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'UNIDECOR: A Unified Deception Corpus for Cross-Corpus Deception Detection'
  tldr: Verbal deception has been studied in psychology, forensics, and computational
    linguistics for a variety of reasons, like understanding behaviour patterns, identifying
    false testimonies, and detecting deception in online communication. Varying motivations
    across research fields lead to differences in
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The scarcity of annotated data is a major impediment to natural language
    processing (NLP) research in Bengali, a language that is considered low-resource.
    In particular, the health and medical domains suffer from a severe paucity of
    annotated data. Thus, this study aims to introduce BanglaSocialHealth, an annotated
    social media health corpus that provides sentence-level annotations of four distinct
    types of expression modes, namely narrative (NAR), informative (INF), suggestive
    (SUG), and inquiring (INQ) modes in Bengali. We provide details regarding the
    annotation procedures and report various statistics, such as the median and mean
    length of words in different sentence modes. Additionally, we apply classical
    machine learning (CML) classifiers and transformer-based language models to classify
    sentence modes. We find that most of the statistical properties are similar in
    different types of sentence modes. To determine the sentence mode, the transformer-based
    M-BERT model provides slightly better efficacy than the CML classifiers. Our developed
    corpus and analysis represent a much-needed contribution to Bengali NLP research
    in medical and health domains and have the potential to facilitate a range of
    downstream tasks, including question-answering, misinformation detection, and
    information retrieval.
  authors:
  - Salim Sazzed
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_11
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Discourse Mode Categorization of Bengali Social Media Health Text
  tldr: The scarcity of annotated data is a major impediment to natural language processing
    (NLP) research in Bengali, a language that is considered low-resource. In particular,
    the health and medical domains suffer from a severe paucity of annotated data.
    Thus, this study aims to introduce BanglaSocialHeal
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: "Paraphrase generation, a.k.a. paraphrasing, is a common and important\
    \ task in natural language processing. Emotional paraphrasing, which changes the\
    \ emotion embodied in a piece of text while preserving its meaning, has many potential\
    \ applications, including moderating online dialogues and preventing cyberbullying.\
    \ We introduce a new task of fine-grained emotional paraphrasing along emotion\
    \ gradients, that is, altering the emotional intensities of the paraphrases in\
    \ fine-grained settings following smooth variations in affective dimensions while\
    \ preserving the meaning of the original text. We reconstruct several widely used\
    \ paraphrasing datasets by augmenting the input and target texts with their fine-grained\
    \ emotion labels. Then, we propose a framework for emotion and sentiment guided\
    \ paraphrasing by leveraging pre-trained language models for conditioned text\
    \ generation. Extensive evaluation of the fine-tuned models suggests that including\
    \ fine-grained emotion labels in the paraphrase task significantly improves the\
    \ likelihood of obtaining high-quality paraphrases that reflect the desired emotions\
    \ while achieving consistently better scores in paraphrase metrics such as BLEU,\
    \ ROUGE, and METEOR.  \n"
  authors:
  - Justin Xie
  - Ameeta Agrawal
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_15
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Emotion and Sentiment Guided Paraphrasing
  tldr: Paraphrase generation, a.k.a. paraphrasing, is a common and important task
    in natural language processing. Emotional paraphrasing, which changes the emotion
    embodied in a piece of text while preserving its meaning, has many potential applications,
    including moderating online dialogues and preventing
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Work on emotion detection is often focused on textual data from i.e. Social
    Media. If multimodal data (i.e. speech) is analysed, the focus again is often
    placed on the transcription. This paper takes a closer look at how crucial acoustic
    information actually is for the recognition of emotions from multimodal data.
    To this end we use the IEMOCAP data, which is one of the larger data sets that
    provides transcriptions, audio recordings and manual emotion categorization. We
    build models for emotion classification using text-only, acoustics-only and combining
    both modalities in order to examine the influence of the various modalities on
    the final categorization. Our results indicate that using text-only models outperform
    acoustics-only models. But combining text-only and acoustic-only models improves
    the results. Additionally, we perform a qualitative analysis and find that a range
    of misclassifications are due to factors not related to the model, but to the
    data such as, recording quality, a challenging classification task and misclassifications
    that are unsurprising for humans.
  authors:
  - Nadine Probol
  - Margot Mieskes
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_16
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Emotions in Spoken Language - Do we need acoustics?
  tldr: Work on emotion detection is often focused on textual data from i.e. Social
    Media. If multimodal data (i.e. speech) is analysed, the focus again is often
    placed on the transcription. This paper takes a closer look at how crucial acoustic
    information actually is for the recognition of emotions from m
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The valence analysis of speakers' utterances or written posts helps to
    understand the activation and variations of the emotional state throughout the
    conversation. More recently, the concept of Emotion Carriers (EC) has been introduced
    to explain the emotion felt by the speaker and its manifestations. In this work,
    we investigate the natural inter-dependency of valence and ECs via a multi-task
    learning approach. We experiment with Pre-trained Language Models (PLM) for single-task,
    two-step, and joint settings for the valence and EC prediction tasks. We compare
    and evaluate the performance of generative (GPT-2) and discriminative (BERT) architectures
    in each setting. We observed that providing the ground truth label of one task
    improves the prediction performance of the models in the other task. We further
    observed that the discriminative model achieves the best trade-off of valence
    and EC prediction tasks in the joint prediction setting. As a result, we attain
    a single model that performs both tasks, thus, saving computation resources at
    training and inference times.
  authors:
  - Gabriel Roccabruna
  - Seyed Mahed Mousavi
  - Giuseppe Riccardi
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_17
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Understanding Emotion Valence is a Joint Deep Learning Task
  tldr: The valence analysis of speakers' utterances or written posts helps to understand
    the activation and variations of the emotional state throughout the conversation.
    More recently, the concept of Emotion Carriers (EC) has been introduced to explain
    the emotion felt by the speaker and its manifestation
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'We present the Verifee dataset: a multimodal dataset of news articles
    with fine-grained trustworthiness annotations. We bring a diverse set of researchers
    from social, media, and computer sciences aboard to study this interdisciplinary
    problem holistically and develop a detailed methodology that assesses the texts
    through the lens of editorial transparency, journalist conventions, and objective
    reporting while penalizing manipulative techniques. We collect over $10,000$ annotated
    articles from $60$ Czech online news sources. Each item is categorized into one
    of the $4$ proposed classes on the credibility spectrum -- ranging from entirely
    trustworthy articles to deceptive ones -- and annotated of its manipulative attributes.
    We fine-tune prominent sequence-to-sequence language models for the trustworthiness
    classification task on our dataset and report the best F-1 score of $0.53$. We
    open-source the dataset, annotation methodology, and annotators'' instructions
    in full length at https://www.verifee.ai/research/ to enable easy build-up work.'
  authors:
  - Matyas Bohacek
  - Michal Bravansky
  - "Filip Trhl\xEDk"
  - Vaclav Moravec
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_18
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Czech-ing the News: Article Trustworthiness Dataset for Czech'
  tldr: 'We present the Verifee dataset: a multimodal dataset of news articles with
    fine-grained trustworthiness annotations. We bring a diverse set of researchers
    from social, media, and computer sciences aboard to study this interdisciplinary
    problem holistically and develop a detailed methodology that ass'
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Manipulated news online is a growing problem which necessitates the use
    of automated systems to curtail its spread. We argue that while misinformation
    and disinformation detection have been studied, there has been a lack of investment
    in the important open challenge of detecting harmful agendas in news articles;
    identifying harmful agendas is critical to flag news campaigns with the greatest
    potential for real world harm. Moreover, due to real concerns around censorship,
    harmful agenda detectors must be interpretable to be effective. In this work,
    we propose this new task and release a dataset, \textsc{NewsAgendas}, of annotated
    news articles for agenda identification. We show how interpretable systems can
    be effective on this task and demonstrate that they can perform comparably to
    black-box models.
  authors:
  - Melanie Subbiah
  - Amrita Bhattacharjee
  - Yilun Hua
  - Tharindu Kumarage
  - Huan Liu
  - Kathleen McKeown
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_21
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Towards Detecting Harmful Agendas in News Articles
  tldr: Manipulated news online is a growing problem which necessitates the use of
    automated systems to curtail its spread. We argue that while misinformation and
    disinformation detection have been studied, there has been a lack of investment
    in the important open challenge of detecting harmful agendas in n
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Sentiment Analysis is an important task for analysing online content across
    languages for tasks such as content moderation and opinion mining. Though a significant
    amount of resources are available for Sentiment Analysis in several Indian languages,
    there do not exist any large-scale, open-access corpora for Gujarati. Our paper
    presents and describes the Gujarati Sentiment Analysis Corpus (GSAC), which has
    been sourced from Twitter and manually annotated by native speakers of the language.
    We describe in detail our collection and annotation processes and conduct extensive
    experiments on our corpus to provide reliable baselines for future work using
    our dataset.
  authors:
  - Monil Gokani
  - Radhika Mamidi
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_22
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'GSAC: A Gujarati Sentiment Analysis Corpus from Twitter'
  tldr: Sentiment Analysis is an important task for analysing online content across
    languages for tasks such as content moderation and opinion mining. Though a significant
    amount of resources are available for Sentiment Analysis in several Indian languages,
    there do not exist any large-scale, open-access co
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: "While deep learning models have greatly improved the performance of many\
    \ tasks related to sentiment analysis and classification, they are often criticized\
    \ for being untrustworthy due to their black-box nature. As a result, numerous\
    \ explainability techniques have been proposed to better understand the model\
    \ predictions and to improve the deep learning models. \nIn this work, we introduce\
    \ InfoBarometer, the first benchmark for examining interpretable methods related\
    \ to sentiment analysis in the German automotive sector based on online news.\
    \ Each news article in our dataset is annotated w.r.t. overall sentiment (i.e.,\
    \ positive, negative and neutral), the target of the sentiment (focusing on innovation-related\
    \ topics such as e.g. electromobility) and the rationales, i.e., textual explanations\
    \ for the sentiment label  that can be leveraged during both training and evaluation.\n\
    For this research, we compare different state-of-the-art approaches to perform\
    \ sentiment analysis and observe that even models that perform very well in classification\
    \ do not score high on explainability metrics like model plausibility and faithfulness.\
    \ \nWe calculated the polarity scores for the best method BERT and got an F-score\
    \ of 73.6.  \nMoreover, we evaluated different interpretability algorithms (LIME,\
    \ SHAP, Integrated Gradients, Saliency) based on explicitly marked rationales\
    \ by human annotators quantitatively and qualitatively.\nOur experiments demonstrate\
    \ that the textual explanations often do not agree with human interpretations,\
    \ and rarely help to justify the models decision. However, local and global features\
    \ provide useful insights to help  uncover spurious features in the model and\
    \ biases within the dataset. We intend to make our dataset public for other researchers"
  authors:
  - Andrea Zielinski
  - Calvin Spolwind
  - Henning Kroll
  - Anna Grimm
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_23
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: A Dataset for Explainable Sentiment Analysis in the German Automotive Industry
  tldr: While deep learning models have greatly improved the performance of many tasks
    related to sentiment analysis and classification, they are often criticized for
    being untrustworthy due to their black-box nature. As a result, numerous explainability
    techniques have been proposed to better understand th
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Opinion summarisation is a task that aims to condense the information
    presented in the source documents while retaining the core message and opinions.
    A summary that only represents the majority opinions will leave the minority opinions
    unrepresented in the summary. In this paper, we use the stance towards a certain
    target as an opinion. We study bias in opinion summarisation from the perspective
    of opinion diversity, which measures whether the model generated summary can cover
    a diverse set of opinions. In addition, we examine opinion similarity, a measure
    of how closely related two opinions are in terms of their stance on a given topic,
    and its relationship with opinion diversity. Through the lense of stances towards
    a topic, we examine opinion diversity and similarity using three debatable topics
    under COVID-19. Experimental results on these topics revealed that a higher degree
    of similarity of opinions did not indicate good diversity or fairly cover the
    various opinions originally presented in the source documents. We found that BART
    and ChatGPT can better capture diverse opinions presented in the source documents.
  authors:
  - Nannan Huang
  - Lin Tian
  - Haytham Fayek
  - Xiuzhen Zhang
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_24
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Examining Bias in Opinion Summarisation through the Perspective of Opinion
    Diversity
  tldr: 'Opinion summarisation is a task that aims to condense the information presented
    in the source documents while retaining the core message and opinions. A summary
    that only represents the majority opinions will leave the minority opinions unrepresented
    in the summary. In this paper, we use the stance '
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Unsupervised text style transfer is a challenging task that aims to alter
    the stylistic attributes of a given text without affecting its original content.
    One of the methods to achieve this is controllable style transfer, which allows
    for the control of the degree of style transfer. However, an issue encountered
    with controllable style transfer is the instability of transferred text fluency
    when the degree of the style transfer changes. To address this problem, we propose
    a novel approach that incorporates additional syntax parsing information during
    style transfer. By leveraging the syntactic information, our model is guided to
    generate natural sentences that effectively reflect the desired style while maintaining
    fluency. Experimental results show that our method achieves robust performance
    and improved fluency compared to previous controllable style transfer methods.
  authors:
  - Ji-Eun Han
  - Kyung-Ah Sohn
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_27
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Fluency Matters! Controllable Style Transfer with Syntax Guidance
  tldr: Unsupervised text style transfer is a challenging task that aims to alter
    the stylistic attributes of a given text without affecting its original content.
    One of the methods to achieve this is controllable style transfer, which allows
    for the control of the degree of style transfer. However, an issu
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper presents a novel framework for quantitatively evaluating the
    interactive ChatGPT model in the context of  suicidality assessment from social
    media posts, utilizing the University of Maryland Reddit suicidality dataset.
    We conduct a technical evaluation of ChatGPT''s performance on this task using
    Zero-Shot and Few-Shot experiments and compare its results with those of two fine-tuned
    transformer-based models. Additionally, we investigate the impact of different
    temperature parameters on ChatGPT''s response generation and discuss the optimal
    temperature based on the inconclusiveness rate of ChatGPT. Our results indicate
    that while ChatGPT attains considerable accuracy in this task, transformer-based
    models fine-tuned on human-annotated datasets exhibit superior performance. Moreover,
    our  analysis sheds light on how adjusting the ChatGPT''s hyperparameters can
    improve its ability to assist mental health professionals in this critical task. '
  authors:
  - Hamideh Ghanadian
  - Isar Nejadgholi
  - Hussein Al Osman
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_28
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'ChatGPT for Suicide Risk Assessment on Social Media: Quantitative Evaluation
    of Model Performance, Potentials and Limitations'
  tldr: This paper presents a novel framework for quantitatively evaluating the interactive
    ChatGPT model in the context of  suicidality assessment from social media posts,
    utilizing the University of Maryland Reddit suicidality dataset. We conduct a
    technical evaluation of ChatGPT's performance on this tas
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Domain adaptation is an important and widely studied problem in natural
    language processing. A large body of literature tries to solve this problem by
    adapting models trained on the source domain to the target domain. In this paper,
    we instead solve this problem from a dataset perspective. We modify the source
    domain dataset with simple lexical transformations to reduce the domain shift
    between the source dataset distribution and the target dataset distribution. We
    find that models trained on the transformed source domain dataset performs significantly
    better than zero-shot models. Using our proposed transformations to convert standard
    English to tweets, we reach an unsupervised part-of-speech (POS) tagging accuracy
    of 92.14% (from 81.54% zero shot accuracy), which is only slightly below the supervised
    performance of 94.45%. We also use our proposed transformations to synthetically
    generate tweets and augment the Twitter dataset to achieve state-of-the-art performance
    for POS tagging.
  authors:
  - Akshat Gupta
  - Xiaomo Liu
  - Sameena Shah
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_29
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Unsupervised Domain Adaptation using Lexical Transformations and Label Injection
    for Twitter Data
  tldr: Domain adaptation is an important and widely studied problem in natural language
    processing. A large body of literature tries to solve this problem by adapting
    models trained on the source domain to the target domain. In this paper, we instead
    solve this problem from a dataset perspective. We modify
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Consumers of services and products exhibit a wide range of behaviors on
    social networks when they are dissatisfied. In this paper, we consider three types
    of cynical expressions negative feelings, specific reasons, and attitude of being
    right and annotate a corpus of 3189 comments in Spanish on car analysis channels
    from YouTube. We evaluate both token classification and text classification settings
    for this problem, and compare performance of different pre-trained models including
    BETO, SpanBERTa, Multilingual Bert, and RoBERTuito. The results show that models
    achieve performance above 0.8 F1 for all types of cynical expressions in the text
    classification setting, but achieve lower performance (around 0.6-0.7 F1) for
    the harder token classification setting.
  authors:
  - Samuel Gonzalez-Lopez
  - Steven Bethard
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_30
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Transformer-based cynical expression detection in a corpus of Spanish YouTube
    reviews
  tldr: 'Consumers of services and products exhibit a wide range of behaviors on social
    networks when they are dissatisfied. In this paper, we consider three types of
    cynical expressions negative feelings, specific reasons, and attitude of being
    right and annotate a corpus of 3189 comments in Spanish on car '
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Emotions are experienced and expressed differently across the world. In
    order to use Large Language Models (LMs) for multilingual tasks that require emotional
    sensitivity, LMs must reflect this cultural variation in emotion. In this study,
    we investigate whether the widely-used multilingual LMs in 2023 reflect differences
    in emotional expressions across cultures and languages. We find that embeddings
    obtained from LMs (e.g., XLM-RoBERTa) are Anglocentric, and generative LMs (e.g.,
    ChatGPT) reflect Western norms, even when responding to prompts in other languages.
    Our results show that multilingual LMs do not successfully learn the culturally
    appropriate nuances of emotion and we highlight possible research directions towards
    correcting this.
  authors:
  - Shreya Havaldar
  - Bhumika Singhal
  - Sunny Rai
  - Langchen Liu
  - Sharath Chandra Guntuku
  - Lyle Ungar
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_31
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Multilingual Language Models are not Multicultural: A Case Study in Emotion'
  tldr: Emotions are experienced and expressed differently across the world. In order
    to use Large Language Models (LMs) for multilingual tasks that require emotional
    sensitivity, LMs must reflect this cultural variation in emotion. In this study,
    we investigate whether the widely-used multilingual LMs in 2
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: As the e-commerce market continues to expand and online transactions proliferate,
    customer reviews have emerged as a critical element in shaping the purchasing
    decisions of prospective buyers. Previous studies have endeavored to identify
    key aspects of customer reviews through the development of sentiment analysis
    models and topic models. However, extracting specific dissatisfaction factors
    remains a challenging task. In this study, we delineate the pain point detection
    problem and propose Painsight, an unsupervised framework for automatically extracting
    distinct dissatisfaction factors from customer reviews without relying on ground
    truth labels. Painsight employs pre-trained language models to construct sentiment
    analysis and topic models, leveraging attribution scores derived from model gradients
    to extract dissatisfaction factors. Upon application of the proposed methodology
    to customer review data spanning five product categories, we successfully identified
    and categorized dissatisfaction factors within each group, as well as isolated
    factors for each type. Notably, Painsight outperformed benchmark methods, achieving
    substantial performance enhancements and exceptional results in human evaluations.
  authors:
  - Yukyung Lee
  - Jaehee Kim
  - Doyoon Kim
  - Yookyung Kho
  - Younsun Kim
  - Pilsung Kang
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_32
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Painsight: An Extendable Opinion Mining Framework for Detecting Pain Points
    Based on Online Customer Reviews'
  tldr: As the e-commerce market continues to expand and online transactions proliferate,
    customer reviews have emerged as a critical element in shaping the purchasing
    decisions of prospective buyers. Previous studies have endeavored to identify
    key aspects of customer reviews through the development of sen
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Emotion Recognition in Conversations (ERC) has been gaining increasing
    importance as conversational agents become more and more common. Recognizing emotions
    is key for effective communication, being a crucial component in the development
    of effective and empathetic conversational agents. Knowledge and understanding
    of the conversational context are extremely valuable for identifying the emotions
    of the interlocutor. We thus approach Emotion Recognition in Conversations leveraging
    the conversational context, i.e., taking into attention previous conversational
    turns. The usual approach to model the conversational context has been to produce
    context-independent representations of each utterance and subsequently perform
    contextual modeling of these. Here we propose context-dependent embedding representations
    of each utterance by leveraging the contextual representational power of pre-trained
    transformer language models. In our approach, we feed the conversational context
    appended to the utterance to be classified as input to the RoBERTa encoder, to
    which we append a simple classification module, thus discarding the need to deal
    with context after obtaining the embeddings since these constitute already an
    efficient representation of such context. We also investigate how the number of
    introduced conversational turns influences our model performance. The effectiveness
    of our approach is validated on the open-domain DailyDialog dataset and on the
    task-oriented EmoWOZ dataset.

    '
  authors:
  - "Patr\xEDcia Pereira"
  - Helena Moniz
  - Isabel Dias
  - Joao Paulo Carvalho
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_33
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Context-Dependent Embedding Utterance Representations for Emotion Recognition
    in Conversations
  tldr: Emotion Recognition in Conversations (ERC) has been gaining increasing importance
    as conversational agents become more and more common. Recognizing emotions is
    key for effective communication, being a crucial component in the development
    of effective and empathetic conversational agents. Knowledge a
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Social media provide a rich source of data that can be mined and used
    for a wide variety of research purposes. However, annotating this data can be
    expensive, yet necessary for state-of-the-art pre-trained language models to achieve
    high prediction performance. Therefore, we combine pool-based active learning
    based on prediction uncertainty (an established method for reducing annotation
    costs) with unsupervised task adaptation through Masked Language Modeling (MLM).
    The results on three different datasets (two social media corpora, one benchmark
    dataset) show that task adaptation significantly improves results and that with
    only a fraction of the available training data, this approach reaches similar
    F1-scores as those achieved by an upper-bound baseline model fine-tuned on all
    training data. We hereby contribute to the scarce corpus of research on active
    learning with pre-trained language models and propose a cost-efficient annotation
    sampling and fine-tuning approach that can be applied to a wide variety of tasks
    and datasets.
  authors:
  - Jens Lemmens
  - Walter Daelemans
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_34
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Combining Active Learning and Task Adaptation with BERT for Cost-Effective
    Annotation of Social Media Datasets
  tldr: Social media provide a rich source of data that can be mined and used for
    a wide variety of research purposes. However, annotating this data can be expensive,
    yet necessary for state-of-the-art pre-trained language models to achieve high
    prediction performance. Therefore, we combine pool-based activ
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In this paper, we leverage the GPT-3.5 language model both using the Chat-GPT
    API interface and the GPT-3.5 API interface to generate realistic examples of
    anti-vaccination tweets in Dutch with the aim of augmenting an imbalanced multi-label
    vaccine hesitancy argumentation classification dataset. In line with previous
    research, we devise a prompt that, on the one hand, instructs the model to generate
    realistic examples based on the gold standard dataset and, on the other hand,
    to assign multiple pseudo-labels (or a single pseudo-label) to the generated instances.
    We then augment our gold standard data with the generated examples and evaluate
    the impact thereof in a cross-validation setting with several state-of-the-art
    Dutch large language models. This augmentation technique predominantly shows improvements
    in F1 for classifying underrepresented classes while increasing the overall recall,
    paired with a slight decrease in precision for more common classes. Furthermore,
    we examine how well the synthetic data generalises to human data in the classification
    task. To our knowledge, we are the first to utilise Chat-GPT and GPT-3.5 for augmenting
    a Dutch multi-label dataset classification task.
  authors:
  - Jens Van Nooten
  - Walter Daelemans
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_35
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Improving Dutch Vaccine Hesitancy Monitoring via Multi-Label Data Augmentation
    with GPT-3.5
  tldr: In this paper, we leverage the GPT-3.5 language model both using the Chat-GPT
    API interface and the GPT-3.5 API interface to generate realistic examples of
    anti-vaccination tweets in Dutch with the aim of augmenting an imbalanced multi-label
    vaccine hesitancy argumentation classification dataset. In
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper introduces the first emotion-annotated dataset for the Dari
    variant of Persian spoken in Afghanistan. The LetHerLearn dataset contains 7,600
    tweets posted in reaction to the Taliban''s ban of women''s rights to education
    in 2022 and has been manually annotated according to Ekman''s emotion categories.
    We here detail the data collection and annotation process, present relevant dataset
    statistics as well as initial experiments on the resulting dataset, benchmarking
    a number of different neural architectures for the task of Dari emotion classification. '
  authors:
  - Mohammad Ali Hussiny
  - "Lilja \xD8vrelid"
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_37
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Emotion Analysis of Tweets Banning Education in Afghanistan
  tldr: This paper introduces the first emotion-annotated dataset for the Dari variant
    of Persian spoken in Afghanistan. The LetHerLearn dataset contains 7,600 tweets
    posted in reaction to the Taliban's ban of women's rights to education in 2022
    and has been manually annotated according to Ekman's emotion c
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The prevalence of hate speech on online platforms has become a pressing
    concern for society, leading to increased attention towards detecting hate speech.
    Prior work in this area has primarily focused on identifying hate speech at the
    utterance level that reflects the complex nature of hate speech. In this paper,
    we propose a targeted and efficient approach to identifying hate speech by detecting
    slurs at the lexical level using contextualized word embeddings. We hypothesize
    that slurs have a systematically different representation than their neutral counterparts,
    making them identifiable through existing methods for discovering semantic dimensions
    in word embeddings. The results demonstrate the effectiveness of our approach
    in predicting slurs, confirming linguistic theory that the meaning of slurs is
    stable across contexts. Our robust hate dimension approach for slur identification
    offers a promising solution to tackle a smaller yet crucial piece of the complex
    puzzle of hate speech detection.
  authors:
  - Sanne Hoeken
  - "Sina Zarrie\xDF"
  - Ozge Alacam
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_38
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Identifying Slurs and Lexical Hate Speech via Light-Weight Dimension Projection
    in Embedding Space
  tldr: 'The prevalence of hate speech on online platforms has become a pressing concern
    for society, leading to increased attention towards detecting hate speech. Prior
    work in this area has primarily focused on identifying hate speech at the utterance
    level that reflects the complex nature of hate speech. '
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: "The popularity of sentiment and emotion analysis has lead to an explosion\
    \ of datasets, approaches, and papers. However, these are often tested in optimal\
    \ settings, where plentiful training and development data are available, and compared\
    \ mainly with recent state-of-the-art models that have been similarly evaluated.\
    \ \n\nIn this paper, we instead present a systematic comparison of sentiment and\
    \ emotion classification methods, ranging from rule- and dictionary-based methods\
    \ to recently proposed few-shot and prompting methods with large language models.\
    \ We test these methods in-domain, out-of-domain, and in cross-lingual settings\
    \ and find that in low-resource settings, rule- and dictionary-based methods perform\
    \ as well or better than few-shot and prompting methods, especially for emotion\
    \ classification. Zero-shot cross-lingual approaches, however, still outperform\
    \ in-language dictionary induction.\n"
  authors:
  - Jeremy Barnes
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_39
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Sentiment and Emotion Classification in Low-resource Settings
  tldr: The popularity of sentiment and emotion analysis has lead to an explosion
    of datasets, approaches, and papers. However, these are often tested in optimal
    settings, where plentiful training and development data are available, and compared
    mainly with recent state-of-the-art models that have been simi
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'The problem of subjectivity detection is often approached as a preparatory
    binary task for sentiment analysis, despite the fact that theoretically subjectivity
    is often defined as a matter of degree. In this work, we approach subjectivity
    analysis as a regression task and test the efficiency of a transformer RoBERTa
    model in annotating subjectivity of online news, including news from social media,
    based on a small subset of human-labeled training data. The results of experiments
    comparing our model to an existing rule-based subjectivity regressor and a state-of-the-art
    binary classifier reveal that: 1) our model highly correlates with the human subjectivity
    ratings and outperforms the widely used rule-based "pattern" subjectivity regressor
    (De Smedt and Daelemans, 2012); 2) our model performs well as a binary classifier
    and generalizes to the benchmark subjectivity dataset (Pang and Lee, 2004); 3)
    in contrast, state-of-the-art classifiers trained on the benchmark dataset show
    catastrophic performance on our human-labeled data. The results bring to light
    the issues of the gold standard subjectivity dataset, and the models trained on
    it, which seem to distinguish between the origin/style of the texts rather than
    subjectivity as perceived by human English speakers.'
  authors:
  - Elena Savinova
  - Fermin Moscoso Del Prado
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_40
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: "Analyzing Subjectivity Using a Transformer-Based Regressor Trained on Na\xEF\
    ve Speakers\u2019 Judgements"
  tldr: The problem of subjectivity detection is often approached as a preparatory
    binary task for sentiment analysis, despite the fact that theoretically subjectivity
    is often defined as a matter of degree. In this work, we approach subjectivity
    analysis as a regression task and test the efficiency of a tr
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: "In this paper we investigate potential bias in fine-tuned transformer\
    \ models for irony detection. Bias is defined in this research as spurious associations\
    \ between word n-grams and class labels, that can cause the system to rely too\
    \ much on superficial cues and miss the essence of the irony. For this purpose,\
    \ we looked for correlations between class labels and words that are prone to\
    \ trigger irony, such as positive adjectives, intensifiers and topical nouns.\
    \ Additionally, we investigate our irony model\u2019s predictions before and after\
    \ manipulating the data set through irony trigger replacements. We further support\
    \ these insights with state-of-the-art explainability techniques (Layer Integrated\
    \ Gradients, Discretized Integrated Gradients and Layer-wise Relevance Propagation).\
    \ Both approaches confirm the hypothesis that transformer models generally encode\
    \ correlations between positive sentiments and ironic texts, with even higher\
    \ correlations between vividly expressed sentiment and irony. Based on these insights,\
    \ we implemented a number of modification strategies to enhance the robustness\
    \ of our irony classifier."
  authors:
  - Aaron Maladry
  - Els Lefever
  - Cynthia Van Hee
  - Veronique Hoste
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_41
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'A Fine Line Between Irony and Sincerity: Identifying Bias in Transformer
    Models for Irony Detection'
  tldr: In this paper we investigate potential bias in fine-tuned transformer models
    for irony detection. Bias is defined in this research as spurious associations
    between word n-grams and class labels, that can cause the system to rely too much
    on superficial cues and miss the essence of the irony. For thi
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: "Humor is a central aspect of human communication that has not been solved\
    \ for artificial agents so far. Large language models (LLMs) are increasingly\
    \ able to capture implicit and contextual information. Especially, OpenAI\u2019\
    s ChatGPT recently gained immense public attention. The GPT3-based model almost\
    \ seems to communicate on a human level and can even tell jokes. Humor is an essential\
    \ component of human communication. But is ChatGPT really funny?\n\nWe put ChatGPT\u2019\
    s sense of humor to the test. In a series of exploratory experiments around jokes,\
    \ i.e., generation, explanation, and detection, we seek to understand ChatGPT\u2019\
    s capability to grasp and reproduce human humor. Since the model itself is not\
    \ accessible, we applied prompt-based experiments. \n\nOur empirical evidence\
    \ indicates that jokes are not hard-coded but mostly also not newly generated\
    \ by the model. Over 90% of 1008 generated jokes were the same 25 Jokes. The system\
    \ accurately explains valid jokes but also comes up with fictional explanations\
    \ for invalid jokes. Joke-typical characteristics can mislead ChatGPT in the classification\
    \ of jokes. ChatGPT has not solved computational humor yet but it can be a big\
    \ leap toward \u201Cfunny\u201D machines."
  authors:
  - Sophie Jentzsch
  - Kristian Kersting
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_43
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: ChatGPT is fun, but it is not funny! Humor is still challenging Large Language
    Models
  tldr: "Humor is a central aspect of human communication that has not been solved\
    \ for artificial agents so far. Large language models (LLMs) are increasingly\
    \ able to capture implicit and contextual information. Especially, OpenAI\u2019\
    s ChatGPT recently gained immense public attention. The GPT3-based model almos"
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Recent advances in the development of large Pretrained Language Models,
    such as GPT, BERT and Bloom, have achieved remarkable performance on a wide range
    of different NLP tasks. However, when used for text generation tasks, these models
    still have limitations when it comes to controlling the content and style of the
    generated text, often producing content that is incorrect, irrelevant, or inappropriate
    in the context of a given task. In this survey paper, we explore methods for controllable
    text generation with a focus on sentiment control. We systematically collect papers
    from the ACL Anthology, create a categorisation scheme based on different control
    techniques and controlled attributes, and use the scheme to categorise and compare
    methods. The result is a detailed and comprehensive overview of state-of-the-art
    techniques for sentiment-controlled text generation categorised on the basis of
    how the control is implemented and what attributes are controlled and providing
    a clear idea of their relative strengths and weaknesses.
  authors:
  - Michela Lorandi
  - Anya Belz
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_44
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'How to Control Sentiment in Text Generation: A Survey of the State-of-the-Art
    in Sentiment-Control Techniques'
  tldr: Recent advances in the development of large Pretrained Language Models, such
    as GPT, BERT and Bloom, have achieved remarkable performance on a wide range of
    different NLP tasks. However, when used for text generation tasks, these models
    still have limitations when it comes to controlling the content
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Emotional reactions to Online Social Network posts have recently gained
    importance in the study of the online ecosystem. Prior to post publication, the
    number of received reactions can be predicted based on either the textual content
    of the post or the related metadata. However, existing approaches suffer from
    both the lack of semantic-aware language understanding models and the limited
    explainability of the prediction models. To overcome these issues, we present
    a new transformer-based method to predict the number of emotional reactions of
    different types to social posts. It leverages the attention mechanism to capture
    arbitrary semantic textual relations neglected by prior works. Furthermore, it
    also provides end-users with textual explanations of the predictions. The results
    achieved on a large collection of Facebook posts confirm the applicability of
    the presented methodology. '
  authors:
  - Irene Benedetto
  - Moreno La Quatra
  - Luca Cagliero
  - Luca Vassio
  - Martino Trevisan
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_45
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Transformer-based Prediction of Emotional Reactions to Online Social Network
    Posts
  tldr: 'Emotional reactions to Online Social Network posts have recently gained importance
    in the study of the online ecosystem. Prior to post publication, the number of
    received reactions can be predicted based on either the textual content of the
    post or the related metadata. However, existing approaches '
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Monolinguals make up a minority of the world''s speakers, and yet most
    language technologies lag behind in handling linguistic behaviours produced by
    bilingual and multilingual speakers. A commonly observed phenomenon in such communities
    is code-mixing, which is prevalent on social media, and thus requires attention
    in NLP research. In this work, we look into the ability of pretrained language
    models to handle code-mixed data, with a focus on the impact of languages present
    in pretraining on the downstream performance of the model as measured on the task
    of sentiment analysis. Ultimately, we find that the pretraining language has little
    effect on performance when the model sees code-mixed data during downstream finetuning.
    We also evaluate the models on code-mixed data in a zero-shot setting, after task-specific
    finetuning on a monolingual dataset. We find that this brings out differences
    in model performance that can be attributed to the pretraining languages. We present
    a thorough analysis of these findings that also looks at model performance based
    on the composition of participating languages in the code-mixed datasets. '
  authors:
  - Kushal Tatariya
  - Heather Lent
  - Miryam De Lhoneux
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_48
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Transfer Learning for Code-Mixed Data: Do Pretraining Languages Matter?'
  tldr: Monolinguals make up a minority of the world's speakers, and yet most language
    technologies lag behind in handling linguistic behaviours produced by bilingual
    and multilingual speakers. A commonly observed phenomenon in such communities
    is code-mixing, which is prevalent on social media, and thus re
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This study evaluated ChatGPT''s ability to understand causal language
    in science papers and news by testing its accuracy in a task of labeling the strength
    of a claim as causal, conditional causal,  correlational, or no relationship.
    The results show that ChatGPT is still behind the existing fine-tuned BERT models
    by a large margin. ChatGPT also had difficulty understanding conditional causal
    claims mitigated by hedges. However, its weakness may be utilized to improve the
    clarity of human annotation guideline. Chain-of-Thoughts were faithful and helpful
    for improving prompt performance, but finding the optimal prompt is difficult
    with inconsistent results and the lack of effective method to establish cause-effect
    between prompts and outcomes, suggesting caution when generalizing prompt engineering
    results across tasks or models.

    '
  authors:
  - Yuheun Kim
  - Lu Guo
  - Bei Yu
  - Yingya Li
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_51
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Can ChatGPT Understand Causal Language in Science Claims?
  tldr: This study evaluated ChatGPT's ability to understand causal language in science
    papers and news by testing its accuracy in a task of labeling the strength of
    a claim as causal, conditional causal,  correlational, or no relationship. The
    results show that ChatGPT is still behind the existing fine-tun
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Very large language models (LLMs) perform extremely well on a spectrum
    of NLP tasks in a zero-shot setting. However, little is known about their performance
    on human-level NLP problems which rely on understanding psychological concepts,
    such as assessing personality traits. In this work, we investigate the zero-shot
    ability of GPT-3 to estimate the Big 5 personality traits from users' social media
    posts. Through a set of systematic experiments, we find that zero-shot GPT-3 performance
    is somewhat close to an existing pre-trained SotA for broad classification upon
    injecting knowledge about the trait in the prompts. However, when prompted to
    provide fine-grained classification, its performance drops to close to a simple
    most frequent class (MFC) baseline. We further analyze where GPT-3 performs better,
    as well as worse, than a pretrained lexical model, illustrating systematic errors
    that suggest ways to improve LLMs on human-level NLP tasks. The code for this
    project is available on Github.
  authors:
  - Adithya V Ganesan
  - Yash Kumar Lal
  - August Nilsson
  - H. Schwartz
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_52
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Systematic Evaluation of GPT-3 for Zero-Shot Personality Estimation
  tldr: Very large language models (LLMs) perform extremely well on a spectrum of
    NLP tasks in a zero-shot setting. However, little is known about their performance
    on human-level NLP problems which rely on understanding psychological concepts,
    such as assessing personality traits. In this work, we investig
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Emerging psychopathology studies are showing that patterns of changes
    in emotional state --- emotion dynamics --- are associated with overall well-being
    and mental health. More recently, there has been some work in tracking emotion
    dynamics through one''s utterances, allowing for data to be collected on a larger
    scale across time and people. However, several questions about how emotion dynamics
    change with age, especially in children, and when determined through children''s
    writing, remain unanswered. In this work, we use both a lexicon and a machine
    learning based approach to quantify characteristics of emotion dynamics determined
    from poems written by children of various ages. We show that both approaches point
    to similar trends: consistent increasing intensities for some emotions (e.g.,
    anger, fear, joy, sadness, arousal, and dominance) with age and a consistent decreasing
    valence with age. We also find increasing emotional variability, rise rates (i.e.,
    emotional reactivity), and recovery rates (i.e., emotional regulation) with age.
    These results act as a useful baselines for further research in how patterns of
    emotions expressed by children change with age, and their association with mental
    health.'
  authors:
  - Daniela Teodorescu
  - Alona Fyshe
  - Saif Mohammad
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_55
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Utterance Emotion Dynamics in Children''s Poems: Emotional Changes Across
    Age'
  tldr: Emerging psychopathology studies are showing that patterns of changes in emotional
    state --- emotion dynamics --- are associated with overall well-being and mental
    health. More recently, there has been some work in tracking emotion dynamics through
    one's utterances, allowing for data to be collected
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In this paper, we present a dataset of subjective views (beliefs and attitudes)
    held by individuals or groups. We analyze the usefulness of the dataset by training
    a neural classifier that identifies belief-containing sentences that are relevant
    for our broader project of interest---scientific modeling of complex systems.
    We also explore and discuss difficulties related to annotation of subjective views  and
    propose ways of addressing them.
  authors:
  - Maria Alexeeva
  - Caroline Hyland
  - Keith Alcock
  - Allegra A. Beal Cohen
  - Hubert Kanyamahanga
  - Isaac Kobby Anni
  - Mihai Surdeanu
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_57
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Annotating and Training for Population Subjective Views
  tldr: In this paper, we present a dataset of subjective views (beliefs and attitudes)
    held by individuals or groups. We analyze the usefulness of the dataset by training
    a neural classifier that identifies belief-containing sentences that are relevant
    for our broader project of interest---scientific model
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Stance Detection is the task of identifying the position of an author
    of a text towards an issue or a target. Previous studies on Stance Detection indicate
    that the existing systems are non-robust to the variations and errors in input
    sentences. Our proposed methodology uses Contrastive Learning to learn sentence
    representations by bringing semantically similar sentences and sentences implying
    the same stance closer to each other in the embedding space. We compare our approach
    to a pretrained transformer model directly finetuned with the stance datasets.
    We use char-level and word-level adversarial perturbation attacks to measure the
    resilience of the models and we show that our approach achieves better performances
    and is more robust to the different adversarial perturbations introduced to the
    test data. The results indicate that our approach performs better on small-sized
    and class-imbalanced stance datasets.
  authors:
  - Udhaya Kumar Rajendran
  - Amine Trabelsi
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_60
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Exploration of Contrastive Learning Strategies toward more Robust Stance
    Detection
  tldr: 'Stance Detection is the task of identifying the position of an author of
    a text towards an issue or a target. Previous studies on Stance Detection indicate
    that the existing systems are non-robust to the variations and errors in input
    sentences. Our proposed methodology uses Contrastive Learning to '
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Social media is an extremely potent tool for influencing public opinion,
    particularly during important events such as elections, pandemics, and national
    conflicts. Emotions are a crucial aspect of this influence, but detecting them
    accurately in the political domain is a significant challenge due to the lack
    of suitable emotion labels and training datasets. In this paper, we present a
    generalized approach to emotion detection that can be adapted to the political
    domain with minimal performance sacrifice. Our approach is designed to be easily
    integrated into existing models without the need for additional training or fine-tuning.
    We demonstrate the zero-shot and few-shot performance of our model on the 2017
    French presidential elections and propose efficient emotion groupings that would
    aid in effectively analyzing influence campaigns and agendas on social media.
  authors:
  - Ankita Bhaumik
  - Andy Bernhardt
  - Gregorios Katsios
  - Ning Sa
  - Tomek Strzalkowski
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_61
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Adapting Emotion Detection to Analyze Influence Campaigns on Social Media
  tldr: Social media is an extremely potent tool for influencing public opinion, particularly
    during important events such as elections, pandemics, and national conflicts.
    Emotions are a crucial aspect of this influence, but detecting them accurately
    in the political domain is a significant challenge due to
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Where do the meaning of emoji come from? Though it is generally assumed
    that emoji are fully iconic, with meanings derived from their visual forms, we
    argue that this is only one component of their meaning. We surveyed users and
    non-users of the Chinese social media platform WeChat for their interpretations
    of emoji specific to WeChat. We find that some emoji show significant differences
    in their interpretations between users and non-users, and based on how familiar
    a person is with the specific emoji. We argue that this reflects a more complex
    process for building the meaning of emoji on a platform than pure iconicity.
  authors:
  - Brianna O'Boyle
  - Gabriel Doyle
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_62
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Not Just Iconic: Emoji Interpretation is Shaped by Use'
  tldr: Where do the meaning of emoji come from? Though it is generally assumed that
    emoji are fully iconic, with meanings derived from their visual forms, we argue
    that this is only one component of their meaning. We surveyed users and non-users
    of the Chinese social media platform WeChat for their interpr
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The dominance of English is a well-known issue in NLP research. In this
    position paper, I turn to state-of-the-art psychological insights to explain why
    this problem is especially persistent in research on automatic emotion detection,
    and why the seemingly promising approach of using multilingual models to include
    lower-resourced languages might not be the desired solution. Instead, I campaign
    for the use of models that acknowledge linguistic and cultural differences in
    emotion conceptualization and verbalization. Moreover, I see much potential in
    NLP to better understand emotions and emotional language use across different
    languages.
  authors:
  - Luna De Bruyne
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_65
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: The Paradox of Multilingual Emotion Detection
  tldr: The dominance of English is a well-known issue in NLP research. In this position
    paper, I turn to state-of-the-art psychological insights to explain why this problem
    is especially persistent in research on automatic emotion detection, and why the
    seemingly promising approach of using multilingual mo
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: People globally quit their jobs at high rates during the COVID-19 pandemic,
    yet there is scant research about emotional trajectories surrounding voluntary
    resignations before or during that era. To explore long-term emotional language
    patterns before and after quitting a job, we amassed a Reddit sample of people
    who indicated resigning on a specific day (n = 7,436), each of whom was paired
    with a comparison user matched on posting history. After excluding people on the
    basis of low posting frequency and word count, we analyzed 150.3 million words
    (53.1% from 5,134 target users who indicated quitting) using SALLEE, a dictionary-based
    syntax-aware tool, and Linguistic Inquiry and Word Count (LIWC) dictionaries.
    Based on posts in the year before and after quitting, people who had quit their
    jobs used more sadness and anxiety language than matched comparison users. Lower
    rates of "I" pronouns and cognitive processing language were associated with less
    sadness and anxiety surrounding quitting. Emotional trajectories during and before
    the pandemic were parallel, though pandemic messages were more negative. The results
    have relevance for strategic self-distancing as a means of regulating negative
    emotions around major life changes.
  authors:
  - Molly Ireland
  - Micah Iserman
  - Kiki Adams
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_66
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Sadness and Anxiety Language in Reddit Messages Before and After Quitting
    a Job
  tldr: People globally quit their jobs at high rates during the COVID-19 pandemic,
    yet there is scant research about emotional trajectories surrounding voluntary
    resignations before or during that era. To explore long-term emotional language
    patterns before and after quitting a job, we amassed a Reddit sam
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Twitter and parliamentary speeches are very different communication channels,
    but many members of parliament (MPs) make use of both. Focusing on the topic of
    climate change, we undertake a comparative analysis of speeches and tweets uttered
    by MPs in Germany in a recent six-year period. By keyword/hashtag analyses and
    topic modeling, we find substantial differences along party lines, with left-leaning
    parties discussing climate change through a crisis frame, while liberal and conservative
    parties try to address climate change through the lens of climate-friendly technology
    and practices. Only the AfD denies the need to adopt climate change mitigating
    measures, demeaning those concerned about a deteriorating climate as climate cult
    or fanatics. Our analysis reveals that climate change communication does not differ
    substantially between Twitter and parliamentary speeches, but across the political
    spectrum.
  authors:
  - Robin Schaefer
  - Christoph Abels
  - Stephan Lewandowsky
  - Manfred Stede
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_67
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Communicating Climate Change: A Comparison Between Tweets and Speeches by
    German Members of Parliament'
  tldr: Twitter and parliamentary speeches are very different communication channels,
    but many members of parliament (MPs) make use of both. Focusing on the topic of
    climate change, we undertake a comparative analysis of speeches and tweets uttered
    by MPs in Germany in a recent six-year period. By keyword/h
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Recent years have seen a proliferation of aggressive social media posts,
    often wreaking even real-world consequences for victims. Aggressive behaviour
    on social media is especially evident during important sociopolitical events such
    as elections, communal incidents, and public protests. In this paper, we introduce
    a dataset in English to model political aggression. The dataset comprises public
    tweets collated across the time-frames of two of the most recent Indian general
    elections. We manually annotate this data for the task of aggression detection
    and analyze this data for aggressive behaviour. To benchmark the efficacy of our
    dataset, we perform experiments by fine-tuning pre-trained language models and
    comparing the results with models trained on an existing but general domain dataset.
    Our models consistently outperform the models trained on existing data. Our best
    model achieves a macro F1-score of 66.66 on our dataset. We also train models
    on a combined version of both datasets, achieving best macro F1-score of 92.77,
    on our dataset. Additionally, we create subsets of code-mixed and non-code-mixed
    data from the combined dataset to observe variations in results due to the Hindi-English
    code-mixing phenomenon. We publicly release the anonymized data, code, and models
    for further research.
  authors:
  - Akash Rawat
  - Nazia Nafis
  - Dnyaneshwar Bhadane
  - Diptesh Kanojia
  - Rudra Murthy
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_69
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Modelling Political Aggression on Social Media Platforms
  tldr: Recent years have seen a proliferation of aggressive social media posts, often
    wreaking even real-world consequences for victims. Aggressive behaviour on social
    media is especially evident during important sociopolitical events such as elections,
    communal incidents, and public protests. In this pape
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper presents the results of the WASSA 2023 shared task on predicting
    empathy, emotion, and personality in conversations and reactions to news articles.
    Participating teams were given access to a new dataset from Omitaomu et al. (2022)
    comprising empathic and emotional reactions to news articles. The dataset included
    formal and informal text, self-report data, and third-party annotations. Specifically,
    the dataset contained news articles (where harm is done to a person, group, or
    other) and crowd-sourced essays written in reaction to the article. After reacting
    via essays, crowd workers engaged in conversations about the news articles. Finally,
    the crowd workers self-reported their empathic concern and distress, personality
    (using the Big Five), and multi-dimensional empathy (via the Interpersonal Reactivity
    Index). A third-party annotated both the conversational turns (for empathy, emotion
    polarity, and emotion intensity) and essays (for multi-label emotions). Thus,
    the dataset contained outcomes (self-reported or third-party annotated) at the
    turn level (within conversations) and the essay level. Participation was encouraged
    in five tracks: (i) predicting turn-level empathy, emotion polarity, and emotion
    intensity in conversations, (ii) predicting state empathy and distress scores,
    (iii) predicting emotion categories, (iv) predicting personality, and (v) predicting
    multi-dimensional trait empathy. In total, 21 teams participated in the shared
    task. We summarize the methods and resources used by the participating teams.'
  authors:
  - Valentin Barriere
  - "Jo\xE3o Sedoc"
  - Shabnam Tafreshi
  - Salvatore Giorgi
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_101
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Findings of WASSA 2023 Shared Task on Empathy, Emotion and Personality Detection
    in Conversation and Reactions to News Articles
  tldr: This paper presents the results of the WASSA 2023 shared task on predicting
    empathy, emotion, and personality in conversations and reactions to news articles.
    Participating teams were given access to a new dataset from Omitaomu et al. (2022)
    comprising empathic and emotional reactions to news articl
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: "This paper describes the system for the YNU-HPCC team in WASSA-2023 Shared\
    \ Task 1: Empathy Detection and Emotion Classification. This task needs to predict\
    \ the empathy, emotion, and personality of the empathic reactions. This system\
    \ is mainly based on the Decoding-enhanced BERT with disentangled attention (DeBERTa)\
    \ model with parameter-efficient fine-tuning (PEFT) and the Robustly Optimized\
    \ BERT Pretraining Approach (RoBERTa). Low-Rank Adaptation (LoRA) fine-tuning\
    \ in PEFT is used to reduce the training parameters of large language models.\
    \ Moreover, back translation is introduced to augment the training dataset. This\
    \ system achieved relatively good results on the competition\u2019s official leaderboard.\
    \ The code of this system is available here."
  authors:
  - Yukun Wang
  - Jin Wang
  - Xuejie Zhang
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_102
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'YNU-HPCC at WASSA-2023 Shared Task 1: Large-scale Language Model with LoRA
    Fine-Tuning for Empathy Detection and Emotion Classification'
  tldr: 'This paper describes the system for the YNU-HPCC team in WASSA-2023 Shared
    Task 1: Empathy Detection and Emotion Classification. This task needs to predict
    the empathy, emotion, and personality of the empathic reactions. This system is
    mainly based on the Decoding-enhanced BERT with disentangled att'
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper presents a study on using the RoBERTa language model for emotion
    classification of essays as part of the 'Shared Task on Empathy Detection, Emotion
    Classification and Personality Detection in Interactions' organized as part of
    'WASSA 2023' at 'ACL 2023'. Emotion classification is a challenging task in natural
    language processing, and imbalanced datasets further exacerbate this challenge.
    In this study, we explore the use of various data balancing techniques in combination
    with RoBERTa to improve the classification performance. We evaluate the performance
    of our approach (denoted by adityapatkar on Codalab) on a benchmark multi-label
    dataset of essays annotated with eight emotion categories, provided by the Shared
    Task organizers. Our results show that the proposed approach achieves the best
    macro F1 score in the competition's training and evaluation phase. Our study provides
    insights into the potential of RoBERTa for handling imbalanced data in emotion
    classification. The results can have implications for the natural language processing
    tasks related to emotion classification.
  authors:
  - Aditya Patkar
  - Suraj Chandrashekhar
  - Ram Mohan Rao Kadiyala
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_103
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'AdityaPatkar at WASSA 2023 Empathy, Emotion, and Personality Shared Task:
    RoBERTa-Based Emotion Classification of Essays, Improving Performance on Imbalanced
    Data'
  tldr: This paper presents a study on using the RoBERTa language model for emotion
    classification of essays as part of the 'Shared Task on Empathy Detection, Emotion
    Classification and Personality Detection in Interactions' organized as part of
    'WASSA 2023' at 'ACL 2023'. Emotion classification is a challe
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: "The WASSA 2023 shared task on predicting empathy, emotion and other personality\
    \ traits consists of essays, conversations and articles in textual form and participants\u2019\
    \ demographic information in numerical form. To address the tasks, our contributions\
    \ include (1) converting numerical information into meaningful text information\
    \ using appropriate templates, (2) summarising lengthy articles, and (3) augmenting\
    \ training data by paraphrasing. To achieve these contributions, we leveraged\
    \ two separate T5-based pre-trained transformers. We then fine-tuned pre-trained\
    \ BERT, DistilBERT and ALBERT for predicting empathy and personality traits. We\
    \ used the Optuna hyperparameter optimisation framework to fine-tune learning\
    \ rates, batch sizes and weight initialisation. Our proposed system achieved its\
    \ highest performance \u2013 a Pearson correlation coefficient of 0.750 \u2013\
    \ on the  onversation-level empathy prediction task1 . The system implementation\
    \ is publicly available at https: //github.com/hasan-rakibul/WASSA23-empathy-emotion."
  authors:
  - Md Rakibul Hasan
  - Md Zakir Hossain
  - Tom Gedeon
  - Susannah Soon
  - Shafin Rahman
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_104
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Curtin OCAI at WASSA 2023 Empathy, Emotion and Personality Shared Task:
    Demographic-Aware Prediction Using Multiple Transformers'
  tldr: "The WASSA 2023 shared task on predicting empathy, emotion and other personality\
    \ traits consists of essays, conversations and articles in textual form and participants\u2019\
    \ demographic information in numerical form. To address the tasks, our contributions\
    \ include (1) converting numerical information into"
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'In this paper, we present Team Hawk''s participation in Track 1 of the
    WASSA 2023 shared task. The objective of the task is to understand the empathy
    that emerges between individuals during their conversations. In our study, we
    developed a multi-tasking framework that is capable of automatically assessing
    empathy, intensity of emotion, and polarity of emotion within participants'' conversations.
    Our proposed core model extends the transformer architecture, utilizing two separate
    RoBERTa-based encoders to encode both the articles and conversations. Subsequently,
    a sequence of self-attention, position-wise feed-forward, and dense layers are
    employed to predict the regression scores for the three sub-tasks: empathy, intensity
    of emotion, and polarity of emotion. Our best model achieved average Pearson''s
    correlation of 0.7710 (Empathy: 0.7843, Emotion Polarity: 0.7917, Emotion Intensity:
    0.7381) on the released  development set and 0.7250 (Empathy: 0.8090, Emotion
    Polarity: 0.7010, Emotion Intensity: 0.6650) on the released test set. These results
    earned us the 3rd position in the test set evaluation phase of Track 1.'
  authors:
  - Addepalli Sai Srinivas
  - Nabarun Barua
  - Santanu Pal
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_105
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Team_Hawk at WASSA 2023 Empathy, Emotion, and Personality Shared Task: Multi-tasking
    Multi-encoder based transformers for Empathy and Emotion Prediction in Conversations'
  tldr: In this paper, we present Team Hawk's participation in Track 1 of the WASSA
    2023 shared task. The objective of the task is to understand the empathy that
    emerges between individuals during their conversations. In our study, we developed
    a multi-tasking framework that is capable of automatically asse
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes our proposed system design for the WASSA 2023 shared
    task 1. We propose a unified architecture of ensemble neural networks to integrate
    the original RoBERTa transformer with two sentiment-enhanced RoBERTa-Twitter and
    EmoBERTa models. For Track 1 at the speech-turn level, our best submission achieved
    an average Pearson correlation score of 0.7236, ranking fourth for empathy, emotion
    polarity and emotion intensity prediction. For Track 2 at the essay-level, our
    best submission obtained an average Pearson correlation score of 0.4178 for predicting
    empathy and distress scores, ranked first among all nine submissions.
  authors:
  - Tzu-Mi Lin
  - Jung-Ying Chang
  - Lung-Hao Lee
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_106
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'NCUEE-NLP at WASSA 2023 Shared Task 1: Empathy and Emotion Prediction Using
    Sentiment-Enhanced RoBERTa Transformers'
  tldr: This paper describes our proposed system design for the WASSA 2023 shared
    task 1. We propose a unified architecture of ensemble neural networks to integrate
    the original RoBERTa transformer with two sentiment-enhanced RoBERTa-Twitter and
    EmoBERTa models. For Track 1 at the speech-turn level, our bes
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This research contributes to the task of predicting empathy and personality
    traits within dialogue, an important aspect of natural language processing, as
    part of our experimental work for the WASSA 2023 Empathy and Emotion Shared Task.
    For predicting empathy, emotion polarity, and emotion intensity on turns within
    a dialogue, we employ adapters trained on social media interactions labeled with
    empathy ratings in a stacked composition with the target task adapters. Furthermore,
    we embed demographic information to predict Interpersonal Reactivity Index (IRI)
    subscales and Big Five Personality Traits utilizing BERT-based models. The results
    from our study provide valuable insights, contributing to advancements in understanding
    human behavior and interaction through text. Our team ranked 2nd on the personality
    and empathy prediction tasks, 4th on the interpersonal reactivity index, and 6th
    on the conversational task.
  authors:
  - Fabio Gruschka
  - Allison Lahnala
  - Charles Welch
  - Lucie Flek
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_107
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Domain Transfer for Empathy, Distress, and Personality Prediction
  tldr: This research contributes to the task of predicting empathy and personality
    traits within dialogue, an important aspect of natural language processing, as
    part of our experimental work for the WASSA 2023 Empathy and Emotion Shared Task.
    For predicting empathy, emotion polarity, and emotion intensity
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'In this paper, we highlight our approach for the "WASSA 2023 Shared-Task
    1: Empathy Detection and Emotion Classification". By accurately identifying emotions
    from textual sources of data, deep learning models can be trained to understand
    and interpret human emotions more effectively. The classification of emotions
    facilitates the creation of more emotionally intelligent systems that can better
    understand and respond to human emotions. We compared multiple transformer-based
    models for multi-label classification. Ensembling and oversampling were used to
    improve the performance of the system. A threshold-based voting mechanism performed
    on three models (Longformer, BERT, BigBird) yields the highest overall macro F1-score
    of 0.6605.'
  authors:
  - Aditya Paranjape
  - Gaurav Kolhatkar
  - Yash Patwardhan
  - Omkar Gokhale
  - Shweta Dharmadhikari
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_108
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Converge at WASSA 2023 Empathy, Emotion and Personality Shared Task: A Transformer-based
    Approach for Multi-Label Emotion Classification'
  tldr: 'In this paper, we highlight our approach for the "WASSA 2023 Shared-Task
    1: Empathy Detection and Emotion Classification". By accurately identifying emotions
    from textual sources of data, deep learning models can be trained to understand
    and interpret human emotions more effectively. The classificat'
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper presents our approach for the WASSA 2023 Empathy, Emotion and
    Personality Shared Task. Empathy and distress are human feelings that are implicitly
    expressed in natural discourses. Empathy and distress detection are crucial challenges
    in Natural Language Processing that can aid our understanding of conversations.
    The provided dataset consists of several long-text examples in the English language,
    with each example associated with a numeric score for empathy and distress. We
    experiment with several BERT-based models as a part of our approach. We also try
    various ensemble methods. Our final submission has a Pearson's r score of 0.346,
    placing us third in the empathy and distress detection subtask.
  authors:
  - Tanmay Chavan
  - Kshitij Deshpande
  - Sheetal Sonawane
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_109
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'PICT-CLRL at WASSA 2023 Empathy, Emotion and Personality Shared Task: Empathy
    and Distress Detection using Ensembles of Transformer Models'
  tldr: This paper presents our approach for the WASSA 2023 Empathy, Emotion and Personality
    Shared Task. Empathy and distress are human feelings that are implicitly expressed
    in natural discourses. Empathy and distress detection are crucial challenges in
    Natural Language Processing that can aid our underst
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes the approach that we used to take part in the multi-label
    multi-class emotion classification as Track 3 of the WASSA 2023 Empathy, Emotion
    and Personality Shared Task at ACL 2023. The overall goal of this track is to
    build models that can predict 8 classes (7 emotions + neutral) based on short
    English essays written in response to news article that talked about events perceived
    as harmful to people. We used OpenAI generative pretrained transformers with full-scale
    APIs for the emotion prediction task by fine-tuning a GPT-3 model and doing prompt
    engineering for zero-shot / few-shot learning with ChatGPT and GPT-4 models based
    on multiple experiments on the dev set. The most efficient method was fine-tuning
    a GPT-3 model which allowed us to beat our baseline character-based XGBoost Classifier
    and rank 2nd among all other participants by achieving a macro F1 score of 0.65
    and a micro F1 score of 0.7 on the final blind test set.
  authors:
  - Andrew Nedilko
  - Yi Chu
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_110
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Team Bias Busters at WASSA 2023 Empathy, Emotion and Personality Shared
    Task: Emotion Detection with Generative Pretrained Transformers'
  tldr: This paper describes the approach that we used to take part in the multi-label
    multi-class emotion classification as Track 3 of the WASSA 2023 Empathy, Emotion
    and Personality Shared Task at ACL 2023. The overall goal of this track is to
    build models that can predict 8 classes (7 emotions + neutral)
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper introduces the participation of team HIT-SCIR to the WASSA
    2023 Shared Task on Empathy Detection and Emotion Classification and Personality
    Detection in Interactions. We focus on three tracks: Track 1 (Empathy and Emotion
    Prediction in Conversations, CONV), Track 2 (Empathy Prediction, EMP) and Track
    3 (Emotion Classification, EMO), and designed three different models to address
    them separately. For Track 1, we designed a direct fine-tuning DeBERTa model for
    three regression tasks at the utterance-level. For Track 2, we designed a multi-task
    learning RoBERTa model for two regression tasks at the essay-level. For Track
    3, we designed a RoBERTa model with data augmentation for the classification task
    at the essay-level. Finally, our team ranked 1st in the Track 1 (CONV), 5th in
    the Track 2 (EMP) and 3rd in the Track 3 (EMO) in the evaluation phase.'
  authors:
  - Xin Lu
  - Zhuojun Li
  - Yanpeng Tong
  - Yanyan Zhao
  - Bing Qin
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_111
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'HIT-SCIR at WASSA 2023: Empathy and Emotion Analysis at the Utterance-Level
    and the Essay-Level'
  tldr: 'This paper introduces the participation of team HIT-SCIR to the WASSA 2023
    Shared Task on Empathy Detection and Emotion Classification and Personality Detection
    in Interactions. We focus on three tracks: Track 1 (Empathy and Emotion Prediction
    in Conversations, CONV), Track 2 (Empathy Prediction, EM'
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Our system, VISU, participated in the WASSA 2023 Shared Task (3) of Emotion
    Classification from essays written in reaction to news articles. Emotion detection
    from complex dialogues is challenging and often requires context/domain understanding.
    Therefore in this research, we have focused on developing deep learning (DL) models
    using the combination of word embedding representations with tailored prepossessing
    strategies to capture the nuances of emotions expressed. Our experiments used
    static and contextual embeddings (individual and stacked) with Bidirectional Long
    short-term memory (BiLSTM) and Transformer based models. We occupied rank tenth
    in the emotion detection task by scoring a Macro F1-Score of 0.2717, validating
    the efficacy of our implemented approaches for small and imbalanced datasets with
    mixed categories of target emotions.
  authors:
  - Vivek Kumar
  - Prayag Tiwari
  - Sushmita Singh
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_112
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'VISU at WASSA 2023 Shared Task: Detecting Emotions in Reaction to News Stories
    Using Transformers and Stacked Embeddings'
  tldr: Our system, VISU, participated in the WASSA 2023 Shared Task (3) of Emotion
    Classification from essays written in reaction to news articles. Emotion detection
    from complex dialogues is challenging and often requires context/domain understanding.
    Therefore in this research, we have focused on develop
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'We present the results of the WASSA 2023 Shared-Task 2: Emotion Classification
    on code-mixed text messages (Roman Urdu + English), which included two tracks
    for emotion classification: multi-label and multi-class. The participants were
    provided with a dataset of code-mixed SMS messages in English and Roman Urdu labeled
    with 12 emotions for both tracks. A total of 5 teams (19 team members) participated
    in the shared task. We summarized the methods, resources, and tools used by the
    participating teams. We also made the data freely available for further improvements
    to the task.'
  authors:
  - Iqra Ameer
  - "Necva B\xF6l\xFCc\xFC"
  - Hua Xu
  - Ali Al Bataineh
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_201
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Findings of WASSA 2023 Shared Task: Multi-Label and Multi-Class Emotion
    Classification on Code-Mixed Text Messages'
  tldr: 'We present the results of the WASSA 2023 Shared-Task 2: Emotion Classification
    on code-mixed text messages (Roman Urdu + English), which included two tracks
    for emotion classification: multi-label and multi-class. The participants were
    provided with a dataset of code-mixed SMS messages in English an'
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Emotion classification on code-mixed text messages is challenging due
    to the multilingual languages and non-literal cues (i.e., emoticons). To solve
    these problems,  we propose an innovative soft prompt tuning method, which is
    lightweight and effective to release potential abilities of the pre-trained language
    models and improve the classification results. Firstly, we transform emoticons
    into textual information to utilize their rich emotional information. Then, variety
    of innovative templates and verbalizers are applied to promote emotion classification.
    Extensive experiments show that transforming emoticons and employing prompt tuning
    both benefit the performance. Finally, as a part of WASSA 2023, we obtain the
    accuracy of 0.972 in track MLEC and 0.892 in track MCEC, yielding the second place
    in both two tracks.
  authors:
  - Jinghui Zhang
  - Dongming Yang
  - Siyu Bao
  - Lina Cao
  - Shunguo Fan
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_202
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Emotion classification on code-mixed text messages via soft prompt tuning
  tldr: Emotion classification on code-mixed text messages is challenging due to the
    multilingual languages and non-literal cues (i.e., emoticons). To solve these
    problems,  we propose an innovative soft prompt tuning method, which is lightweight
    and effective to release potential abilities of the pre-train
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Code-mixing refers to the phenomenon of using two or more languages interchangeably
    within a speech or discourse context. This practice is particularly prevalent
    on social media platforms, and determining the embedded affects in a code-mixed
    sentence remains as a challenging problem. In this submission we describe our
    system for WASSA 2023 Shared Task on Emotion Detection in English-Urdu code-mixed
    text. In our system we implement a multiclass emotion detection model with label
    space of 11 emotions. Samples are code-mixed English-Urdu text, where Urdu is
    written in romanised form. Our submission is limited to one of the subtasks -
    Multi Class classification and we leverage transformer-based Multilingual Large
    Language Models (MLLMs), XLM-RoBERTa and Indic-BERT. We fine-tune MLLMs on the
    released data splits, with and without pre-processing steps (translation to english),
    for classifying texts into the appropriate emotion category. Our methods did not
    surpass the baseline, and our submission is ranked sixth overall.
  authors:
  - Bhaskara Hanuma Vedula
  - Prashant Kodali
  - Manish Shrivastava
  - Ponnurangam Kumaraguru
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_203
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'PrecogIIITH@WASSA2023: Emotion Detection for Urdu-English Code-mixed Text'
  tldr: Code-mixing refers to the phenomenon of using two or more languages interchangeably
    within a speech or discourse context. This practice is particularly prevalent
    on social media platforms, and determining the embedded affects in a code-mixed
    sentence remains as a challenging problem. In this submiss
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In this era of digital communication and social media, texting and chatting
    among individuals occur mainly through code-mixed or Romanized versions of the
    native language prevalent in the region. The presence of Romanized and code-mixed
    language develops the need to build NLP systems in these domains to leverage the
    digital content for various use cases. This paper describes our contribution to
    the subtask MCEC of the shared task WASSA 2023:Shared Task on Multi-Label and
    Multi-Class Emotion Classification on Code-Mixed Text Messages. We explore how
    one can build sentence transformers models for low-resource languages using unsupervised
    data by leveraging contrastive learning techniques described in the SIMCSE paper
    and using the sentence transformer developed to build classification models using
    the SetFit approach. Additionally, we'll publish our code and models on GitHub
    and HuggingFace, two open-source hosting services.
  authors:
  - Bhavish Pahwa
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_204
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'BpHigh at WASSA 2023: Using Contrastive Learning to build Sentence Transformer
    models for Multi-Class Emotion Classification in Code-mixed Urdu'
  tldr: In this era of digital communication and social media, texting and chatting
    among individuals occur mainly through code-mixed or Romanized versions of the
    native language prevalent in the region. The presence of Romanized and code-mixed
    language develops the need to build NLP systems in these domain
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Emotion classification on code-mixed texts has been widely used in real-world
    applications. In this paper, we build a system that participates in the WASSA
    2023 Shared Task 2 for emotion classification on code-mixed text messages from
    Roman Urdu and English. The main goal of the proposed method is to adopt a text-mixed
    data augmentation for robust code-mixed text representation. We mix texts with
    both multi-label (track 1) and multi-class (track 2) annotations in a unified
    multilingual pre-trained model, i.e., XLM-RoBERTa, for both subtasks. Our results
    show that the proposed text-mixed method performs competitively, ranking first
    in both tracks, achieving an average Macro F1 score of 0.9782 on the multi-label
    track and of 0.9329 on the multi-class track.
  authors:
  - Xuqiao Ran
  - You Zhang
  - Jin Wang
  - Dan Xu
  - Xuejie Zhang
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_205
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'YNU-HPCC at WASSA 2023: Using Text-Mixed Data Augmentation for Emotion Classification
    on Code-Mixed Text Message'
  tldr: Emotion classification on code-mixed texts has been widely used in real-world
    applications. In this paper, we build a system that participates in the WASSA
    2023 Shared Task 2 for emotion classification on code-mixed text messages from
    Roman Urdu and English. The main goal of the proposed method is t
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes the approach that we utilized to participate in the
    shared task for multi-label and multi-class emotion classification organized as
    part of WASSA 2023 at ACL 2023. The objective was to build mod- els that can predict
    11 classes of emotions, or the lack thereof (neutral class) based on code- mixed
    Roman Urdu and English SMS text messages. We participated in Track 2 of this task
    - multi-class emotion classification (MCEC). We used generative pretrained transformers,
    namely ChatGPT because it has a commercially available full-scale API, for the
    emotion detec- tion task by leveraging the prompt engineer- ing and zero-shot
    / few-shot learning method- ologies based on multiple experiments on the dev set.
    Although this was the first time we used a GPT model for the purpose, this ap-
    proach allowed us to beat our own baseline character-based XGBClassifier, as well
    as the baseline model trained by the organizers (bert- base-multilingual-cased).
    We ranked 4th and achieved the macro F1 score of 0.7038 and the accuracy of 0.7313
    on the blind test set.
  authors:
  - Andrew Nedilko
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WASSA
  forum: ''
  id: WASSA_206
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Generative Pretrained Transformers for Emotion Detection in a Code-Switching
    Setting
  tldr: This paper describes the approach that we utilized to participate in the shared
    task for multi-label and multi-class emotion classification organized as part
    of WASSA 2023 at ACL 2023. The objective was to build mod- els that can predict
    11 classes of emotions, or the lack thereof (neutral class) ba
  track: The 13th Workshop on Computational Approaches to Subjectivity, Sentiment,
    & Social Media Analysis
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'In 2023, the third iteration of the DISRPT Shared Task (Discourse Relation
    Parsing and Treebanking) was held, dedicated to the underlying units used in discourse
    parsing across formalisms. Following the success of the 2019and 2021 tasks on
    Elementary Discourse Unit Segmentation, Connective Detection, and Relation Classification,
    this iteration has added 10 new corpora, including 2 new languages (Thai and Italian)
    and 3 discourse treebanks annotated in the discourse dependency representation
    in addition to the previously included frameworks: RST, SDRT, and PDTB. In this
    paper, we review the data included in the Shared Task, which covers 26 datasets
    across 13 languages, survey and compare submitted systems, and report on system
    performance on each task for both annotated and plain-tokenized versions of the
    data.'
  authors:
  - "Chlo\xE9 Braud"
  - Yang Janet Liu
  - Eleni Metheniti
  - Philippe Muller
  - "Laura Rivi\xE8re"
  - Attapol Rutherford
  - Amir Zeldes
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - DISRPT
  forum: ''
  id: ACL-CODI_4
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: all
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: The DISRPT 2023 Shared Task on Elementary Discourse Unit Segmentation, Connective
    Detection, and Relation Classification
  tldr: In 2023, the third iteration of the DISRPT Shared Task (Discourse Relation
    Parsing and Treebanking) was held, dedicated to the underlying units used in discourse
    parsing across formalisms. Following the success of the 2019and 2021 tasks on
    Elementary Discourse Unit Segmentation, Connective Detection
  track: The Shared Task on Discourse Relation Parsing and Treebanking
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper introduces DiscoFlan, a multilingual discourse relation classifier
    submitted for DISRPT 2023. Our submission represents the first attempt at building
    a multilingual discourse relation classifier for the DISRPT 2023 shared task.
    By our model addresses the issue to mismatches caused by hallucination in a seq2seq
    model by utilizing the label distribution information for label generation. In
    contrast to the previous state-of-the-art model, our approach eliminates the need
    for hand-crafted features in computing the discourse relation classes. Furthermore,
    we propose a novel label generation mechanism that anchors the labels to a fixed
    set by selectively enhancing training on the decoder model. Our experimental results
    demonstrate that our model surpasses the current state-of-the-art performance
    in 11 out of the 26 datasets considered, however the submitted model compatible
    with provided evaluation scripts is better in 7 out of 26 considered datasets,
    while demonstrating competitive results in the rest. Overall, DiscoFlan showcases
    promising advancements in multilingual discourse relation classification for the
    DISRPT 2023 shared task.
  authors:
  - Kaveri Anuranjana
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - DISRPT
  forum: ''
  id: ACL-CODI_3
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: relations
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'DiscoFlan: Instruction Fine-tuning and Refined Text Generation for Discourse
    Relation Label Classification'
  tldr: This paper introduces DiscoFlan, a multilingual discourse relation classifier
    submitted for DISRPT 2023. Our submission represents the first attempt at building
    a multilingual discourse relation classifier for the DISRPT 2023 shared task.
    By our model addresses the issue to mismatches caused by hall
  track: The Shared Task on Discourse Relation Parsing and Treebanking
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper presents the results obtained by the MELODI team for the three
    tasks proposed within the DISRPT 2023 shared task on discourse: segmentation,
    connective identification, and relation classification. The competition involves
    corpora in various languages in several underlying frameworks, and proposes two
    tracks depending on the presence or not of annotations of sentence boundaries
    and syntactic information. For these three tasks, we rely on a transformer-based
    architecture, and investigate several optimizations of the models, including hyper-parameter
    search and layer freezing.For discourse relations, we also explore the use of
    adapters---a lightweight solution for model fine-tuning---and introduce relation
    mappings to partially deal with the label set explosion we are facing within the
    setting of the shared task in a multi-corpus perspective. In the end, we propose
    one single architecture for segmentation and connectives, based on XLM-RoBERTa
    large, freezed at lower layers, with new state-of-the-art results for segmentation,
    and we propose 3 different models for relations, since the task makes it harder
    to generalize across all corpora.'
  authors:
  - Eleni Metheniti
  - "Chlo\xE9 Braud"
  - Philippe Muller
  - "Laura Rivi\xE8re"
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - DISRPT
  forum: ''
  id: ACL-CODI_1
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: all
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'DisCut and DiscReT: MELODI at DISRPT 2023'
  tldr: 'This paper presents the results obtained by the MELODI team for the three
    tasks proposed within the DISRPT 2023 shared task on discourse: segmentation,
    connective identification, and relation classification. The competition involves
    corpora in various languages in several underlying frameworks, and '
  track: The Shared Task on Discourse Relation Parsing and Treebanking
  underline_id: null
  underline_url: null
  video_url: null
- abstract: HITS participated in the Discourse Segmentation (DS, Task 1) and Connective
    Detection (CD, Task 2) tasks at the DISRPT 2023. Task 1 focuses on segmenting
    the text into discourse units, while Task 2 aims to detect the discourse connectives.
    We deployed a framework based on different pre-trained models according to the
    target language for these two tasks.HITS also participated in the Relation Classification
    track (Task 3). The main task was recognizing the discourse relation between text
    spans from different languages. We designed a joint model for languages with a
    small corpus while separate models for large corpora. The adversarial training
    strategy is applied to enhance the robustness of relation classifiers.
  authors:
  - Wei Liu
  - Yi Fan
  - Michael Strube
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - DISRPT
  forum: ''
  id: ACL-CODI_2
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: all
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'HITS at DISRPT 2023: Discourse Segmentation, Connective Detection, and Relation
    Classification'
  tldr: HITS participated in the Discourse Segmentation (DS, Task 1) and Connective
    Detection (CD, Task 2) tasks at the DISRPT 2023. Task 1 focuses on segmenting
    the text into discourse units, while Task 2 aims to detect the discourse connectives.
    We deployed a framework based on different pre-trained model
  track: The Shared Task on Discourse Relation Parsing and Treebanking
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Automatic Speech Recognition (ASR) in medical contexts has the potential
    to save time, cut costs, increase report accuracy, and reduce physician burnout.
    However, the healthcare industry has been slower to adopt this technology, in
    part due to the importance of avoiding medically-relevant transcription mistakes.
    In this work, we present the Clinical BERTScore (CBERTScore), an ASR metric that
    penalizes clinically-relevant mistakes more than others. We collect a benchmark
    of 18 clinician preferences on 149 realistic medical sentences called the Clinician
    Transcript Preference benchmark (CTP) and make it publicly available for the community
    to further develop clinically-aware ASR metrics. To our knowledge, this is the
    first public dataset of its kind. We demonstrate that our metric more closely
    aligns with clinician preferences on medical sentences as compared to other metrics
    (WER, BLUE, METEOR, etc), sometimes by wide margins.
  authors:
  - Joel Shor
  - Ruyue Agnes Bi
  - Subhashini Venugopalan
  - Steven Ibara
  - Roman Goldenberg
  - Ehud Rivlin
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_2
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Clinical BERTScore: An Improved Measure of Automatic Speech Recognition
    Performance in Clinical Settings'
  tldr: Automatic Speech Recognition (ASR) in medical contexts has the potential to
    save time, cut costs, increase report accuracy, and reduce physician burnout.
    However, the healthcare industry has been slower to adopt this technology, in
    part due to the importance of avoiding medically-relevant transcript
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Assessing the capacity of numerical understanding of vision-and-language
    models over images and texts is crucial for real vision-and-language applications,
    such as systems for automated medical image analysis.

    We provide a visual reasoning dataset focusing on numerical understanding in the
    medical domain.

    The experiments using our dataset show that current vision-and-language models
    fail to perform numerical inference in the medical domain.

    However, the data augmentation with only a small amount of our dataset improves
    the model performance, while maintaining the performance in the general domain.'
  authors:
  - Hitomi Yanaka
  - Yuta Nakamura
  - Yuki Chida
  - Tomoya Kurosawa
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_3
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Medical Visual Textual Entailment for Numerical Understanding of Vision-and-Language
    Models
  tldr: 'Assessing the capacity of numerical understanding of vision-and-language
    models over images and texts is crucial for real vision-and-language applications,
    such as systems for automated medical image analysis.

    We provide a visual reasoning dataset focusing on numerical understanding in the
    medical d'
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Valuable datasets that contain sensitive information are not shared due
    to privacy and copyright concerns. This hinders progress in many areas and prevents
    the use of machine learning solutions to solve relevant tasks. One possible solution
    is sharing models that are trained on such datasets. However, this is also associated
    with potential privacy risks due to data extraction attacks. In this work, we
    propose a solution based on sharing parts of the model's parameters, and using
    a proxy dataset for complimentary knowledge transfer. Our experiments show encouraging
    results, and reduced risk to potential training data identification attacks. We
    present a viable solution to sharing knowledge with data-disadvantaged parties,
    that do not have the resources to produce high-quality data, with reduced privacy
    risks to the sharing parties. We make our code publicly available.
  authors:
  - Paul Youssef
  - "J\xF6rg Schl\xF6tterer"
  - Christin Seifert
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_4
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Privacy-Preserving Knowledge Transfer through Partial Parameter Sharing
  tldr: Valuable datasets that contain sensitive information are not shared due to
    privacy and copyright concerns. This hinders progress in many areas and prevents
    the use of machine learning solutions to solve relevant tasks. One possible solution
    is sharing models that are trained on such datasets. Howeve
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Alzheimer's Disease (AD) is a neurodegenerative disorder that affects
    cognitive abilities and memory, especially in older adults. One of the challenges
    of AD is that it can be difficult to diagnose in its early stages. However, recent
    research has shown that changes in language, including speech decline and difficulty
    in processing information, can be important indicators of AD and may help with
    early detection. Hence, the speech narratives of the patients can be useful in
    diagnosing the early stages of Alzheimer's disease. While the previous works have
    presented the potential of using speech narratives to diagnose AD in high-resource
    languages, this work explores the possibility of using a low-resourced language,
    i.e., Hindi language, to diagnose AD. In this paper, we present a dataset specifically
    for analyzing AD in the Hindi language, along with experimental results using
    various state-of-the-art algorithms to assess the diagnostic potential of speech
    narratives in Hindi. Our analysis suggests that speech narratives in the Hindi
    language have the potential to aid in the diagnosis of AD. Our dataset and code
    are made publicly available at https://github.com/rkritesh210/DementiaBankHindi.
  authors:
  - Kritesh Rauniyar
  - Shuvam Shiwakoti
  - Sweta Poudel
  - Surendrabikram Thapa
  - Usman Naseem
  - Mehwish Nasim
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_5
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Breaking Barriers: Exploring the Diagnostic Potential of Speech Narratives
    in Hindi for Alzheimer''s Disease'
  tldr: Alzheimer's Disease (AD) is a neurodegenerative disorder that affects cognitive
    abilities and memory, especially in older adults. One of the challenges of AD
    is that it can be difficult to diagnose in its early stages. However, recent research
    has shown that changes in language, including speech dec
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Massively multilingual pre-trained language models (MMPLMs) are developed
    in recent years demonstrating superpowers and the pre-knowledge they acquire for
    downstream tasks.

    This work investigates whether MMPLMs can be applied to clinical domain machine
    translation (MT) towards entirely unseen languages via transfer learning.

    We carry out an experimental investigation using Meta-AI''s MMPLMs ``wmt21-dense-24-wide-en-X
    and X-en (WMT21fb)'''' which were pre-trained on 7 language pairs and 14 translation
    directions including English to Czech, German, Hausa, Icelandic, Japanese, Russian,
    and Chinese, and the opposite direction.

    We fine-tune these MMPLMs towards English-\textit{Spanish} language pair which
    \textit{did not exist at all} in their original pre-trained corpora both implicitly
    and explicitly.

    We prepare carefully aligned \textit{clinical} domain data for this fine-tuning,
    which is different from their original mixed domain knowledge.

    Our experimental result shows that the fine-tuning is very successful using just
    250k well-aligned in-domain EN-ES segments for three sub-task translation testings:
    clinical cases, clinical terms, and ontology concepts. It achieves very close
    evaluation scores to another MMPLM NLLB from Meta-AI, which included Spanish as
    a high-resource setting in the pre-training.

    To the best of our knowledge, this is the first work on using MMPLMs towards \textit{clinical
    domain transfer-learning NMT} successfully for totally unseen languages during
    pre-training.'
  authors:
  - Lifeng Han
  - 'Gleb '
  - Irina Sorokina
  - Serge Gladkoff
  - Goran Nenadic
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_6
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Investigating Massive Multilingual Pre-Trained Machine Translation Models
    for Clinical Domain via Transfer Learning
  tldr: 'Massively multilingual pre-trained language models (MMPLMs) are developed
    in recent years demonstrating superpowers and the pre-knowledge they acquire for
    downstream tasks.

    This work investigates whether MMPLMs can be applied to clinical domain machine
    translation (MT) towards entirely unseen langua'
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: "The Coronavirus pandemic has heightened the demand for technological\
    \ solutions capable of gathering and monitoring data automatically, quickly, and\
    \ securely. To achieve this need, the Plant\xE3o Coronavirus chatbot has been\
    \ made available to the population of Cear\xE1 State in Brazil. This chatbot employs\
    \ automated symptom detection technology through Natural Language Processing (NLP).\
    \ The proposal of this work is a symptom tracker, which is a neural network that\
    \ processes texts and captures symptoms in messages exchanged between citizens\
    \ of the state and the Plant\xE3o Coronavirus nurse/doctor, i.e., clinical conversations.\
    \ The model has the ability to recognize new patterns and has identified a high\
    \ incidence of altered psychological behaviors, including anguish, anxiety, and\
    \ sadness, among users who tested positive or negative for Covid-19. As a result,\
    \ the tool has emphasized the importance of expanding coverage through community\
    \ mental health services in the state."
  authors:
  - Ticiana Coelho Da Silva
  - "Jos\xE9 Fernandes De Mac\xEAdo"
  - "R\xE9gis Magalh\xE3es"
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_7
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Tracking the Evolution of Covid-19 Symptoms through Clinical Conversations
  tldr: "The Coronavirus pandemic has heightened the demand for technological solutions\
    \ capable of gathering and monitoring data automatically, quickly, and securely.\
    \ To achieve this need, the Plant\xE3o Coronavirus chatbot has been made available\
    \ to the population of Cear\xE1 State in Brazil. This chatbot employs"
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'In the rapidly evolving landscape of medical research, accurate and concise
    summarization of clinical studies is crucial to support evidence-based practice.
    This paper presents a novel approach to clinical studies summarization, leveraging
    reinforcement learning to enhance factual consistency and align with human annotator
    preferences. Our work focuses on two tasks: Conclusion Generation and Review Generation.
    We train a CONFIT summarization model that outperforms GPT-3 and previous state-of-the-art
    models on the same datasets and collects expert and crowd-worker annotations to
    evaluate the quality and factual consistency of the generated summaries. These
    annotations enable us to measure the correlation of various automatic metrics,
    including modern factual evaluation metrics like QAFactEval, with human-assessed
    factual consistency. By employing top-correlated metrics as objectives for a reinforcement
    learning model, we demonstrate improved factuality in generated summaries that
    are preferred by human annotators. '
  authors:
  - Xiangru Tang
  - Arman Cohan
  - Mark Gerstein
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_8
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Aligning Factual Consistency for Clinical Studies Summarization through Reinforcement
    Learning
  tldr: In the rapidly evolving landscape of medical research, accurate and concise
    summarization of clinical studies is crucial to support evidence-based practice.
    This paper presents a novel approach to clinical studies summarization, leveraging
    reinforcement learning to enhance factual consistency and al
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Pretrained language models leverage self-supervised learning to use large
    amounts of unlabeled text for learning contextual representations of sequences.
    However, in the domain of medical conversations, the availability of large, public
    datasets is limited due to issues of privacy and data management. In this paper,
    we study the effectiveness of dialog-aware pretraining objectives and multiphase
    training in using unlabeled data to improve LMs training for medical utterance
    classification. The objectives of pretraining for dialog awareness involve tasks
    that take into account the structure of conversations, including features such
    as turn-taking and the roles of speakers. The multiphase training process uses
    unannotated data in a sequence that prioritizes similarities and connections between
    different domains. We empirically evaluate these methods on conversational dialog
    classification tasks in the medical and counseling domains, and find that multiphase
    training can help achieve higher performance than standard pretraining or finetuning.
  authors:
  - Do June Min
  - Veronica Perez-Rosas
  - Rada Mihalcea
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_12
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Navigating Data Scarcity: Pretraining for Medical Utterance Classification'
  tldr: Pretrained language models leverage self-supervised learning to use large
    amounts of unlabeled text for learning contextual representations of sequences.
    However, in the domain of medical conversations, the availability of large, public
    datasets is limited due to issues of privacy and data managemen
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In developing countries like India, doctors and healthcare professionals
    working in public health spend significant time answering health queries that
    are fact-based and repetitive. Therefore, we propose an automated way to answer
    maternal and child health-related queries. A database of Frequently Asked Questions
    (FAQs) and their corresponding answers generated by experts is curated from rural
    health workers and young mothers. We develop a Hindi chatbot that identifies k
    relevant Question and Answer (QnA) pairs from the database in response to a healthcare
    query (q) written in Devnagri script or Hindi-English (Hinglish) code-mixed script.
    The curated database covers 80% of all the queries that a user of our study is
    likely to ask. We experimented with (i) rule-based methods, (ii) sentence embeddings,
    and (iii) a paraphrasing classifier, to calculate the q-Q similarity. We observed
    that paraphrasing classifier gives the best result when trained first on an open-domain
    text and then on the healthcare domain. Our chatbot uses an ensemble of all three
    approaches. We observed that if a given q can be answered using the database,
    then our chatbot can provide at least one relevant QnA pair among its top three
    suggestions for up to 70% of the queries.
  authors:
  - Ritwik Mishra
  - Simranjeet Singh
  - Jasmeet Kaur
  - Pushpendra Singh
  - Rajiv Shah
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_13
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Hindi Chatbot for Supporting Maternal and Child Health Related Queries in
    Rural India
  tldr: In developing countries like India, doctors and healthcare professionals working
    in public health spend significant time answering health queries that are fact-based
    and repetitive. Therefore, we propose an automated way to answer maternal and
    child health-related queries. A database of Frequently A
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Generative artificial intelligence (AI) is a promising direction for augmenting
    clinical diagnostic decision support and reducing diagnostic errors, a leading
    contributor to medical errors. To further the development of clinical AI systems,
    the Diagnostic Reasoning Benchmark (DR.BENCH) was introduced as a comprehensive
    generative AI framework, comprised of six tasks representing key components in
    clinical reasoning. We present a comparative analysis of in-domain versus out-of-domain
    language models as well as multi-task versus single task training with a focus
    on the problem summarization task in DR.BENCH. We demonstrate that a multi-task,
    clinically-trained language model outperforms its general domain counterpart by
    a large margin, establishing a new state-of-the-art performance, with a ROUGE-L
    score of 28.55. This research underscores the value of domain-specific training
    for optimizing clinical diagnostic reasoning tasks.
  authors:
  - Brihat Sharma
  - Yanjun Gao
  - Timothy Miller
  - Matthew Churpek
  - Majid Afshar
  - Dmitriy Dligach
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_16
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Multi-Task Training with In-Domain Language Models for Diagnostic Reasoning
  tldr: Generative artificial intelligence (AI) is a promising direction for augmenting
    clinical diagnostic decision support and reducing diagnostic errors, a leading
    contributor to medical errors. To further the development of clinical AI systems,
    the Diagnostic Reasoning Benchmark (DR.BENCH) was introduce
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Accurately capturing medication history is crucial in delivering high-quality
    medical care. The extraction of medication events from unstructured clinical notes,
    however, is challenging because the information is presented in complex narratives.
    We address this challenge by leveraging the newly released Contextualized Medication
    Event Dataset (CMED) as part of our participation in the 2022 National NLP Clinical
    Challenges (n2c2) shared task. Our study evaluates the performance of various
    pretrained language models in this task. Further, we find that data augmentation
    coupled with domain-specific training provides notable improvements. With experiments,
    we also underscore the importance of careful data preprocessing in medical event
    detection.
  authors:
  - Noushin Salek Faramarzi
  - Meet Patel
  - Sai Harika Bandarupally
  - Ritwik Banerjee
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_19
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Context-aware Medication Event Extraction from Unstructured Text
  tldr: Accurately capturing medication history is crucial in delivering high-quality
    medical care. The extraction of medication events from unstructured clinical notes,
    however, is challenging because the information is presented in complex narratives.
    We address this challenge by leveraging the newly rele
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: International Classification of Diseases (ICD) coding is the task of assigning
    a patient's electronic health records into standardized codes, which is crucial
    for enhancing medical services and reducing healthcare costs. In Korea, automatic
    Korean Standard Classification of Diseases (KCD) coding has been hindered by limited
    resources, differences in ICD systems, and language-specific characteristics.
    Therefore, we construct the Korean Dataset for Automatic KCD coding (KoDAK) by
    collecting and preprocessing Korean clinical documents. In addition, we propose
    a tokenization method optimized for Korean clinical documents. Our experiments
    show that our proposed method outperforms Korean Medical BERT (KM-BERT) in Macro-F1
    performance by 0.14%p while using fewer model parameters, demonstrating its effectiveness
    in Korean clinical documents.
  authors:
  - Geunyeong Jeong
  - Juoh Sun
  - Seokwon Jeong
  - Hyunjin Shin
  - Harksoo Kim
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_20
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Improving Automatic KCD Coding: Introducing the KoDAK and an Optimized Tokenization
    Method for Korean Clinical Documents'
  tldr: International Classification of Diseases (ICD) coding is the task of assigning
    a patient's electronic health records into standardized codes, which is crucial
    for enhancing medical services and reducing healthcare costs. In Korea, automatic
    Korean Standard Classification of Diseases (KCD) coding has
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Natural language processing (NLP) has shown great potential for Alzheimer''s
    disease (AD) detection, particularly due to the adverse effect of AD on spontaneous
    speech. The current body of literature has directed attention toward context-based
    models, especially Bidirectional Encoder Representations from Transformers (BERTs),
    owing to their exceptional abilities to integrate contextual information in a
    wide range of NLP tasks.

    This comes at the cost of added model opacity and computational requirements.
    Taking this into consideration, we propose a Word2Vec-based model for AD detection
    in 108 age- and sex-matched participants who were asked to describe the Cookie
    Theft picture. We also investigate the effectiveness of our model by fine-tuning
    BERT-based sequence classification models, as well as incorporating linguistic
    features. Our results demonstrate that our lightweight and easy-to-implement model
    outperforms some of the state-of-the-art models available in the literature, as
    well as BERT models.'
  authors:
  - Behrad Taghibeyglou
  - Frank Rudzicz
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_22
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: "Who needs context? Classical techniques for Alzheimer\u2019s disease detection"
  tldr: 'Natural language processing (NLP) has shown great potential for Alzheimer''s
    disease (AD) detection, particularly due to the adverse effect of AD on spontaneous
    speech. The current body of literature has directed attention toward context-based
    models, especially Bidirectional Encoder Representations '
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'In the medical field, there are many clinical texts such as electronic
    medical records, and research on Japanese natural language processing using these
    texts has been conducted.One such research involves Recognizing Textual Entailment
    (RTE) in clinical texts using a semantic analysis and logical inference system,
    ccg2lambda.However, it is difficult for existing inference systems to correctly
    determine the entailment relations , if the input sentence contains medical domain
    specific paraphrases such as disease names.

    In this study, we propose a method to supplement the equivalence relations of
    disease names as axioms by identifying candidates for paraphrases that lack in
    theorem proving.Candidates of paraphrases are identified by using a model for
    the NER task for disease names and a disease name dictionary.We also construct
    an inference test set that requires knowledge injection of disease names and evaluate
    our inference system.Experiments showed that our inference system was able to
    correctly infer for 106 out of 149 inference test sets.'
  authors:
  - Natsuki Murakami
  - Mana Ishida
  - Yuta Takahashi
  - Hitomi Yanaka
  - Daisuke Bekki
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_26
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Knowledge Injection for Disease Names in Logical Inference between Japanese
    Clinical Texts
  tldr: In the medical field, there are many clinical texts such as electronic medical
    records, and research on Japanese natural language processing using these texts
    has been conducted.One such research involves Recognizing Textual Entailment (RTE)
    in clinical texts using a semantic analysis and logical in
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This work introduces a novel three-class annotation scheme for text-based
    dementia classification in patients, based on their recorded visit interactions.
    Multiple models were developed utilising BERT, RoBERTa and DistilBERT. Two approaches
    were employed to improve the representation of dementia samples: oversampling
    the underrepresented data points in the original Pitt dataset and combining the
    Pitt with the Holland and Kempler datasets. The DistilBERT models trained on either
    an oversampled Pitt dataset or the combined dataset performed best in classifying
    the dementia class. Specifically, the model trained on the oversampled Pitt dataset
    and the one trained on the combined dataset obtained state-of-the-art performance
    with 98.8% overall accuracy and 98.6% macro-averaged F1-score, respectively. The
    models'' outputs were manually inspected through saliency highlighting, using
    Local Interpretable Model-agnostic Explanations (LIME), to provide a better understanding
    of its predictions.'
  authors:
  - Nadine Abdelhalim
  - Ingy Abdelhalim
  - Riza Batista-Navarro
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_27
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Training Models on Oversampled Data and a Novel Multi-class Annotation Scheme
    for Dementia Detection
  tldr: This work introduces a novel three-class annotation scheme for text-based
    dementia classification in patients, based on their recorded visit interactions.
    Multiple models were developed utilising BERT, RoBERTa and DistilBERT. Two approaches
    were employed to improve the representation of dementia sam
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Text in electronic health records is organized into sections, and classifying
    those sections into section categories is useful for downstream tasks. In this
    work, we attempt to improve the transferability of section classification models
    by combining the dataset-specific knowledge in supervised learning models with
    the world knowledge inside large language models (LLMs). Surprisingly, we find
    that zero-shot LLMs out-perform supervised BERT-based models applied to out-of-domain
    data. We also find that their strengths are synergistic, so that a simple ensemble
    technique leads to additional performance gains.
  authors:
  - Weipeng Zhou
  - Majid Afshar
  - Dmitriy Dligach
  - Yanjun Gao
  - Timothy Miller
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_30
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Improving the Transferability of Clinical Note Section Classification Models
    with BERT and Large Language Model Ensembles
  tldr: Text in electronic health records is organized into sections, and classifying
    those sections into section categories is useful for downstream tasks. In this
    work, we attempt to improve the transferability of section classification models
    by combining the dataset-specific knowledge in supervised lear
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: "Recent advances in large language models (LLMs) have generated significant\
    \ interest in their application across various domains including healthcare. However,\
    \ there is limited data on their safety and performance in real-world scenarios.\
    \ This study uses data collected using an autonomous telemedicine clinical assistant.\
    \ The assistant asks symptom-based questions to elicit patient concerns and allows\
    \ patients to ask questions about their post-operative recovery. We utilise real-world\
    \ postoperative questions posed to the assistant by a cohort of 120 patients to\
    \ examine the safety and appropriateness of responses generated by a recent popular\
    \ LLM by OpenAI, ChatGPT. We demonstrate that LLMs have the potential to helpfully\
    \ address routine patient queries following routine surgery.  However, important\
    \ limitations around the safety of today\u2019s models exist which must be considered.\
    \ \n"
  authors:
  - Mohita Chowdhury
  - Ernest Lim
  - Aisling Higham
  - Rory McKinnon
  - Nikoletta Ventoura
  - Yajie He
  - Nick De Pennington
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_31
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Can Large Language Models Safely Address Patient Questions Following Cataract
    Surgery?
  tldr: Recent advances in large language models (LLMs) have generated significant
    interest in their application across various domains including healthcare. However,
    there is limited data on their safety and performance in real-world scenarios.
    This study uses data collected using an autonomous telemedicin
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We present our work on building large scale sequence-to-sequence models
    for generating clinical note from patient-doctor conversation. This is formulated
    as an abstractive summarization task for which we use encoder-decoder transformer
    model with pointer-generator. We discuss various modeling enhancements to this
    baseline model which include using subword and multiword tokenization scheme,
    prefixing the targets with a chain-of-clinical-facts, and training with contrastive
    loss that is defined over various candidate summaries. We also use flash attention
    during training and query chunked attention during inference to be able to process
    long input and output sequences and to improve computational efficiency. Experiments
    are conducted on a dataset containing about 900K encounters from around 1800 healthcare
    providers covering 27 specialties. The results are broken down into primary care
    and non-primary care specialties. Consistent accuracy improvements are observed
    across both of these categories.
  authors:
  - Gagandeep Singh
  - Yue Pan
  - Jesus Andres-Ferrer
  - Miguel Del-Agua
  - Frank Diehl
  - Joel Pinto
  - Paul Vozila
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_33
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Large Scale Sequence-to-Sequence Models for Clinical Note Generation from
    Patient-Doctor Conversations
  tldr: We present our work on building large scale sequence-to-sequence models for
    generating clinical note from patient-doctor conversation. This is formulated
    as an abstractive summarization task for which we use encoder-decoder transformer
    model with pointer-generator. We discuss various modeling enhanc
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Clinical Natural Language Processing has been an increasingly popular
    research area in the NLP community. With the rise of large language models (LLMs)
    and their impressive abilities in NLP tasks, it is crucial to pay attention to
    their clinical applications. Sequence to sequence generative approaches with LLMs
    have been widely used in recent years. To be a part of the research in clinical
    NLP with recent advances in the field, we participated in task A of MEDIQA-Chat
    at ACL-ClinicalNLP Workshop 2023. In this paper, we explain our methods and findings
    as well as our comments on our results and limitations. '
  authors:
  - Kadir Bulut Ozler
  - Steven Bethard
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_35
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'clulab at MEDIQA-Chat 2023: Summarization and classification of medical
    dialogues'
  tldr: Clinical Natural Language Processing has been an increasingly popular research
    area in the NLP community. With the rise of large language models (LLMs) and their
    impressive abilities in NLP tasks, it is crucial to pay attention to their clinical
    applications. Sequence to sequence generative approach
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Early detection and automated classification of dementia has recently
    gained considerable attention using neuroimaging data and spontaneous speech.
    In this paper, we validate the possibility of dementia detection with in-hospital
    clinical notes. We collected 954 patients'' clinical notes from a local hospital
    and assign dementia/non-dementia labels to those patients based on clinical assessment
    and telephone interview. Given the labeled dementia data sets, we fine tune a
    ClinicalBioBERT based on some filtered clinical notes and conducted experiments
    on both binary and three class dementia classification. Our experiment results
    show that the fine tuned ClinicalBioBERT achieved satisfied performance on binary
    classification but failed on three class dementia classification. Further analysis
    suggests that more human prior knowledge should be considered. '
  authors:
  - Ming Liu
  - Richard Beare
  - Taya Collyer
  - Nadine Andrew
  - Velandai Srikanth
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_36
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Leveraging Natural Language Processing and Clinical Notes for Dementia Detection
  tldr: Early detection and automated classification of dementia has recently gained
    considerable attention using neuroimaging data and spontaneous speech. In this
    paper, we validate the possibility of dementia detection with in-hospital clinical
    notes. We collected 954 patients' clinical notes from a local
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We propose a method to automate orthodontic diagnosis with natural language
    processing. It is worthwhile to assist dentists with such technology to prevent
    errors by inexperienced dentists and to reduce the workload of experienced ones.
    However, text length and style inconsistencies in medical findings make an automated
    orthodontic diagnosis with deep-learning models difficult. In this study, we improve
    the performance of automatic diagnosis utilizing short summaries of medical findings
    written in a consistent style by experienced dentists. Experimental results on
    970 Japanese medical findings show that summarization consistently improves the
    performance of various machine learning models for automated orthodontic diagnosis.
    Although BERT is the model that gains the most performance with the proposed method,
    the convolutional neural network achieved the best performance.
  authors:
  - Takumi Ohtsuka
  - Tomoyuki Kajiwara
  - Chihiro Tanikawa
  - Yuujin Shimizu
  - Hajime Nagahara
  - Takashi Ninomiya
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_37
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Automated Orthodontic Diagnosis from a Summary of Medical Findings
  tldr: We propose a method to automate orthodontic diagnosis with natural language
    processing. It is worthwhile to assist dentists with such technology to prevent
    errors by inexperienced dentists and to reduce the workload of experienced ones.
    However, text length and style inconsistencies in medical findi
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Recent advancements in natural language processing (NLP) have been driven
    by large language models (LLMs), thereby revolutionizing the field. Our study
    investigates the impact of diverse pre-training strategies on the performance
    of Turkish clinical language models in a multi-label classification task involving
    radiology reports, with a focus on overcoming language resource limitations. Additionally,
    for the first time, we evaluated the simultaneous pre-training approach by utilizing
    limited clinical task data. We developed four models: TurkRadBERT-task v1, TurkRadBERT-task
    v2, TurkRadBERT-sim v1, and TurkRadBERT-sim v2. Our results revealed superior
    performance from BERTurk and TurkRadBERT-task v1, both of which leverage a broad
    general-domain corpus. Although task-adaptive pre-training is capable of identifying
    domain-specific patterns, it may be prone to overfitting because of the constraints
    of the task-specific corpus. Our findings highlight the importance of domain-specific
    vocabulary during pre-training to improve performance. They also affirmed that
    a combination of general domain knowledge and task-specific fine-tuning is crucial
    for optimal performance across various categories. This study offers key insights
    for future research on pre-training techniques in the clinical domain, particularly
    for low-resource languages.'
  authors:
  - "Hazal T\xFCrkmen"
  - Oguz Dikenelli
  - Cenk Eraslan
  - Mehmet Calli
  - Suha Ozbek
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_38
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Harnessing the Power of BERT in the Turkish Clinical Domain: Pretraining
    Approaches for Limited Data Scenarios'
  tldr: Recent advancements in natural language processing (NLP) have been driven
    by large language models (LLMs), thereby revolutionizing the field. Our study
    investigates the impact of diverse pre-training strategies on the performance
    of Turkish clinical language models in a multi-label classification ta
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Over the last years, an increasing number of publicly available, semantically
    annotated medical corpora have been released for the German language. While their
    annotations cover comparable semantic classes, the synergies of such efforts have
    not been explored, yet. This is due to substantial differences in the data schemas
    (syntax) and annotated entities (semantics), which hinder the creation of common
    meta-datasets. For instance, it is unclear whether named entity recognition (NER)
    taggers trained on one or more of such datasets are useful to detect entities
    in any of the other datasets.  In this work, we create harmonized versions of
    German medical corpora using the BigBIO framework, and make them available to
    the community. Using these as a meta-dataset, we perform a series of cross-corpus
    evaluation experiments on two settings of aligned labels. These consist in fine-tuning
    various pre-trained Transformers on different combinations of training sets, and
    testing them against each dataset separately. We find that a) trained NER models
    generalize poorly, with F1 scores dropping approx. 20 pp. on unseen test data,
    and b) current pre-trained Transformer models for the German language do not systematically
    alleviate this issue. However, our results suggest that models benefit from additional
    training corpora in most cases, even if these belong to different medical fields
    or text genres.
  authors:
  - Ignacio Llorca
  - Florian Borchert
  - Matthieu-P. Schapranow
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_39
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'A Meta-dataset of German Medical Corpora: Harmonization of Annotations and
    Cross-corpus NER Evaluation'
  tldr: Over the last years, an increasing number of publicly available, semantically
    annotated medical corpora have been released for the German language. While their
    annotations cover comparable semantic classes, the synergies of such efforts have
    not been explored, yet. This is due to substantial differe
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: "Post-stroke speech and language deficits (aphasia) significantly impact\
    \ patients\u2019 quality of life. Many with mild symptoms remain undiagnosed,\
    \ and the majority do not receive the intensive doses of therapy recommended,\
    \ due to healthcare costs and/or inadequate services. Automatic Speech Recognition\
    \ (ASR) may help overcome these difficulties by improving diagnostic rates and\
    \ providing feedback during tailored therapy. However, its performance is often\
    \ unsatisfactory due to the high variability in speech errors and scarcity of\
    \ training datasets. This study assessed the performance of Whisper, a recently\
    \ released end-to-end model, in patients with post-stroke aphasia (PWA). We tuned\
    \ its hyperparameters to achieve the lowest word error rate (WER) on aphasic speech.\
    \ WER was significantly higher in PWA compared to age-matched controls (10.3%\
    \ vs 38.5%, $p<0.001$). We demonstrated that worse WER was related to the more\
    \ severe aphasia as measured by expressive (overt naming, and spontaneous speech\
    \ production) and receptive (written and spoken comprehension) language assessments.\
    \ Stroke lesion size did not affect the performance of Whisper. Linear mixed models\
    \ accounting for demographic factors, therapy duration, and time since stroke,\
    \ confirmed worse Whisper performance with left hemispheric frontal lesions.\n\
    We discuss the implications of these findings for how future ASR can be improved\
    \ in PWA."
  authors:
  - Giulia Sanguedolce
  - Patrick Naylor
  - Fatemeh Geranmayeh
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_40
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Uncovering the Potential for a Weakly Supervised End-to-End Model in Recognising
    Speech from Patient with Post-Stroke Aphasia
  tldr: "Post-stroke speech and language deficits (aphasia) significantly impact patients\u2019\
    \ quality of life. Many with mild symptoms remain undiagnosed, and the majority\
    \ do not receive the intensive doses of therapy recommended, due to healthcare\
    \ costs and/or inadequate services. Automatic Speech Recognition "
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'We explore temporal dependency graph (TDG) parsing in the clinical domain.
    We leverage existing annotations on the THYME dataset to semi-automatically construct
    a TDG corpus. Then we propose a new natural language inference (NLI) approach
    to TDG parsing, and evaluate it both on general domain TDGs from wikinews and
    the newly constructed clinical TDG corpus. We achieve competitive performance
    on general domain TDGs with a much simpler model than prior work. On the clinical
    TDGs, our method establishes the first result of TDG parsing on clinical data
    with 0.79/0.88 micro/macro F1. '
  authors:
  - Jiarui Yao
  - Steven Bethard
  - Kristin Wright-Bettner
  - Eli Goldner
  - David Harris
  - Guergana Savova
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_42
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Textual Entailment for Temporal Dependency Graph Parsing
  tldr: We explore temporal dependency graph (TDG) parsing in the clinical domain.
    We leverage existing annotations on the THYME dataset to semi-automatically construct
    a TDG corpus. Then we propose a new natural language inference (NLI) approach
    to TDG parsing, and evaluate it both on general domain TDGs f
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'A medical provider''s summary of a patient visit serves several critical
    purposes, including clinical decision-making, facilitating hand-offs between providers,
    and as a reference for the patient. An effective summary is required to be coherent
    and accurately capture all the medically relevant information in the dialogue,
    despite the complexity of patient-generated language. Even minor inaccuracies
    in visit summaries (for example, summarizing "patient does not have a fever" when
    a fever is present) can be detrimental to the outcome of care for the patient.


    This paper tackles the problem of medical conversation summarization by discretizing
    the task into several smaller dialogue-understanding tasks that are sequentially
    built upon. First, we identify medical entities and their affirmations within
    the conversation to serve as building blocks. We study dynamically constructing
    few-shot prompts for tasks by conditioning on relevant patient information and
    use GPT-3 as the backbone for our experiments. We also develop GPT-derived summarization
    metrics to measure performance against reference summaries quantitatively. Both
    our human evaluation study and metrics for medical correctness show that summaries
    generated using this approach are clinically accurate and outperform the baseline
    approach of summarizing the dialog in a zero-shot, single-prompt setting.'
  authors:
  - Varun Nair
  - Elliot Schumacher
  - Anitha Kannan
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_43
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Generating medically-accurate summaries of patient-provider dialogue: A
    multi-stage approach using large language models'
  tldr: A medical provider's summary of a patient visit serves several critical purposes,
    including clinical decision-making, facilitating hand-offs between providers,
    and as a reference for the patient. An effective summary is required to be coherent
    and accurately capture all the medically relevant inform
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Detecting duplicate patient participation in clinical trials is a major
    challenge because repeated patients can undermine the credibility and accuracy
    of the trial''s findings and result in significant health and financial risks.
    Developing accurate automated speaker verification (ASV) models is crucial to
    verify the identity of enrolled individuals and remove duplicates, but the size
    and quality of data influence ASV performance. However, there has been limited
    investigation into the factors that can affect ASV capabilities in clinical environments.
    In this paper, we bridge the gap by conducting analysis of how participant demographic
    characteristics, audio quality criteria, and severity level of Alzheimer''s disease
    (AD) impact the performance of ASV utilizing a dataset of speech recordings from
    659 participants with varying levels of AD, obtained through multiple speech tasks.
    Our results indicate that ASV performance: 1) is slightly better on male speakers
    than on female speakers; 2) degrades for individuals who are above 70 years old;
    3) is comparatively better for non-native English speakers than for native English
    speakers; 4) is negatively affected by clinician interference, noisy background,
    and unclear participant speech; 5) tends to decrease with an increase in the severity
    level of AD. Our study finds that voice biometrics raise fairness concerns as
    certain subgroups exhibit different ASV performances owing to their inherent voice
    characteristics. Moreover, the performance of ASV is influenced by the quality
    of speech recordings, which underscores the importance of improving the data collection
    settings in clinical trials.'
  authors:
  - Malikeh Ehghaghi
  - Marija Stanojevic
  - Ali Akram
  - Jekaterina Novikova
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_45
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: "Factors Affecting the Performance of Automated Speaker Verification in Alzheimer\u2019\
    s Disease Clinical Trials"
  tldr: Detecting duplicate patient participation in clinical trials is a major challenge
    because repeated patients can undermine the credibility and accuracy of the trial's
    findings and result in significant health and financial risks. Developing accurate
    automated speaker verification (ASV) models is cruc
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes Team Cadence's winning submission to Task C of the
    MEDIQA-Chat 2023 shared tasks. We also present the set of methods, including a
    novel N-pass strategy to summarize a mix of clinical dialogue and an incomplete
    summarized note, used to complete Task A and Task B, ranking highly on the leaderboard
    amongst stable and reproducible code submissions. The shared tasks invited participants
    to summarize, classify and generate patient-doctor conversations. Considering
    the small volume of training data available, we took a data-augmentation-first
    approach to the three tasks by focusing on the dialogue generation task, i.e.,
    Task C. It proved effective in improving our models' performance on Task A and
    Task B. We also found the BART architecture to be highly versatile, as it formed
    the base for all our submissions. Finally, based on the results shared by the
    organizers, we note that Team Cadence was the only team to submit stable and reproducible
    runs to all three tasks.
  authors:
  - Ashwyn Sharma
  - David Feldman
  - Aneesh Jain
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_48
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Team Cadence at MEDIQA-Chat 2023: Generating, augmenting and summarizing
    clinical dialogue with large language models'
  tldr: This paper describes Team Cadence's winning submission to Task C of the MEDIQA-Chat
    2023 shared tasks. We also present the set of methods, including a novel N-pass
    strategy to summarize a mix of clinical dialogue and an incomplete summarized
    note, used to complete Task A and Task B, ranking highly o
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Annotated clinical text corpora are essential for machine learning studies
    that model and predict care processes and disease progression. However, few studies
    describe the necessary experimental design of the annotation guideline and annotation
    phases. This makes replication, reuse, and adoption challenging.


    Using clinical questions about sepsis, we designed a semantic annotation guideline
    to capture sepsis signs from clinical text. The clinical questions aid guideline
    design, application, and evaluation. Our method incrementally evaluates each change
    in the guideline by testing the resulting annotated corpus using clinical questions.
    Additionally, our method uses inter-annotator agreement to judge the annotator
    compliance and quality of the guideline. We show that the method, combined with
    controlled design increments, is simple and allows the development and measurable
    improvement of a purpose-built semantic annotation guideline. We believe that
    our approach is useful for incremental design of semantic annotation guidelines
    in general.'
  authors:
  - Melissa Yan
  - Lise Gustad
  - "Lise H\xF8vik"
  - "\xD8ystein Nytr\xF8"
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_49
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Method for Designing Semantic Annotation of Sepsis Signs in Clinical Text
  tldr: Annotated clinical text corpora are essential for machine learning studies
    that model and predict care processes and disease progression. However, few studies
    describe the necessary experimental design of the annotation guideline and annotation
    phases. This makes replication, reuse, and adoption cha
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: "Prompt tuning offers an efficient approach to domain adaptation for pretrained\
    \ language models, which predominantly focus on masked language modeling or generative\
    \ objectives. \nHowever, the potential of discriminative language models in biomedical\
    \ tasks remains underexplored.\nTo bridge this gap, we develop BioDLM, a method\
    \ tailored for biomedical domain adaptation of discriminative language models\
    \ that incorporates prompt-based continual pretraining and prompt tuning for downstream\
    \ tasks. \nBioDLM aims to maximize the potential of discriminative language models\
    \ in low-resource scenarios by reformulating these tasks as span-level corruption\
    \ detection, thereby enhancing performance on domain-specific tasks and improving\
    \ the efficiency of continual pertaining.\nIn this way, BioDLM provides a data-efficient\
    \ domain adaptation method for discriminative language models, effectively enhancing\
    \ performance on discriminative tasks within the biomedical domain."
  authors:
  - Keming Lu
  - Peter Potash
  - Xihui Lin
  - Yuwen Sun
  - Zihan Qian
  - Zheng Yuan
  - Tristan Naumann
  - Tianxi Cai
  - Junwei Lu
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_50
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Prompt Discriminative Language Models for Domain Adaptation
  tldr: "Prompt tuning offers an efficient approach to domain adaptation for pretrained\
    \ language models, which predominantly focus on masked language modeling or generative\
    \ objectives. \nHowever, the potential of discriminative language models in biomedical\
    \ tasks remains underexplored.\nTo bridge this gap, we "
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Information extraction from clinical text has the potential to facilitate
    clinical research and personalized clinical care, but annotating large amounts
    of data for each set of target tasks is prohibitive. We present a German medical
    Named Entity Recognition (NER) system capable of cross-domain knowledge transferring.
    The system builds on a pre-trained German language model and a token-level binary
    classifier, employing semantic types sourced from the Unified Medical Language
    System (UMLS) as entity labels to identify corresponding entity spans within the
    input text. To enhance the system's performance and robustness, we pre-train it
    using a medical literature corpus that incorporates UMLS semantic term annotations.
    We evaluate the system's effectiveness on two German annotated datasets obtained
    from different clinics in zero- and few-shot settings. The results show that our
    approach outperforms task-specific Condition Random Fields (CRF) classifiers in
    terms of accuracy. Our work contributes to developing robust and transparent German
    medical NER models that can support the extraction of information from various
    clinical texts.
  authors:
  - Siting Liang
  - Mareike Hartmann
  - Daniel Sonntag
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_51
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Cross-domain German Medical Named Entity Recognition using a Pre-Trained
    Language Model and Unified Medical Semantic Types
  tldr: Information extraction from clinical text has the potential to facilitate
    clinical research and personalized clinical care, but annotating large amounts
    of data for each set of target tasks is prohibitive. We present a German medical
    Named Entity Recognition (NER) system capable of cross-domain know
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Graph-based techniques have gained traction for representing and analyzing
    data in various natural language processing (NLP) tasks. Knowledge graph-based
    language representation models have shown promising results in leveraging domain-specific
    knowledge for NLP tasks, particularly in the biomedical NLP field. However, such
    models have limitations, including knowledge noise and neglect of contextual relationships,
    leading to potential semantic errors and reduced accuracy. To address these issues,
    this paper proposes two novel methods. The first method combines knowledge graph-based
    language model with nearest-neighbor models to incorporate semantic and category
    information from neighboring instances. The second method involves integrating
    knowledge graph-based language model with graph neural networks (GNNs) to leverage
    feature information from neighboring nodes in the graph. Experiments on relation
    extraction (RE) and classification tasks in English and Chinese language datasets
    demonstrate significant performance improvements with both methods, highlighting
    their potential for enhancing the performance of language models and improving
    NLP applications in the biomedical domain.
  authors:
  - Usman Naseem
  - Surendrabikram Thapa
  - Qi Zhang
  - Liang Hu
  - Anum Masood
  - Mehwish Nasim
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_52
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Reducing Knowledge Noise for Improved Semantic Analysis in Biomedical Natural
    Language Processing Applications
  tldr: 'Graph-based techniques have gained traction for representing and analyzing
    data in various natural language processing (NLP) tasks. Knowledge graph-based
    language representation models have shown promising results in leveraging domain-specific
    knowledge for NLP tasks, particularly in the biomedical '
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Artificial intelligence based diagnosis systems have emerged as powerful
    tools to reform traditional medical care. Each clinician now wants to have his
    own intelligent diagnostic partner to expand the range of services he can provide.  When
    reading a clinical note, experts make inferences with relevant knowledge. However,
    medical knowledge appears to be heterogeneous, including structured and unstructured
    knowledge. Existing approaches are incapable of uniforming them well. Besides,
    the descriptions of clinical findings in clinical notes, which are reasoned to
    diagnosis, vary a lot for different diseases or patients. To address these problems,
    we propose a Medical Knowledge-enhanced Prompt Learning (MedKPL) model for diagnosis
    classification. First, to overcome the heterogeneity of knowledge, given the knowledge
    relevant to diagnosis, MedKPL extracts and normalizes the relevant knowledge into
    a prompt sequence. Then, MedKPL integrates the knowledge prompt with the clinical
    note into a designed prompt for representation. Therefore, MedKPL can integrate
    medical knowledge into the models to enhance diagnosis and effectively transfer
    learned diagnosis capacity to unseen diseases using alternating relevant disease
    knowledge. The experimental results on two medical datasets show that our method
    can obtain better medical text classification results and can perform better in
    transfer and few-shot settings among datasets of different diseases. '
  authors:
  - Yuxing Lu
  - Xukai Zhao
  - Jinzhuo Wang
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_54
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Medical knowledge-enhanced prompt learning for diagnosis classification from
    clinical text
  tldr: Artificial intelligence based diagnosis systems have emerged as powerful tools
    to reform traditional medical care. Each clinician now wants to have his own intelligent
    diagnostic partner to expand the range of services he can provide.  When reading
    a clinical note, experts make inferences with relev
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Natural language tasks like Named Entity Recognition (NER) in the clinical
    domain on non-English texts can be very time-consuming and expensive due to the
    lack of annotated data. Cross-lingual transfer (CLT) is a way to circumvent this
    issue thanks to the ability of multilingual large language models to be fine-tuned
    on a specific task in one language and to provide high accuracy for the same task
    in another language. However, other methods leveraging translation models can
    be used to perform NER without annotated data in the target language, by either
    translating the training set or test set. This paper compares cross-lingual transfer
    with these two alternative methods, to perform clinical NER in French and in German
    without any training data in those languages. To this end, we release MedNERF
    a medical NER test set extracted from French drug prescriptions and annotated
    with the same guidelines as an English dataset. Through extensive experiments
    on this dataset and on a German medical dataset (Frei and Kramer, 2021), we show
    that translation-based methods can achieve similar performance to CLT but require
    more care in their design. And while they can take advantage of monolingual clinical
    language models, those do not guarantee better results than large general-purpose
    multilingual models, whether with cross-lingual transfer or translation.
  authors:
  - "F\xE9lix Gaschi"
  - Xavier Fontaine
  - Parisa Rastin
  - Yannick Toussaint
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_55
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Multilingual Clinical NER: Translation or Cross-lingual Transfer?'
  tldr: Natural language tasks like Named Entity Recognition (NER) in the clinical
    domain on non-English texts can be very time-consuming and expensive due to the
    lack of annotated data. Cross-lingual transfer (CLT) is a way to circumvent this
    issue thanks to the ability of multilingual large language model
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Pre-trained transformer language models (LMs) have in recent years become
    the dominant paradigm in applied NLP. These models have achieved state-of-the-art
    performance on tasks such as information extraction, question answering, sentiment
    analysis, document classification and many others. In the biomedical domain, significant
    progress has been made in adapting this paradigm to NLP tasks that require the
    integration of domain-specific knowledge as well as statistical modelling of language.
    In particular, research in this area has focused on the question of how best to
    construct LMs that take into account not only the patterns of token distribution
    in medical text, but also the wealth of structured information contained in terminology
    resources such as the UMLS. This work contributes a data-centric paradigm for
    enriching the language representations of biomedical transformer-encoder LMs by
    extracting text sequences from the UMLS.

    This allows for graph-based learning objectives to be combined with masked-language
    pre-training. Preliminary results from experiments in the extension of pre-trained
    LMs as well as training from scratch show that this framework improves downstream
    performance on multiple biomedical and clinical Named Entity Recognition (NER)
    tasks. All pre-trained models, data processing pipelines and evaluation scripts
    will be made publicly available.'
  authors:
  - Aidan Mannion
  - Didier Schwab
  - Lorraine Goeuriot
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_56
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'UMLS-KGI-BERT: Data-Centric Knowledge Integration in Transformers for Biomedical
    Entity Recognition'
  tldr: Pre-trained transformer language models (LMs) have in recent years become
    the dominant paradigm in applied NLP. These models have achieved state-of-the-art
    performance on tasks such as information extraction, question answering, sentiment
    analysis, document classification and many others. In the bio
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper describes our submission to the MEDIQA-Chat 2023 shared task
    for automatic clinical note generation from doctor-patient conversations. We report
    results for two approaches: the first fine-tunes a pre-trained language model
    (PLM) on the shared task data, and the second uses few-shot in-context learning
    (ICL) with a large language model (LLM). Both achieve high performance as measured
    by automatic metrics (e.g. ROUGE, BERTScore) and ranked second and first, respectively,
    of all submissions to the shared task. Expert human scrutiny indicates that notes
    generated via the ICL-based approach with GPT-4 are preferred about as often as
    human-written notes, making it a promising path toward automated note generation
    from doctor-patient conversations.'
  authors:
  - John Giorgi
  - Augustin Toma
  - Ronald Xie
  - Sondra Chen
  - Kevin An
  - Grace Zheng
  - BO Wang
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_57
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'WangLab at MEDIQA-Chat 2023: Clinical Note Generation from Doctor-Patient
    Conversations using Large Language Models'
  tldr: 'This paper describes our submission to the MEDIQA-Chat 2023 shared task for
    automatic clinical note generation from doctor-patient conversations. We report
    results for two approaches: the first fine-tunes a pre-trained language model
    (PLM) on the shared task data, and the second uses few-shot in-con'
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The disease coding task involves assigning a unique identifier from a
    controlled vocabulary to each disease mentioned in a clinical document. This task
    is relevant since it allows information extraction from unstructured data to perform,
    for example, epidemiological studies about the incidence and prevalence of diseases
    in a determined context. However, the manual coding process is subject to errors
    as it requires medical personnel to be competent in coding rules and terminology.
    In addition, this process consumes a lot of time and energy, which could be allocated
    to more clinically relevant tasks. These difficulties can be addressed by developing
    computational systems that automatically assign codes to diseases. In this way,
    we propose a two-step system for automatically coding diseases in referrals from
    the Chilean public healthcare system. Specifically, our model uses a state-of-the-art
    NER model for recognizing disease mentions and a search engine system based on
    Elasticsearch for assigning the most relevant codes associated with these disease
    mentions. The system's performance was evaluated on referrals manually coded by
    clinical experts. Our system obtained a MAP score of 0.63 for the subcategory
    level and 0.83 for the category level, close to the best-performing models in
    the literature. This system could be a support tool for health professionals,
    optimizing the coding and management process. Finally, to guarantee reproducibility,
    we publicly release the code of our models and experiments.
  authors:
  - "Fabi\xE1n Villena"
  - "Mat\xEDas Rojas"
  - Felipe Arias
  - Jorge Pacheco
  - Paulina Vera
  - Jocelyn Dunstan
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_58
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Automatic Coding at Scale: Design and Deployment of a Nationwide System
    for Normalizing Referrals in the Chilean Public Healthcare System'
  tldr: The disease coding task involves assigning a unique identifier from a controlled
    vocabulary to each disease mentioned in a clinical document. This task is relevant
    since it allows information extraction from unstructured data to perform, for
    example, epidemiological studies about the incidence and p
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper explores methods for extracting information from radiology
    reports that generalize across exam modalities to reduce requirements for annotated
    data. We demonstrate that multi-pass T5-based text-to-text generative models exhibit
    better generalization across exam modalities compared to approaches that employ
    BERT-based task-specific classification layers.  We then develop methods that
    reduce the inference cost of the model, making large-scale corpus processing more
    feasible for clinical applications. Specifically, we introduce a generative technique
    that decomposes complex tasks into smaller subtask blocks, which improves a single-pass
    model when combined with multitask training. In addition, we leverage target-domain
    contexts during inference to enhance domain adaptation, enabling use of smaller
    models. Analyses offer insights into the benefits of different cost reduction
    strategies. '
  authors:
  - Sitong Zhou
  - Meliha Yetisgen
  - Mari Ostendorf
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_62
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Building blocks for complex tasks:  Robust generative event extraction for
    radiology reports under domain shifts'
  tldr: This paper explores methods for extracting information from radiology reports
    that generalize across exam modalities to reduce requirements for annotated data.
    We demonstrate that multi-pass T5-based text-to-text generative models exhibit
    better generalization across exam modalities compared to appr
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Detecting testimonial injustice is an essential element of addressing
    inequities and promoting inclusive healthcare practices, many of which are life-critical.
    However, using a single demographic factor to detect testimonial injustice does
    not fully encompass the nuanced identities that contribute to a patient''s experience.
    Further, some injustices may only be evident when examining the nuances that arise
    through the lens of intersectionality. Ignoring such injustices can result in
    poor quality of care or life-endangering events. Thus, considering intersectionality
    could result in more accurate classifications and just decisions. To illustrate
    this, we use real-world medical data to determine whether medical records exhibit
    words that could lead to testimonial injustice, employ fairness metrics (e.g.
    demographic parity, differential intersectional fairness, and subgroup fairness)
    to assess the severity to which subgroups are experiencing testimonial injustice,
    and analyze how the intersectionality of demographic features (e.g. gender and
    race) make a difference in uncovering testimonial injustice. From our analysis
    we found that with intersectionality we can better see disparities in how subgroups
    are treated and there are differences in how someone is treated based on the intersection
    of their demographic attributes. This has not been previously studied in clinical
    records, nor has it been proven through empirical study. '
  authors:
  - Kenya Andrews
  - Bhuvni Shah
  - Lu Cheng
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_65
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Intersectionality and Testimonial Injustice in Medical Records
  tldr: Detecting testimonial injustice is an essential element of addressing inequities
    and promoting inclusive healthcare practices, many of which are life-critical.
    However, using a single demographic factor to detect testimonial injustice does
    not fully encompass the nuanced identities that contribute t
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Motivated by the scarcity of high-quality labeled biomedical text, as
    well as the success of data programming, we introduce KRISS-Search. By leveraging
    the Unified Medical Language Systems (UMLS) ontology, KRISS-Search addresses an
    interactive few-shot span recommendation task that we propose. We first introduce
    unsupervised KRISS-Search and show that our method outperforms existing methods
    in identifying spans that are semantically similar to a given span of interest,
    with >50% AUPRC improvement relative to PubMedBERT. We then introduce supervised
    KRISS-Search, which leverages human interaction to improve the notion of similarity
    used by unsupervised KRISS-Search. Through simulated human feedback, we demonstrate
    an enhanced F1 score of 0.68 in classifying spans as semantically similar or different
    in the low-label setting, outperforming PubMedBERT by 2 F1 points. Finally, supervised
    KRISS-Search demonstrates competitive or superior performance compared to PubMedBERT
    in few-shot biomedical named entity recognition (NER) across five benchmark datasets,
    with an average improvement of 5.6 F1 points. We envision KRISS-Search increasing
    the efficiency of programmatic data labeling and also providing broader utility
    as an interactive biomedical search engine.
  authors:
  - Louis Blankemeier
  - Theodore Zhao
  - Robert Tinn
  - Sid Kiblawi
  - Yu Gu
  - Akshay Chaudhari
  - Hoifung Poon
  - Sheng Zhang
  - Mu Wei
  - J. Preston
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_66
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Interactive Span Recommendation for Biomedical Text
  tldr: Motivated by the scarcity of high-quality labeled biomedical text, as well
    as the success of data programming, we introduce KRISS-Search. By leveraging the
    Unified Medical Language Systems (UMLS) ontology, KRISS-Search addresses an interactive
    few-shot span recommendation task that we propose. We fi
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Social determinants of health (SDOH) documented in the electronic health
    record through unstructured text are increasingly being studied to understand
    how SDOH impacts patient health outcomes. In this work, we utilize the Social
    History Annotation Corpus (SHAC), a multi-institutional corpus of de-identified
    social history sections annotated for SDOH, including substance use, employment,
    and living status information. We explore the automatic extraction of SDOH information
    with SHAC in both standoff and inline annotation formats using GPT-4 in a one-shot
    prompting setting. We compare GPT-4 extraction performance with a high-performing
    supervised approach and perform thorough error analyses. Our prompt-based GPT-4
    method achieved an overall 0.652 F1 on the SHAC test set, similar to the 7th best-performing
    system among all teams in the n2c2 challenge with SHAC. '
  authors:
  - Giridhar Kaushik Ramachandran
  - Yujuan Fu
  - Bin Han
  - Kevin Lybarger
  - Nic Dobbins
  - Ozlem Uzuner
  - Meliha Yetisgen
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_68
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Prompt-based Extraction of Social Determinants of Health Using Few-shot Learning
  tldr: Social determinants of health (SDOH) documented in the electronic health record
    through unstructured text are increasingly being studied to understand how SDOH
    impacts patient health outcomes. In this work, we utilize the Social History Annotation
    Corpus (SHAC), a multi-institutional corpus of de-id
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In this paper, we introduce the design and various attempts for TaskB
    of MEDIQA-Chat 2023. The goal of TaskB in MEDIQA-Chat 2023 is to generate full
    clinical note from doctor-patient consultation dialogues. This task has several
    challenging issues, such as lack of training data, handling long dialogue inputs,
    and generating semi-structured clinical note which have section heads. To address
    these issues, we conducted various experiments and analyzed their results. We
    utilized the DialogLED model pre-trained on long dialogue data to handle long
    inputs, and we pre-trained on other dialogue datasets to address the lack of training
    data. We also attempted methods such as using prompts and contrastive learning
    for handling sections. This paper provides insights into clinical note generation
    through analyzing experimental methods and results, and it suggests future research
    directions.
  authors:
  - Yongbin Jeong
  - Ju-Hyuck Han
  - Kyung Min Chae
  - Yousang Cho
  - Hyunbin Seo
  - KyungTae Lim
  - Key-Sun Choi
  - Younggyun Hahm
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_69
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Teddysum at MEDIQA-Chat 2023: an analysis of fine-tuning strategy for long
    dialog summarization'
  tldr: In this paper, we introduce the design and various attempts for TaskB of MEDIQA-Chat
    2023. The goal of TaskB in MEDIQA-Chat 2023 is to generate full clinical note
    from doctor-patient consultation dialogues. This task has several challenging
    issues, such as lack of training data, handling long dialog
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Multi-label clinical text classification, such as automatic ICD coding,
    has always been a challenging subject in Natural Language Processing, due to its
    long, domain-specific documents and long-tail distribution over a large label
    set. Existing methods adopt different model architectures to encode the clinical
    notes. Whereas without digging out the useful connections between labels, the
    model presents a huge gap in predicting performances between rare and frequent
    codes. In this work, we propose a novel method for further mining the helpful
    relations between different codes via a relation-enhanced code encoder to improve
    the rare code performance. Starting from the simple code descriptions, the model
    reaches comparable, even better performances than models with heavy external knowledge.
    Our proposed method is evaluated on MIMIC-III, a common dataset in the medical
    domain. It outperforms the previous state-of-art models on both overall metrics
    and rare code performances. Moreover, the interpretation results further prove
    the effectiveness of our methods. Our code is publicly available at https://github.com/jiaminchen-1031/Rare-ICD.
  authors:
  - Jiamin Chen
  - Xuhong Li
  - Junting Xi
  - Lei Yu
  - Haoyi Xiong
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_70
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Rare Codes Count: Mining Inter-code Relations for Long-tail Clinical Text
    Classification'
  tldr: Multi-label clinical text classification, such as automatic ICD coding, has
    always been a challenging subject in Natural Language Processing, due to its long,
    domain-specific documents and long-tail distribution over a large label set. Existing
    methods adopt different model architectures to encode t
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper presents the MEDIQA-Chat 2023 shared task organized at the
    ACL-Clinical NLP workshop. The shared task is motivated by the need to develop
    methods to automatically generate clinical notes from doctor-patient conversations.
    In this paper, we present our submission for \textit{MEDIQA-Chat 2023 Task A:
    Short Dialogue2Note Summarization}. Manual creation of these clinical notes requires
    extensive human efforts, thus making it a time-consuming and expensive process.
    To address this, we propose an ensemble-based method over GPT-3, BART, BERT variants,
    and Rule-based systems to automatically generate clinical notes from these conversations.
    The proposed system achieves a score of 0.730 and 0.544 for both the sub-tasks
    on the test set (ranking 8th on the leaderboard for both tasks) and shows better
    performance compared to a baseline system using BART variants. '
  authors:
  - Prakhar Mishra
  - Ravi Theja Desetty
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_71
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'NewAgeHealthWarriors at MEDIQA-Chat 2023 Task A: Summarizing Short Medical
    Conversation with Transformers'
  tldr: This paper presents the MEDIQA-Chat 2023 shared task organized at the ACL-Clinical
    NLP workshop. The shared task is motivated by the need to develop methods to automatically
    generate clinical notes from doctor-patient conversations. In this paper, we present
    our submission for \textit{MEDIQA-Chat 20
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Aphasia and dysarthria are both common symptoms of stroke, affecting around
    30% and 50% of acute ischemic stroke patients. In this paper, we propose a storyline-centric
    approach to detect aphasia and dysarthria in acute stroke patients using transcribed
    picture descriptions alone. Our pipeline enriches the training set with healthy
    data to address the lack of acute stroke patient data and utilizes knowledge distillation
    to significantly improve upon a document classification baseline, achieving an
    AUC of 0.814 (aphasia) and 0.764 (dysarthria) on a patient-only validation set.
  authors:
  - Peiqi Sui
  - Kelvin Wong
  - Xiaohui Yu
  - John Volpi
  - Stephen Wong
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_72
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Storyline-Centric Detection of Aphasia and Dysarthria in Stroke Patient Transcripts
  tldr: Aphasia and dysarthria are both common symptoms of stroke, affecting around
    30% and 50% of acute ischemic stroke patients. In this paper, we propose a storyline-centric
    approach to detect aphasia and dysarthria in acute stroke patients using transcribed
    picture descriptions alone. Our pipeline enric
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The field of clinical natural language processing (NLP) can extract useful
    information from clinical text. Since 2017, the NLP field has shifted towards
    using pre-trained language models (PLMs), improving performance in several tasks.
    Most of the research in this field has focused on English text, but there are
    some available PLMs in Spanish. In this work, we use clinical PLMs to analyze
    text from admission and medical reports in Spanish for an insurance and health
    provider to give a probability of no coverage in a labor insurance process. Our
    results show that fine-tuning a PLM pre-trained with the provider's data leads
    to better results, but this process is time-consuming and computationally expensive.
    At least for this task, fine-tuning publicly available clinical PLM leads to comparable
    results to a custom PLM, but in less time and with fewer resources. Analyzing
    large volumes of insurance requests is burdensome for employers, and models can
    ease this task by pre-classifying reports that are likely not to have coverage.
    Our approach of entirely using clinical-related text improves the current models
    while reinforcing the idea of clinical support systems that simplify human labor
    but do not replace it. To our knowledge, the clinical corpus collected for this
    study is the largest one reported for the Spanish language.
  authors:
  - Claudio Aracena
  - "Nicol\xE1s Rodr\xEDguez"
  - Victor Rocco
  - Jocelyn Dunstan
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_74
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Pre-trained language models in Spanish for health insurance coverage
  tldr: The field of clinical natural language processing (NLP) can extract useful
    information from clinical text. Since 2017, the NLP field has shifted towards
    using pre-trained language models (PLMs), improving performance in several tasks.
    Most of the research in this field has focused on English text, b
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'In response to the global challenge of mental health problems, we proposes
    a Logical Neural Network (LNN) based Neuro-Symbolic AI method for the diagnosis
    of mental disorders. Due to the lack of effective therapy coverage for mental
    disorders, there is a need for an AI solution that can assist therapists with
    the diagnosis. However, current Neural Network models lack explainability and
    may not be trusted by therapists. The LNN is a Recurrent Neural Network architecture
    that combines the learning capabilities of neural networks with the reasoning
    capabilities of classical logic-based AI. The proposed system uses input predicates
    from clinical interviews to output a mental disorder class, and different predicate
    pruning techniques are used to achieve scalability and higher scores. In addition,
    we provide an insight extraction method to aid therapists with their diagnosis.
    The proposed system addresses the lack of explainability of current Neural Network
    models and provides a more trustworthy solution for mental disorder diagnosis.

    '
  authors:
  - Yeldar Toleubay
  - Don Joven Agravante
  - Daiki Kimura
  - Baihan Lin
  - Djallel Bouneffouf
  - Michiaki Tatsubori
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_75
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Utterance Classification with Logical Neural Network: Explainable AI for
    Mental Disorder Diagnosis'
  tldr: In response to the global challenge of mental health problems, we proposes
    a Logical Neural Network (LNN) based Neuro-Symbolic AI method for the diagnosis
    of mental disorders. Due to the lack of effective therapy coverage for mental
    disorders, there is a need for an AI solution that can assist thera
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Medical Report Generation (MRG) is a sub-task of  Natural Language Generation
    (NLG) and aims to present information from various sources in textual form and
    synthesize salient information, with the goal of reducing the time spent by domain
    experts in writing medical reports and providing support information for decision-making.
    Given the specificity of the medical domain, the evaluation of automatically generated
    medical reports is of paramount importance to the validity of these systems. Therefore,
    in this paper, we focus on the evaluation of automatically generated medical reports
    from the perspective of automatic and human evaluation. We present evaluation
    methods for general NLG evaluation and how they have been applied to domain-specific
    medical tasks. The study shows that MRG evaluation methods are very diverse, and
    that further work is needed to build shared evaluation methods. The state of the
    art also emphasizes that such an evaluation must be task specific and include
    human assessments, requesting the participation of experts in the field.
  authors:
  - Yongxin Zhou
  - Fabien Ringeval
  - "Fran\xE7ois Portet"
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_76
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: A Survey of Evaluation Methods of Generated Medical Textual Reports
  tldr: Medical Report Generation (MRG) is a sub-task of  Natural Language Generation
    (NLG) and aims to present information from various sources in textual form and
    synthesize salient information, with the goal of reducing the time spent by domain
    experts in writing medical reports and providing support inf
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper presents UMASS_BioNLP team participation in the MEDIQA-Chat
    2023 shared task for Task-A and Task-C. We focus especially on Task-C and propose
    a novel LLMs cooperation system named a doctor-patient loop to generate high-quality
    conversation data sets. The experiment results demonstrate that our approaches
    yield reasonable performance as evaluated by automatic metrics such as ROUGE,
    medical concept recall, BLEU, and Self-BLEU.  Furthermore, we conducted a comparative
    analysis between our proposed method and ChatGPT and GPT-4. This analysis also
    investigates the potential of utilizing cooperation LLMs to generate high-quality
    datasets.
  authors:
  - Junda Wang
  - Zonghai Yao
  - Avijit Mitra
  - Samuel Osebe
  - Zhichao Yang
  - Hong yu
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_78
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'UMASS_BioNLP at MEDIQA-Chat 2023:   Can LLMs generate high-quality synthetic
    note-oriented doctor-patient conversations?'
  tldr: This paper presents UMASS_BioNLP team participation in the MEDIQA-Chat 2023
    shared task for Task-A and Task-C. We focus especially on Task-C and propose a
    novel LLMs cooperation system named a doctor-patient loop to generate high-quality
    conversation data sets. The experiment results demonstrate tha
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In recent years, we have seen many Transformer based models being created
    to address Dialog Summarization problem. While there has been a lot of work on
    understanding how these models stack against each other in summarizing regular
    conversations such as the ones found in DialogSum dataset, there haven't been
    many analysis of these models on Clinical Dialog Summarization. In this article,
    we describe our solution to MEDIQA-Chat 2023 Shared Tasks as part of ACL-ClinicalNLP
    2023 workshop which benchmarks some of the popular Transformer Architectures such
    as BioBart, Flan-T5, DialogLED, and OpenAI GPT3 on the problem of Clinical Dialog
    Summarization. We analyse their performance on two tasks - summarizing short conversations
    and long conversations. In addition to this, we also benchmark two popular summarization
    ensemble methods and report their performance.
  authors:
  - Kunal Suri
  - Saumajit Saha
  - Atul Singh
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_79
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'HealthMavericks@MEDIQA-Chat 2023: Benchmarking different Transformer based
    models for Clinical Dialogue Summarization'
  tldr: In recent years, we have seen many Transformer based models being created
    to address Dialog Summarization problem. While there has been a lot of work on
    understanding how these models stack against each other in summarizing regular
    conversations such as the ones found in DialogSum dataset, there hav
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Medical dialogue summarization is challenging due to the unstructured
    nature of medical conversations, the use of medical terminology

    in gold summaries, and the need to identify key information across multiple symptom
    sets. We present a novel system for the Dialogue2Note Medical Summarization tasks
    in the MEDIQA 2023 Shared Task. Our approach for sectionwise summarization (Task
    A) is a two-stage process of selecting semantically similar dialogues and using
    the top-k similar dialogues as in-context examples for GPT-4. For full-note summarization
    (Task B), we use a similar solution with k=1. We achieved 3rd place in Task A
    (2nd among all teams), 4th place in Task B Division Wise Summarization (2nd among
    all teams), 15th place in Task A Section Header Classification (9th among all
    teams), and 8th place among all teams in Task B. Our results highlight the effectiveness
    of few-shot prompting for this task, though we also identify several weaknesses
    of prompting-based approaches. We compare GPT-4 performance with several finetuned
    baselines. We find that GPT-4 summaries are more abstractive and shorter. We make
    our code publicly available.'
  authors:
  - Yash Mathur
  - Sanketh Rangreji
  - Raghav Kapoor
  - Medha Palavalli
  - Amanda Bertsch
  - Matthew Gormley
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_80
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'SummQA at MEDIQA-Chat 2023: In-Context Learning with GPT-4 for Medical Summarization'
  tldr: 'Medical dialogue summarization is challenging due to the unstructured nature
    of medical conversations, the use of medical terminology

    in gold summaries, and the need to identify key information across multiple symptom
    sets. We present a novel system for the Dialogue2Note Medical Summarization tasks '
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Automatic generation of clinical notes from doctor-patient conversations
    can play a key role in reducing daily doctors'' workload and improving their interactions
    with the patients. MEDIQA-Chat 2023 aims to advance and promote research on effective
    solutions through shared tasks on the automatic summarization of doctor-patient
    conversations and on the generation of synthetic dialogues from clinical notes
    for data augmentation. Seventeen teams participated in the challenge and experimented
    with a broad range of approaches and models. In this paper, we describe the three
    MEDIQA-Chat 2023 tasks, the datasets, and the participants'' results and methods.
    We hope that these shared tasks will lead to additional research efforts and insights
    on the automatic generation and evaluation of clinical notes. '
  authors:
  - Asma Ben Abacha
  - Wen-wai Yim
  - Griffin Adams
  - Neal Snider
  - Meliha Yetisgen
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_81
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Overview of the MEDIQA-Chat 2023 Shared Tasks on the Summarization & Generation
    of Doctor-Patient Conversations
  tldr: Automatic generation of clinical notes from doctor-patient conversations can
    play a key role in reducing daily doctors' workload and improving their interactions
    with the patients. MEDIQA-Chat 2023 aims to advance and promote research on effective
    solutions through shared tasks on the automatic summ
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We propose a transfer learning method that adapts a high-resource English
    clinical NER model to low-resource languages and domains using only small amounts
    of in-domain annotated data. Our approach involves translating in-domain datasets
    to English, fine-tuning the English model on the translated data, and then transferring
    it to the target language/domain. Experiments on Spanish, French, and conversational
    clinical text datasets show accuracy gains over models trained on target data
    alone. Our method achieves state-of-the-art performance and can enable clinical
    NLP in more languages and modalities with limited resources.
  authors:
  - Nevasini Sasikumar
  - Krishna Sri Ipsit Mantri
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_83
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Transfer Learning for Low-Resource Clinical Named Entity Recognition
  tldr: We propose a transfer learning method that adapts a high-resource English
    clinical NER model to low-resource languages and domains using only small amounts
    of in-domain annotated data. Our approach involves translating in-domain datasets
    to English, fine-tuning the English model on the translated da
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Clinical conversation summarization has become an important application
    of Natural language Processing. In this work, we intend to analyze summarization
    model ensembling approaches, that can be utilized to improve the overall accuracy
    of the generated medical report called chart note. The work starts with a single
    summarization model creating the baseline. Then leads to an ensemble of summarization
    models trained on a separate section of the chart note. This leads to the final
    approach of passing the generated results to another summarization model in a
    multi-layer/stage fashion for better coherency of the generated text. Our results
    indicate that although an ensemble of models specialized in each section produces
    better results, the multi-layer/stage approach does not improve accuracy. The
    code for the above paper is available at https://github.com/dhananjay-srivastava/MEDIQA-Chat-2023-iuteam1.git
  authors:
  - Dhananjay Srivastava
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_84
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'IUTEAM1 at MEDIQA-Chat 2023: Is simple fine tuning effective for multi layer
    summarization of clinical conversations?'
  tldr: Clinical conversation summarization has become an important application of
    Natural language Processing. In this work, we intend to analyze summarization
    model ensembling approaches, that can be utilized to improve the overall accuracy
    of the generated medical report called chart note. The work start
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Summarizing medical conversations is one of the tasks proposed by MEDIQA-Chat
    to promote research on automatic clinical note generation from doctor-patient
    conversations. In this paper, we present our submission to this task using fine-tuned
    language models, including T5, BART and BioGPT models. The fine-tuned models are
    evaluated using ensemble metrics including ROUGE, BERTScore and

    BLEURT. Among the fine-tuned models, Flan-T5 achieved the highest aggregated score
    for dialogue summarization.'
  authors:
  - Amal Alqahtani
  - Rana Salama
  - Mona Diab
  - Abdou Youssef
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_85
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Care4Lang at MEDIQA-Chat 2023: Fine-tuning Language Models for Classifying
    and Summarizing Clinical Dialogues'
  tldr: Summarizing medical conversations is one of the tasks proposed by MEDIQA-Chat
    to promote research on automatic clinical note generation from doctor-patient
    conversations. In this paper, we present our submission to this task using fine-tuned
    language models, including T5, BART and BioGPT models. The
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper presents our system for the MEDIQA-Chat 2023 shared task on
    medical conversation summarization. Our approach involves finetuning a LongT5
    model on multiple tasks simultaneously, which we demonstrate improves the model's
    overall performance while reducing the number of factual errors and hallucinations
    in the generated summary. Furthermore, we investigated the effect of augmenting
    the data with in-text annotations from a clinical named entity recognition model,
    finding that this approach decreased summarization quality. Lastly, we explore
    using different text generation strategies for medical note generation based on
    the length of the note. Our findings suggest that the application of our proposed
    approach can be beneficial for improving the accuracy and effectiveness of medical
    conversation summarization.
  authors:
  - Kirill Milintsevich
  - Navneet Agarwal
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_86
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Calvados at MEDIQA-Chat 2023: Improving Clinical Note Generation with Multi-Task
    Instruction Finetuning'
  tldr: This paper presents our system for the MEDIQA-Chat 2023 shared task on medical
    conversation summarization. Our approach involves finetuning a LongT5 model on
    multiple tasks simultaneously, which we demonstrate improves the model's overall
    performance while reducing the number of factual errors and h
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper presents the results of the Data Science for Digital Health
    (DS4DH) group in the MEDIQA-Chat Tasks at ACL-ClinicalNLP 2023. Our study combines
    the power of a classical machine learning method, Support Vector Machine, for
    classifying medical dialogues, along with the implementation of one-shot prompts
    using GPT-3.5. We employ dialogues and summaries from the same category as prompts
    to generate summaries for novel dialogues. Our findings exceed the average benchmark
    score, offering a robust reference for assessing performance in this field.
  authors:
  - Boya Zhang
  - Rahul Mishra
  - Douglas Teodoro
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_87
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'DS4DH at MEDIQA-Chat 2023: Leveraging SVM and GPT-3 Prompt Engineering for
    Medical Dialogue Classification and Summarization'
  tldr: This paper presents the results of the Data Science for Digital Health (DS4DH)
    group in the MEDIQA-Chat Tasks at ACL-ClinicalNLP 2023. Our study combines the
    power of a classical machine learning method, Support Vector Machine, for classifying
    medical dialogues, along with the implementation of one-
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper presents our contribution to the MEDIQA-2023 Dialogue2Note
    shared task, encompassing both subtask A and subtask B. We approach the task as
    a dialogue summarization problem and implement two distinct pipelines: (a) a fine-tuning
    of a pre-trained dialogue summarization model and GPT-3, and (b) few-shot in-context
    learning (ICL) using a large language model, GPT-4. Both methods achieve excellent
    results in terms of ROUGE-1 F1, BERTScore F1 (deberta-xlarge-mnli), and BLEURT,
    with scores of 0.4011, 0.7058, and 0.5421, respectively. Additionally, we predict
    the associated section headers using RoBERTa and SciBERT based classification
    models. Our team ranked fourth among all teams, while each team is allowed to
    submit three runs as part of their submission. We also utilize expert annotations
    to demonstrate that the notes generated through the ICL GPT-4 are better than
    all other baselines. The code for our submission is available.'
  authors:
  - Xiangru Tang
  - Andrew Tran
  - Jeffrey Tan
  - Mark Gerstein
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Clinical-NLP
  forum: ''
  id: ClinicalNLP_88
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'GersteinLab at MEDIQA-Chat 2023: Clinical Note Summarization from Doctor-Patient
    Conversations through Fine-tuning and In-context Learning'
  tldr: 'This paper presents our contribution to the MEDIQA-2023 Dialogue2Note shared
    task, encompassing both subtask A and subtask B. We approach the task as a dialogue
    summarization problem and implement two distinct pipelines: (a) a fine-tuning
    of a pre-trained dialogue summarization model and GPT-3, and '
  track: The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: State-of-the-art sign language generation frameworks lack expressivity
    and naturalness which is the result of only focusing manual signs, neglecting
    the affective, grammatical and semantic functions of facial expressions. The purpose
    of this work is to  augment semantic representation of sign language through grounding
    facial expressions. We study the effect of modeling the relationship between text,
    gloss, and facial expressions on the performance of the sign generation systems.
    In particular, we propose a Dual Encoder Transformer able to generate manual signs
    as well as facial expressions by capturing the similarities and differences found
    in text and sign gloss annotation. We take into consideration the role of facial
    muscle activity to express intensities of manual signs by being the first to employ
    facial action units in sign language generation. We perform a series of experiments
    showing that our proposed model improves the quality of automatically generated
    sign language.
  authors:
  - Carla Viegas
  - Mert Inan
  - Lorna Quandt
  - Malihe Alikhani
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SEM
  forum: ''
  id: StarSEM_2
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Including Facial Expressions in Contextual Embeddings for Sign Language Generation
  tldr: State-of-the-art sign language generation frameworks lack expressivity and
    naturalness which is the result of only focusing manual signs, neglecting the
    affective, grammatical and semantic functions of facial expressions. The purpose
    of this work is to  augment semantic representation of sign langua
  track: The 12th Joint Conference on Lexical and Computational Semantics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Multimodal embeddings aim to enrich the semantic information in neural
    representations of language compared to text-only models. While different embeddings
    exhibit different applicability and performance on downstream tasks, little is
    known about the systematic representation differences attributed to the visual
    modality.  Our paper compares word embeddings from three vision-and-language models
    (CLIP, OpenCLIP and Multilingual CLIP, Radford et al. 2021; Ilharco et al. 2021;
    Carlsson et al. 2022) and three text-only models, with static (FastText, Bojanowski
    et al. 2017) as well as contextual representations (multilingual BERT Devlin et
    al. 2018; XLM-RoBERTa, Conneau et al. 2019). This is the first large-scale study
    of the effect of visual grounding on language representations, including 46 semantic
    parameters. We identify meaning properties and relations that characterize words
    whose embeddings are most affected by the inclusion of visual modality in the
    training data; that is, points where visual grounding turns out most important.
    We find that the effect of visual modality correlates most with denotational semantic
    properties related to concreteness, but is also detected for several specific
    semantic classes, as well as for valence, a sentiment-related connotational property
    of linguistic expressions.
  authors:
  - Lisa Bylinina
  - Denis Paperno
  - Alexey Tikhonov
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SEM
  forum: ''
  id: StarSEM_3
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Leverage Points in Modality Shifts: Comparing Language-only and Multimodal
    Word Representations'
  tldr: 'Multimodal embeddings aim to enrich the semantic information in neural representations
    of language compared to text-only models. While different embeddings exhibit different
    applicability and performance on downstream tasks, little is known about the systematic
    representation differences attributed '
  track: The 12th Joint Conference on Lexical and Computational Semantics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Negation scope resolution is the process of detecting the negated part
    of a sentence. Unlike the syntax-based approach employed in previous research,
    state-of-the-art methods performed better without the explicit use of syntactic
    structure. This work revisits the syntax-based approach and re-evaluates the effectiveness
    of syntactic structure in negation scope resolution. We replace the parser utilized
    in the prior works with state-of-the-art parsers and modify the syntax-based heuristic
    rules. The experimental results demonstrate that the simple modifications enhance
    the performance of the prior syntax-based method to the same level as state-of-the-art
    end-to-end neural-based methods.
  authors:
  - Asahi Yoshida
  - Yoshihide Kato
  - Shigeki Matsubara
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SEM
  forum: ''
  id: StarSEM_4
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Revisiting Syntax-Based Approach in Negation Scope Resolution
  tldr: Negation scope resolution is the process of detecting the negated part of
    a sentence. Unlike the syntax-based approach employed in previous research, state-of-the-art
    methods performed better without the explicit use of syntactic structure. This
    work revisits the syntax-based approach and re-evaluat
  track: The 12th Joint Conference on Lexical and Computational Semantics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In this paper, we focus on the ability of large language models (LLMs)
    to accommodate different pragmatic sentence types, such as questions, commands,
    as well as sentence fragments for natural language inference (NLI). On the commonly
    used notion of logical inference, nothing can be inferred from a question, an
    order, or an incomprehensible sentence fragment. We find MNLI, arguably the most
    important NLI dataset, and hence models fine-tuned on this dataset, insensitive
    to this fact. Using a symbolic semantic parser, we develop and make publicly available,
    fine-tuning datasets designed specifically to address this issue, with promising
    results. We also make a first exploration of ChatGPT's concept of entailment.
  authors:
  - Reto Gubelmann
  - Aikaterini-lida Kalouli
  - Christina Niklaus
  - Siegfried Handschuh
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SEM
  forum: ''
  id: StarSEM_8
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: When Truth Matters - Addressing Pragmatic Categories in Natural Language
    Inference (NLI) by Large Language Models (LLMs)
  tldr: 'In this paper, we focus on the ability of large language models (LLMs) to
    accommodate different pragmatic sentence types, such as questions, commands, as
    well as sentence fragments for natural language inference (NLI). On the commonly
    used notion of logical inference, nothing can be inferred from a '
  track: The 12th Joint Conference on Lexical and Computational Semantics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Using Japanese honorifics is challenging because it requires not only
    knowledge of the grammatical rules but also contextual information, such as social
    relationships. It remains unclear whether pre-trained large language models (LLMs)
    can flexibly handle Japanese honorifics like humans. To analyze this, we introduce
    an honorific conversion task that considers social relationships among people
    mentioned in a conversation. We construct a Japanese honorifics dataset from problem
    templates of various sentence structures to investigate the syntactic generalization
    capacity of GPT-3, one of the leading LLMs, on this task under two settings: fine-tuning
    and prompt learning.Our results showed that the fine-tuned GPT-3 performed better
    in a context-aware honorific conversion task than the prompt-based one.The fine-tuned
    model demonstrated overall syntactic generalizability towards compound honorific
    sentences, except when tested with the data involving direct speech.'
  authors:
  - Ryo Sekizawa
  - Hitomi Yanaka
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SEM
  forum: ''
  id: StarSEM_9
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Analyzing Syntactic Generalization Capacity of Pre-trained Language Models
    on Japanese Honorific Conversion
  tldr: 'Using Japanese honorifics is challenging because it requires not only knowledge
    of the grammatical rules but also contextual information, such as social relationships.
    It remains unclear whether pre-trained large language models (LLMs) can flexibly
    handle Japanese honorifics like humans. To analyze '
  track: The 12th Joint Conference on Lexical and Computational Semantics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Geocoding is the task of converting location mentions in text into structured
    data that encodes the geospatial semantics. We propose a new architecture for
    geocoding, GeoNorm. GeoNorm first uses information retrieval techniques to generate
    a list of candidate entries from the geospatial ontology.Then it reranks the candidate
    entries using a transformer-based neural network that incorporates information
    from the ontology such as the entry''s population. This generate-and-rerank process
    is applied twice: first to resolve the less ambiguous countries, states, and counties,
    and second to resolve the remaining location mentions, using the identified countries,
    states, and counties as context. Our proposed toponym resolution framework achieves
    state-of-the-art performance on multiple datasets.Code and models are available
    at \textbackslash{}url\{https://github.com/clulab/geonorm\}.'
  authors:
  - Zeyu Zhang
  - Steven Bethard
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SEM
  forum: ''
  id: StarSEM_13
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Improving Toponym Resolution with Better Candidate Generation, Transformer-based
    Reranking, and Two-Stage Resolution
  tldr: Geocoding is the task of converting location mentions in text into structured
    data that encodes the geospatial semantics. We propose a new architecture for
    geocoding, GeoNorm. GeoNorm first uses information retrieval techniques to generate
    a list of candidate entries from the geospatial ontology.The
  track: The 12th Joint Conference on Lexical and Computational Semantics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Automatic image comprehension is an important yet challenging task that
    includes identifying actions in an image and corresponding action participants.  Most
    current approaches to this task, now termed Grounded Situation Recognition (GSR),
    start by predicting a verb that describes the action and then predict the nouns
    that can participate in the action as arguments to the verb. This problem formulation
    limits each image to a single action even though several actions could be depicted.  In
    contrast, text-based Semantic Role Labeling (SRL) aims to label all actions in
    a sentence, typically resulting in at least two or three predicate argument structures
    per sentence.  We hypothesize that expanding GSR to follow the more liberal SRL
    text-based approach to action and participant identification could improve image
    comprehension results. To test this hypothesis and to preserve generalization
    capabilities, we use general-purpose vision and language components as a front-end.
    This paper presents our results, a substantial 28.6 point jump in performance
    on the SWiG dataset, which confirm our hypothesis. We also discuss the benefits
    of loosely coupled broad-coverage off-the-shelf components which generalized well
    to out of domain images, and can decrease the need for manual image semantic role
    annotation.
  authors:
  - Abhidip Bhattacharyya
  - Martha Palmer
  - Christoffer Heckman
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SEM
  forum: ''
  id: StarSEM_15
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: CRAPES:Cross-modal Annotation Projection for Visual Semantic Role Labeling
  tldr: Automatic image comprehension is an important yet challenging task that includes
    identifying actions in an image and corresponding action participants.  Most current
    approaches to this task, now termed Grounded Situation Recognition (GSR), start
    by predicting a verb that describes the action and the
  track: The 12th Joint Conference on Lexical and Computational Semantics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Counterhate arguments can effectively fight and limit the spread of hate
    speech. However, they can also exacerbate the hate, as some people may respond
    with aggression if they feel threatened or targeted by the counterhate. In this
    paper, we investigate replies to counterhate arguments beyond whether the reply
    agrees or disagrees with the counterhate argument. We present a corpus with 2,621
    replies to counterhate arguments countering hateful tweets, and annotate them
    with fine-grained characteristics. We show that (a) half of the replies (51\%)
    to the counterhate arguments disagree with the argument, and (b) this kind of
    reply often supports the hateful tweet (40\%). We also analyze the language of
    counterhate arguments that elicit certain types of replies. Experimental results
    show that it is feasible to anticipate the kind of replies a counterhate argument
    will elicit.
  authors:
  - Abdullah Albanyan
  - Ahmed Hassan
  - Eduardo Blanco
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SEM
  forum: ''
  id: StarSEM_23
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Not All Counterhate Tweets Elicit the Same Replies: A Fine-Grained Analysis'
  tldr: Counterhate arguments can effectively fight and limit the spread of hate speech.
    However, they can also exacerbate the hate, as some people may respond with aggression
    if they feel threatened or targeted by the counterhate. In this paper, we investigate
    replies to counterhate arguments beyond whethe
  track: The 12th Joint Conference on Lexical and Computational Semantics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Automated evaluation of text generation systems has recently seen increasing
    attention, particularly checking whether generated text stays truthful to input
    sources.Existing methods frequently rely on an evaluation using task-specific
    language models, which in turn allows for little interpretability of generated
    scores.We introduce SRLScore, a reference-free evaluation metric designed with
    text summarization in mind. Our approach generates fact tuples constructed from
    Semantic Role Labels, applied to both input and summary texts.A final factuality
    score is computed by an adjustable scoring mechanism, which allows for easy adaption
    of the method across domains.  Correlation with human judgments on English summarization
    datasets shows that SRLScore is competitive with state-of-the-art methods and
    exhibits stable generalization across datasets without requiring further training
    or hyperparameter tuning.We experiment with an optional co-reference resolution
    step, but find that the performance boost is mostly outweighed by the additional
    compute required.Our metric is available online at: https://github.com/heyjing/SRLScore'
  authors:
  - Jing Fan
  - Dennis Aumiller
  - Michael Gertz
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SEM
  forum: ''
  id: StarSEM_24
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Evaluating Factual Consistency of Texts with Semantic Role Labeling
  tldr: Automated evaluation of text generation systems has recently seen increasing
    attention, particularly checking whether generated text stays truthful to input
    sources.Existing methods frequently rely on an evaluation using task-specific
    language models, which in turn allows for little interpretability
  track: The 12th Joint Conference on Lexical and Computational Semantics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Negation has been shown to be a major bottleneck for masked language models,
    such as BERT. However, whether this finding still holds for larger-sized auto-regressive
    language models ("LLMs") has not been studied comprehensively. With the ever-increasing
    volume of research and applications of LLMs, we take a step back to evaluate the
    ability of current-generation LLMs to handle negation, a fundamental linguistic
    phenomenon that is central to language understanding. We evaluate different LLMs
    - including the open-source GPT-neo, GPT-3, and InstructGPT - against a wide range
    of negation benchmarks. Through systematic experimentation with varying model
    sizes and prompts, we show that LLMs have several limitations including insensitivity
    to the presence of negation, an inability to capture the lexical semantics of
    negation, and a failure to reason under negation.
  authors:
  - Thinh Hung Truong
  - Timothy Baldwin
  - Karin Verspoor
  - Trevor Cohn
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SEM
  forum: ''
  id: StarSEM_25
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Language models are not naysayers: an analysis of language models on negation
    benchmarks'
  tldr: Negation has been shown to be a major bottleneck for masked language models,
    such as BERT. However, whether this finding still holds for larger-sized auto-regressive
    language models ("LLMs") has not been studied comprehensively. With the ever-increasing
    volume of research and applications of LLMs, w
  track: The 12th Joint Conference on Lexical and Computational Semantics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'We propose a graph-based event extraction framework JSEEGraph that approaches
    the task of event extraction as general graph parsing in the tradition of Meaning
    Representation Parsing. It explicitly encodes entities and events in a single
    semantic graph, and further has the flexibility to encode a wider range of additional
    IE relations and jointly infer individual tasks. JSEEGraph performs in an end-to-end
    manner via general graph parsing: (1) instead of flat sequence labelling, nested
    structures between entities/triggers are efficiently encoded as separate nodes
    in the graph, allowing for nested and overlapping entities and triggers; (2) both
    entities, relations, and events can be encoded in the same graph, where entities
    and event triggers are represented as nodes and entity relations and event arguments
    are constructed via edges;  (3) joint inference avoids error propagation and enhances
    the interpolation of different IE tasks. We experiment on two benchmark datasets
    of varying structural complexities; ACE05 and Rich ERE, covering three languages:
    English, Chinese, and Spanish. Experimental results show that JSEEGraph can handle
    nested event structures, that it is beneficial to solve different IE tasks jointly,
    and that event argument extraction in particular benefits from entity extraction.
    Our code and models are released as open-source.'
  authors:
  - Huiling You
  - Lilja Vrelid
  - Samia Touileb
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SEM
  forum: ''
  id: StarSEM_26
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'JSEEGraph: Joint Structured Event Extraction as Graph Parsing'
  tldr: We propose a graph-based event extraction framework JSEEGraph that approaches
    the task of event extraction as general graph parsing in the tradition of Meaning
    Representation Parsing. It explicitly encodes entities and events in a single
    semantic graph, and further has the flexibility to encode a wi
  track: The 12th Joint Conference on Lexical and Computational Semantics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Aspect sentiment quad prediction (ASQP) analyzes the aspect terms, opinion
    terms, sentiment polarity, and aspect categories in a text. One challenge in this
    task is the scarcity of data owing to the high annotation cost. Data augmentation
    techniques are commonly used to address this issue. However, existing approaches
    simply rewrite texts in the training data, restricting the semantic diversity
    of the generated data and impairing the quality due to the inconsistency between
    text and quads. To address these limitations, we augment quads and train a quads-to-text
    model to generate corresponding texts. Furthermore, we designed novel strategies
    to filter out low-quality data and balance the sample difficulty distribution
    of the augmented dataset. Empirical studies on two ASQP datasets demonstrate that
    our method outperforms other data augmentation methods and achieves state-of-the-art
    performance on the benchmarks. The source code will be released upon acceptance.
  authors:
  - An Wang
  - Junfeng Jiang
  - Youmi Ma
  - Ao Liu
  - Naoaki Okazaki
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SEM
  forum: ''
  id: StarSEM_27
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Generative Data Augmentation for Aspect Sentiment Quad Prediction
  tldr: 'Aspect sentiment quad prediction (ASQP) analyzes the aspect terms, opinion
    terms, sentiment polarity, and aspect categories in a text. One challenge in this
    task is the scarcity of data owing to the high annotation cost. Data augmentation
    techniques are commonly used to address this issue. However, '
  track: The 12th Joint Conference on Lexical and Computational Semantics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In psycholinguistics, semantic attraction is a sentence processing phenomenon
    in which a given argument violates the selectional requirements of a verb, but
    this violation is not perceived by comprehenders due to its attraction to another
    noun in the same sentence, which is syntactically unrelated but semantically sound.In
    our study, we use autoregressive language models to compute the sentence-level
    and the target phrase-level Surprisal scores of a psycholinguistic dataset on
    semantic attraction. Our results show that the models are sensitive to semantic
    attraction, leading to reduced Surprisal scores, although none of them perfectly
    matches the human behavioral pattern.
  authors:
  - Yan Cong
  - Emmanuele Chersoni
  - Yu-yin Hsu
  - Alessandro Lenci
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SEM
  forum: ''
  id: StarSEM_28
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Are Language Models Sensitive to Semantic Attraction? A Study on Surprisal
  tldr: In psycholinguistics, semantic attraction is a sentence processing phenomenon
    in which a given argument violates the selectional requirements of a verb, but
    this violation is not perceived by comprehenders due to its attraction to another
    noun in the same sentence, which is syntactically unrelated b
  track: The 12th Joint Conference on Lexical and Computational Semantics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Recent advances in large language models have prompted researchers to
    examine their abilities across a variety of linguistic tasks, but little has been
    done to investigate how models handle the interactions in meaning across words
    and larger syntactic forms---i.e. phenomena at the intersection of syntax and
    semantics. We present the semantic notion of agentivity as a case study for probing
    such interactions. We created a novel evaluation dataset by utilitizing the unique
    linguistic properties of a subset of optionally transitive English verbs. This
    dataset was used to prompt varying sizes of three model classes to see if they
    are sensitive to agentivity at the lexical level, and if they can appropriately
    employ these word-level priors given a specific syntactic context. Overall, GPT-3
    text-davinci-003 performs extremely well across all experiments, outperforming
    all other models tested by far. In fact, the results are even better correlated
    with human judgements than both syntactic and semantic corpus statistics. This
    suggests that LMs may potentially serve as more useful tools for linguistic annotation,
    theory testing, and discovery than select corpora for certain tasks.
  authors:
  - Lindia Tjuatja
  - Emmy Liu
  - Lori Levin
  - Graham Neubig
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SEM
  forum: ''
  id: StarSEM_29
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Syntax and Semantics Meet in the "Middle": Probing the Syntax-Semantics
    Interface of LMs Through Agentivity'
  tldr: Recent advances in large language models have prompted researchers to examine
    their abilities across a variety of linguistic tasks, but little has been done
    to investigate how models handle the interactions in meaning across words and
    larger syntactic forms---i.e. phenomena at the intersection of sy
  track: The 12th Joint Conference on Lexical and Computational Semantics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: For Pretrained Language Models (PLMs), their susceptibility to noise has
    recently been linked to subword segmentation. However, it is unclear which aspects
    of segmentation affect their understanding. This study assesses the robustness
    of PLMs against various disrupted segmentation caused by noise. An evaluation
    framework for subword segmentation, named Contrastive Lexical Semantic (CoLeS)
    probe, is proposed. It provides a systematic categorization of segmentation corruption
    under noise and evaluation protocols by generating contrastive datasets with canonical-noisy
    word pairs. Experimental results indicate that PLMs are unable to accurately compute
    word meanings if the noise introduces completely different subwords, small subword
    fragments, or a large number of additional subwords, particularly when they are
    inserted within other subwords.
  authors:
  - Xinzhe Li
  - Ming Liu
  - Shang Gao
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SEM
  forum: ''
  id: StarSEM_31
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Can Pretrained Language Models Derive Correct Semantics from Corrupt Subwords
    under Noise?
  tldr: For Pretrained Language Models (PLMs), their susceptibility to noise has recently
    been linked to subword segmentation. However, it is unclear which aspects of segmentation
    affect their understanding. This study assesses the robustness of PLMs against
    various disrupted segmentation caused by noise. A
  track: The 12th Joint Conference on Lexical and Computational Semantics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Idioms such as "call it a day" and "piece of cake," are prevalent in natural
    language. How do Transformer language models process idioms? This study examines
    this question by analysing three models - BERT, Multilingual BERT, and DistilBERT.
    We compare the embeddings of idiomatic and literal expressions across all layers
    of the networks at both the sentence and word levels. Additionally, we investigate
    the attention directed from other sentence tokens towards a word within an idiom
    as opposed to in a literal context. Results indicate that while the three models
    exhibit slightly different internal mechanisms, they all represent idioms distinctively
    compared to literal language, with attention playing a critical role. These findings
    suggest that idioms are semantically and syntactically idiosyncratic, not only
    for humans but also for language models.
  authors:
  - Ye Tian
  - Isobel James
  - Hye Son
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SEM
  forum: ''
  id: StarSEM_32
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: How Are Idioms Processed Inside Transformer Language Models?
  tldr: Idioms such as "call it a day" and "piece of cake," are prevalent in natural
    language. How do Transformer language models process idioms? This study examines
    this question by analysing three models - BERT, Multilingual BERT, and DistilBERT.
    We compare the embeddings of idiomatic and literal expressi
  track: The 12th Joint Conference on Lexical and Computational Semantics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Some applications of artificial intelligence make it desirable that logical
    formulae be converted computationally to comprehensible natural language sentences.
    As there are many logical equivalents to a given formula, finding the most suitable
    equivalent to be used as input for such a "logic-to-text" generation system is
    a difficult challenge. In this paper, we focus on the role of brevity: Are the
    shortest formulae the most suitable? We focus on propositional logic (PL), framing
    formula minimization (i.e., the problem of finding the shortest equivalent of
    a given formula) as a Quantified Boolean Formulae (QBFs) satisfiability problem.
    We experiment with several generators and selection strategies to prune the resulting
    candidates. We conduct exhaustive automatic and human evaluations of the comprehensibility
    and fluency of the generated texts. The results suggest that while, in many cases,
    minimization has a positive impact on the quality of the sentences generated,
    formula minimization may ultimately not be the best strategy.'
  authors:
  - Eduardo Cal
  - Jordi Levy
  - Albert Gatt
  - Kees Van Deemter
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SEM
  forum: ''
  id: StarSEM_34
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Is Shortest Always Best? The Role of Brevity in Logic-to-Text Generation
  tldr: Some applications of artificial intelligence make it desirable that logical
    formulae be converted computationally to comprehensible natural language sentences.
    As there are many logical equivalents to a given formula, finding the most suitable
    equivalent to be used as input for such a "logic-to-text
  track: The 12th Joint Conference on Lexical and Computational Semantics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'The automatic extraction of hypernym knowledge from large language models
    like BERT is an open problem, and it is unclear whether methods fail due to a
    lack of knowledge in the model or shortcomings of the extraction methods. In particular,
    methods fail on challenging cases which include rare or abstract concepts, and
    perform inconsistently under paraphrased prompts. In this study, we revisit the
    long line of work on pattern-based hypernym extraction, and use it as a diagnostic
    tool to thoroughly examine the hypernomy knowledge encoded in BERT and the limitations
    of hypernym extraction methods. We propose to construct prompts from established
    pattern structures: definitional (X is a Y); lexico-syntactic (Y such as X); and
    their anchored versions (Y such as X or Z). We devise an automatic method for
    anchor prediction, and compare different patterns in: (i) their effectiveness
    for hypernym retrieval from BERT across six English data sets; (ii) on challenge
    sets of rare and abstract concepts; and (iii) on consistency under paraphrasing.
    We show that anchoring is particularly useful for abstract concepts and in enhancing
    consistency across paraphrases, demonstrating how established methods in the field
    can inform prompt engineering.'
  authors:
  - Chunhua Liu
  - Trevor Cohn
  - Lea Frermann
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SEM
  forum: ''
  id: StarSEM_37
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Seeking Clozure: Robust Hypernym extraction from BERT with Anchored Prompts'
  tldr: The automatic extraction of hypernym knowledge from large language models
    like BERT is an open problem, and it is unclear whether methods fail due to a
    lack of knowledge in the model or shortcomings of the extraction methods. In particular,
    methods fail on challenging cases which include rare or abs
  track: The 12th Joint Conference on Lexical and Computational Semantics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Model explanations that shed light on the model's predictions are becoming
    a desired additional output of NLP models, alongside their predictions. Challenges
    in creating these explanations include making them trustworthy and faithful to
    the model's predictions. In this work, we propose a novel framework for guiding
    model explanations by supervising them explicitly. To this end, our method, LEXplain,
    uses task-related lexicons to directly supervise model explanations. This approach
    consistently improves the model's explanations without sacrificing performance
    on the task, as we demonstrate on sentiment analysis and toxicity detection. Our
    analyses show that our method also demotes spurious correlations (i.e., with respect
    to African American English dialect) when performing the task, improving fairness.
  authors:
  - Orevaoghene Ahia
  - Hila Gonen
  - Vidhisha Balachandran
  - Yulia Tsvetkov
  - Noah A. Smith
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SEM
  forum: ''
  id: StarSEM_43
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'LEXPLAIN: Improving Model Explanations via Lexicon Supervision'
  tldr: Model explanations that shed light on the model's predictions are becoming
    a desired additional output of NLP models, alongside their predictions. Challenges
    in creating these explanations include making them trustworthy and faithful to
    the model's predictions. In this work, we propose a novel frame
  track: The 12th Joint Conference on Lexical and Computational Semantics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The ability of knowledge graphs to represent complex relationships at
    scale has led to their adoption for various needs including knowledge representation,
    question-answering, and recommendation systems. Knowledge graphs are often incomplete
    in the information they represent, necessitating the need for knowledge graph
    completion tasks. Pre-trained and fine-tuned language models have shown promise
    in these tasks although these models ignore the intrinsic information encoded
    in the knowledge graph, namely the entity and relation types. In this work, we
    propose the Knowledge Graph Language Model (KGLM) architecture, where we introduce
    a new entity/relation embedding layer that learns to differentiate distinctive
    entity and relation types, therefore allowing the model to learn the structure
    of the knowledge graph. In this work, we show that further pre-training the language
    models with this additional embedding layer using the triples extracted from the
    knowledge graph, followed by the standard fine-tuning phase sets a new state-of-the-art
    performance for the link prediction task on the benchmark datasets.
  authors:
  - Jason Youn
  - Ilias Tagkopoulos
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SEM
  forum: ''
  id: StarSEM_44
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'KGLM: Integrating Knowledge Graph Structure in Language Models for Link
    Prediction'
  tldr: 'The ability of knowledge graphs to represent complex relationships at scale
    has led to their adoption for various needs including knowledge representation,
    question-answering, and recommendation systems. Knowledge graphs are often incomplete
    in the information they represent, necessitating the need '
  track: The 12th Joint Conference on Lexical and Computational Semantics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: As the size of the pre-trained language model (PLM) continues to increase,
    numerous parameter-efficient transfer learning methods have been proposed recently
    to compensate for the high cost of fine-tuning. While large PLMs and various PETL
    methods have achieved impressive results on various benchmarks, it is uncertain
    whether they can effectively handle inputs that have been distributionally shifted.
    In this study, we systematically explore how the ability to detect out-of-distribution
    (OOD) changes as the size of the PLM grows or the transfer methods are altered.
    Specifically, we evaluated various PETL techniques, including fine-tuning, Adapter,
    LoRA, and prefix-tuning, with various language models with different scales.
  authors:
  - Hyunsoo Cho
  - Choonghyun Park
  - Junyeob Kim
  - Hyuhng Joon Kim
  - Kang Min Yoo
  - Sang-goo Lee
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SEM
  forum: ''
  id: StarSEM_45
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Probing Out-of-Distribution Robustness of Language Models with Parameter-Efficient
    Transfer Learning
  tldr: As the size of the pre-trained language model (PLM) continues to increase,
    numerous parameter-efficient transfer learning methods have been proposed recently
    to compensate for the high cost of fine-tuning. While large PLMs and various PETL
    methods have achieved impressive results on various benchmar
  track: The 12th Joint Conference on Lexical and Computational Semantics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: With the advent of large language models (LLMs), the trend in NLP has
    been to train LLMs on vast amounts of data to solve diverse language understanding
    and generation tasks.  The list of LLM successes is long and varied. Nevertheless,
    several recent papers provide empirical evidence that LLMs fail to capture important
    aspects of linguistic meaning. Focusing on universal quantification, we provide
    a theoretical foundation for these empirical findings by proving that LLMs cannot
    learn certain fundamental semantic properties including semantic entailment and
    consistency as they are defined in formal semantics.  More generally, we show
    that LLMs are unable to learn concepts beyond the first level of the Borel Hierarchy,
    which imposes severe limits on the ability of LMs, both large and small, to capture
    many aspects of linguistic meaning.  This means that LLMs will operate without
    formal guarantees on tasks that require entailments and deep linguistic understanding.
  authors:
  - Nicholas Asher
  - Swarnadeep Bhar
  - Akshay Chaturvedi
  - Julie Hunter
  - Soumya Paul
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SEM
  forum: ''
  id: StarSEM_46
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Limits for learning with language models
  tldr: 'With the advent of large language models (LLMs), the trend in NLP has been
    to train LLMs on vast amounts of data to solve diverse language understanding
    and generation tasks.  The list of LLM successes is long and varied. Nevertheless,
    several recent papers provide empirical evidence that LLMs fail '
  track: The 12th Joint Conference on Lexical and Computational Semantics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Even in the era of massive language models, it has been suggested that
    character-level representations improve the performance of neural models. The
    state-of-the-art neural semantic parser for Discourse Representation Structures
    uses character-level representations, improving performance in the four languages
    (i.e., English, German, Dutch, and Italian) in the Parallel Meaning Bank dataset.
    However, how and why character-level information improves the parser's performance
    remains unclear. This study provides an in-depth analysis of performance changes
    by order of character sequences. In the experiments, we compare F1-scores by shuffling
    the order and randomizing character sequences after testing the performance of
    character-level information. Our results indicate that incorporating character-level
    information does not improve the performance in English and German. In addition,
    we find that the parser is not sensitive to correct character order in Dutch.
    Nevertheless, performance improvements are observed when using character-level
    information.
  authors:
  - Tomoya Kurosawa
  - Hitomi Yanaka
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SEM
  forum: ''
  id: StarSEM_50
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Does Character-level Information Always Improve DRS-based Semantic Parsing?
  tldr: Even in the era of massive language models, it has been suggested that character-level
    representations improve the performance of neural models. The state-of-the-art
    neural semantic parser for Discourse Representation Structures uses character-level
    representations, improving performance in the four
  track: The 12th Joint Conference on Lexical and Computational Semantics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Paraphrase detection is useful in many natural language understanding
    applications. Current works typically formulate this problem as a sentence pair
    binary classification task. However, this setup is not a good fit for many of
    the intended applications of paraphrase models. In particular, such applications
    often involve finding the closest paraphrases of the target sentence from a group
    of candidate sentences where they exhibit different degrees of semantic overlap
    with the target sentence. To apply models to this paraphrase retrieval scenario,
    the model must be sensitive to the degree to which two sentences are paraphrases
    of one another. However, many existing datasets ignore and fail to test models
    in this setup. In response, we propose adversarial paradigms to create evaluation
    datasets, which could examine the sensitivity to different degrees of semantic
    overlap. Empirical results show that, while paraphrase models and different sentence
    encoders appear successful on standard evaluations, measuring the degree of semantic
    overlap still remains a big challenge for them.
  authors:
  - Qiwei Peng
  - David Weir
  - Julie Weeds
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SEM
  forum: ''
  id: StarSEM_54
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Testing Paraphrase Models on Recognising Sentence Pairs at Different Degrees
    of Semantic Overlap
  tldr: Paraphrase detection is useful in many natural language understanding applications.
    Current works typically formulate this problem as a sentence pair binary classification
    task. However, this setup is not a good fit for many of the intended applications
    of paraphrase models. In particular, such appl
  track: The 12th Joint Conference on Lexical and Computational Semantics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'How does the word analogy task fit in the modern NLP landscape? Given
    the rarity of comparable multilingual benchmarks and the lack of a consensual
    evaluation protocol for contextual models, this remains an open question. In this
    paper, we introduce MATS: a multilingual analogy dataset, covering forty analogical
    relations in six languages, and evaluate human as well as static and contextual
    embedding performances on the task. We find that not all analogical relations
    are equally straightforward for humans, static models remain competitive with
    contextual embeddings, and optimal settings vary across languages and analogical
    relations. Several key challenges remain, including creating benchmarks that align
    with human reasoning and understanding what drives differences across methodologies.'
  authors:
  - Timothee Mickus
  - Eduardo Cal
  - Lo Jacqmin
  - Denis Paperno
  - Mathieu Constant
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SEM
  forum: ''
  id: StarSEM_64
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Mann" is to "Donna asis to  Reine  Adapting the Analogy Task for Multilingual
    and Contextual Embeddings
  tldr: 'How does the word analogy task fit in the modern NLP landscape? Given the
    rarity of comparable multilingual benchmarks and the lack of a consensual evaluation
    protocol for contextual models, this remains an open question. In this paper,
    we introduce MATS: a multilingual analogy dataset, covering for'
  track: The 12th Joint Conference on Lexical and Computational Semantics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Joint vision-language models have shown great performance over a diverse
    set of tasks. However, little is known about their limitations, as the high dimensional
    space learned by these models makes it difficult to identify semantic errors.
    Recent work has addressed this problem by designing highly controlled probing
    task benchmarks. Our paper introduces a more scalable solution that relies on
    already annotated benchmarks. Our method consists of extracting a large set of
    diverse features from a vision-language benchmark and measuring their correlation
    with the output of the target model. We confirm previous findings that CLIP behaves
    like a bag of words model and performs better with nouns and verbs; we also uncover
    novel insights such as CLIP getting confused by concrete words. Our framework
    is available at https://github.com/MichiganNLP/Scalable-VLM-Probing and can be
    used with other multimodal models and benchmarks.
  authors:
  - Santiago Castro
  - Oana Ignat
  - Rada Mihalcea
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SEM
  forum: ''
  id: StarSEM_67
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Scalable Performance Analysis for Vision-Language Models
  tldr: Joint vision-language models have shown great performance over a diverse set
    of tasks. However, little is known about their limitations, as the high dimensional
    space learned by these models makes it difficult to identify semantic errors.
    Recent work has addressed this problem by designing highly co
  track: The 12th Joint Conference on Lexical and Computational Semantics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Existing work on controlled text generation (CTG) assumes a control interface
    of categorical attributes. In this work, we propose a natural language (NL) interface,
    where we craft a PCFG to embed the control attributes into natural language commands,
    and propose variants of existing CTG models that take commands as input. In our
    experiments, we design tailored setups to test the model's generalization abilities.
    We find our PCFG-based command generation approach is effective for handling unseen
    commands compared to fix-set templates. Further, our proposed NL models can effectively
    generalize to unseen attributes (a new ability enabled by the NL interface), as
    well as unseen attribute combinations. Interestingly, in model comparisons, the
    simple conditional generation approach, enhanced with our proposed NL interface,
    is shown to be a strong baseline in those challenging settings.
  authors:
  - Jingyu Zhang
  - James Glass
  - Tianxing He
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SEM
  forum: ''
  id: StarSEM_68
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: PCFG-Based Natural Language Interface Improves Generalization for Controlled
    Text Generation
  tldr: 'Existing work on controlled text generation (CTG) assumes a control interface
    of categorical attributes. In this work, we propose a natural language (NL) interface,
    where we craft a PCFG to embed the control attributes into natural language commands,
    and propose variants of existing CTG models that '
  track: The 12th Joint Conference on Lexical and Computational Semantics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Large language models (LLMs) have demonstrated solid zero-shot reasoning
    capabilities, which is reflected in their performance on the current test tasks.
    This calls for a more challenging benchmark requiring highly advanced reasoning
    ability to be solved. In this paper, we introduce such a benchmark, consisting
    of 191 long-form (1200 words on average) mystery narratives constructed as detective
    puzzles. Puzzles are sourced from the "5 Minute Mystery" platform and include
    a multiple-choice question for evaluation. Only 47\% of humans solve a puzzle
    successfully on average, while the best human solvers achieve over 80\% success
    rate. We show that GPT-3 models barely outperform random on this benchmark (with
    28\% accuracy) while state-of-the-art GPT-4 solves only 38\% of puzzles. This
    indicates that there is still a significant gap in the deep reasoning abilities
    of LLMs and humans and highlights the need for further research in this area.
    Our work introduces a challenging benchmark for future studies on reasoning in
    language models and contributes to a better understanding of the limits of LLMs'
    abilities.
  authors:
  - Maksym Del
  - Mark Fishel
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SEM
  forum: ''
  id: StarSEM_69
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3
    and Challenging for GPT-4'
  tldr: Large language models (LLMs) have demonstrated solid zero-shot reasoning capabilities,
    which is reflected in their performance on the current test tasks. This calls
    for a more challenging benchmark requiring highly advanced reasoning ability to
    be solved. In this paper, we introduce such a benchmark
  track: The 12th Joint Conference on Lexical and Computational Semantics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Sequence-to-sequence paraphrase generation models often struggle with
    the generation of diverse paraphrases. This deficiency constrains the viability
    of leveraging paraphrase generation in different Natural Language Processing tasks.
    We propose a translation-based guided paraphrase generation model that learns
    useful features for promoting surface form variation in generated paraphrases
    from cross-lingual parallel data. Our proposed method leverages multilingual neural
    machine translation pretraining to learn zero-shot paraphrasing. Furthermore,
    we incorporate dedicated prefix tokens into the training of the machine translation
    models to promote variation. The prefix tokens are designed to affect various
    linguistic features related to surface form realizations, and can be applied during
    inference to guide the decoding process towards a desired solution. We assess
    the proposed guided model on paraphrase generation in three languages, English,
    Finnish, and Swedish, and provide analysis on the feasibility of the prefix tokens
    to guided paraphrasing. Our analysis suggests that the attributes represented
    by the prefix tokens are useful in promoting variation, by pushing the paraphrases
    generated by the guided model to diverge from the input sentence while preserving
    semantics conveyed by the sentence well.
  authors:
  - Teemu Vahtola
  - Mathias Creutz
  - Jrg Tiedemann
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SEM
  forum: ''
  id: StarSEM_70
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Guiding Zero-Shot Paraphrase Generation with Fine-Grained Control Tokens
  tldr: 'Sequence-to-sequence paraphrase generation models often struggle with the
    generation of diverse paraphrases. This deficiency constrains the viability of
    leveraging paraphrase generation in different Natural Language Processing tasks.
    We propose a translation-based guided paraphrase generation model '
  track: The 12th Joint Conference on Lexical and Computational Semantics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Lexical Semantic Change is the study of how the meaning of words evolves
    through time. Another related question is whether and how lexical relations over
    pairs of words, such as synonymy, change over time. There are currently two competing,
    apparently opposite hypotheses in the historical linguistic literature regarding
    how synonymous words evolve: the Law of Differentiation (LD) argues that synonyms
    tend to take on different meanings over time, whereas the Law of Parallel Change
    (LPC) claims that synonyms tend to undergo the same semantic change and therefore
    remain synonyms. So far, there has been little research using distributional models
    to assess to what extent these laws apply on historical corpora.In this work,
    we take a first step toward detecting whether LD or LPC operates for given word
    pairs. After recasting the problem into a more tractable task, we combine two
    linguistic resources to propose the first complete evaluation framework on this
    problem and provide empirical evidence in favor of a dominance of LD. We then
    propose various computational approaches to the problem using Distributional Semantic
    Models and grounded in recent literature on Lexical Semantic Change detection.
    Our best approaches achieve a balanced accuracy above 0.6 on our dataset. We discuss
    challenges still faced by these approaches, such as polysemy or the potential
    confusion between synonymy and hypernymy.'
  authors:
  - Bastien Lietard
  - Mikaela Keller
  - Pascal Denis
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SEM
  forum: ''
  id: StarSEM_71
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'A Tale of Two Laws of Semantic Change: Predicting Synonym Changes with Distributional
    Semantic Models'
  tldr: Lexical Semantic Change is the study of how the meaning of words evolves through
    time. Another related question is whether and how lexical relations over pairs
    of words, such as synonymy, change over time. There are currently two competing,
    apparently opposite hypotheses in the historical linguistic
  track: The 12th Joint Conference on Lexical and Computational Semantics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Prior work has shown that coupling sequential latent variable models
    with semantic ontological knowledge can improve the representational capabilities
    of event modeling approaches. In this work, we present a novel, doubly hierarchical,
    semi-supervised event modeling framework that provides structural hierarchy while
    also accounting for ontological hierarchy. Our approach consistsof multiple layers
    of structured latent variables, where each successive layer compresses and abstracts
    the previous layers. We guide this compression through the injection of structured
    ontological knowledge that is defined at the type level of events: importantly,
    our model allows for partial injection of semantic knowledge and it does not depend
    on observing instances at any particular level of the semantic ontology. Across
    two different datasets and four different evaluation metrics, we demonstrate that
    our approach is able to out-perform the previous state-of-the-art approaches by
    up to 8.5\%, demonstrating the benefits of structured and semantic hierarchical
    knowledge for event modeling.'
  authors:
  - Shubhashis Roy Dipta
  - Mehdi Rezaee
  - Francis Ferraro
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SEM
  forum: ''
  id: StarSEM_72
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Semantically-informed Hierarchical Event Modeling
  tldr: Prior work has shown that coupling sequential latent variable models with
    semantic ontological knowledge can improve the representational capabilities of
    event modeling approaches. In this work, we present a novel, doubly hierarchical,
    semi-supervised event modeling framework that provides structura
  track: The 12th Joint Conference on Lexical and Computational Semantics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The representation space of pretrained Language Models (LMs) encodes rich
    information about words and their relationships (e.g., similarity, hypernymy,
    polysemy) as well as abstract semantic notions (e.g., intensity). In this paper,
    we demonstrate that lexical stylistic notions such as complexity, formality, and
    figurativeness, can also be identified in this space. We show that it is possible
    to derive a vector representation for each of these stylistic notions from only
    a small number of seed pairs. Using these vectors, we can characterize new texts
    in terms of these dimensions by performing simple calculations in the corresponding
    embedding space. We conduct experiments on five datasets and find that static
    embeddings encode these features more accurately at the level of words and phrases,
    whereas contextualized LMs perform better on sentences. The lower performance
    of contextualized representations at the word level is partially attributable
    to the anisotropy of their vector space, which can be corrected to some extent
    using techniques like standardization.
  authors:
  - Qing Lyu
  - Marianna Apidianaki
  - Chris Callison-burch
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SEM
  forum: ''
  id: StarSEM_74
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Representation of Lexical Stylistic Features in Language Models' Embedding
    Space
  tldr: The representation space of pretrained Language Models (LMs) encodes rich
    information about words and their relationships (e.g., similarity, hypernymy,
    polysemy) as well as abstract semantic notions (e.g., intensity). In this paper,
    we demonstrate that lexical stylistic notions such as complexity, f
  track: The 12th Joint Conference on Lexical and Computational Semantics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The task of entity state tracking aims to automatically analyze procedural
    texts -- texts that describe a step-by-step process (e.g. a baking recipe). Specifically,
    the goal is to track various states of the entities participating in a given process.
    Some of the challenges for this NLP task include annotated data scarcity and annotators'
    reliance on commonsense knowledge to annotate implicit state information. Zhang
    et al. (2021) successfully incorporated commonsense entity-centric knowledge from
    ConceptNet into their BERT-based neural-symbolic architecture. Since English mostly
    encodes state change information in verbs, we attempted to test whether injecting
    semantic knowledge of events (retrieved from the state-of-the-art VerbNet parser)
    into a neural model can also improve the performance on this task. To achieve
    this, we adapt the methodology introduced by Zhang et al. (2021) for incorporating
    symbolic entity information from ConceptNet to the incorporation of VerbNet event
    semantics. We evaluate the performance of our model on the ProPara dataset (Mishra
    et al., 2018). In addition, we introduce a purely symbolic model for entity state
    tracking that uses a simple set of case statements, and is informed mostly by
    linguistic knowledge retrieved from various computational lexical resources. Our
    approach is inherently domain-agnostic, and our model is explainable and achieves
    state-of-the-art results on the Recipes dataset (Bosselut et al., 2017).
  authors:
  - Ghazaleh Kazeminejad
  - Martha Palmer
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SEM
  forum: ''
  id: StarSEM_75
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Event Semantic Knowledge in Procedural Text Understanding
  tldr: 'The task of entity state tracking aims to automatically analyze procedural
    texts -- texts that describe a step-by-step process (e.g. a baking recipe). Specifically,
    the goal is to track various states of the entities participating in a given process.
    Some of the challenges for this NLP task include '
  track: The 12th Joint Conference on Lexical and Computational Semantics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In this paper we investigate the application of active learning to semantic
    role labeling (SRL) using Bayesian Active Learning by Disagreement (BALD). Our
    new predicate-focused selection method quickly improves efficiency on three different
    specialised domain corpora. This is encouraging news for researchers wanting to
    port SRL to domain specific applications.  Interestingly, with the large and diverse
    \textbackslash{}textit\{OntoNotes\} corpus, the sentence selection approach, that
    collects a larger number of predicates, taking more time to annotate, fares better
    than the predicate approach. In this paper, we analyze both the selections made
    by our two selections methods for the various domains and the differences between
    these corpora in detail.
  authors:
  - Skatje Myers
  - Martha Palmer
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SEM
  forum: ''
  id: StarSEM_76
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Leveraging Active Learning to Minimise SRL Annotation Across Corpora
  tldr: In this paper we investigate the application of active learning to semantic
    role labeling (SRL) using Bayesian Active Learning by Disagreement (BALD). Our
    new predicate-focused selection method quickly improves efficiency on three different
    specialised domain corpora. This is encouraging news for re
  track: The 12th Joint Conference on Lexical and Computational Semantics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Prior work typically describes out-of-domain (OOD) or out-of-distribution
    (OODist) samples as those that originate from dataset(s) or source(s) different
    from the training set but for the same task. When compared to in-domain (ID) samples,
    the models have been known to usually perform poorer on OOD samples, although
    this observation is not consistent. Another thread of research has focused on
    OOD detection, albeit mostly using supervised approaches. In this work, we first
    consolidate and present a systematic analysis of multiple definitions of OOD and
    OODist  as discussed in prior literature. Then, we analyze the performance of
    a model under ID and OOD/OODist settings in a principled way. Finally, we seek
    to identify an unsupervised method for reliably identifying OOD/OODist samples
    without using a trained model. The results of our extensive evaluation using 12
    datasets from 4 different tasks suggest the promising potential of unsupervised
    metrics in this task.
  authors:
  - Rhitabrat Pokharel
  - Ameeta Agrawal
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SEM
  forum: ''
  id: StarSEM_79
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Estimating Semantic Similarity between In-Domain and Out-of-Domain Samples
  tldr: 'Prior work typically describes out-of-domain (OOD) or out-of-distribution
    (OODist) samples as those that originate from dataset(s) or source(s) different
    from the training set but for the same task. When compared to in-domain (ID) samples,
    the models have been known to usually perform poorer on OOD '
  track: The 12th Joint Conference on Lexical and Computational Semantics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In this study, we propose using the GPT-3 as a query generator for the
    backend of CLIP as an implicit word sense disambiguation (WSD) component for the
    SemEval 2023 shared task Visual Word Sense Disambiguation (VWSD). We confirmed
    previous findings --- human-like prompts adapted for WSD with quotes benefit both
    CLIP and GPT-3, whereas plain phrases or poorly templated prompts give the worst
    results.
  authors:
  - Xiaomeng Pan
  - Zhousi Chen
  - Mamoru Komachi
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SEM
  forum: ''
  id: StarSEM_80
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Query Generation Using GPT-3 for CLIP-Based Word Sense Disambiguation for
    Image Retrieval
  tldr: 'In this study, we propose using the GPT-3 as a query generator for the backend
    of CLIP as an implicit word sense disambiguation (WSD) component for the SemEval
    2023 shared task Visual Word Sense Disambiguation (VWSD). We confirmed previous
    findings --- human-like prompts adapted for WSD with quotes '
  track: The 12th Joint Conference on Lexical and Computational Semantics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Functional Distributional Semantics is a linguistically motivated framework
    for modelling lexical and sentence-level semantics with truth-conditional functions
    using distributional information. Previous implementations of the framework focus
    on subjectverbobject (SVO) triples only, which largely limits the contextual information
    available for training and thus the capability of the learnt model. In this paper,
    we discuss the challenges of extending the previous architectures to training
    on arbitrary sentences. We address the challenges by proposing a more expressive
    lexical model that works over a continuous semantic space. This improves the flexibility
    and computational efficiency of the model, as well as its compatibility with present-day
    machine-learning frameworks. Our proposal allows the model to be applied to a
    wider range of semantic tasks, and improved performances are demonstrated from
    experimental results.
  authors:
  - Chun Hei Lo
  - Hong Cheng
  - Wai Lam
  - Guy Emerson
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SEM
  forum: ''
  id: StarSEM_81
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Functional Distributional Semantics at Scale
  tldr: Functional Distributional Semantics is a linguistically motivated framework
    for modelling lexical and sentence-level semantics with truth-conditional functions
    using distributional information. Previous implementations of the framework focus
    on subjectverbobject (SVO) triples only, which largely lim
  track: The 12th Joint Conference on Lexical and Computational Semantics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Transformers have been shown to work well for the task of English euphemism
    disambiguation, in which a potentially euphemistic term (PET) is classified as
    euphemistic or non-euphemistic in a particular context. In this study, we expand
    on the task in two ways. First, we annotate PETs for vagueness, a linguistic property
    associated with euphemisms, and find that transformers are generally better at
    classifying vague PETs, suggesting linguistic differences in the data that impact
    performance. Second, we present novel euphemism corpora in three different languages:
    Yoruba, Spanish, and Mandarin Chinese. We perform euphemism disambiguation experiments
    in each language using multilingual transformer models mBERT and XLM-RoBERTa,
    establishing preliminary results from which to launch future work.'
  authors:
  - Patrick Lee
  - Iyanuoluwa Shode
  - Alain Trujillo
  - Yuan Zhao
  - Olumide Ojo
  - Diana Plancarte
  - Anna Feldman
  - Jing Peng
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SEM
  forum: ''
  id: StarSEM_83
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'FEED PETs: Further Experimentation and Expansion on the Disambiguation of
    Potentially Euphemistic Terms'
  tldr: 'Transformers have been shown to work well for the task of English euphemism
    disambiguation, in which a potentially euphemistic term (PET) is classified as
    euphemistic or non-euphemistic in a particular context. In this study, we expand
    on the task in two ways. First, we annotate PETs for vagueness, '
  track: The 12th Joint Conference on Lexical and Computational Semantics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We tackle the problem of monolingual phrase alignment conforming to syntactic
    structures. The existing method formalises the problem as unordered tree mapping;
    hence, the alignment quality is easily affected by syntactic ambiguities. We address
    this problem by expanding the method to align parse forests rather than 1-best
    trees, where syntactic structures and phrase alignment are simultaneously identified.
    The proposed method achieves efficient alignment by mapping forests on a packed
    structure. The experimental results indicated that our method improves the phrase
    alignment quality of the state-of-the-art method by aligning forests rather than
    1-best trees.
  authors:
  - Sora Kadotani
  - Yuki Arase
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SEM
  forum: ''
  id: StarSEM_85
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Monolingual Phrase Alignment as Parse Forest Mapping
  tldr: We tackle the problem of monolingual phrase alignment conforming to syntactic
    structures. The existing method formalises the problem as unordered tree mapping;
    hence, the alignment quality is easily affected by syntactic ambiguities. We address
    this problem by expanding the method to align parse for
  track: The 12th Joint Conference on Lexical and Computational Semantics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In this work we build upon negative results from an attempt at language
    modeling with predicted semantic structure, in order to establish empirical lower
    bounds on what could have made the attempt successful.More specifically, we design
    a concise binary vector representation of semantic structure at the lexical level
    and evaluate in-depth how good an incremental tagger needs to be in order to achieve
    better-than-baseline performance with an end-to-end semantic-bootstrapping language
    model. We envision such a system as consisting of a (pretrained) sequential-neural
    component and a hierarchical-symbolic component working together to generate text
    with low surprisal and high linguistic interpretability.We find that (a) dimensionality
    of the semantic vector representation can be dramatically reduced without losing
    its main advantages and (b) lower bounds on prediction quality cannot be established
    via a single score alone, but need to take the distributions of signal and noise
    into account.
  authors:
  - Jakob Prange
  - Emmanuele Chersoni
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SEM
  forum: ''
  id: StarSEM_86
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Empirical Sufficiency Lower Bounds for Language Modeling with Locally-Bootstrapped
    Semantic Structures
  tldr: In this work we build upon negative results from an attempt at language modeling
    with predicted semantic structure, in order to establish empirical lower bounds
    on what could have made the attempt successful.More specifically, we design a
    concise binary vector representation of semantic structure at
  track: The 12th Joint Conference on Lexical and Computational Semantics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Words of Estimative Probability (WEP) are phrases used to express the
    plausibility of a statement. Examples include terms like \textbackslash{}textit\{probably,
    maybe, likely, doubt, unlikely\}, and \textbackslash{}textit\{impossible\}. Surveys
    have shown that human evaluators tend to agree when assigning numerical probability
    levels to these WEPs. For instance, the term \textbackslash{}textit\{highly likely\}
    equates to a median probability of \$0.90\{\textbackslash{}pm\}0.08\$ according
    to a survey by \textbackslash{}citet\{fagen-ulmschneider\}.In this study, our
    focus is to gauge the competency of neural language processing models in accurately
    capturing the consensual probability level associated with each WEP. Our first
    approach is utilizing the UNLI dataset \textbackslash{}cite\{chen-etal-2020-uncertain\},
    which links premises and hypotheses with their perceived joint probability \$p\$.
    From this, we craft prompts in the form: "[\textbackslash{}textsc\{Premise\}].
    [\textbackslash{}textsc\{Wep\}], [\textbackslash{}textsc\{Hypothesis\}]." This
    allows us to evaluate whether language models can predict if the consensual probability
    level of a WEP aligns closely with \$p\$.In our second approach, we develop a
    dataset based on WEP-focused probabilistic reasoning to assess if language models
    can logically process WEP compositions. For example, given the prompt "[\textbackslash{}textsc\{EventA\}]
    \textbackslash{}textit\{is likely\}. [\textbackslash{}textsc\{EventB\}] \textbackslash{}textit\{is
    impossible\}.", a well-functioning language model should not conclude that [\textbackslash{}textsc\{EventA\$\textbackslash{}\&\$B\}]
    is likely.Through our study, we observe that both tasks present challenges to
    out-of-the-box English language models. However, we also demonstrate that fine-tuning
    these models can lead to significant and transferable improvements.'
  authors:
  - Damien Sileo
  - Marie-francine Moens
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SEM
  forum: ''
  id: StarSEM_100
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Probing neural language models for understanding of words of estimative probability
  tldr: Words of Estimative Probability (WEP) are phrases used to express the plausibility
    of a statement. Examples include terms like \textbackslash{}textit\{probably,
    maybe, likely, doubt, unlikely\}, and \textbackslash{}textit\{impossible\}. Surveys
    have shown that human evaluators tend to agree when ass
  track: The 12th Joint Conference on Lexical and Computational Semantics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'State-of-the-art pretrained language models tend to perform below their
    capabilities when applied out-of-the-box on tasks that require understanding and
    working with numbers (usually referred to as numeracy). Recent work suggests two
    main reasons for this: (1) popular tokenisation algorithms have limited expressiveness
    for numbers, and (2) common pretraining objectives do not target numeracy. Approaches
    that address these shortcomings usually require architectural changes or pretraining
    from scratch. In this paper, we propose a new extended pretraining approach called
    Arithmetic-Based Pretraining that jointly addresses both in one extended pretraining
    step without requiring architectural changes or pretraining from scratch. Arithmetic-Based
    Pretraining combines contrastive learning to improve the number representation,
    and a novel extended pretraining objective called Inferable Number Prediction
    Task to improve numeracy. Our experiments show the effectiveness of Arithmetic-Based
    Pretraining in three different tasks that require improved numeracy, i.e., reading
    comprehension in the DROP dataset, inference-on-tables in the InfoTabs dataset,
    and table-to-text generation in the WikiBio and SciGen datasets.'
  authors:
  - Dominic Petrak
  - Nafise Sadat Moosavi
  - Iryna Gurevych
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SEM
  forum: ''
  id: StarSEM_101
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Arithmetic-Based Pretraining  Improving Numeracy of Pretrained Language Models
  tldr: 'State-of-the-art pretrained language models tend to perform below their capabilities
    when applied out-of-the-box on tasks that require understanding and working with
    numbers (usually referred to as numeracy). Recent work suggests two main reasons
    for this: (1) popular tokenisation algorithms have li'
  track: The 12th Joint Conference on Lexical and Computational Semantics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Stance detection deals with identifying an author's stance towards a target.
    Most existing stance detection models are limited because they do not consider
    relevant contextual information which allows for inferring the stance correctly.Complementary
    context can be found in knowledge bases but integrating the context into pretrained
    language models is non-trivial due to the graph structure of standard knowledge
    bases. To overcome this, we explore an approach to integrate contextual information
    as text which allows for integrating contextual information from heterogeneous
    sources, such as structured knowledge sources and by prompting large language
    models.Our approach can outperform competitive baselines on a large and diverse
    stance detection benchmark in a cross-target setup, i.e. for targets unseen during
    training. We demonstrate that it is more robust to noisy context and can regularize
    for unwanted correlations between labels and target-specific vocabulary. Finally,
    it is independent of the pretrained language model in use.
  authors:
  - Tilman Beck
  - Andreas Waldis
  - Iryna Gurevych
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SEM
  forum: ''
  id: StarSEM_104
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Robust Integration of Contextual Information for Cross-Target Stance Detection
  tldr: Stance detection deals with identifying an author's stance towards a target.
    Most existing stance detection models are limited because they do not consider
    relevant contextual information which allows for inferring the stance correctly.Complementary
    context can be found in knowledge bases but integr
  track: The 12th Joint Conference on Lexical and Computational Semantics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper begins with the premise that adverbs are neglected in computational
    linguistics. This view derives from two analyses: a literature review and a novel
    adverb dataset to probe a state-of-the-art language model, thereby uncovering
    systematic gaps in accounts for adverb meaning. We suggest that using Frame Semantics
    for characterizing word meaning, as in FrameNet, provides a promising approach
    to adverb analysis, given its ability to describe ambiguity, semantic roles, and
    null instantiation.'
  authors:
  - Dmitry Nikolaev
  - Collin Baker
  - Miriam R. L. Petruck
  - Sebastian Pad
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SEM
  forum: ''
  id: StarSEM_105
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Adverbs, Surprisingly
  tldr: 'This paper begins with the premise that adverbs are neglected in computational
    linguistics. This view derives from two analyses: a literature review and a novel
    adverb dataset to probe a state-of-the-art language model, thereby uncovering
    systematic gaps in accounts for adverb meaning. We suggest th'
  track: The 12th Joint Conference on Lexical and Computational Semantics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: While many real-life tasks require reasoning over multi-step sequential
    instructions, collecting fine-grained annotations for each intermediate step can
    be prohibitively expensive. In this work, we study how general pretrained sequence-to-sequence
    transformers perform under varying types of annotation for sequential instruction
    understanding. We conduct experiments using T5 (Raffel et al., 2020) on a commonly-used
    multi-step instruction understanding dataset SCONE (Long et al., 2016) that includes
    three sub-tasks. First, we show that with only gold supervision for the final
    step of a multi-step instruction sequence, depending on the sequential properties
    of different tasks, transformers may exhibit extremely bad performance on intermediate
    steps, in stark contrast with their performance on the final step. Next, we explore
    two directions to relieve this problem. We show that with the same limited annotation
    budget, using supervision uniformly distributed across different steps (instead
    of only final-step supervision), we can greatly improve the performance on intermediate
    steps with a drop in final-step performance. Further, we explore a contrastive
    learning approach to provide training signals on intermediate steps with zero
    intermediate gold supervision. This, however, achieves mixed results. It significantly
    improves the model's bad intermediate-step performance on one subtask, but also
    shows decreased performance on another subtask.
  authors:
  - Xiang Zhou
  - Aditya Gupta
  - Shyam Upadhyay
  - Mohit Bansal
  - Manaal Faruqui
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SEM
  forum: ''
  id: StarSEM_106
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Can Sequence-to-Sequence Transformers Naturally Understand Sequential Instructions?
  tldr: While many real-life tasks require reasoning over multi-step sequential instructions,
    collecting fine-grained annotations for each intermediate step can be prohibitively
    expensive. In this work, we study how general pretrained sequence-to-sequence
    transformers perform under varying types of annotati
  track: The 12th Joint Conference on Lexical and Computational Semantics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: "Deverbal nouns are nominal forms of verbs commonly used in written English\
    \ texts to describe events or actions, as well as their arguments. However, many\
    \ NLP systems, and in particular pattern-based ones, neglect to handle such nominalized\
    \ constructions. The solutions that do exist for handling arguments of nominalized\
    \ constructions are based on semantic annotation and require semantic ontologies,\
    \ making their applications restricted to a small set of nouns. We propose to\
    \ adopt instead a more syntactic approach, which maps the arguments of deverbal\
    \ nouns to the universal-dependency relations of the corresponding verbal construction.\
    \ We present an unsupervised mechanism\u2014based on contextualized word representations\u2014\
    which allows to enrich universal-dependency trees with dependency arcs denoting\
    \ arguments of deverbal nouns, using the same labels as the corresponding verbal\
    \ cases. By sharing the same label set as in the verbal case, patterns that were\
    \ developed for verbs can be applied without modification but with high accuracy\
    \ also to the nominal constructions."
  authors:
  - Aviv Weinstein
  - Yoav Goldberg
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SEM
  forum: ''
  id: StarSEM_F1
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Unsupervised Mapping of Arguments of Deverbal Nouns to Their Corresponding
    Verbal Labels
  tldr: Deverbal nouns are nominal forms of verbs commonly used in written English
    texts to describe events or actions, as well as their arguments. However, many
    NLP systems, and in particular pattern-based ones, neglect to handle such nominalized
    constructions. The solutions that do exist for handling argu
  track: The 12th Joint Conference on Lexical and Computational Semantics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: "Over the last few years, Masked Language Modeling (MLM) pre-training\
    \ has resulted in remarkable advancements in many Natural Language Understanding\
    \ (NLU) tasks, which sparked an interest in researching alternatives and extensions\
    \ to the MLM objective. In this paper, we tackle the absence of explicit semantic\
    \ grounding in MLM and propose Descriptive Masked Language Modeling (DMLM), a\
    \ knowledge-enhanced reading comprehension objective, where the model is required\
    \ to predict the most likely word in a context, being provided with the word\u2019\
    s definition. For instance, given the sentence \u201CI was going to the _\u201D\
    , if we provided as definition \u201Cfinancial institution\u201D, the model would\
    \ have to predict the word \u201Cbank\u201D; if, instead, we provided \u201Csandy\
    \ seashore\u201D, the model should predict \u201Cbeach\u201D. Our evaluation highlights\
    \ the effectiveness of DMLM in comparison with standard MLM, showing improvements\
    \ on a number of well-established NLU benchmarks, as well as other semantics-focused\
    \ tasks, e.g., Semantic Role Labeling. Furthermore, we demonstrate how it is possible\
    \ to take full advantage of DMLM to embed explicit semantics in downstream tasks,\
    \ explore several properties of DMLM-based contextual representations and suggest\
    \ a number of future directions to investigate."
  authors:
  - Edoardo Barba
  - "Niccol\xF2 Campolungo"
  - Roberto Navigli
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SEM
  forum: ''
  id: StarSEM_F2
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'DMLM: Descriptive Masked Language Modeling'
  tldr: Over the last few years, Masked Language Modeling (MLM) pre-training has resulted
    in remarkable advancements in many Natural Language Understanding (NLU) tasks,
    which sparked an interest in researching alternatives and extensions to the MLM
    objective. In this paper, we tackle the absence of explicit
  track: The 12th Joint Conference on Lexical and Computational Semantics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: "Although we have witnessed impressive progress in Semantic Role Labeling\
    \ (SRL), most of the research in the area is carried out assuming that the majority\
    \ of predicates are verbs. Conversely, predicates can also be expressed using\
    \ other parts of speech, e.g., nouns and adjectives. However, non-verbal predicates\
    \ appear in the benchmarks we commonly use to measure progress in SRL less frequently\
    \ than in some real-world settings \u2013 newspaper headlines, dialogues, and\
    \ tweets, among others. In this paper, we put forward a new PropBank dataset which\
    \ boasts wide coverage of multiple predicate types. Thanks to it, we demonstrate\
    \ empirically that standard benchmarks do not provide an accurate picture of the\
    \ current situation in SRL and that state-of-the-art systems are still incapable\
    \ of transferring knowledge across different predicate types. Having observed\
    \ these issues, we also present a novel, manually-annotated challenge set designed\
    \ to give equal importance to verbal, nominal, and adjectival predicate-argument\
    \ structures. We use such dataset to investigate whether we can leverage different\
    \ linguistic resources to promote knowledge transfer. In conclusion, we claim\
    \ that SRL is far from \u201Csolved\u201D, and its integration with other semantic\
    \ tasks might enable significant improvements in the future, especially for the\
    \ long tail of non-verbal predicates, thereby facilitating further research on\
    \ SRL for non-verbal predicates. We release our software and datasets at https://github.com/sapienzanlp/exploring-srl."
  authors:
  - Riccardo Orlando
  - Simone Conia
  - Roberto Navigli
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SEM
  forum: ''
  id: StarSEM_F3
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Exploring Non-Verbal Predicates in Semantic Role Labeling: Challenges and
    Opportunities'
  tldr: Although we have witnessed impressive progress in Semantic Role Labeling (SRL),
    most of the research in the area is carried out assuming that the majority of
    predicates are verbs. Conversely, predicates can also be expressed using other
    parts of speech, e.g., nouns and adjectives. However, non-verba
  track: The 12th Joint Conference on Lexical and Computational Semantics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We propose an unsupervised approach to multiword expression (MWE) paraphrasing
    in context. Our model uses only monolingual corpus data and pre-trained language
    models (without fine-tuning), without external resources such as dictionaries.
    We evaluate our method on the SemEval 2022 idiomatic semantic text similarity
    task, and show that it outperforms all unsupervised systems and rivals supervised
    systems.
  authors:
  - Takashi Wada
  - Yuji Matsumoto
  - Timothy Baldwin
  - Jey Han Lau
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SEM
  forum: ''
  id: StarSEM_F4
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Unsupervised Paraphrasing of Multiword Expressions
  tldr: We propose an unsupervised approach to multiword expression (MWE) paraphrasing
    in context. Our model uses only monolingual corpus data and pre-trained language
    models (without fine-tuning), without external resources such as dictionaries.
    We evaluate our method on the SemEval 2022 idiomatic semantic
  track: The 12th Joint Conference on Lexical and Computational Semantics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Abstract Meaning Representation (AMR) is a Semantic Parsing formalism
    that aims at providing a semantic graph abstraction representing a given text.
    Current approaches are based on autoregressive language models such as BART or
    T5, fine-tuned through Teacher Forcing to obtain a linearized version of the AMR
    graph from a sentence. In this paper, we present LeakDistill, a model and method
    that explores a modification to the Transformer architecture, using structural
    adapters to explicitly incorporate graph information into the learned representations
    and improve AMR parsing performance. Our experiments show how, by employing word-to-node
    alignment to embed graph structural information into the encoder at training time,
    we can obtain state-of-the-art AMR parsing through self-knowledge distillation,
    even without the use of additional data. We release the code at http://www.github.com/sapienzanlp/LeakDistill.
  authors:
  - Pavlo Vasylenko
  - "Pere-Llu\xEDs Huguet Cabot"
  - "Abelardo Carlos Mart\xEDnez Lorenzo"
  - Roberto Navigli
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SEM
  forum: ''
  id: StarSEM_F5
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Incorporating Graph Information in Transformer-based AMR Parsing
  tldr: Abstract Meaning Representation (AMR) is a Semantic Parsing formalism that
    aims at providing a semantic graph abstraction representing a given text. Current
    approaches are based on autoregressive language models such as BART or T5, fine-tuned
    through Teacher Forcing to obtain a linearized version of
  track: The 12th Joint Conference on Lexical and Computational Semantics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Automatic extraction of party (dis)similarities from texts such as party
    election manifestos or parliamentary speeches plays an increasing role in computational
    political science. However, existing approaches are fundamentally limited to targeting
    only global party (dis)similarity: they condense the relationship between a pair
    of parties into a single figure, their similarity. In aggregating over all policy
    domains (e.g., health or foreign policy), they do not provide any qualitative
    insights into which domains parties agree or disagree on. This paper proposes
    a workflow for estimating policy domain aware party similarity that overcomes
    this limitation. The workflow covers (a) definition of suitable policy domains;
    (b) automatic labeling of domains, if no manual labels are available; (c) computation
    of domain-level similarities and aggregation at a global level; (d) extraction
    of interpretable party positions on major policy axes via multidimensional scaling.
    We evaluate our workflow on manifestos from the German federal elections. We find
    that our method (a) yields high correlation when predicting party similarity at
    a global level and (b) provides accurate party-specific positions, even with automatically
    labelled policy domains.'
  authors:
  - Tanise Ceron
  - Dmitry Nikolaev
  - "Sebastian Pad\xF3"
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SEM
  forum: ''
  id: StarSEM_F6
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Additive manifesto decomposition: A policy domain aware method for understanding
    party positioning'
  tldr: 'Automatic extraction of party (dis)similarities from texts such as party
    election manifestos or parliamentary speeches plays an increasing role in computational
    political science. However, existing approaches are fundamentally limited to targeting
    only global party (dis)similarity: they condense the'
  track: The 12th Joint Conference on Lexical and Computational Semantics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Existing metrics for evaluating the quality of automatically generated
    questions such as BLEU, ROUGE, BERTScore, and BLEURT compare the reference and
    predicted questions, providing a high score when there is a considerable lexical
    overlap or semantic similarity between the candidate and the reference questions.
    This approach has two major shortcomings. First, we need expensive human-provided
    reference questions. Second, it penalises valid questions that may not have high
    lexical or semantic similarity to the reference questions. In this paper, we propose
    a new metric, RQUGE, based on the answerability of the candidate question given
    the context. The metric consists of a question-answering and a span scorer modules,
    using pre-trained models from existing literature, thus it can be used without
    any further training. We demonstrate that RQUGE has a higher correlation with
    human judgment without relying on the reference question. Additionally, RQUGE
    is shown to be more robust to several adversarial corruptions. Furthermore, we
    illustrate that we can significantly improve the performance of QA models on out-of-domain
    datasets by fine-tuning on synthetic data generated by a question generation model
    and re-ranked by RQUGE.
  authors:
  - Alireza Mohammadshahi
  - Thomas Scialom
  - Majid Yazdani
  - Pouya Yanki
  - Angela Fan
  - James Henderson
  - Marzieh Saeidi
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SEM
  forum: ''
  id: StarSEM_F7
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'RQUGE: Reference-Free Metric for Evaluating Question Generation by Answering
    the Question'
  tldr: Existing metrics for evaluating the quality of automatically generated questions
    such as BLEU, ROUGE, BERTScore, and BLEURT compare the reference and predicted
    questions, providing a high score when there is a considerable lexical overlap
    or semantic similarity between the candidate and the referenc
  track: The 12th Joint Conference on Lexical and Computational Semantics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Recent years have witnessed a growing interest in investigating what Transformer-based
    language models (TLMs) actually learn from the training data. This is especially
    relevant for complex tasks such as the understanding of non-literal meaning. In
    this work, we probe the performance of three black-box TLMs and two intrinsically
    transparent white-box models on figurative language classification of sarcasm,
    similes, idioms, and metaphors. We conduct two studies on the classification results
    to provide insights into the inner workings of such models. With our first analysis
    on feature importance, we identify crucial differences in model behavior. With
    our second analysis using an online experiment with human participants, we inspect
    different linguistic characteristics of the four figurative language types.
  authors:
  - Hyewon Jang
  - Qi Yu
  - Diego Frassinelli
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SEM
  forum: ''
  id: StarSEM_F8
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Figurative Language Processing: A Linguistically Informed Feature Analysis
    of the Behavior of Language Models and Humans'
  tldr: Recent years have witnessed a growing interest in investigating what Transformer-based
    language models (TLMs) actually learn from the training data. This is especially
    relevant for complex tasks such as the understanding of non-literal meaning. In
    this work, we probe the performance of three black-b
  track: The 12th Joint Conference on Lexical and Computational Semantics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Seyed Mahed Mousavi
  - Simone Caldarella
  - Giuseppe Riccardi
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLP4ConvAI
  forum: ''
  id: NLP4ConvAI_7
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Response Generation in Longitudinal Dialogues: Which Knowledge Representation
    Helps?'
  tldr: ''
  track: The 5th Workshop on NLP for Conversational AI
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Naoki Otani
  - Jun Araki
  - HyeongSik Kim
  - Eduard Hovy
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLP4ConvAI
  forum: ''
  id: NLP4ConvAI_12
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: On the Underspecification of Situations in Open-domain Conversational Datasets
  tldr: ''
  track: The 5th Workshop on NLP for Conversational AI
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Parker Glenn
  - Parag Pravin Dakle
  - Preethi Raghavan
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLP4ConvAI
  forum: ''
  id: NLP4ConvAI_14
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Correcting Semantic Parses with Natural Language through Dynamic Schema Encoding
  tldr: ''
  track: The 5th Workshop on NLP for Conversational AI
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Longfei Yang
  - Jiyi Li
  - Sheng Li
  - Takahiro Shinozaki
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLP4ConvAI
  forum: ''
  id: NLP4ConvAI_17
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Dialogue State Tracking with Sparse Local Slot Attention
  tldr: ''
  track: The 5th Workshop on NLP for Conversational AI
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Yen-Ting Lin
  - Yun-Nung Chen
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLP4ConvAI
  forum: ''
  id: NLP4ConvAI_19
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain
    Conversations with Large Language Models'
  tldr: ''
  track: The 5th Workshop on NLP for Conversational AI
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Anirudh S. Sundar
  - Larry Heck
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLP4ConvAI
  forum: ''
  id: NLP4ConvAI_21
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'cTBLS: Augmenting Large Language Models with Conversational Tables'
  tldr: ''
  track: The 5th Workshop on NLP for Conversational AI
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Maarten De Raedt
  - Fr\'{e}deric Godin
  - Thomas Demeester
  - Chris Develder
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLP4ConvAI
  forum: ''
  id: NLP4ConvAI_22
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'IDAS: Intent Discovery with Abstractive Summarization'
  tldr: ''
  track: The 5th Workshop on NLP for Conversational AI
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Qiusi Zhan
  - Xiaojie Guo
  - Heng Ji
  - Lingfei Wu
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLP4ConvAI
  forum: ''
  id: NLP4ConvAI_23
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: User Simulator Assisted Open-ended Conversational Recommendation System
  tldr: ''
  track: The 5th Workshop on NLP for Conversational AI
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Divyanshu Aggarwal
  - Vivek Gupta
  - Anoop Kunchukuttan
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLP4ConvAI
  forum: ''
  id: NLP4ConvAI_35
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Evaluating Inter-Bilingual Semantic Parsing for Indian Languages
  tldr: ''
  track: The 5th Workshop on NLP for Conversational AI
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Ze-Song Xu
  - Yun-Nung Chen
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLP4ConvAI
  forum: ''
  id: NLP4ConvAI_37
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Zero-Shot Dialogue Relation Extraction by Relating Explainable Triggers and
    Relation Names
  tldr: ''
  track: The 5th Workshop on NLP for Conversational AI
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Gaetan Lopez Latouche
  - Laurence Marcotte
  - Ben Swanson
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLP4ConvAI
  forum: ''
  id: NLP4ConvAI_40
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Generating Video Game Scripts with Style
  tldr: ''
  track: The 5th Workshop on NLP for Conversational AI
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Ananya Ganesh
  - Martha Palmer
  - Katharina Kann
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLP4ConvAI
  forum: ''
  id: NLP4ConvAI_50
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: A Survey of Challenges and Methods in the Computational Modeling of Multi-Party
    Dialog
  tldr: ''
  track: The 5th Workshop on NLP for Conversational AI
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Raghav Gupta
  - Renat Aksitov
  - Samrat Phatale
  - Simral Chaudhary
  - Harrison Lee
  - Abhinav Rastogi
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLP4ConvAI
  forum: ''
  id: NLP4ConvAI_53
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Conversational Recommendation as Retrieval: A Simple, Strong Baseline'
  tldr: ''
  track: The 5th Workshop on NLP for Conversational AI
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Francesco Cazzaro
  - Davide Locatelli
  - Ariadna Julieta Quattoni
  - Xavier Carreras
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLP4ConvAI
  forum: ''
  id: NLP4ConvAI_F1
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Translate First Reorder Later: Leveraging Monotonicity in Semantic Parsing'
  tldr: ''
  track: The 5th Workshop on NLP for Conversational AI
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Hui-Chi Kuo
  - Yun-Nung Chen
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLP4ConvAI
  forum: ''
  id: NLP4ConvAI_F2
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Zero-Shot Prompting for Implicit Intent Prediction and Recommendation with
    Commonsense Reasoning
  tldr: ''
  track: The 5th Workshop on NLP for Conversational AI
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Nikita Moghe
  - Evgeniia Razumovskaia
  - Liane K Guillou
  - Ivan Vuli\'{c}
  - Anna Korhonen
  - Alexandra Birch
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLP4ConvAI
  forum: ''
  id: NLP4ConvAI_F3
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Multi3NLU++: A Multilingual, Multi-Intent, Multi-Domain Dataset for Natural
    Language Understanding in Task-Oriented Dialogue'
  tldr: ''
  track: The 5th Workshop on NLP for Conversational AI
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Vishal Vivek Saley
  - Rocktim Jyoti Das
  - Dinesh Raghu
  - Mausam None
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLP4ConvAI
  forum: ''
  id: NLP4ConvAI_F4
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'DKAF: KB Arbitration for Learning Task-Oriented Dialog Systems with Dialog-KB
    Inconsistencies'
  tldr: ''
  track: The 5th Workshop on NLP for Conversational AI
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Yi-Lin Tuan
  - Alon Albalak
  - Wenda Xu
  - Michael S Saxon
  - Connor F Pryor
  - Lise Getoor
  - William Yang Wang
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLP4ConvAI
  forum: ''
  id: NLP4ConvAI_F5
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'CausalDialogue: Modeling Utterance-level Causality in Conversations'
  tldr: ''
  track: The 5th Workshop on NLP for Conversational AI
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Duong Minh Le
  - Ruohao Guo
  - Wei Xu
  - Alan Ritter
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLP4ConvAI
  forum: ''
  id: NLP4ConvAI_F6
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Improved Instruction Ordering in Recipe-Grounded Conversation
  tldr: ''
  track: The 5th Workshop on NLP for Conversational AI
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Swaroop Mishra
  - Elnaz Nouri
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - NLP4ConvAI
  forum: ''
  id: NLP4ConvAI_F7
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'HELP ME THINK: A Simple Prompting Strategy for Non-experts to Create Customized
    Content with Models'
  tldr: ''
  track: The 5th Workshop on NLP for Conversational AI
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In this paper, we present the interim results of a transformer-based annotation
    pipeline for Ancient and Medieval Greek. As the texts in the Database of Byzantine
    Book Epigrams have not been normalised, they pose more challenges for manual and
    automatic annotation than Ancient Greek, normalised texts do. As a result, the
    existing annotation tools perform poorly. We compiled three data sets for the
    development of an automatic annotation tool and carried out an inter-annotator
    agreement study, with a promising agreement score. The experimental results show
    that our part-of-speech tagger yields accuracy scores that are almost 50 percentage
    points higher than the widely used rule-based system Morpheus. In addition, error
    analysis revealed problems related to phenomena also occurring in current social
    media language.
  authors:
  - Colin Swaelens
  - Ilse De Vos
  - Els Lefever
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - LAW
  forum: ''
  id: LAW_7
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long paper (8 pages)
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Medieval Social Media: Manual and Automatic Annotation of Byzantine Greek
    Marginal Writing'
  tldr: In this paper, we present the interim results of a transformer-based annotation
    pipeline for Ancient and Medieval Greek. As the texts in the Database of Byzantine
    Book Epigrams have not been normalised, they pose more challenges for manual and
    automatic annotation than Ancient Greek, normalised text
  track: The 17th Linguistic Annotation Workshop (LAW-XVII) \\ @ ACL 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'The mythological domain has various ways of expressing events and background
    knowledge. Using data extracted according to the hylistic approach (Zgoll, 2019),
    we annotated a data set of 6315 sentences from various mythological contexts and
    geographical origins, like Ancient Greece and Rome or Mesopotamia, into four categories:
    single-point events (e.g. actions), durative-constant (background knowledge, continuous
    states), durative-initial, and durative-resultativ. This data is used to train
    a classifier, which is able to reliably distinguish event types.'
  authors:
  - Franziska Pannach
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - LAW
  forum: ''
  id: LAW_8
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long paper (8 pages)
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: '"Orpheus Came to His End by Being Struck by a Thunderbolt": Annotating Events
    in Mythological Sequences'
  tldr: The mythological domain has various ways of expressing events and background
    knowledge. Using data extracted according to the hylistic approach (Zgoll, 2019),
    we annotated a data set of 6315 sentences from various mythological contexts and
    geographical origins, like Ancient Greece and Rome or Mesopo
  track: The 17th Linguistic Annotation Workshop (LAW-XVII) \\ @ ACL 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'In this paper, we give a brief survey of the difficulties in handling
    the syntax of mathematical expressions in Universal Dependencies, focusing on
    examples from English language corpora. We first examine the prevalence and current
    handling of mathematical expressions in UD corpora. We then examine several strategies
    for how to approach the handling of syntactic dependencies for such expressions:
    as multi-word expressions, as a domain appropriate for code-switching, or as approximate
    to other types of natural language. Ultimately, we argue that  mathematical expressions
    should primarily be analyzed as natural language, and we offer recommendations
    for the treatment of basic mathematical expressions as analogous to English natural
    language.'
  authors:
  - Lauren Levine
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - LAW
  forum: ''
  id: LAW_9
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long paper (8 pages)
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Difficulties in Handling Mathematical Expressions in Universal Dependencies
  tldr: 'In this paper, we give a brief survey of the difficulties in handling the
    syntax of mathematical expressions in Universal Dependencies, focusing on examples
    from English language corpora. We first examine the prevalence and current handling
    of mathematical expressions in UD corpora. We then examine '
  track: The 17th Linguistic Annotation Workshop (LAW-XVII) \\ @ ACL 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'We present a novel dataset for physical and abstract plausibility of
    events in English. Based on naturally occurring sentences extracted from Wikipedia,
    we infiltrate degrees of abstractness, and automatically generate perturbed pseudo-implausible
    events. We annotate a filtered and balanced subset for plausibility using crowd-sourcing,
    and perform extensive cleansing to ensure annotation quality. In-depth quantitative
    analyses indicate that annotators favor plausibility over implausibility and disagree
    more on implausible events. Furthermore, our plausibility dataset is the first
    to capture abstractness in events to the same extent as concreteness, and we find
    that event abstractness has an impact on plausibility ratings: more concrete event
    participants trigger a perception of implausibility.'
  authors:
  - Annerose Eichel
  - Sabine Schulte Im Walde
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - LAW
  forum: ''
  id: LAW_11
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long paper (8 pages)
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: A Dataset for Physical and Abstract Plausibility and Sources of Human Disagreement
  tldr: We present a novel dataset for physical and abstract plausibility of events
    in English. Based on naturally occurring sentences extracted from Wikipedia, we
    infiltrate degrees of abstractness, and automatically generate perturbed pseudo-implausible
    events. We annotate a filtered and balanced subset f
  track: The 17th Linguistic Annotation Workshop (LAW-XVII) \\ @ ACL 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The Turkish particle dA is a focus-associated enclitic, and it can act
    as a discourse connective conveying multiple senses, like additive, contrastive,
    causal etc. Like many other linguistic expressions, it is subject to usage ambiguity
    and creates a challenge in natural language automatization tasks. For the first
    time, we annotate the discourse and non-discourse connnective occurrences of dA
    in Turkish with the PDTB principles. Using a minimal set of linguistic features,
    we develop binary classifiers to distinguish its discourse connective usage from
    its other usages. We show that despite its ability to cliticize to any syntactic
    type, variable position in the sentence and having a wide argument span, its discourse/non-discourse
    connective usage can be annotated reliably and its discourse usage can be disambiguated
    by exploiting local cues.
  authors:
  - "Ebru Ers\xF6yleyen"
  - Deniz Zeyrek
  - "F\u0131rat \xD6ter"
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - LAW
  forum: ''
  id: LAW_12
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long paper (8 pages)
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Annotating and Disambiguating the Discourse Usage of the Enclitic dA in Turkish
  tldr: The Turkish particle dA is a focus-associated enclitic, and it can act as
    a discourse connective conveying multiple senses, like additive, contrastive,
    causal etc. Like many other linguistic expressions, it is subject to usage ambiguity
    and creates a challenge in natural language automatization task
  track: The 17th Linguistic Annotation Workshop (LAW-XVII) \\ @ ACL 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: High-quality labeled data is paramount to the performance of modern machine
    learning models. However, annotating data is a time-consuming and costly process
    that requires human experts to examine large collections of raw data. For conversational
    agents in production settings with access to large amounts of user-agent conversations,
    the challenge is to decide what data should be annotated first. We consider the
    Natural Language Understanding (NLU) component of a conversational agent deployed
    in a real-world setup with limited resources. We present an active learning pipeline
    for offline detection of classification errors that leverages two strong classifiers.
    Then, we perform topic modeling on the potentially mis-classified samples to ease
    data analysis and to reveal error patterns. In our experiments, we show on a real-world
    dataset that by using our method to prioritize data annotation we reach 100\%
    of the performance annotating only 36\% of the data. Finally, we present an analysis
    of some of the error patterns revealed and argue that our pipeline is a valuable
    tool to detect critical errors and reduce the workload of annotators.
  authors:
  - Damian Pascual
  - Aritz Bercher
  - Akansha Bhardwaj
  - Mingbo Cui
  - Dominic Kohler
  - Liam Van Der Poel
  - Paolo Rosso
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - LAW
  forum: ''
  id: LAW_13
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short paper (4 pages)
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: An Active Learning Pipeline for NLU Error Detection in Conversational Agents
  tldr: High-quality labeled data is paramount to the performance of modern machine
    learning models. However, annotating data is a time-consuming and costly process
    that requires human experts to examine large collections of raw data. For conversational
    agents in production settings with access to large amo
  track: The 17th Linguistic Annotation Workshop (LAW-XVII) \\ @ ACL 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This work presents two corpora based on excerpts from two novels with
    an informal narration style in German. We performed fine-grained multi-layer annotations
    of animate referents, assigning local and global prominence-lending features to
    the annotated referring expressions. In addition, our corpora include annotations
    of intra-sentential segments, which can serve as a more reliable unit of length
    measurement. Furthermore, we present two exemplary studies demonstrating how to
    use these corpora.
  authors:
  - Magdalena Repp
  - Petra B. Schumacher
  - Fahime Same
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - LAW
  forum: ''
  id: LAW_18
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long paper (8 pages)
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Multi-layered Annotation of Conversation-like Narratives in German
  tldr: This work presents two corpora based on excerpts from two novels with an informal
    narration style in German. We performed fine-grained multi-layer annotations of
    animate referents, assigning local and global prominence-lending features to the
    annotated referring expressions. In addition, our corpora
  track: The 17th Linguistic Annotation Workshop (LAW-XVII) \\ @ ACL 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Most tasks in NLP require labeled data. Data labeling is often done on
    crowdsourcing platforms due to scalability reasons. However, publishing data on
    public platforms can only be done if no privacy-relevant information is included.
    Textual data often contains sensitive information like person names or locations.
    In this work, we investigate how removing personally identifiable information
    (PII) as well as applying differential privacy (DP) rewriting can enable text
    with privacy-relevant information to be used for crowdsourcing. We find that DP-rewriting
    before crowdsourcing can preserve privacy while still leading to good label quality
    for certain tasks and data. PII-removal led to good label quality in all examined
    tasks, however, there are no privacy guarantees given.
  authors:
  - Nina Mouhammad
  - Johannes Daxenberger
  - Benjamin Schiller
  - Ivan Habernal
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - LAW
  forum: ''
  id: LAW_20
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long paper (8 pages)
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Crowdsourcing on Sensitive Data with Privacy-Preserving Text Rewriting
  tldr: Most tasks in NLP require labeled data. Data labeling is often done on crowdsourcing
    platforms due to scalability reasons. However, publishing data on public platforms
    can only be done if no privacy-relevant information is included. Textual data
    often contains sensitive information like person names
  track: The 17th Linguistic Annotation Workshop (LAW-XVII) \\ @ ACL 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In this project, we have investigated the use of advanced machine learning
    methods, specifically fine-tuned large language models, for pre-annotating data
    for a lexical extension task, namely adding descriptive words (verbs) to an existing
    (but incomplete, as of yet) ontology of event types. Several research questions
    have been focused on, from the investigation of a possible heuristics to provide
    at least hints to annotators which verbs to include and which are outside the
    current version of the ontology, to the possible use of the automatic scores to
    help the annotators to be more efficient in finding a threshold for identifying
    verbs that cannot be assigned to any existing class and therefore they are to
    be used as seeds for a new class. We have also carefully examined the correlation
    of the automatic scores with the human annotation. While the correlation turned
    out to be strong, its influence on the annotation proper is modest due to its
    near linearity, even though the mere fact of such pre-annotation leads to relatively
    short annotation times.
  authors:
  - "Jana Strakov\xE1"
  - "Eva Fu\u010D\xEDkov\xE1"
  - "Jan Haji\u010D"
  - "Zde\u0148ka Ure\u0161ov\xE1"
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - LAW
  forum: ''
  id: LAW_22
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long paper (8 pages)
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Extending an Event-type Ontology: Adding Verbs and Classes Using Fine-tuned
    LLMs Suggestions'
  tldr: In this project, we have investigated the use of advanced machine learning
    methods, specifically fine-tuned large language models, for pre-annotating data
    for a lexical extension task, namely adding descriptive words (verbs) to an existing
    (but incomplete, as of yet) ontology of event types. Several
  track: The 17th Linguistic Annotation Workshop (LAW-XVII) \\ @ ACL 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Much work in natural language processing (NLP) relies on human annotation.
    The majority of this implicitly assumes that annotator's labels are temporally
    stable, although the reality is that human judgements are rarely consistent over
    time. As a subjective annotation task, hate speech labels depend on annotator's
    emotional and moral reactions to the language used to convey the message. Studies
    in Cognitive Science reveal a `foreign language effect', whereby people take differing
    moral positions and perceive offensive phrases to be weaker in their second languages.
    Does this affect annotations as well? We conduct an experiment to investigate
    the impacts of (1) time and (2) different language conditions (English and German)
    on measurements of intra-annotator agreement in a hate speech labelling task.
    While we do not observe the expected lower stability in the different language
    condition, we find that overall agreement is significantly lower than is implicitly
    assumed in annotation tasks, which has important implications for dataset reproducibility
    in NLP.
  authors:
  - Gavin Abercrombie
  - Dirk Hovy
  - Vinodkumar Prabhakaran
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - LAW
  forum: ''
  id: LAW_26
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short paper (4 pages)
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Temporal and Second Language Influence on Intra-Annotator Agreement and Stability
    in Hate Speech Labelling
  tldr: 'Much work in natural language processing (NLP) relies on human annotation.
    The majority of this implicitly assumes that annotator''s labels are temporally
    stable, although the reality is that human judgements are rarely consistent over
    time. As a subjective annotation task, hate speech labels depend '
  track: The 17th Linguistic Annotation Workshop (LAW-XVII) \\ @ ACL 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Coreference Resolution is a well studied problem in NLP. While widely
    studied for English and other resource-rich languages, research on coreference
    resolution in Bengali largely remains unexplored due to the absence of relevant
    datasets. Bengali, being a low-resource language, exhibits greater morphological
    richness compared to English. In this article, we introduce a new dataset, BenCoref,
    comprising coreference annotations for Bengali texts gathered from four distinct
    domains. This relatively small dataset contains 5200 mention annotations forming
    502 mention clusters within 48,569 tokens. We describe the process of creating
    this dataset and report performance of multiple models trained using BenCoref.
    We anticipate that our work sheds some light on the variations in coreference
    phenomena across multiple domains in Bengali and encourages the development of
    additional resources for Bengali. Furthermore, we found poor crosslingual performance
    at zero-shot  setting from English, highlighting the need for more language-specific
    resources for this task.
  authors:
  - Shadman Rohan
  - Mojammel Hossain
  - Mohammad Rashid
  - Nabeel Mohammed
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - LAW
  forum: ''
  id: LAW_27
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long paper (8 pages)
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'BenCoref: A Multi-Domain Dataset of Nominal Phrases and Pronominal Reference
    Annotations'
  tldr: Coreference Resolution is a well studied problem in NLP. While widely studied
    for English and other resource-rich languages, research on coreference resolution
    in Bengali largely remains unexplored due to the absence of relevant datasets.
    Bengali, being a low-resource language, exhibits greater morp
  track: The 17th Linguistic Annotation Workshop (LAW-XVII) \\ @ ACL 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'The availability of annotated legal corpora is crucial for a number of
    tasks, such as legal search, legal information retrieval, and predictive justice.
    Annotation is mostly assumed to be a straightforward task: as long as the annotation
    scheme is well defined and the guidelines are clear, annotators are expected to
    agree on the labels. This is not always the case, especially in legal annotation,
    which can be extremely difficult even for expert annotators. We propose a legal
    annotation procedure that takes into account annotator certainty and improves
    it through negotiation. We also collect annotator feedback and show that our approach
    contributes to a positive annotation environment. Our work invites reflection
    on often neglected ethical concerns regarding legal annotation.'
  authors:
  - Emma Zanoli
  - Matilde Barbini
  - Davide Riva
  - Sergio Picascia
  - Emanuela Furiosi
  - Stefano D'Ancona
  - Cristiano Chesi
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - LAW
  forum: ''
  id: LAW_28
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long paper (8 pages)
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Annotators-in-the-loop: Testing a Novel Annotation Procedure on Italian
    Case Law'
  tldr: 'The availability of annotated legal corpora is crucial for a number of tasks,
    such as legal search, legal information retrieval, and predictive justice. Annotation
    is mostly assumed to be a straightforward task: as long as the annotation scheme
    is well defined and the guidelines are clear, annotator'
  track: The 17th Linguistic Annotation Workshop (LAW-XVII) \\ @ ACL 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: "This submission reports on a three-part series of original methods geared\
    \ towards producing semantic annotations for the decompositional marker \"again\"\
    . The three methods are (i) exhaustive expert annotation based on a comprehensive\
    \ set of guidelines, (ii) extension of expert annotation by predicting presuppositions\
    \ with a Multinomial Na\xEFve Bayes classifier in the context of a meta-analysis\
    \ to optimize feature selection and (iii) quality-controlled crowdsourcing with\
    \ ensuing evaluation and KMeans clustering of annotation vectors."
  authors:
  - Martin Kopf
  - Remus Gergel
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - LAW
  forum: ''
  id: LAW_33
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long paper (8 pages)
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Annotating Decomposition in Time: Three Approaches for Again'
  tldr: This submission reports on a three-part series of original methods geared
    towards producing semantic annotations for the decompositional marker "again".
    The three methods are (i) exhaustive expert annotation based on a comprehensive
    set of guidelines, (ii) extension of expert annotation by predictin
  track: The 17th Linguistic Annotation Workshop (LAW-XVII) \\ @ ACL 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Annotating cross-document event coreference links is a time-consuming
    and cognitively demanding task that can compromise annotation quality and efficiency.
    To address this, we propose a model-in-the-loop annotation approach for event
    coreference resolution, where a machine learning model suggests likely corefering
    event pairs only. We evaluate the effectiveness of this approach by first simulating
    the annotation process and then, using a novel annotator-centric Recall-Annotation
    effort trade-off metric, we compare the results of various underlying models and
    datasets. We finally present a method for obtaining 97\% recall while substantially
    reducing the workload required by a fully manual annotation process.
  authors:
  - Shafiuddin Rehan Ahmed
  - Abhijnan Nath
  - Michael Regan
  - Adam Pollins
  - Nikhil Krishnaswamy
  - James H. Martin
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - LAW
  forum: ''
  id: LAW_34
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short paper (4 pages)
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: How Good Is the Model in Model-in-the-loop Event Coreference Resolution Annotation?
  tldr: Annotating cross-document event coreference links is a time-consuming and
    cognitively demanding task that can compromise annotation quality and efficiency.
    To address this, we propose a model-in-the-loop annotation approach for event
    coreference resolution, where a machine learning model suggests li
  track: The 17th Linguistic Annotation Workshop (LAW-XVII) \\ @ ACL 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The annotation task we elaborated aims at describing the contextual factors
    that influence the appearance and interpretation of moral predicates, in newspaper
    articles on police brutality, in French and in English. The paper provides a brief
    review of the literature on moral predicates and their relation with context.
    The paper also describes the elaboration of the corpus and the ontology. Our hypothesis
    is that the use of moral adjectives and their appearance in context could change
    depending on the political orientation of the journal. We elaborated an annotation
    task to investigate the precise contexts discussed in articles on police brutality.
    The paper concludes by describing the study and the annotation task in details.
  authors:
  - Tess Feyen
  - Alda Mari
  - Paul Portner
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - LAW
  forum: ''
  id: LAW_38
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short paper (4 pages)
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Pragmatic Annotation of Articles Related to Police Brutality
  tldr: The annotation task we elaborated aims at describing the contextual factors
    that influence the appearance and interpretation of moral predicates, in newspaper
    articles on police brutality, in French and in English. The paper provides a brief
    review of the literature on moral predicates and their rel
  track: The 17th Linguistic Annotation Workshop (LAW-XVII) \\ @ ACL 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: "We present the RST Continuity Corpus (RST-CC), a corpus of discourse\
    \ relations annotated for continuity dimensions. Continuity or discontinuity (maintaining\
    \ or shifting deictic centres across discourse segments) is an important property\
    \ of discourse relations, but the two are correlated in greatly varying ways.\
    \ To analyse this correlation, the relations in the RST-CC are annotated using\
    \ operationalised versions of Giv\xF3n's (1993) continuity dimensions. We also\
    \ report on the inter-annotator agreement, and discuss recurrent annotation issues.\
    \ First results show substantial variation of continuity dimensions within and\
    \ across relation types."
  authors:
  - Debopam Das
  - Markus Egg
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - LAW
  forum: ''
  id: LAW_40
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long paper (8 pages)
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: The RST Continuity Corpus
  tldr: We present the RST Continuity Corpus (RST-CC), a corpus of discourse relations
    annotated for continuity dimensions. Continuity or discontinuity (maintaining
    or shifting deictic centres across discourse segments) is an important property
    of discourse relations, but the two are correlated in greatly v
  track: The 17th Linguistic Annotation Workshop (LAW-XVII) \\ @ ACL 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'We present GENTLE, a new mixed-genre English challenge corpus totaling
    17K tokens and consisting of 8 unusual text types for out-of-domain evaluation:
    dictionary entries, esports commentaries, legal documents, medical notes, poetry,
    mathematical proofs, syllabuses, and threat letters. GENTLE is manually annotated
    for a variety of popular NLP tasks, including syntactic dependency parsing, entity
    recognition, coreference resolution, and discourse parsing. We evaluate state-of-the-art
    NLP systems on GENTLE and find severe degradation for at least some genres in
    their performance on all tasks, which indicates GENTLE''s utility as an evaluation
    dataset for NLP systems.'
  authors:
  - Tatsuya Aoyama
  - Shabnam Behzad
  - Luke Gessler
  - Lauren Levine
  - Jessica Lin
  - Yang Janet Liu
  - Siyao Peng
  - Yilun Zhu
  - Amir Zeldes
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - LAW
  forum: ''
  id: LAW_41
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long paper (8 pages)
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'GENTLE: A Genre-Diverse Multilayer Challenge Set for English NLP and Linguistic
    Evaluation'
  tldr: 'We present GENTLE, a new mixed-genre English challenge corpus totaling 17K
    tokens and consisting of 8 unusual text types for out-of-domain evaluation: dictionary
    entries, esports commentaries, legal documents, medical notes, poetry, mathematical
    proofs, syllabuses, and threat letters. GENTLE is manu'
  track: The 17th Linguistic Annotation Workshop (LAW-XVII) \\ @ ACL 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The task of summarisation is notoriously difficult to evaluate, with agreement
    even between expert raters unlikely to be perfect. One technique for summary evaluation
    relies on collecting comparison data by presenting annotators with generated summaries
    and tasking them with selecting the best one. This paradigm is currently being
    exploited in reinforcement learning using human feedback, whereby a reward function
    is trained using pairwise choice data. Comparisons are an easier way to elicit
    human feedback for summarisation, however, such decisions can be bottle necked
    by the usability of the annotator interface. In this paper, we present the results
    of a pilot study exploring how the user interface impacts annotator agreement
    when judging summary quality.
  authors:
  - Sian Gooding
  - Lucas Werner
  - "Victor C\u0103rbune"
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - LAW
  forum: ''
  id: LAW_42
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long paper (8 pages)
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: A Study on Annotation Interfaces for Summary Comparison
  tldr: 'The task of summarisation is notoriously difficult to evaluate, with agreement
    even between expert raters unlikely to be perfect. One technique for summary evaluation
    relies on collecting comparison data by presenting annotators with generated summaries
    and tasking them with selecting the best one. '
  track: The 17th Linguistic Annotation Workshop (LAW-XVII) \\ @ ACL 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Within the research presented in this article, we created a new question
    answering benchmark database for Hungarian called MILQA. When creating the dataset,
    we basically followed the principles of the English SQuAD 2.0, however, like in
    some more recent English question answering datasets, we introduced a number of
    innovations beyond SQuAD: e.g., yes/no-questions, list-like answers consisting
    of several text spans, long answers, questions requiring calculation and other
    question types where you cannot simply copy the answer from the text. For all
    these non-extractive question types, the pragmatically adequate form of the answer
    was also added to make the training of generative models possible. We implemented
    and evaluated a set of baseline retrieval and answer span extraction models on
    the dataset. BM25 performed better than any vector-based solution for retrieval.
    Cross-lingual transfer from English significantly improved span extraction models.'
  authors:
  - "Attila Nov\xE1k"
  - "Borb\xE1la Nov\xE1k"
  - "Tam\xE1s Zombori"
  - "Gerg\u0151 Szab\xF3"
  - "Zsolt Sz\xE1nt\xF3"
  - "Rich\xE1rd Farkas"
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - LAW
  forum: ''
  id: LAW_44
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long paper (8 pages)
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: A Question Answering Benchmark Database for Hungarian
  tldr: Within the research presented in this article, we created a new question answering
    benchmark database for Hungarian called MILQA. When creating the dataset, we basically
    followed the principles of the English SQuAD 2.0, however, like in some more recent
    English question answering datasets, we introd
  track: The 17th Linguistic Annotation Workshop (LAW-XVII) \\ @ ACL 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Natural Language Inference (NLI) has been a cornerstone task in evaluating
    language models' inferential reasoning capabilities. However, the standard three-way
    classification scheme used in NLI has well-known shortcomings in evaluating models'
    ability to capture the nuances of natural human reasoning. In this paper, we argue
    that the operationalization of the neutral label in current NLI datasets has low
    validity, is interpreted inconsistently, and that at least one important sense
    of neutrality is often ignored. We uncover the detrimental impact of these shortcomings,
    which in some cases leads to annotation datasets that actually decrease performance
    on downstream tasks. We compare approaches of handling annotator disagreement
    and identify flaws in a recent NLI dataset that designs an annotator study based
    on a problematic operationalization. Our findings highlight the need for a more
    refined evaluation framework for NLI, and we hope to spark further discussion
    and action in the NLP community.
  authors:
  - Animesh Nighojkar
  - Antonio Laverghetta Jr.
  - John Licato
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - LAW
  forum: ''
  id: LAW_47
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long paper (8 pages)
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'No Strong Feelings One Way or Another: Re-operationalizing Neutrality in
    Natural Language Inference'
  tldr: Natural Language Inference (NLI) has been a cornerstone task in evaluating
    language models' inferential reasoning capabilities. However, the standard three-way
    classification scheme used in NLI has well-known shortcomings in evaluating models'
    ability to capture the nuances of natural human reasonin
  track: The 17th Linguistic Annotation Workshop (LAW-XVII) \\ @ ACL 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: UMR-Writer is a web-based tool for annotating semantic graphs with the
    Uniform Meaning Representation (UMR) scheme. UMR is a graph-based semantic representation
    that can be applied cross-linguistically for deep semantic analysis of texts.
    In this work, we implemented a new keyboard interface in UMR-Writer 2.0, which
    is a powerful addition to the original mouse interface, supporting faster annotation
    for more experienced annotators. The new interface also addresses issues with
    the original mouse interface. Additionally, we demonstrate an efficient workflow
    for annotation project management in UMR-Writer 2.0, which has been applied to
    many projects.
  authors:
  - Sijia Ge
  - Jin Zhao
  - Kristin Wright-bettner
  - Skatje Myers
  - Nianwen Xue
  - Martha Palmer
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - LAW
  forum: ''
  id: LAW_48
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short paper (4 pages)
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'UMR-Writer 2.0: Incorporating a New Keyboard Interface and Workflow into
    UMR-Writer'
  tldr: UMR-Writer is a web-based tool for annotating semantic graphs with the Uniform
    Meaning Representation (UMR) scheme. UMR is a graph-based semantic representation
    that can be applied cross-linguistically for deep semantic analysis of texts.
    In this work, we implemented a new keyboard interface in UMR-
  track: The 17th Linguistic Annotation Workshop (LAW-XVII) \\ @ ACL 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We investigate whether the Cambridge Grammar of the English Language (2002)
    and its extensive descriptions work well as a corpus annotation scheme. We develop
    annotation guidelines and in the process outline some interesting linguistic uncertainties
    that we had to resolve. To test the applicability of CGEL to real-world corpora,
    we conduct an interannotator study on sentences from the English Web Treebank,
    showing that consistent annotation of even complex syntactic phenomena like gapping
    using the CGEL formalism is feasible. Why introduce yet another formalism for
    English syntax? We argue that CGEL is attractive due to its exhaustive analysis
    of English syntactic phenomena, its labeling of both constituents and functions,
    and its accessibility. We look towards expanding CGELBank and augmenting it with
    automatic conversions from existing treebanks in the future.
  authors:
  - Brett Reynolds
  - Aryaman Arora
  - Nathan Schneider
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - LAW
  forum: ''
  id: LAW_50
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long paper (8 pages)
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Unified Syntactic Annotation of English in the CGEL Framework
  tldr: 'We investigate whether the Cambridge Grammar of the English Language (2002)
    and its extensive descriptions work well as a corpus annotation scheme. We develop
    annotation guidelines and in the process outline some interesting linguistic uncertainties
    that we had to resolve. To test the applicability '
  track: The 17th Linguistic Annotation Workshop (LAW-XVII) \\ @ ACL 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Patent descriptions are a crucial component of patent applications, as
    they are key to understanding the invention and play a significant role in securing
    patent grants. While discursive analyses have been undertaken for scientific articles,
    they have not been as thoroughly explored for patent descriptions, despite the
    increasing importance of Intellectual Property and the constant rise of the number
    of patent applications. In this study, we propose an annotation scheme containing
    16 classes that allows categorizing each sentence in patent descriptions according
    to their discursive roles. We publish an experimental human-annotated corpus of
    16 patent descriptions and analyze challenges that may be encountered in such
    work. This work can be base for an automated annotation and thus contribute to
    enriching linguistic resources in the patent domain.
  authors:
  - Lufei Liu
  - Xu Sun
  - "Fran\xE7ois Veltz"
  - Kim Gerdes
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - LAW
  forum: ''
  id: LAW_51
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long paper (8 pages)
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Annotating Discursive Roles of Sentences in Patent Descriptions
  tldr: Patent descriptions are a crucial component of patent applications, as they
    are key to understanding the invention and play a significant role in securing
    patent grants. While discursive analyses have been undertaken for scientific articles,
    they have not been as thoroughly explored for patent descr
  track: The 17th Linguistic Annotation Workshop (LAW-XVII) \\ @ ACL 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Cross-lingual annotation projection is a practical method for improving
    performance on low resource structured prediction tasks. An important step in
    annotation projection is obtaining alignments between the source and target texts,
    which enables the mapping of annotations across the texts. By manually correcting
    automatically generated alignments, we examine the impact of alignment quality---automatic,
    manual, and mixed---on downstream performance for two information extraction tasks
    and quantify the trade-off between annotation effort and model performance.
  authors:
  - Shabnam Behzad
  - Seth Ebner
  - Marc Marone
  - Benjamin Van Durme
  - Mahsa Yarmohammadi
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - LAW
  forum: ''
  id: LAW_52
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short paper (4 pages)
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: The Effect of Alignment Correction on Cross-Lingual Annotation Projection
  tldr: Cross-lingual annotation projection is a practical method for improving performance
    on low resource structured prediction tasks. An important step in annotation projection
    is obtaining alignments between the source and target texts, which enables the
    mapping of annotations across the texts. By manua
  track: The 17th Linguistic Annotation Workshop (LAW-XVII) \\ @ ACL 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Annotators are not fungible. Their demographics, life experiences, and
    backgrounds all contribute to how they label data. However, NLP has only recently
    considered how annotator identity might influence their decisions. Here, we present
    POPQUORN (the Potato-Prolific dataset for Question-Answering, Offensiveness, text
    Rewriting and politeness rating with demographic Nuance). POPQUORN contains 45,000
    annotations from 1,484 annotators, drawn from a representative sample regarding
    sex, age, and race as the US population. Through a series of analyses, we show
    that annotators' background plays a significant role in their judgments. Further,
    our work shows that backgrounds not previously considered in NLP (e.g., education),
    are meaningful and should be considered. Our study suggests that understanding
    the background of annotators and collecting labels from a demographically balanced
    pool of crowd workers is important to reduce the bias of datasets. The dataset,
    annotator background, and annotation interface are available at https://github.com/Jiaxin-Pei/potato-prolific-dataset.
  authors:
  - Jiaxin Pei
  - David Jurgens
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - LAW
  forum: ''
  id: LAW_54
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long paper (8 pages)
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: When Do Annotator Demographics Matter? Measuring the Influence of Annotator
    Demographics with the POPQUORN Dataset
  tldr: Annotators are not fungible. Their demographics, life experiences, and backgrounds
    all contribute to how they label data. However, NLP has only recently considered
    how annotator identity might influence their decisions. Here, we present POPQUORN
    (the Potato-Prolific dataset for Question-Answering, O
  track: The 17th Linguistic Annotation Workshop (LAW-XVII) \\ @ ACL 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'In this paper we address the scarcity of annotated data for NArabizi,
    a Romanized form of North African Arabic used mostly on  social media, which poses
    challenges for Natural Language Processing  (NLP). We introduce an enriched version
    of NArabizi Treebank (Seddah et al., 2020) with three main contributions: the
    addition of two novel annotation layers (named entity recognition and offensive
    language detection) and a re-annotation of the tokenization, morpho-syntactic
    and syntactic layers that ensure annotation consistency. Our experimental results,
    using different tokenization schemes, showcase the value of our contributions
    and highlight the impact of working with non-gold tokenization for NER and dependency
    parsing. To facilitate future research, we make these annotations publicly available.
    Our enhanced NArabizi Treebank paves the way for creating sophisticated language
    models and NLP tools for this under-represented language.'
  authors:
  - Arij Riabi
  - Menel Mahamdi
  - "Djam\xE9 Seddah"
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - LAW
  forum: ''
  id: LAW_58
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long paper (8 pages)
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Enriching the NArabizi Treebank: A Multifaceted Approach to Supporting an
    Under-Resourced Language'
  tldr: In this paper we address the scarcity of annotated data for NArabizi, a Romanized
    form of North African Arabic used mostly on  social media, which poses challenges
    for Natural Language Processing  (NLP). We introduce an enriched version of NArabizi
    Treebank (Seddah et al., 2020) with three main cont
  track: The 17th Linguistic Annotation Workshop (LAW-XVII) \\ @ ACL 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Good datasets are a foundation of NLP research, and form the basis for
    training and evaluating models of language use. While creating datasets, the standard
    practice is to verify the annotation consistency using a committee of human annotators.
    This norm assumes that multiple annotators are available, which is not the case
    for highly specialized tasks or low-resource languages. In this paper, we ask:
    Can we evaluate the quality of a dataset constructed by a single human annotator?
    To address this question, we propose four weak verifiers to help estimate dataset
    quality, and outline when each may be employed. We instantiate these strategies
    for the task of semantic analysis of adpositions in Gujarati, a low-resource language,
    and show that our weak verifiers concur with a double-annotation study. As an
    added contribution, we also release the first dataset with semantic annotations
    in Gujarati along with several model baselines.'
  authors:
  - Maitrey Mehta
  - Vivek Srikumar
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - LAW
  forum: ''
  id: LAW_F1
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Verifying Annotation Agreement without Multiple Experts: A Case Study with
    Gujarati SNACS'
  tldr: Good datasets are a foundation of NLP research, and form the basis for training
    and evaluating models of language use. While creating datasets, the standard practice
    is to verify the annotation consistency using a committee of human annotators.
    This norm assumes that multiple annotators are availabl
  track: The 17th Linguistic Annotation Workshop (LAW-XVII) \\ @ ACL 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The task of textual geolocation retrieving the coordinates of a place
    based on a free-form language description calls for not only grounding but also
    natural language understanding and geospatial reasoning. Even though there are
    quite a few datasets in English used for geolocation, they are currently based
    on open-source data (Wikipedia and Twitter), where the location of the described
    place is mostly implicit, such that the location retrieval resolution is limited.
    Furthermore, there are no datasets available for addressing the problem of textual
    geolocation in morphologically rich and resource-poor languages, such as Hebrew.
    In this paper, we present the Hebrew Geo-Location (HeGeL) corpus, designed to
    collect literal place descriptions and analyze lingual geospatial reasoning. We
    crowdsourced 5,649 literal Hebrew place descriptions of various place types in
    three cities in Israel. Qualitative and empirical analysis show that the data
    exhibits abundant use of geospatial reasoning and requires a novel environmental
    representation.
  authors:
  - Tzuf Paz-Argaman
  - Tal Bauman
  - Itai Mondshine
  - Itzhak Omer
  - Sagi Dalyot
  - Reut Tsarfaty
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - LAW
  forum: ''
  id: LAW_F2
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'HeGeL: A Novel Dataset for Geo-Location from Hebrew Text'
  tldr: The task of textual geolocation retrieving the coordinates of a place based
    on a free-form language description calls for not only grounding but also natural
    language understanding and geospatial reasoning. Even though there are quite a
    few datasets in English used for geolocation, they are currentl
  track: The 17th Linguistic Annotation Workshop (LAW-XVII) \\ @ ACL 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper presents HaVQA, the first multi-modal dataset for visual question-answering
    (VQA) tasks in the Hausa language. The dataset was created by manually translating
    6,022 English question-answer pairs, which are associated with 1,555 unique images
    from the Visual Genome dataset. As a result, the dataset provides 12,044 gold
    standard English-Hausa parallel sentences that were translated in a fashion that
    guarantees their semantic match with the corresponding visual information. We
    conducted several baseline experiments on the dataset, including visual question
    answering, visual question elicitation, text-only and multi-modal machine translation.
  authors:
  - Shantipriya Parida
  - Idris Abdulmumin
  - Shamsuddeen Muhammad
  - Aneesh Bose
  - Guneet Kohli
  - Ibrahim Ahmad
  - Ketan Kotwal
  - Sayan Deb Sarkar
  - "Ond\u0159ej Bojar"
  - Habeebah Kakudi
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - LAW
  forum: ''
  id: LAW_F3
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'HaVQA: A Dataset for Visual Question Answering and Multimodal Research in
    Hausa Language'
  tldr: This paper presents HaVQA, the first multi-modal dataset for visual question-answering
    (VQA) tasks in the Hausa language. The dataset was created by manually translating
    6,022 English question-answer pairs, which are associated with 1,555 unique images
    from the Visual Genome dataset. As a result, th
  track: The 17th Linguistic Annotation Workshop (LAW-XVII) \\ @ ACL 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'With a growing focus on morphological inflection systems for languages
    where high-quality data is scarce, training data noise is a serious but so far
    largely ignored concern. We aim at closing this gap by investigating the types
    of noise encountered within a pipeline for truly unsupervised morphological paradigm
    completion and its impact on morphological inflection systems: First, we propose
    an error taxonomy and annotation pipeline for inflection training data. Then,
    we compare the effect of different types of noise on multiple state-of-the-art
    inflection models. Finally, we propose a novel character-level masked language
    modeling (CMLM) pretraining objective and explore its impact on the models'' resistance
    to noise.  Our experiments show that various architectures are impacted differently
    by separate types of noise, but encoder-decoders tend to be more robust to noise
    than models trained with a copy bias. CMLM pretraining helps transformers, but
    has lower impact on LSTMs.'
  authors:
  - Adam Wiemerslage
  - Changbing Yang
  - Garrett Nicolai
  - Miikka Silfverberg
  - Katharina Kann
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - LAW
  forum: ''
  id: LAW_F4
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: An Investigation of Noise in Morphological Inflection
  tldr: With a growing focus on morphological inflection systems for languages where
    high-quality data is scarce, training data noise is a serious but so far largely
    ignored concern. We aim at closing this gap by investigating the types of noise
    encountered within a pipeline for truly unsupervised morpholog
  track: The 17th Linguistic Annotation Workshop (LAW-XVII) \\ @ ACL 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Metrics for Inter-Annotator Agreement (IAA), like Cohen's Kappa, are crucial
    for validating annotated datasets. Although high agreement is often used to show
    the reliability of annotation procedures, it is insufficient to ensure validity
    or reproducibility. While researchers are encouraged to increase annotator agreement,
    this can lead to specific and tailored annotation guidelines. We hypothesize that
    this may result in diverging annotations from different groups. To study this,
    we first propose the Lee et al. Protocol (LEAP), a standardized and codified annotation
    protocol. LEAP strictly enforces transparency in the annotation process, which
    ensures reproducibility of annotation guidelines. Using LEAP to annotate a dialog
    dataset, we empirically show that while research groups may create reliable guidelines
    by raising agreement, this can cause divergent annotations across different research
    groups, thus questioning the validity of the annotations. Therefore, we caution
    NLP researchers against using reliability as a proxy for reproducibility and validity.
  authors:
  - Seunggun Lee
  - Alexandra DeLucia
  - Nikita Nangia
  - Praneeth Ganedi
  - Ryan Guan
  - Rubing Li
  - Britney Ngaw
  - Aditya Singhal
  - Shalaka Vaidya
  - Zijun Yuan
  - Lining Zhang
  - "Jo\xE3o Sedoc"
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - LAW
  forum: ''
  id: LAW_F5
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Common Law Annotations: Investigating the Stability of Dialog System Output
    Annotations'
  tldr: Metrics for Inter-Annotator Agreement (IAA), like Cohen's Kappa, are crucial
    for validating annotated datasets. Although high agreement is often used to show
    the reliability of annotation procedures, it is insufficient to ensure validity
    or reproducibility. While researchers are encouraged to increa
  track: The 17th Linguistic Annotation Workshop (LAW-XVII) \\ @ ACL 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Figurative language permeates human communication, but at the same time
    is relatively understudied in NLP. Datasets have been created in English to accelerate
    progress towards measuring and improving figurative language processing in language
    models (LMs). However, the use of figurative language is an expression of our
    cultural and societal experiences, making it difficult for these phrases to be
    universally applicable. In this work, we create a figurative language inference
    dataset, MABL, for seven diverse languages associated with a variety of cultures:
    Hindi, Indonesian, Javanese, Kannada, Sundanese, Swahili and Yoruba. Our dataset
    reveals that each language relies on cultural and regional concepts for figurative
    expressions, with the highest overlap between languages originating from the same
    region. We assess multilingual LMs'' abilities to interpret figurative language
    in zero-shot and few-shot settings. All languages exhibit a significant deficiency
    compared to English, with variations in performance reflecting the availability
    of pre-training and fine-tuning data, emphasizing the need for LMs to be exposed
    to a broader range of linguistic and cultural variation during training.'
  authors:
  - Anubha Kabra
  - Emmy Liu
  - Simran Khanuja
  - Alham Fikri Aji
  - Genta Winata
  - Samuel Cahyawijaya
  - Anuoluwapo Aremu
  - Perez Ogayo
  - Graham Neubig
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - LAW
  forum: ''
  id: LAW_F6
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Multi-lingual and Multi-cultural Figurative Language Understanding
  tldr: 'Figurative language permeates human communication, but at the same time is
    relatively understudied in NLP. Datasets have been created in English to accelerate
    progress towards measuring and improving figurative language processing in language
    models (LMs). However, the use of figurative language is '
  track: The 17th Linguistic Annotation Workshop (LAW-XVII) \\ @ ACL 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Text Style Transfer (TST) evaluation is, in practice, inconsistent. Therefore,
    we conduct a meta-analysis on human and automated TST evaluation and experimentation
    that thoroughly examines existing literature in the field. The meta-analysis reveals
    a substantial standardization gap in human and automated evaluation. In addition,
    we also find a validation gap: only few automated metrics have been validated
    using human experiments. To this end, we thoroughly scrutinize both the standardization
    and validation gap and reveal the resulting pitfalls. This work also paves the
    way to close the standardization and validation gap in TST evaluation by calling
    out requirements to be met by future research.'
  authors:
  - Phil Ostheimer
  - Mayank Kumar Nagda
  - Marius Kloft
  - Sophie Fellenz
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - LAW
  forum: ''
  id: LAW_F7
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: A Call for Standardization and Validation of Text Style Transfer Evaluation
  tldr: Text Style Transfer (TST) evaluation is, in practice, inconsistent. Therefore,
    we conduct a meta-analysis on human and automated TST evaluation and experimentation
    that thoroughly examines existing literature in the field. The meta-analysis reveals
    a substantial standardization gap in human and auto
  track: The 17th Linguistic Annotation Workshop (LAW-XVII) \\ @ ACL 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Automatic summarization with pre-trained language models has led to impressively
    fluent results, but is prone to `hallucinations', low performance on non-news
    genres, and outputs which are not exactly summaries. Targeting ACL 2023's 'Reality
    Check' theme, we present GUMSum, a small but carefully crafted dataset of English
    summaries in 12 written and spoken genres for evaluation of abstractive summarization.
    Summaries are highly constrained, focusing on substitutive potential, factuality,
    and faithfulness. We present guidelines and evaluate human agreement as well as
    subjective judgments on recent system outputs, comparing general-domain untuned
    approaches, a fine-tuned one, and a prompt-based approach, to human performance.
    Results show that while GPT3 achieves impressive scores, it still underperforms
    humans, with varying quality across genres. Human judgments reveal different types
    of errors in supervised, prompted, and human-generated summaries, shedding light
    on the challenges of producing a good summary.
  authors:
  - Yang Janet Liu
  - Amir Zeldes
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - LAW
  forum: ''
  id: LAW_F8
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'GUMSum: Multi-Genre Data and Evaluation for English Abstractive Summarization'
  tldr: Automatic summarization with pre-trained language models has led to impressively
    fluent results, but is prone to `hallucinations', low performance on non-news
    genres, and outputs which are not exactly summaries. Targeting ACL 2023's 'Reality
    Check' theme, we present GUMSum, a small but carefully cra
  track: The 17th Linguistic Annotation Workshop (LAW-XVII) \\ @ ACL 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We demonstrate that coreference resolution in procedural texts is significantly
    improved when performing transformation-based entity linking prior to coreference
    relation identification. When events in the text introduce changes to the state
    of participating entities, it is often impossible to accurately link entities
    in anaphoric and coreference relations without an understanding of the transformations
    those entities undergo. We show how adding event semantics helps to better model
    entity coreference. We argue that all transformation predicates, not just creation
    verbs, introduce a new entity into the discourse, as a kind of generalized Result
    Role, which is typically not textually mentioned. This allows us to model procedural
    texts as process graphs and to compute the coreference type for any two entities
    in the recipe. We present our annotation methodology and the corpus generated
    as well as describe experiments on coreference resolution of entity mentions under
    a process-oriented model of events.
  authors:
  - Kyeongmin Rim
  - Jingxuan Tu
  - Bingyang Ye
  - Marc Verhagen
  - Eben Holderness
  - James Pustejovsky
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - LAW
  forum: ''
  id: LAW_F9
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'The Coreference under Transformation Labeling Dataset: Entity Tracking in
    Procedural Texts Using Event Models'
  tldr: We demonstrate that coreference resolution in procedural texts is significantly
    improved when performing transformation-based entity linking prior to coreference
    relation identification. When events in the text introduce changes to the state
    of participating entities, it is often impossible to accur
  track: The 17th Linguistic Annotation Workshop (LAW-XVII) \\ @ ACL 2023
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Recent work attributes progress in NLP to large language models (LMs)
    with increased model size and large quantities of pretraining data. Despite this,
    current state-of-the-art LMs for Hebrew are both under-parameterized and under-trained
    compared to LMs in other languages. Additionally, previous work on pretrained
    Hebrew LMs focused on encoder-only models. While the encoder-only architecture
    is beneficial for classification tasks, it does not cater well for sub-word prediction
    tasks, such as Named Entity Recognition, when considering the morphologically
    rich nature of Hebrew. In this paper we argue that sequence-to-sequence generative
    architectures are more suitable for large LMs in morphologically rich languages
    (MRLs) such as Hebrew. We demonstrate this by casting tasks in the Hebrew NLP
    pipeline as text-to-text tasks, for which we can leverage powerful multilingual,
    pretrained sequence-to-sequence models as mT5, eliminating the need for a separate,
    specialized, morpheme-based, decoder. Using this approach, our experiments show
    substantial improvements over previously published results on all existing Hebrew
    NLP benchmarks. These results suggest that multilingual sequence-to-sequence models
    present a promising building block for NLP for MRLs.
  authors:
  - Matan Eyal
  - Hila Noga
  - Roee Aharoni
  - Idan Szpektor
  - Reut Tsarfaty
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SIGMORPHON
  forum: ''
  id: ACL_1
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Multilingual Sequence-to-Sequence Models for Hebrew NLP
  tldr: Recent work attributes progress in NLP to large language models (LMs) with
    increased model size and large quantities of pretraining data. Despite this, current
    state-of-the-art LMs for Hebrew are both under-parameterized and under-trained
    compared to LMs in other languages. Additionally, previous wo
  track: The 20th SIGMORPHON workshop on Computational Morphology, Phonology, and
    Phonetics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We investigate how well words in the polysynthetic language Inuktitut
    can be translated by combining dictionary definitions, without use of a neural
    machine translation model trained on parallel text. Such a translation system
    would allow natural language technology to benefit from resources designed for
    community use in a language revitalization or education program, rather than requiring
    a separate parallel corpus. We show that the text-to-text generation capabilities
    of GPT-3 allow it to perform this task with BLEU scores of up to 18.5. We investigate
    prompting GPT-3 to provide multiple translations, which can help slightly, and
    providing it with grammar information, which is mostly ineffective. Finally, we
    test GPT-3's ability to derive morpheme definitions from whole-word translations,
    but find this process is prone to errors including hallucinations.
  authors:
  - Micha Elsner
  - Jordan Needle
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SIGMORPHON
  forum: ''
  id: ACL_6
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Translating a low-resource language using GPT-3 and a human-readable dictionary
  tldr: We investigate how well words in the polysynthetic language Inuktitut can
    be translated by combining dictionary definitions, without use of a neural machine
    translation model trained on parallel text. Such a translation system would allow
    natural language technology to benefit from resources designe
  track: The 20th SIGMORPHON workshop on Computational Morphology, Phonology, and
    Phonetics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Recent advances in pretrained multilingual models such as Multilingual
    T5 (mT5) have facilitated cross-lingual transfer by learning shared representations
    across languages. Leveraging pretrained multilingual models for scaling morphology
    analyzers to low-resource languages is a unique opportunity that has been under-explored
    so far. We investigate this line of research in the context of Indian languages,
    focusing on two important morphological sub-tasks: root word extraction and tagging
    morphosyntactic descriptions (MSD), viz., gender, number, and person (GNP). We
    experiment with six Indian languages from two language families (Dravidian and
    Indo-Aryan) to train a multilingual morphology analyzers for the first time for
    Indian languages. We demonstrate the usability of multilingual models for few-shot
    cross-lingual transfer through an average 7\% increase in GNP tagging in a cross-lingual
    setting as compared to a monolingual setting through controlled experiments. We
    provide an overview of the state of the datasets available related to our tasks
    and point-out a few modeling limitations due to datasets. Lastly, we analyze the
    cross-lingual transfer of morphological tags for verbs and nouns, which provides
    a proxy for the quality of representations of word markings learned by the model.'
  authors:
  - Siddhesh Pawar
  - Pushpak Bhattacharyya
  - Partha Talukdar
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SIGMORPHON
  forum: ''
  id: ACL_7
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Evaluating Cross Lingual Transfer for Morphological Analysis: a Case Study
    of Indian Languages'
  tldr: Recent advances in pretrained multilingual models such as Multilingual T5
    (mT5) have facilitated cross-lingual transfer by learning shared representations
    across languages. Leveraging pretrained multilingual models for scaling morphology
    analyzers to low-resource languages is a unique opportunity th
  track: The 20th SIGMORPHON workshop on Computational Morphology, Phonology, and
    Phonetics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Due to the lack of data resources, rule-based or transfer learning is
    mainly used in the morphological tagging of low-resource languages. However, these
    methods require expert knowledge, ignore contextual features, and have error propagation.
    Therefore, we propose a joint morphological tagger for low-resource agglutinative
    languages to alleviate the above challenges. First, we represent the contextual
    input with multi-dimensional features of agglutinative words. Second, joint training
    reduces the direct impact of part-of-speech errors on morphological features and
    increases the indirect influence between the two types of labels through a fusion
    mechanism. Finally, our model separately predicts part-of-speech and morphological
    features. Part-of-speech tagging is regarded as sequence tagging. When predicting
    morphological features, two-label adjacency graphs are dynamically reconstructed
    by integrating multilingual global features and monolingual local features. Then,
    a graph convolution network is used to learn the higher-order intersection of
    labels. A series of experiments show that the proposed model in this paper is
    superior to other comparative models.
  authors:
  - Gulinigeer Abudouwaili
  - Kahaerjiang Abiderexiti
  - Nian Yi
  - Aishan Wumaier
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SIGMORPHON
  forum: ''
  id: ACL_11
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Joint Learning Model for Low-Resource Agglutinative Language Morphological
    Tagging
  tldr: Due to the lack of data resources, rule-based or transfer learning is mainly
    used in the morphological tagging of low-resource languages. However, these methods
    require expert knowledge, ignore contextual features, and have error propagation.
    Therefore, we propose a joint morphological tagger for lo
  track: The 20th SIGMORPHON workshop on Computational Morphology, Phonology, and
    Phonetics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: UniMorph--the Universal Morphology project is a collaborative initiative
    to create and maintain morphological data and organize numerous related tasks
    for various language processing communities. The morphological data is provided
    by linguists for over 160 languages in the latest version of UniMorph 4.0. This
    paper sheds light on the Central Kurdish data on UniMorph 4.0 by analyzing the
    existing data, its fallacies, and systematic morphological errors. It also presents
    an approach to creating more reliable morphological data by considering various
    specific phenomena in Central Kurdish that have not been addressed previously,
    such as Izafe and several enclitics.
  authors:
  - Sina Ahmadi
  - Aso Mahmudi
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SIGMORPHON
  forum: ''
  id: ACL_12
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Revisiting and Amending Central Kurdish Data on UniMorph 4.0
  tldr: UniMorph--the Universal Morphology project is a collaborative initiative to
    create and maintain morphological data and organize numerous related tasks for
    various language processing communities. The morphological data is provided by
    linguists for over 160 languages in the latest version of UniMorph
  track: The 20th SIGMORPHON workshop on Computational Morphology, Phonology, and
    Phonetics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'While the deep learning revolution has led to significant performance
    improvements in speech recognition, accented speech remains a challenge. Current
    approaches to this challenge typically do not seek to understand and provide explanations
    for the variations of accented speech, whether they stem from native regional
    variation or non-native error patterns. This paper seeks to address non-native
    speaker variations from both a knowledge-based and a data-driven perspective.
    We propose to approximate non-native accented-speech pronunciation patterns by
    the means of two approaches: based on phonetic and phonological knowledge on the
    one hand and inferred from a text-to-speech system on the other. Artificial speech
    is then generated with a range of variants which have been captured in confusion
    matrices representing phoneme similarities. We then show that non-native accent
    confusions actually propagate to the transcription from the ASR, thus suggesting
    that the inference of accent specific phoneme confusions is achievable from artificial
    speech.'
  authors:
  - Margot Masson
  - Julie Carson-berndsen
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SIGMORPHON
  forum: ''
  id: ACL_13
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Investigating Phoneme Similarity with Artificially Accented Speech
  tldr: While the deep learning revolution has led to significant performance improvements
    in speech recognition, accented speech remains a challenge. Current approaches
    to this challenge typically do not seek to understand and provide explanations
    for the variations of accented speech, whether they stem fr
  track: The 20th SIGMORPHON workshop on Computational Morphology, Phonology, and
    Phonetics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: "Interlinear glossing provides a vital type of morphosyntactic annotation,\
    \ both for linguists and language revitalists, and numerous conventions exist\
    \ for representing it formally and computationally. Some of these formats are\
    \ human readable; others are machine readable. Some are easy to edit with general-purpose\
    \ tools. Few represent non-concatentative processes like infixation, reduplication,\
    \ mutation, truncation, and tonal overwriting in a consistent and formally rigorous\
    \ way (on par with affixation). We propose an annotation convention\xE2\u20AC\u201D\
    Generalized Glossing Guidelines (GGG) that combines all of these positive properties\
    \ using an Item-and-Process (IP) framework. We describe the format, demonstrate\
    \ its linguistic adequacy, and compare it with two other interlinear glossed text\
    \ annotation schemes."
  authors:
  - David R. Mortensen
  - Ela Gulsen
  - Taiqi He
  - Nathaniel Robinson
  - Jonathan Amith
  - Lindia Tjuatja
  - Lori Levin
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SIGMORPHON
  forum: ''
  id: ACL_15
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable,
    Item-and-Process Convention for Morphological Annotation'
  tldr: Interlinear glossing provides a vital type of morphosyntactic annotation,
    both for linguists and language revitalists, and numerous conventions exist for
    representing it formally and computationally. Some of these formats are human
    readable; others are machine readable. Some are easy to edit with ge
  track: The 20th SIGMORPHON workshop on Computational Morphology, Phonology, and
    Phonetics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We introduce JAMBU, a cognate database of South Asian languages which
    unifies dozens of previous sources in a structured and accessible format. The
    database includes nearly 287k lemmata from 602 lects, grouped together in 23k
    sets of cognates. We outline the data wrangling necessary to compile the dataset
    and train neural models for reflex prediction on the Indo- Aryan subset of the
    data. We hope that JAMBU is an invaluable resource for all historical linguists
    and Indologists, and look towards further improvement and expansion of the database.
  authors:
  - Aryaman Arora
  - Adam Farris
  - Samopriya Basu
  - Suresh Kolichala
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SIGMORPHON
  forum: ''
  id: ACL_16
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Jambu: A historical linguistic database for South Asian languages'
  tldr: We introduce JAMBU, a cognate database of South Asian languages which unifies
    dozens of previous sources in a structured and accessible format. The database
    includes nearly 287k lemmata from 602 lects, grouped together in 23k sets of cognates.
    We outline the data wrangling necessary to compile the d
  track: The 20th SIGMORPHON workshop on Computational Morphology, Phonology, and
    Phonetics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Linguistic analysis is a core task in the process of documenting, analyzing,
    and describing endangered and less-studied languages. In addition to providing
    insight into the properties of the language being studied, having tools to automatically
    label words in a language for grammatical category and morphological features
    can support a range of applications useful for language pedagogy and revitalization.
    At the same time, most modern NLP methods for these tasks require both large amounts
    of data in the language and compute costs well beyond the capacity of most research
    groups and language communities.  In this paper, we present a gloss-to-gloss (g2g)
    model for linguistic analysis (specifically, morphological analysis and part-of-speech
    tagging) that is lightweight in terms of both data requirements and computational
    expense. The model is designed for the interlinear glossed text (IGT) format,
    in which we expect the source text of a sentence in a low-resource language, a
    translation of that sentence into a language of wider communication, and a detailed
    glossing of the morphological properties of each word in the sentence. We first
    produce silver standard parallel glossed data by automatically labeling the high-resource
    translation. The model then learns to transform source language morphological
    labels into output labels for the target language, mediated by a structured linguistic
    representation layer. We test the model on both low-resource and high-resource
    languages, and find that our simple CNN-based model achieves comparable performance
    to a state-of-the-art transformer-based model, at a fraction of the computational
    cost.
  authors:
  - Bhargav Shandilya
  - Alexis Palmer
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SIGMORPHON
  forum: ''
  id: ACL_17
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Lightweight morpheme labeling in context: Using structured linguistic representations
    to support linguistic analysis for the language documentation context'
  tldr: 'Linguistic analysis is a core task in the process of documenting, analyzing,
    and describing endangered and less-studied languages. In addition to providing
    insight into the properties of the language being studied, having tools to automatically
    label words in a language for grammatical category and '
  track: The 20th SIGMORPHON workshop on Computational Morphology, Phonology, and
    Phonetics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The process of lexical blending is difficult to reliably predict. This
    difficulty has been shown by machine learning approaches in blend modeling, including
    attempts using then state-of-the-art LSTM deep neural networks trained on character
    embeddings, which were able to predict lexical blends given the ordered constituent
    words in less than half of cases, at maximum. This project introduces a novel
    model architecture which dramatically increases the correct prediction rates for
    lexical blends, using only Polynomial regression and Random Forest models. This
    is achieved by generating multiple possible blend candidates for each input word
    pairing and evaluating them based on observable linguistic features. The success
    of this model architecture illustrates the potential usefulness of observable
    linguistic features for problems that elude more advanced models which utilize
    only features discovered in the latent space.
  authors:
  - Jarem Saunders
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SIGMORPHON
  forum: ''
  id: ACL_18
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Improving Automated Prediction of English Lexical Blends Through the Use
    of Observable Linguistic Features
  tldr: The process of lexical blending is difficult to reliably predict. This difficulty
    has been shown by machine learning approaches in blend modeling, including attempts
    using then state-of-the-art LSTM deep neural networks trained on character embeddings,
    which were able to predict lexical blends given
  track: The 20th SIGMORPHON workshop on Computational Morphology, Phonology, and
    Phonetics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: "Colexification refers to the linguistic phenomenon where a single lexical\
    \ form is used to convey multiple meanings. By studying cross-lingual colexifications,\
    \ researchers have gained valuable insights into fields such as psycholinguistics\
    \ and cognitive sciences (Jack- son et al., 2019; Xu et al., 2020; Karjus et al.,\
    \ 2021; Schapper and Koptjevskaja-Tamm, 2022; Fran\xC3\xA7ois, 2022). While several\
    \ multilingual colexification datasets exist, there is untapped potential in using\
    \ this information to bootstrap datasets across such semantic features. In this\
    \ paper, we aim to demonstrate how colexifications can be leveraged to create\
    \ such cross-lingual datasets. We showcase curation procedures which result in\
    \ a dataset covering 142 languages across 21 language families across the world.\
    \ The dataset includes ratings of concreteness and affectiveness, mapped with\
    \ phonemes and phonological features. We further analyze the dataset along different\
    \ dimensions to demonstrate potential of the proposed procedures in facilitating\
    \ further interdisciplinary research in psychology, cognitive science, and multilingual\
    \ natural language processing (NLP). Based on initial investigations, we observe\
    \ that i) colexifications that are closer in concreteness/affectiveness are more\
    \ likely to colexify ; ii) certain initial/last phonemes are significantly correlated\
    \ with concreteness/affectiveness intra language families, such as /k/ as the\
    \ initial phoneme in both Turkic and Tai-Kadai correlated with concreteness, and\
    \ /p/ in Dravidian and Sino-Tibetan correlated with Valence; iii) the type-to-token\
    \ ratio (TTR) of phonemes are positively correlated with concreteness across several\
    \ language families, while the length of phoneme segments are negatively correlated\
    \ with concreteness; iv) certain phonological features are negatively correlated\
    \ with concreteness across languages. The dataset is made public online for further\
    \ research."
  authors:
  - Yiyi Chen
  - Johannes Bjerva
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SIGMORPHON
  forum: ''
  id: ACL_19
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Colexifications for Bootstrapping Cross-lingual Datasets: The Case of Phonology,
    Concreteness, and Affectiveness'
  tldr: Colexification refers to the linguistic phenomenon where a single lexical
    form is used to convey multiple meanings. By studying cross-lingual colexifications,
    researchers have gained valuable insights into fields such as psycholinguistics
    and cognitive sciences (Jack- son et al., 2019; Xu et al., 20
  track: The 20th SIGMORPHON workshop on Computational Morphology, Phonology, and
    Phonetics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper evaluates various character alignment methods on the task of
    sentence-level standardization of dialect transcriptions. We compare alignment
    methods from different scientific traditions (dialectometry, speech processing,
    machine translation) and apply them to Finnish, Norwegian and Swiss German dialect
    datasets. In the absence of gold alignments, we evaluate the methods on a set
    of characteristics that are deemed undesirable for the task. We find that trained
    alignment methods only show marginal benefits to simple Levenshtein distance.
    On this particular task, eflomal outperforms related methods such as GIZA++ or
    fast_align by a large margin.
  authors:
  - Yves Scherrer
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SIGMORPHON
  forum: ''
  id: ACL_21
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Character alignment methods for dialect-to-standard normalization
  tldr: This paper evaluates various character alignment methods on the task of sentence-level
    standardization of dialect transcriptions. We compare alignment methods from different
    scientific traditions (dialectometry, speech processing, machine translation)
    and apply them to Finnish, Norwegian and Swiss G
  track: The 20th SIGMORPHON workshop on Computational Morphology, Phonology, and
    Phonetics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: "The 2023 SIGMORPHON\u2013UniMorph shared task on typologically diverse\
    \ morphological inflection included a wide range of languages: 26 languages from\
    \ 9 primary language families. The data this year was all lemma-split, to allow\
    \ testing models' generalization ability, and structured along the new hierarchical\
    \ schema presented in (Batsuren et al., 2022). The systems submitted this year,\
    \ 9 in number, showed ingenuity and innovativeness, including hard attention for\
    \ explainability and bidirectional decoding. Special treatment was also given\
    \ by many participants to the newly-introduced data in Japanese, due to the high\
    \ abundance of unseen Kanji characters in its test set."
  authors:
  - Omer Goldman
  - Khuyagbaatar Batsuren
  - Salam Khalifa
  - Aryaman Arora
  - Garrett Nicolai
  - Reut Tsarfaty
  - Ekaterina Vylomova
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SIGMORPHON
  forum: ''
  id: ACL_Inf1
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: "SIGMORPHON\u2013UniMorph 2023 Shared Task 0: Typologically Diverse Morphological\
    \ Inflection"
  tldr: "The 2023 SIGMORPHON\u2013UniMorph shared task on typologically diverse morphological\
    \ inflection included a wide range of languages: 26 languages from 9 primary language\
    \ families. The data this year was all lemma-split, to allow testing models' generalization\
    \ ability, and structured along the new hierarch"
  track: The 20th SIGMORPHON workshop on Computational Morphology, Phonology, and
    Phonetics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper summarises data collection and curation for Part 2 of the 2023
    SIGMORPHON-UniMorph Shared Task 0, which focused on modeling speaker knowledge
    and generalization of a pair of interacting phonological processes in Korean.
    We briefly describe how modeling the generalization task could be of interest
    to researchers in both Natural Language Processing and linguistics, and then summarise
    the traditional description of the phonological processes that are at the center
    of the modeling challenge. We then describe the criteria we used to select and
    code cases of process application in two Korean speech corpora, which served as
    the primary learning data. We also report the technical details of the experiment
    we carried out that served as the primary test data.
  authors:
  - Canaan Breiss
  - Jinyoung Jo
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SIGMORPHON
  forum: ''
  id: ACL_Inf2
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: "SIGMORPHON\u2013UniMorph 2023 Shared Task 0, Part 2: Cognitively Plausible\
    \ Morphophonological Generalization in Korean"
  tldr: 'This paper summarises data collection and curation for Part 2 of the 2023
    SIGMORPHON-UniMorph Shared Task 0, which focused on modeling speaker knowledge
    and generalization of a pair of interacting phonological processes in Korean.
    We briefly describe how modeling the generalization task could be of '
  track: The 20th SIGMORPHON workshop on Computational Morphology, Phonology, and
    Phonetics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes the submission by the University of Arizona to the
    SIGMORPHON 2023 Shared Task on typologically diverse morphological (re-)infection.
    In our submission, we investigate the role of frequency, length, and weighted
    transducers in addressing the challenge of morphological reinflection. We start
    with the non-neural baseline provided for the task and show how some improvement
    can be gained by integrating length and frequency in prefix selection. We also
    investigate using weighted finite-state transducers, jump-started from edit distance
    and directly augmented with frequency. Our specific technique is promising and
    quite simple, but we see only modest improvements for some languages here.
  authors:
  - Alice Kwak
  - Michael Hammond
  - Cheyenne Wing
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SIGMORPHON
  forum: ''
  id: ACL_30
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Morphological reinflection with weighted finite-state transducers
  tldr: This paper describes the submission by the University of Arizona to the SIGMORPHON
    2023 Shared Task on typologically diverse morphological (re-)infection. In our
    submission, we investigate the role of frequency, length, and weighted transducers
    in addressing the challenge of morphological reinflecti
  track: The 20th SIGMORPHON workshop on Computational Morphology, Phonology, and
    Phonetics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper presents our submission to the SIGMORPHON 2023 task 2 of Cognitively
    Plausible Morphophonological Generalization in Korean. We implemented both Linear
    Discriminative Learning and Transformer models and found that the Linear Discriminative
    Learning model trained on a combination of corpus and experimental data showed
    the best performance with the overall accuracy of around 83%. We found that the
    best model must be trained on both corpus data and the experimental data of one
    particular participant. Our examination of speaker-variability and speaker-specific
    information did not explain why a particular participant combined well with the
    corpus data. We recommend Linear Discriminative Learning models as a future non-neural
    baseline system, owning to its training speed, accuracy, model interpretability
    and cognitive plausibility. In order to improve the model performance, we suggest
    using bigger data and/or performing data augmentation and incorporating speaker-
    and item-specifics considerably.
  authors:
  - Cheonkam Jeong
  - Dominic Schmitz
  - Akhilesh Kakolu Ramarao
  - Anna Stein
  - Kevin Tang
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SIGMORPHON
  forum: ''
  id: ACL_24
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Linear Discriminative Learning: a competitive non-neural baseline for morphological
    inflection'
  tldr: 'This paper presents our submission to the SIGMORPHON 2023 task 2 of Cognitively
    Plausible Morphophonological Generalization in Korean. We implemented both Linear
    Discriminative Learning and Transformer models and found that the Linear Discriminative
    Learning model trained on a combination of corpus '
  track: The 20th SIGMORPHON workshop on Computational Morphology, Phonology, and
    Phonetics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes our systems participating in the 2023 SIGMORPHON
    Shared Task on Morphological Inflection and in the 2023 SIGMORPHON Shared Task
    on Interlinear Glossing. We propose methods to enrich predictions from neural
    models with discrete, i.e. interpretable, information. For morphological inflection,
    our models learn deterministic mappings from subsets of source lemma characters
    and morphological tags to individual target characters, which introduces interpretability.
    For interlinear glossing, our models learn a shallow morpheme segmentation in
    an unsupervised way jointly with predicting glossing lines. Estimated segmentation
    may be useful when no ground-truth segmentation is available. As both methods
    introduce discreteness into neural models, our technical contribution is to show
    that straight-through gradient estimators are effective to train hard attention
    models.
  authors:
  - Leander Girrbach
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SIGMORPHON
  forum: ''
  id: ACL_25
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: "T\xFC-CL at SIGMORPHON 2023: Straight-Through Gradient Estimation for Hard\
    \ Attention"
  tldr: 'This paper describes our systems participating in the 2023 SIGMORPHON Shared
    Task on Morphological Inflection and in the 2023 SIGMORPHON Shared Task on Interlinear
    Glossing. We propose methods to enrich predictions from neural models with discrete,
    i.e. interpretable, information. For morphological '
  track: The 20th SIGMORPHON workshop on Computational Morphology, Phonology, and
    Phonetics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: "This paper presents the submission by the MeLeL team to the SIGMORPHON\u2013\
    UniMorph Shared Task on Typologically Diverse and Acquisition-Inspired Morphological\
    \ Inflection Generation Part 3: Models of Acquisition of Inflectional Noun Morphology\
    \ in Polish, Estonian, and Finnish. This task requires us to produce the word\
    \ form given a lemma and a grammatical case, while trying to produce the same\
    \ error-rate as in children. We approach this task with a reduced-size character-based\
    \ transformer model, multilingual training and an upsampling method to introduce\
    \ bias."
  authors:
  - Gal Astrach
  - Yuval Pinter
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SIGMORPHON
  forum: ''
  id: ACL_32
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: The BGU-MeLeL System for the SIGMORPHON 2023 Shared Task on Morphological
    Inflection
  tldr: "This paper presents the submission by the MeLeL team to the SIGMORPHON\u2013\
    UniMorph Shared Task on Typologically Diverse and Acquisition-Inspired Morphological\
    \ Inflection Generation Part 3: Models of Acquisition of Inflectional Noun Morphology\
    \ in Polish, Estonian, and Finnish. This task requires us to p"
  track: The 20th SIGMORPHON workshop on Computational Morphology, Phonology, and
    Phonetics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes our systems participating in the 2023 SIGMORPHON
    Shared Task on Morphological Inflection and in the 2023 SIGMORPHON Shared Task
    on Interlinear Glossing. We propose methods to enrich predictions from neural
    models with discrete, i.e. interpretable, information. For morphological inflection,
    our models learn deterministic mappings from subsets of source lemma characters
    and morphological tags to individual target characters, which introduces interpretability.
    For interlinear glossing, our models learn a shallow morpheme segmentation in
    an unsupervised way jointly with predicting glossing lines. Estimated segmentation
    may be useful when no ground-truth segmentation is available. As both methods
    introduce discreteness into neural models, our technical contribution is to show
    that straight-through gradient estimators are effective to train hard attention
    models.
  authors:
  - Leander Girrbach
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SIGMORPHON
  forum: ''
  id: ACL_25
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: "T\xFC-CL at SIGMORPHON 2023: Straight-Through Gradient Estimation for Hard\
    \ Attention"
  tldr: 'This paper describes our systems participating in the 2023 SIGMORPHON Shared
    Task on Morphological Inflection and in the 2023 SIGMORPHON Shared Task on Interlinear
    Glossing. We propose methods to enrich predictions from neural models with discrete,
    i.e. interpretable, information. For morphological '
  track: The 20th SIGMORPHON workshop on Computational Morphology, Phonology, and
    Phonetics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: "This paper presents the findings of the SIGMORPHON 2023 Shared Task on\
    \ Interlinear Glossing. This first iteration of the shared task explores glossing\
    \ of a set of six typologically diverse languages: Arapaho, Gitksan, Lezgi, Nat\xFC\
    gu, Tsez and Uspanteko. The shared task encompasses two tracks: a resource-scarce\
    \ closed track and an open track, where participants are allowed to utilize external\
    \ data resources.  Five teams participated in the shared task. The winning team\
    \ T\xFC-CL achieved a 23.99%-point improvement over a baseline RoBERTa system\
    \ in the closed track and a 17.42%-point improvement in the open track."
  authors:
  - Michael Ginn
  - Sarah Moeller
  - Alexis Palmer
  - Anna Stacey
  - Garrett Nicolai
  - Mans Hulden
  - Miikka Silfverberg
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SIGMORPHON
  forum: ''
  id: ACL_Gloss
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Findings of the SIGMORPHON 2023 Shared Task on Interlinear Glossing
  tldr: "This paper presents the findings of the SIGMORPHON 2023 Shared Task on Interlinear\
    \ Glossing. This first iteration of the shared task explores glossing of a set\
    \ of six typologically diverse languages: Arapaho, Gitksan, Lezgi, Nat\xFCgu,\
    \ Tsez and Uspanteko. The shared task encompasses two tracks: a resou"
  track: The 20th SIGMORPHON workshop on Computational Morphology, Phonology, and
    Phonetics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes LISN"'"s submission to the second track (open track)
    of the shared task on Interlinear Glossing for SIGMORPHON 2023. Our systems are
    based on Lost, a variation of linear Conditional Random Fields initially developed
    as a probabilistic translation model and then adapted to the glossing task. This
    model allows us to handle one of the main challenges posed by glossing, i.e. the
    fact that the list of potential labels for lexical morphemes is not fixed in advance
    and needs to be extended dynamically when labelling units are not seen in training.
    In such situations, we show how to make use of candidate lexical glosses found
    in the translation and discuss how such extension affects the training and inference
    procedures. The resulting automatic glossing systems prove to yield very competitive
    results, especially in low-resource settings.
  authors:
  - Shu Okabe
  - "Fran\xE7ois Yvon"
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SIGMORPHON
  forum: ''
  id: ACL_26
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: LISN @ SIGMORPHON 2023 Shared Task on Interlinear Glossing
  tldr: This paper describes LISN"'"s submission to the second track (open track)
    of the shared task on Interlinear Glossing for SIGMORPHON 2023. Our systems are
    based on Lost, a variation of linear Conditional Random Fields initially developed
    as a probabilistic translation model and then adapted to the gl
  track: The 20th SIGMORPHON workshop on Computational Morphology, Phonology, and
    Phonetics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'In our submission to the SIGMORPHON 2023 Shared Task on interlinear glossing
    (IGT), we explore approaches to data augmentation and modeling across seven low-resource
    languages. For data augmentation, we explore two approaches: creating artificial
    data from the provided training data and utilizing existing IGT resources in other
    languages. On the modeling side, we test an enhanced version of the provided token
    classification baseline as well as a pretrained multilingual seq2seq model. Additionally,
    we apply post-correction using a dictionary for Gitksan, the language with the
    smallest amount of data. We find that our token classification models are the
    best performing, with the highest word-level accuracy for Arapaho and highest
    morpheme-level accuracy for Gitksan out of all submissions. We also show that
    data augmentation is an effective strategy, though applying artificial data pretraining
    has very different effects across both models tested.'
  authors:
  - Taiqi He
  - Lindia Tjuatja
  - Nathaniel Robinson
  - Shinji Watanabe
  - David R. Mortensen
  - Graham Neubig
  - Lori Levin
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SIGMORPHON
  forum: ''
  id: ACL_27
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing
  tldr: 'In our submission to the SIGMORPHON 2023 Shared Task on interlinear glossing
    (IGT), we explore approaches to data augmentation and modeling across seven low-resource
    languages. For data augmentation, we explore two approaches: creating artificial
    data from the provided training data and utilizing ex'
  track: The 20th SIGMORPHON workshop on Computational Morphology, Phonology, and
    Phonetics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper presents my submission to Track 1 of the 2023 SIGMORPHON shared
    task on interlinear glossed text (IGT). There are a wide amount of techniques
    for building and training IGT models (see Moeller and Hulden, 2018; McMillan-Major,
    2020; Zhao et al., 2020). I describe my ensembled sequence-to-sequence approach,
    perform experiments, and share my submission's test-set accuracy. I also discuss
    future areas of research in low-resource token classification methods for IGT.
  authors:
  - Edith Coates
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SIGMORPHON
  forum: ''
  id: ACL_28
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: An Ensembled Encoder-Decoder System for Interlinear Glossed Text
  tldr: This paper presents my submission to Track 1 of the 2023 SIGMORPHON shared
    task on interlinear glossed text (IGT). There are a wide amount of techniques
    for building and training IGT models (see Moeller and Hulden, 2018; McMillan-Major,
    2020; Zhao et al., 2020). I describe my ensembled sequence-to-s
  track: The 20th SIGMORPHON workshop on Computational Morphology, Phonology, and
    Phonetics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper presents several different neural subword modelling based approaches
    to interlinear glossing for seven under-resourced languages as a part of the 2023
    SIGMORPHON shared task on interlinear glossing. We experiment with various augmentation
    and tokenization strategies for both the open and closed tracks of data. We found
    that while byte-level models may perform well for greater amounts of data, character
    based approaches remain competitive in their performance in lower resource settings.
  authors:
  - Ziggy Cross
  - Michelle Yun
  - Ananya Apparaju
  - Jata MacCabe
  - Garrett Nicolai
  - Miikka Silfverberg
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SIGMORPHON
  forum: ''
  id: ACL_29
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Glossy Bytes: Neural Glossing using Subword Encoding'
  tldr: 'This paper presents several different neural subword modelling based approaches
    to interlinear glossing for seven under-resourced languages as a part of the 2023
    SIGMORPHON shared task on interlinear glossing. We experiment with various augmentation
    and tokenization strategies for both the open and '
  track: The 20th SIGMORPHON workshop on Computational Morphology, Phonology, and
    Phonetics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Despite their successes in NLP, Transformer-based language models still
    require extensive computing resources and suffer in low-resource or low-compute
    settings. In this paper, we present AxomiyaBERTa, a novel BERT model for Assamese,
    a morphologically-rich lowresource language (LRL) of Eastern India. AxomiyaBERTa
    is trained only on the masked language modeling (MLM) task, without the typical
    additional next sentence prediction (NSP) objective, and our results show that
    in resourcescarce settings for very low-resource languages like Assamese, MLM
    alone can be successfully leveraged for a range of tasks. AxomiyaBERTa achieves
    SOTA on token-level tasks like Named Entity Recognition and also performs well
    on "longer-context" tasks like Cloze-style QA and Wiki Title Prediction, with
    the assistance of a novel embedding disperser and phonological signals respectively.
    Moreover, we show that AxomiyaBERTa can leverage phonological signals for even
    more challenging tasks, such as a novel cross-document coreference task on a translated
    version of the ECB+ corpus, where we present a new SOTA result for an LRL.
  authors:
  - Abhijnan Nath
  - Sheikh Mannan
  - Nikhil Krishnaswamy
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SIGMORPHON
  forum: ''
  id: ACL_F1
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: AxomiyaBERTa:A Phonologically-aware Transformer Model for Assamese
  tldr: Despite their successes in NLP, Transformer-based language models still require
    extensive computing resources and suffer in low-resource or low-compute settings.
    In this paper, we present AxomiyaBERTa, a novel BERT model for Assamese, a morphologically-rich
    lowresource language (LRL) of Eastern Indi
  track: The 20th SIGMORPHON workshop on Computational Morphology, Phonology, and
    Phonetics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: "Neural sequence-to-sequence models have been very successful at tasks\
    \ in phonology and morphology that seemingly require a capacity for intricate\
    \ linguistic generalisations. In this paper, we perform a detailed breakdown of\
    \ the power of such models to capture various phonological generalisations and\
    \ to benefit from exposure to one phonological rule to infer the behaviour of\
    \ another similar rule. We present two types of experiments, one of which establishes\
    \ the efficacy of the transformer model on 29 different processes. The second\
    \ experiment type follows a priming and held-out case split where our model is\
    \ exposed to two (or more) phenomena; one which is used as a primer to make the\
    \ model aware of a linguistic category (e.g. voiceless stops) and a second one\
    \ which contains a rule with a withheld case that the model is expected to infer\
    \ (e.g. word-final devoicing with a missing training example such as b\u2192p).\
    \ Our results show that the transformer model can successfully model all 29 phonological\
    \ phenomena considered, regardless of perceived pro-cess difficulty. We also show\
    \ that the model can generalise linguistic categories and structures, such as\
    \ vowels and syllables, through priming processes."
  authors:
  - "Saliha Murado\u01E7lu"
  - Mans Hulden
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SIGMORPHON
  forum: ''
  id: ACL_F2
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Do Transformer Models do Phonology like a Linguist?
  tldr: Neural sequence-to-sequence models have been very successful at tasks in phonology
    and morphology that seemingly require a capacity for intricate linguistic generalisations.
    In this paper, we perform a detailed breakdown of the power of such models to
    capture various phonological generalisations and
  track: The 20th SIGMORPHON workshop on Computational Morphology, Phonology, and
    Phonetics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: "Grapheme-to-phoneme conversion is an important component in many speech\
    \ technologies, but until recently there were no multilingual benchmarks for this\
    \ task. The third iteration of the SIGMORPHON shared task on multilingual grapheme-to-phoneme\
    \ conversion features many improvements from the previous year\u2019s task (Ashby\
    \ et al., 2021), including additional languages, three subtasks varying the amount\
    \ of available resources, extensive quality assurance procedures, and automated\
    \ error analyses. Three teams submitted a total of fifteen systems, at best achieving\
    \ relative reductions of word error rate of 14% in the crosslingual subtask and\
    \ 14% in the very-low resource subtask. The generally consistent result is that\
    \ cross-lingual transfer substantially helps grapheme-to-phoneme modeling, but\
    \ not to the same degree as in-language examples."
  authors:
  - Arya D. McCarthy
  - Jackson L. Lee
  - Alexandra DeLucia
  - Travis Bartley
  - Milind Agarwal
  - Lucas F.E. Ashby
  - Luca Del Signore
  - Cameron Gibson
  - Reuben Raff
  - Winston Wu
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SIGMORPHON
  forum: ''
  id: ACL_G2P
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: The SIGMORPHON 2022 Shared Task on Cross-lingual and Low-Resource Grapheme-to-Phoneme
    Conversion
  tldr: 'Grapheme-to-phoneme conversion is an important component in many speech technologies,
    but until recently there were no multilingual benchmarks for this task. The third
    iteration of the SIGMORPHON shared task on multilingual grapheme-to-phoneme conversion
    features many improvements from the previous '
  track: The 20th SIGMORPHON workshop on Computational Morphology, Phonology, and
    Phonetics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes our participation in the Third SIGMORPHON Shared
    Task on Grapheme-to-Phoneme Conversion (Low-Resource and Cross-Lingual) (McCarthy
    et al.,2022). Our models rely on different sequence labelling methods. The main
    model predicts multiple phonemes from each grapheme and is trained using CTC loss
    (Graves et al., 2006). We find that sequence labelling methods yield worse performance
    than the baseline when enough data is available, but can still be used when very
    little data is available. Furthermore, we demonstrate that alignments learned
    by the sequence labelling models can be easily inspected.
  authors:
  - Leander Girrbach
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SIGMORPHON
  forum: ''
  id: ACL_G2P1
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'SIGMORPHON 2022 Shared Task on Grapheme-to-Phoneme Conversion Submission
    Description: Sequence Labelling for G2P'
  tldr: This paper describes our participation in the Third SIGMORPHON Shared Task
    on Grapheme-to-Phoneme Conversion (Low-Resource and Cross-Lingual) (McCarthy et
    al.,2022). Our models rely on different sequence labelling methods. The main model
    predicts multiple phonemes from each grapheme and is trained u
  track: The 20th SIGMORPHON workshop on Computational Morphology, Phonology, and
    Phonetics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In this paper we explore a very simple nonneural approach to mapping orthography
    to phonetic transcription in a low-resource context with transfer data from a
    related language. We start from a baseline system and focus our efforts on data
    augmentation. We make three principal moves. First, we start with an HMMbased
    system (Novak et al., 2012). Second, we augment our basic system by recombining
    legal substrings in restricted fashion (Ryan and Hulden, 2020). Finally, we limit
    our transfer data by only using training pairs where the phonetic form shares
    all bigrams with the target language.
  authors:
  - Michael Hammond
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SIGMORPHON
  forum: ''
  id: ACL_G2P2
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Low-resource grapheme-to-phoneme mapping with phonetically-conditioned transfer
  tldr: 'In this paper we explore a very simple nonneural approach to mapping orthography
    to phonetic transcription in a low-resource context with transfer data from a
    related language. We start from a baseline system and focus our efforts on data
    augmentation. We make three principal moves. First, we start '
  track: The 20th SIGMORPHON workshop on Computational Morphology, Phonology, and
    Phonetics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: "We propose a universal grapheme-phoneme transduction model using neuralized\
    \ finite-state transducers.  Many computational models of grapheme-phoneme transduction\
    \ nowadays are based on the (autoregressive) sequence-to-sequence string transduction\
    \ paradigm. While such models have achieved state-of-the-art performance, they\
    \ suffer from theoretical limitations of autoregressive models. On the other hand,\
    \ neuralized finite-state transducers (NFSTs) have shown promising results on\
    \ various string transduction tasks. NFSTs can be seen as a generalization of\
    \ weighted finite-state transducers (WFSTs), and can be seen as pairs of a featurized\
    \ finite-state machine (\u2018marked finite-state transducer\u2019 or MFST in\
    \ NFST terminology), and a string scoring function. Instead of taking a product\
    \ of local contextual feature weights on FST arcs, NFSTs can employ arbitrary\
    \ scoring functions to weight global contextual features of a string transduction,\
    \ and therefore break the Markov property. Furthermore, NFSTs can be formally\
    \ shown to be more expressive than (autoregressive) seq2seq models. Empirically,\
    \ joint grapheme-phoneme transduction NFSTs have consistently outperformed vanilla\
    \ seq2seq models on grapheme-tophoneme and phoneme-to-grapheme transduction tasks\
    \ for English. Furthermore, they provide interpretable aligned string transductions,\
    \ thanks to their finite-state machine component. In this talk, we propose a multilingual\
    \ extension of the joint grapheme-phoneme NFST. We achieve this goal by modeling\
    \ typological and phylogenetic features of languages and scripts as optional latent\
    \ variables using a finite-state machine. The result is a versatile graphemephoneme\
    \ transduction model: in addition to standard monolingual and multilingual transduction,\
    \ the proposed multilingual NFST can also be used in various controlled generation\
    \ scenarios, such as phoneme-to-grapheme transduction of an unseen language-script\
    \ pair. We also plan to release an NFST software package."
  authors:
  - Chu-Cheng Lin Lin
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SIGMORPHON
  forum: ''
  id: ACL_G2P3
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: A future for universal grapheme-phoneme transduction modeling with neuralized
    finite-state transducers
  tldr: We propose a universal grapheme-phoneme transduction model using neuralized
    finite-state transducers.  Many computational models of grapheme-phoneme transduction
    nowadays are based on the (autoregressive) sequence-to-sequence string transduction
    paradigm. While such models have achieved state-of-the
  track: The 20th SIGMORPHON workshop on Computational Morphology, Phonology, and
    Phonetics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: "Grapheme-to-phoneme (G2P) conversion is a task that is inherently related\
    \ to both written and spoken language. Therefore, our submission to the G2P shared\
    \ task builds off of mSLAM (Bapna et al., 2022), a 600M parameter encoder model\
    \ pretrained simultaneously on text from 101 languages and speech from 51 languages.\
    \ For fine-tuning a G2P model, we combined mSLAM\u2019s text encoder, which uses\
    \ characters as its input tokens, with an uninitialized single-layer RNN-T decoder\
    \ (Graves, 2012) whose vocabulary is the set of all 381 phonemes appearing in\
    \ the shared task data. We took an explicitly multilingual approach to modeling\
    \ the G2P tasks, fine-tuning and evaluating a single model that covered all the\
    \ languages in each task, and adding language codes as prefixes to the input strings\
    \ as a means of specifying the language of each example. Our models perform well\
    \ in the shared task\u2019s \u201Chigh\u201D setting (in which they were trained\
    \ on 1,000 words from each language), though they do poorly in the \u201Clow\u201D\
    \ task setting (training on only 100 words from each language). Our models also\
    \ perform reasonably in the \u201Cmixed\u201D setting (training on 100 words in\
    \ the target language and 1000 words in a related language), hinting that mSLAM\u2019\
    s multilingual pretraining may be enabling useful cross-lingual sharing."
  authors:
  - Dan Garrette
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SIGMORPHON
  forum: ''
  id: ACL_G2P4
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Fine-tuning mSLAM for the SIGMORPHON 2022 Shared Task on Grapheme-to-Phoneme
    Conversion
  tldr: Grapheme-to-phoneme (G2P) conversion is a task that is inherently related
    to both written and spoken language. Therefore, our submission to the G2P shared
    task builds off of mSLAM (Bapna et al., 2022), a 600M parameter encoder model
    pretrained simultaneously on text from 101 languages and speech fro
  track: The 20th SIGMORPHON workshop on Computational Morphology, Phonology, and
    Phonetics
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Positive emotion elicitation aims at evoking positive emotion states
    in human users in open-domain dialogue generation. However, most work focuses
    on inducing a single-dimension of positive sentiment using human annotated datasets,
    which limits the scale of the training dataset. In this paper, we propose to model
    various emotions in large unannotated conversations, such as joy, trust and anticipation,
    by leveraging a latent variable to control the emotional intention of the response.
    Our proposed emotion-eliciting-Conditional-Variational-AutoEncoder (EE-CVAE) model
    generates more diverse and emotionally-intelligent responses compared to single-dimension
    baseline models in human evaluation. '
  authors:
  - Ziwei Gong
  - Qingkai Min
  - Yue Zhang
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SICon
  forum: ''
  id: SICon_4
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short (novel;
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Eliciting Rich Positive Emotions in Dialogue Generation
  tldr: Positive emotion elicitation aims at evoking positive emotion states in human
    users in open-domain dialogue generation. However, most work focuses on inducing
    a single-dimension of positive sentiment using human annotated datasets, which
    limits the scale of the training dataset. In this paper, we pr
  track: The 1st Workshop on Social Influence in Conversations (SICon)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The expression of opinions, stances, and moral foundations on social media
    often coincide with toxic, divisive, or inflammatory language that can make constructive
    discourse across communities difficult. Natural language generation methods could
    provide a means to reframe or reword such expressions in a way that fosters more
    civil discourse, yet current Large Language Model (LLM) methods tend towards language
    that is too generic or formal to seem authentic for social media discussions.
    We present preliminary work on training LLMs to maintain authenticity while presenting
    a community's ideas and values in a constructive, non-toxic manner.
  authors:
  - Ritwik Bose
  - Ian Perera
  - Bonnie Dorr
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SICon
  forum: ''
  id: SICon_13
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: short (novel;
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Detoxifying Online Discourse: A Guided Response Generation Approach for
    Reducing Toxicity in User-Generated Text'
  tldr: 'The expression of opinions, stances, and moral foundations on social media
    often coincide with toxic, divisive, or inflammatory language that can make constructive
    discourse across communities difficult. Natural language generation methods could
    provide a means to reframe or reword such expressions '
  track: The 1st Workshop on Social Influence in Conversations (SICon)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: "Two studies tested the hypothesis that a Large Language Model (LLM) can\
    \ be used to model psychological change following exposure to influential input.\
    \ The first study tested a generic mode of influence - the Illusory Truth Effect\
    \ (ITE) - where earlier exposure to a statement boosts a later truthfulness test\
    \ rating. Analysis of newly collected data from human and LLM-simulated subjects\
    \ (1000 of each) showed the same pattern of effects in both populations; although\
    \ with greater per statement variability for the LLM. The second study concerns\
    \ a specific mode of influence \u2013 populist framing of news to increase its\
    \ persuasion and political mobilization. Newly collected data from simulated subjects\
    \ was compared to previously published data from a 15 country experiment on 7286\
    \ human participants. Several effects from the human study were replicated by\
    \ the simulated study, including ones that surprised the authors of the human\
    \ study by contradicting their theoretical expectations; but some significant\
    \ relationships found in human data were not present in the LLM data. Together\
    \ the two studies support the view that LLMs have potential to act as models of\
    \ the effect of influence."
  authors:
  - Lewis Griffin
  - Bennett Kleinberg
  - Maximilian Mozes
  - Kimberly Mai
  - Maria Do Mar Vau
  - Matthew Caldwell
  - Augustine Mavor-Parker
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SICon
  forum: ''
  id: SICon_5
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long (novel;
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Large Language Models respond to Influence like Humans
  tldr: Two studies tested the hypothesis that a Large Language Model (LLM) can be
    used to model psychological change following exposure to influential input. The
    first study tested a generic mode of influence - the Illusory Truth Effect (ITE)
    - where earlier exposure to a statement boosts a later truthfuln
  track: The 1st Workshop on Social Influence in Conversations (SICon)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'When harmful social stereotypes are expressed on a public platform, they
    must be addressed in a way that educates and informs both the original poster
    and other readers, without causing offence or perpetuating new stereotypes. In
    this paper, we synthesize findings from psychology and computer science to propose
    a set of potential counter-stereotype strategies. We then automatically generate
    such counter-stereotypes using ChatGPT, and analyze their correctness and expected
    effectiveness at reducing stereotypical associations. We identify the strategies
    of denouncing stereotypes, warning of consequences, and using an empathetic tone
    as three promising strategies to be further tested. '
  authors:
  - Kathleen Fraser
  - Svetlana Kiritchenko
  - Isar Nejadgholi
  - Anna Kerkhof
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SICon
  forum: ''
  id: SICon_7
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long (novel;
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: What Makes a Good Counter-Stereotype? Evaluating Strategies for Automated
    Responses to Stereotypical Text
  tldr: When harmful social stereotypes are expressed on a public platform, they must
    be addressed in a way that educates and informs both the original poster and other
    readers, without causing offence or perpetuating new stereotypes. In this paper,
    we synthesize findings from psychology and computer scienc
  track: The 1st Workshop on Social Influence in Conversations (SICon)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Facilitating healthy online deliberation in terms of sensemaking and
    collaboration of discussion participants proves extremely challenging due to a
    number of known negative effects of online communication on social media platforms.
    We start from concerns and aspirations about the use of existing online discussion
    systems as distilled in previous literature, we then combine them with lessons
    learned on design and engineering practices from our research team, to inform
    the design of an easy-to-use tool (BCause.app) that enables higher quality discussions
    than traditional social media. We describe the design of this tool, highlighting
    the main interaction features that distinguish it from common social media, namely:
    i. the low-cost argumentation structuring of the conversations with direct replies;
    ii. and the distinctive use of reflective feedback rather than appreciative-only
    feedback. We then present the results of a controlled A/B experiment in which
    we show that the presence of argumentative and cognitive reflective discussion
    elements produces better social interaction with less polarization and promotes
    a more cohesive discussion than common social media-like interactions.'
  authors:
  - Lucas Anastasiou
  - Anna De Libbo
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SICon
  forum: ''
  id: SICon_8
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long (novel;
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'BCause: Reducing group bias and promoting cohesive discussion in online
    deliberation processes through a simple and engaging  online deliberation tool'
  tldr: Facilitating healthy online deliberation in terms of sensemaking and collaboration
    of discussion participants proves extremely challenging due to a number of known
    negative effects of online communication on social media platforms. We start from
    concerns and aspirations about the use of existing onl
  track: The 1st Workshop on Social Influence in Conversations (SICon)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Dialog participants sometimes align their linguistic styles, e.g., they
    use the same words and syntactic constructions as their interlocutors. We propose
    to investigate the notion of lexico-semantic alignment: to what extent do speakers
    convey the same meaning when they use the same words? We design measures of lexico-semantic
    alignment relying on contextualized word representations. We show that they reflect
    interesting semantic differences between the two sides of a debate and that they
    can assist in the task of debate''s winner prediction.

    '
  authors:
  - "Aina Gar\xED Soler"
  - Matthieu Labeau
  - "Chlo\xE9 Clavel"
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SICon
  forum: ''
  id: SICon_11
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long (novel;
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Measuring Lexico-Semantic Alignment in Debates with Contextualized Word Representations
  tldr: 'Dialog participants sometimes align their linguistic styles, e.g., they use
    the same words and syntactic constructions as their interlocutors. We propose
    to investigate the notion of lexico-semantic alignment: to what extent do speakers
    convey the same meaning when they use the same words? We design'
  track: The 1st Workshop on Social Influence in Conversations (SICon)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Linguistic style matching (LSM) in conversations can be reflective of
    several aspects of social influence such as power or persuasion. However, how
    LSM relates to the outcomes of online communication on platforms such as Reddit
    is an unknown question. In this study, we analyze a large corpus of two-party
    conversation threads in Reddit where we identify all occurrences of LSM using
    two types of style: the use of function words and formality. Using this framework,
    we examine how levels of LSM differ in conversations depending on several social
    factors within Reddit: post and subreddit features, conversation depth, user tenure,
    and the controversiality of a comment. Finally, we measure the change of LSM following
    loss of status after community banning. Our findings reveal the interplay of LSM
    in Reddit conversations with several community metrics, suggesting the importance
    of understanding conversation engagement when understanding community dynamics.'
  authors:
  - Aparna Ananthasubramaniam
  - Hong Chen
  - Jason Yan
  - Kenan Alkiek
  - Jiaxin Pei
  - Agrima Seth
  - Lavinia Dunagan
  - Minje Choi
  - Benjamin Litterer
  - David Jurgens
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SICon
  forum: ''
  id: SICon_15
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long (novel;
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Exploring Linguistic Style Matching in Online Communities: The Role of Social
    Context and Conversation Dynamics'
  tldr: Linguistic style matching (LSM) in conversations can be reflective of several
    aspects of social influence such as power or persuasion. However, how LSM relates
    to the outcomes of online communication on platforms such as Reddit is an unknown
    question. In this study, we analyze a large corpus of two-
  track: The 1st Workshop on Social Influence in Conversations (SICon)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Feature attribution methods highlight the important input tokens as explanations
    to model predictions, which have been widely applied to deep neural networks towards
    trustworthy AI. However, recent works show that explanations provided by these
    methods face challenges of being faithful and robust. In this paper, we propose
    a method with Robustness improvement and Explanation Guided training towards more
    faithful EXplanations (REGEX) for text classification. First, we improve model
    robustness by input gradient regularization technique and virtual adversarial
    training. Secondly, we use salient ranking to mask noisy tokens and maximize the
    similarity between model attention and feature attribution, which can be seen
    as a self-training procedure without importing other external information. We
    conduct extensive experiments on six datasets with five attribution methods, and
    also evaluate the faithfulness in the out-of-domain setting. The results show
    that REGEX improves fidelity metrics of explanations in all settings and further
    achieves consistent gains based on two randomization tests. Moreover, we show
    that using highlight explanations produced by REGEX to train select-then-predict
    models results in comparable task performance to the end-to-end method.
  authors:
  - Dongfang Li
  - Baotian Hu
  - Qingcai Chen
  - Shan He
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - TrustNLP
  forum: ''
  id: TrustNLP_2
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Towards Faithful Explanations for Text Classification with Robustness Improvement
    and Explanation Guided Training
  tldr: Feature attribution methods highlight the important input tokens as explanations
    to model predictions, which have been widely applied to deep neural networks towards
    trustworthy AI. However, recent works show that explanations provided by these
    methods face challenges of being faithful and robust. I
  track: The Third Workshop on Trustworthy Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Metric Differential Privacy enables text-to-text privatization by adding
    calibrated noise to the vector of a word derived from an embedding space and projecting
    this noisy vector back to a discrete vocabulary using a nearest neighbor search.
    Since words are substituted without context, this mechanism is expected to fall
    short at finding substitutes for words with ambiguous meanings, such as 'bank'.
    To account for these ambiguous words, we leverage a sense embedding and incorporate
    a sense disambiguation step prior to noise injection. We encompass our modification
    to the privatization mechanism with an estimation of privacy and utility. For
    word sense disambiguation on the Words in Context dataset, we demonstrate a substantial
    increase in classification accuracy by 6.05\%.
  authors:
  - Stefan Arnold
  - Dilara Yesilbas
  - Sven Weinzierl
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - TrustNLP
  forum: ''
  id: TrustNLP_3
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Driving Context into Text-to-Text Privatization
  tldr: Metric Differential Privacy enables text-to-text privatization by adding calibrated
    noise to the vector of a word derived from an embedding space and projecting this
    noisy vector back to a discrete vocabulary using a nearest neighbor search. Since
    words are substituted without context, this mechanis
  track: The Third Workshop on Trustworthy Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We analyze sentiment analysis and toxicity detection models to detect
    the presence of explicit bias against people with disability (PWD). We employ
    the bias identification framework of Perturbation Sensitivity Analysis to examine
    conversations related to PWD on social media platforms, specifically Twitter and
    Reddit, in order to gain insight into how disability bias is disseminated in real-world
    social settings. We then create the Bias Identification Test in Sentiment (BITS)
    corpus to quantify explicit disability bias in any sentiment analysis and toxicity
    detection models. Our study utilizes BITS to uncover significant biases in four
    open AIaaS (AI as a Service) sentiment analysis tools, namely TextBlob, VADER,
    Google Cloud Natural Language API, DistilBERT and two toxicity detection models,
    namely two versions of Toxic-BERT. Our findings indicate that all of these models
    exhibit statistically significant explicit bias against PWD.
  authors:
  - Pranav Narayanan Venkit
  - Mukund Srinath
  - Shomir Wilson
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - TrustNLP
  forum: ''
  id: TrustNLP_5
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Automated Ableism: An Exploration of Explicit Disability Biases in Sentiment
    and Toxicity Analysis Models'
  tldr: We analyze sentiment analysis and toxicity detection models to detect the
    presence of explicit bias against people with disability (PWD). We employ the
    bias identification framework of Perturbation Sensitivity Analysis to examine
    conversations related to PWD on social media platforms, specifically T
  track: The Third Workshop on Trustworthy Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The textual adversarial attack refers to an attack method in which the
    attacker adds imperceptible perturbations to the original texts by elaborate design
    so that the NLP (natural language processing) model produces false judgments.
    This method is also used to evaluate the robustness of NLP models. Currently,
    most of the research in this field focuses on English, and there is also a certain
    amount of research on Chinese. However, to the best of our knowledge, there is
    little research targeting Chinese minority languages. Textual adversarial attacks
    are a new challenge for the information processing of Chinese minority languages.
    In response to this situation, we propose a Tibetan syllable-level black-box textual
    adversarial attack called TSAttacker based on syllable cosine distance and scoring
    mechanism. And then, we conduct TSAttacker on six models generated by fine-tuning
    two PLMs (pre-trained language models) for three downstream tasks. The experiment
    results show that TSAttacker is effective and generates high-quality adversarial
    samples. In addition, the robustness of the involved models still has much room
    for improvement.
  authors:
  - Xi Cao
  - Dolma Dawa
  - Nuo Qun
  - Trashi Nyima
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - TrustNLP
  forum: ''
  id: TrustNLP_6
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Pay Attention to the Robustness of Chinese Minority Language Models! Syllable-level
    Textual Adversarial Attack on Tibetan Script
  tldr: 'The textual adversarial attack refers to an attack method in which the attacker
    adds imperceptible perturbations to the original texts by elaborate design so
    that the NLP (natural language processing) model produces false judgments. This
    method is also used to evaluate the robustness of NLP models. '
  track: The Third Workshop on Trustworthy Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ChatGPT, the first large language model with mass adoption, has demonstrated
    remarkableperformance in numerous natural language tasks. Despite its evident
    usefulness, evaluatingChatGPT's performance in diverse problem domains remains
    challenging due to the closednature of the model and its continuous updates via
    Reinforcement Learning from HumanFeedback (RLHF). We highlight the issue of data
    contamination in ChatGPT evaluations, with a case study in stance detection. We
    discuss the challenge of preventing data contamination and ensuring fair model
    evaluation in the age of closed and continuously trained models.
  authors:
  - Rachith Aiyappa
  - Jisun An
  - Haewoon Kwak
  - Yong-yeol Ahn
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - TrustNLP
  forum: ''
  id: TrustNLP_8
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Can we trust the evaluation on ChatGPT?
  tldr: ChatGPT, the first large language model with mass adoption, has demonstrated
    remarkableperformance in numerous natural language tasks. Despite its evident
    usefulness, evaluatingChatGPT's performance in diverse problem domains remains
    challenging due to the closednature of the model and its continuou
  track: The Third Workshop on Trustworthy Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Modern abstractive summarization models often generate summaries that
    contain hallucinated or contradictory information. In this paper, we propose a
    simple but effective contrastive learning framework that incorporates recent developments
    in reward learning and factuality metrics. Empirical studies demonstrate that
    the proposed framework enables summarization models to learn from feedback of
    factuality metrics using contrastive reward learning, leading to more factual
    summaries by human evaluations. This suggests that further advances in learning
    and evaluation algorithms can feed directly into providing more factual summaries.
    Code and human evaluation results will be publicly available at \textbackslash{}url\{https://github.com/EthanC111/factuality\_summarization\}.
  authors:
  - I-chun Chern
  - Zhiruo Wang
  - Sanjan Das
  - Bhavuk Sharma
  - Pengfei Liu
  - Graham Neubig
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - TrustNLP
  forum: ''
  id: TrustNLP_10
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Improving Factuality of Abstractive Summarization via Contrastive Reward
    Learning
  tldr: 'Modern abstractive summarization models often generate summaries that contain
    hallucinated or contradictory information. In this paper, we propose a simple
    but effective contrastive learning framework that incorporates recent developments
    in reward learning and factuality metrics. Empirical studies '
  track: The Third Workshop on Trustworthy Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: As language models continue to be integrated into applications of personal
    and societal relevance, ensuring these models' trustworthiness is crucial, particularly
    with respect to producing consistent outputs regardless of sensitive attributes.
    Given that first names may serve as proxies for (intersectional) socio-demographic
    representations, it is imperative to examine the impact of first names on commonsense
    reasoning capabilities. In this paper, we study whether a model's reasoning given
    a specific input differs based on the first names provided. Our underlying assumption
    is that the reasoning about Alice should not differ from the reasoning about James.
    We propose and implement a controlled experimental framework to measure the causal
    effect of first names on commonsense reasoning, enabling us to distinguish between
    model predictions due to chance and caused by actual factors of interest. Our
    results indicate that the frequency of first names has a direct effect on model
    prediction, with less frequent names yielding divergent predictions compared to
    more frequent names. To gain insights into the internal mechanisms of models that
    are contributing to these behaviors, we also conduct an in-depth explainable analysis.
    Overall, our findings suggest that to ensure model robustness, it is essential
    to augment datasets with more diverse first names during the configuration stage.
  authors:
  - Sullam Jeoung
  - Jana Diesner
  - Halil Kilicoglu
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - TrustNLP
  forum: ''
  id: TrustNLP_11
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Examining the Causal Impact of First Names on Language Models: The Case
    of Social Commonsense Reasoning'
  tldr: As language models continue to be integrated into applications of personal
    and societal relevance, ensuring these models' trustworthiness is crucial, particularly
    with respect to producing consistent outputs regardless of sensitive attributes.
    Given that first names may serve as proxies for (interse
  track: The Third Workshop on Trustworthy Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Large language models (LLMs) have become mainstream technology with their
    versatile use cases and impressive performance. Despite the countless out-of-the-box
    applications, LLMs are still not reliable. A lot of work is being done to improve
    the factual accuracy, consistency, and ethical standards of these models through
    fine-tuning, prompting, and Reinforcement Learning with Human Feedback (RLHF),
    but no systematic analysis of the responses of these models to different categories
    of statements, or on their potential vulnerabilities to simple prompting changes
    is available. In this work, we analyze what confuses GPT-3: how the model responds
    to certain sensitive topics and what effects the prompt wording has on the model
    response. We find that GPT-3 correctly disagrees with obvious Conspiracies and
    Stereotypes but makes mistakes with common Misconceptions and Controversies. The
    model responses are inconsistent across prompts and settings, highlighting GPT-3''s
    unreliability.'
  authors:
  - Aisha Khatun
  - Daniel Brown
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - TrustNLP
  forum: ''
  id: TrustNLP_12
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Reliability Check: An Analysis of GPT-3''s Response to Sensitive Topics
    and Prompt Wording'
  tldr: Large language models (LLMs) have become mainstream technology with their
    versatile use cases and impressive performance. Despite the countless out-of-the-box
    applications, LLMs are still not reliable. A lot of work is being done to improve
    the factual accuracy, consistency, and ethical standards of
  track: The Third Workshop on Trustworthy Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Large language models (LLMs) are excellent few-shot learners. They can
    perform a wide variety of tasks purely based on natural language prompts provided
    to them. These prompts contain data of a specific downstream task---often the
    private dataset of a party, e.g., a company that wants to leverage the LLM on
    their purposes. We show that deploying prompted models presents a significant
    privacy risk for the data used within the prompt by proposing a highly effective
    membership inference attack.We also observe that the privacy risk of prompted
    models exceeds fine-tuned models at the same utility levels. After identifying
    the model's sensitivity to their prompts---in form of a significantly higher prediction
    confidence on the prompted data---as a cause for the increased risk, we propose
    ensembling as a mitigation strategy. By aggregating over multiple different versions
    of a prompted model, membership inference risk can be decreased.
  authors:
  - Haonan Duan
  - Adam Dziedzic
  - Mohammad Yaghini
  - Nicolas Papernot
  - Franziska Boenisch
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - TrustNLP
  forum: ''
  id: TrustNLP_13
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: On the Privacy Risk of In-context Learning
  tldr: Large language models (LLMs) are excellent few-shot learners. They can perform
    a wide variety of tasks purely based on natural language prompts provided to them.
    These prompts contain data of a specific downstream task---often the private dataset
    of a party, e.g., a company that wants to leverage th
  track: The Third Workshop on Trustworthy Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Adversarial attack research in natural language processing (NLP) has made
    significant progress in designing powerful attack methods and defence approaches.
    However, few efforts have sought to identify which source samples are the most
    attackable or robust, i.e. can we determine for an unseen target model, which
    samples are the most vulnerable to an adversarial attack. This work formally extends
    the definition of sample attackability/robustness for NLP attacks. Experiments
    on two popular NLP datasets, four state of the art models and four different NLP
    adversarial attack methods, demonstrate that sample uncertainty is insufficient
    for describing characteristics of attackable/robust samples and hence a deep learning
    based detector can perform much better at identifying the most attackable and
    robust samples for an unseen target model. Nevertheless, further analysis finds
    that there is little agreement in which samples are considered the most attackable/robust
    across different NLP attack methods, explaining a lack of portability of attackability
    detection methods across attack methods.
  authors:
  - Vyas Raina
  - Mark Gales
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - TrustNLP
  forum: ''
  id: TrustNLP_16
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Sample Attackability in Natural Language Adversarial Attacks
  tldr: 'Adversarial attack research in natural language processing (NLP) has made
    significant progress in designing powerful attack methods and defence approaches.
    However, few efforts have sought to identify which source samples are the most
    attackable or robust, i.e. can we determine for an unseen target '
  track: The Third Workshop on Trustworthy Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Harmful content detection models tend to have higher false positive rates
    for content from marginalized groups. In the context of marginal abuse modeling
    on Twitter, such disproportionate penalization poses the risk of reduced visibility,
    where marginalized communities lose the opportunity to voice their opinion on
    the platform. Current approaches to algorithmic harm mitigation, and bias detection
    for NLP models are often very ad hoc and subject to human bias. We make two main
    contributions in this paper. First, we design a novel methodology, which provides
    a principled approach to detecting and measuring the severity of potential harms
    associated with a text-based model. Second, we apply our methodology to audit
    Twitter's English marginal abuse model, which is used for removing amplification
    eligibility of marginally abusive content. Without utilizing demographic labels
    or dialect classifiers, we are still able to detect and measure the severity of
    issues related to the over-penalization of the speech of marginalized communities,
    such as the use of reclaimed speech, counterspeech, and identity related terms.
    In order to mitigate the associated harms, we experiment with adding additional
    true negative examples and find that doing so provides improvements to our fairness
    metrics without large degradations in model performance.
  authors:
  - Kyra Yee
  - Alice Schoenauer Sebag
  - Olivia Redfield
  - Matthias Eck
  - Emily Sheng
  - Luca Belli
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - TrustNLP
  forum: ''
  id: TrustNLP_17
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: A Keyword Based Approach to Understanding the Overpenalization of Marginalized
    Groups by English Marginal Abuse Models on Twitter
  tldr: 'Harmful content detection models tend to have higher false positive rates
    for content from marginalized groups. In the context of marginal abuse modeling
    on Twitter, such disproportionate penalization poses the risk of reduced visibility,
    where marginalized communities lose the opportunity to voice '
  track: The Third Workshop on Trustworthy Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Large-scale Pre-Trained Language Models (PTLMs) capture knowledge from
    massive human-written data which contains latent societal biases and toxic contents.
    In this paper, we leverage the primary task of PTLMs, i.e., language modeling,
    and propose a new metric to quantify manifested implicit representational harms
    in PTLMs towards 13 marginalized demographics. Using this metric, we conducted
    an empirical analysis of 24 widely used PTLMs. Our analysis provides insights
    into the correlation between the proposed metric in this work and other related
    metrics for representational harm. We observe that our metric correlates with
    most of the gender-specific metrics in the literature. Through extensive experiments,
    we explore the connections between PTLMs architectures and representational harms
    across two dimensions: depth and width of the networks. We found that prioritizing
    depth over width, mitigates representational harms in some PTLMs. Our code and
    data can be found at [place holder].'
  authors:
  - Saghar Hosseini
  - Hamid Palangi
  - Ahmed Hassan Awadallah
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - TrustNLP
  forum: ''
  id: TrustNLP_18
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: An Empirical Study of Metrics to Measure Representational Harms in Pre-Trained
    Language Models
  tldr: Large-scale Pre-Trained Language Models (PTLMs) capture knowledge from massive
    human-written data which contains latent societal biases and toxic contents. In
    this paper, we leverage the primary task of PTLMs, i.e., language modeling, and
    propose a new metric to quantify manifested implicit represen
  track: The Third Workshop on Trustworthy Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We investigate the phenomenon of an LLM's untruthful response using a
    large set of 220 handcrafted linguistic features. We focus on GPT-3 models and
    find that the linguistic profiles of responses are similar across model sizes.
    That is, how varying-sized LLMs respond to given prompts stays similar on the
    linguistic properties level. We expand upon this finding by training support vector
    machines that rely only upon the stylistic components of model responses to classify
    the truthfulness of statements. Though the dataset size limits our current findings,
    we present promising evidence that truthfulness detection is possible without
    evaluating the content itself. We release our code and raw data.
  authors:
  - Bruce W. Lee
  - Benedict Florance Arockiaraj
  - Helen Jin
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - TrustNLP
  forum: ''
  id: TrustNLP_20
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Linguistic Properties of Truthful Response
  tldr: We investigate the phenomenon of an LLM's untruthful response using a large
    set of 220 handcrafted linguistic features. We focus on GPT-3 models and find
    that the linguistic profiles of responses are similar across model sizes. That
    is, how varying-sized LLMs respond to given prompts stays similar o
  track: The Third Workshop on Trustworthy Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Despite the remarkable performances in various applications, machine learning
    (ML) models could potentially discriminate. They may result in biasness in decision-making,
    leading to an impact negatively on individuals and society. Recently, various
    methods have been developed to mitigate biasness and achieve significant performance.
    Attention mechanisms are a fundamental component of many state-of-the-art ML models
    and may potentially impact the fairness of ML models. However, how they explicitly
    influence fairness has yet to be thoroughly explored. In this paper, we investigate
    how different attention mechanisms affect the fairness of ML models, focusing
    on models used in Natural Language Processing (NLP) models. We evaluate the performance
    of fairness of several models with and without different attention mechanisms
    on widely used benchmark datasets. Our results indicate that the majority of attention
    mechanisms that have been assessed can improve the fairness performance of Bidirectional
    Gated Recurrent Unit (BiGRU) and Bidirectional Long Short-Term Memory (BiLSTM)
    in all three datasets regarding religious and gender-sensitive groups, however,
    with varying degrees of trade-offs in accuracy measures. Our findings highlight
    the possibility of fairness being affected by adopting specific attention mechanisms
    in machine learning models for certain datasets
  authors:
  - Shijing Chen
  - Usman Naseem
  - Imran Razzak
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - TrustNLP
  forum: ''
  id: TrustNLP_21
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Debunking Biases in Attention
  tldr: Despite the remarkable performances in various applications, machine learning
    (ML) models could potentially discriminate. They may result in biasness in decision-making,
    leading to an impact negatively on individuals and society. Recently, various
    methods have been developed to mitigate biasness and
  track: The Third Workshop on Trustworthy Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Metric Differential Privacy is a generalization of differential privacy
    tailored to address the unique challenges of text-to-text privatization. By adding
    noise to the representation of words in the geometric space of embeddings, words
    are replaced with words located in the proximity of the noisy representation.
    Since embeddings are trained based on word co-occurrences, this mechanism ensures
    that substitutions stem from a common semantic context. Without considering the
    grammatical category of words, however, this mechanism cannot guarantee that substitutions
    play similar syntactic roles. We analyze the capability of text-to-text privatization
    to preserve the grammatical category of words after substitution and find that
    surrogate texts consist almost exclusively of nouns. Lacking the capability to
    produce surrogate texts that correlate with the structure of the sensitive texts,
    we encompass our analysis by transforming the privatization step into a candidate
    selection problem in which substitutions are directed to words with matching grammatical
    properties. We demonstrate a substantial improvement in the performance of downstream
    tasks by up to 4.66\% while retaining comparative privacy guarantees.
  authors:
  - Stefan Arnold
  - Dilara Yesilbas
  - Sven Weinzierl
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - TrustNLP
  forum: ''
  id: TrustNLP_22
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Guiding Text-to-Text Privatization by Syntax
  tldr: Metric Differential Privacy is a generalization of differential privacy tailored
    to address the unique challenges of text-to-text privatization. By adding noise
    to the representation of words in the geometric space of embeddings, words are
    replaced with words located in the proximity of the noisy re
  track: The Third Workshop on Trustworthy Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: An important question in deploying large language models (LLMs) is how
    to augment LLMs with private data.We propose Differentially Private In-context
    Learning (DP-ICL) to enable LLMs to adapt to new tasks while maintaining privacy
    guarantees. DP-ICL performs private inference by establishing a noisy consensus
    over an ensemble of exemplars using the Report-Noisy-Max mechanism. We evaluate
    DP-ICL on four benchmarks and find that it achieves comparable performance (\textless{}
    2\textbackslash{}\% degradation) with non-private ICL.
  authors:
  - Ashwinee Panda
  - Tong Wu
  - Jiachen Wang
  - Prateek Mittal
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - TrustNLP
  forum: ''
  id: TrustNLP_23
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Differentially Private In-Context learning
  tldr: An important question in deploying large language models (LLMs) is how to
    augment LLMs with private data.We propose Differentially Private In-context Learning
    (DP-ICL) to enable LLMs to adapt to new tasks while maintaining privacy guarantees.
    DP-ICL performs private inference by establishing a noisy
  track: The Third Workshop on Trustworthy Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper presents novel experiments shedding light on the shortcomings
    of current metrics for assessing biases of gender discrimination made by machine
    learning algorithms on textual data. We focus on the Bios dataset, and our learning
    task is to predict the occupation of individuals, based on their biography. Such
    prediction tasks are common in commercial Natural Language Processing (NLP) applications
    such as automatic job recommendations. We address an important limitation of theoretical
    discussions dealing with group-wise fairness metrics: they focus on large datasets,
    although the norm in many industrial NLP applications is to use small to reasonably
    large linguistic datasets for which the main practical constraint is to get a
    good prediction accuracy. We then question how reliable are different popular
    measures of bias when the size of the training set is simply sufficient to learn
    reasonably accurate predictions.Our experiments sample the Bios dataset and learn
    more than 200 models on different sample sizes. This allows us to statistically
    study our results and to confirm that common gender bias indices provide diverging
    and sometimes unreliable results when applied to relatively small training and
    test samples.  This highlights the crucial importance of variance calculations
    for providing sound results in this field.'
  authors:
  - Fanny Jourdan
  - Laurent Risser
  - Jean-michel Loubes
  - Nicholas Asher
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - TrustNLP
  forum: ''
  id: TrustNLP_25
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Are fairness metric scores enough to assess discrimination biases in machine
    learning?
  tldr: This paper presents novel experiments shedding light on the shortcomings of
    current metrics for assessing biases of gender discrimination made by machine
    learning algorithms on textual data. We focus on the Bios dataset, and our learning
    task is to predict the occupation of individuals, based on the
  track: The Third Workshop on Trustworthy Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Wikipedia articles are a common source of training data for Natural Language
    Processing (NLP) research, especially as a source for corpora in languages other
    than English. However, research has shown that not all Wikipedia editions are
    produced organically by native speakers, and there are substantial levels of automation
    and translation activities in the Wikipedia project that could negatively impact
    the degree to which they truly represent the language and the culture of native
    speakers. To encourage transparency in the Wikipedia project, Wikimedia Foundation
    introduced the depth metric as an indication of the degree of collaboration or
    how frequently users edit a Wikipedia edition's articles. While a promising start,
    this depth metric suffers from a few serious problems, like a lack of adequate
    handling of inflation of edits metric and a lack of full utilization of users-related
    metrics. In this paper, we propose the DEPTH+ metric, provide its mathematical
    definitions, and describe how it reflects a better representation of the depth
    of human collaborativeness. We also quantify the bot activities in Wikipedia and
    offer a bot-free depth metric after the removal of the bot-created articles and
    the bot-made edits on the Wikipedia articles.
  authors:
  - Saied Alshahrani
  - Norah Alshahrani
  - Jeanna Matthews
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - TrustNLP
  forum: ''
  id: TrustNLP_26
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'DEPTH+: An Enhanced Depth Metric for Wikipedia Corpora Quality'
  tldr: Wikipedia articles are a common source of training data for Natural Language
    Processing (NLP) research, especially as a source for corpora in languages other
    than English. However, research has shown that not all Wikipedia editions are
    produced organically by native speakers, and there are substanti
  track: The Third Workshop on Trustworthy Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: As generative NLP can now produce content nearly indistinguishable from
    human writing, it becomes difficult to identify genuine research contributions
    in academic writing and scientific publications. Moreover, information in NLP-generated
    text can potentially be factually wrong or even entirely fabricated. This study
    introduces a novel benchmark dataset, containing human-written and machine-generated
    scientific papers from SCIgen, GPT-2, GPT-3, ChatGPT, and Galactica. After describing
    the generation and extraction pipelines, we also experiment with four distinct
    classifiers as a baseline for detecting the authorship of scientific text. A strong
    focus is put on generalization capabilities and explainability to highlight the
    strengths and weaknesses of detectors. We believe our work serves as an important
    step towards creating more robust methods for distinguishing between human-written
    and machine-generated scientific papers, ultimately ensuring the integrity of
    scientific literature.
  authors:
  - Edoardo Mosca
  - Mohamed Hesham Ibrahim Abdalla
  - Paolo Basso
  - Margherita Musumeci
  - Georg Groh
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - TrustNLP
  forum: ''
  id: TrustNLP_27
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Distinguishing Fact from Fiction: A Benchmark Dataset for Identifying Machine-Generated
    Scientific Papers in the LLM Era.'
  tldr: As generative NLP can now produce content nearly indistinguishable from human
    writing, it becomes difficult to identify genuine research contributions in academic
    writing and scientific publications. Moreover, information in NLP-generated text
    can potentially be factually wrong or even entirely fabr
  track: The Third Workshop on Trustworthy Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Large language models are trained on increasing quantities of unstructured
    text, the largest sources of which are scraped from the Web. These Web scrapes
    are mainly composed of heterogeneous collections of text from multiple domains
    with minimal documentation. While some work has been done to identify and remove
    toxic, biased, or sexual language, the topic of personal information (PI) in textual
    data used for training Natural Language Processing (NLP) models is relatively
    under-explored. In this work, we draw from definitions of PI across multiple countries
    to define the first PI taxonomy of its kind, categorized by type and risk level.
    We then conduct a case study on the Colossal Clean Crawled Corpus (C4) and the
    Pile, to detect some of the highest-risk personal information, such as email addresses
    and credit card numbers, and examine the differences between automatic and regular
    expression-based approaches for their detection. We identify shortcomings in modern
    approaches for PI detection, and propose a reframing of the problem that is informed
    by global perspectives and the goals in personal information detection.
  authors:
  - Nishant Subramani
  - Sasha Luccioni
  - Jesse Dodge
  - Margaret Mitchell
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - TrustNLP
  forum: ''
  id: TrustNLP_28
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Detecting Personal Information in Training Corpora: an Analysis'
  tldr: Large language models are trained on increasing quantities of unstructured
    text, the largest sources of which are scraped from the Web. These Web scrapes
    are mainly composed of heterogeneous collections of text from multiple domains
    with minimal documentation. While some work has been done to identi
  track: The Third Workshop on Trustworthy Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Textual counterfactual examples explain a prediction by modifying the
    tokens of an initial instance in order to flip the outcome of a classifier. Even
    under sparsity constraint, counterfactual generation can lead to numerous changes
    from the initial text, making the explanation hard to understand. We propose Counterfactual
    Feature Importance, a method to make non-sparse counterfactual explanations more
    intelligible. Counterfactual Feature Importance assesses token change importance
    between an instance to explain and its counterfactual example. We develop two
    ways of computing Counterfactual Feature Importance, respectively based on classifier
    gradient computation and counterfactual generator loss evolution during counterfactual
    search. Then we design a global version of Counterfactual Feature Importance,
    providing rich information about semantic fields globally impacting classifier
    predictions. Counterfactual Feature Importance enables to focus on impacting parts
    of counterfactual explanations, making counterfactual explanations involving numerous
    changes more understandable.
  authors:
  - Milan Bhan
  - Jean-noel Vittaut
  - Nicolas Chesneau
  - Marie-jeanne Lesot
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - TrustNLP
  forum: ''
  id: TrustNLP_29
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Enhancing textual counterfactual explanation intelligibility through Counterfactual
    Feature Importance
  tldr: Textual counterfactual examples explain a prediction by modifying the tokens
    of an initial instance in order to flip the outcome of a classifier. Even under
    sparsity constraint, counterfactual generation can lead to numerous changes from
    the initial text, making the explanation hard to understand. W
  track: The Third Workshop on Trustworthy Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This work investigates the effectiveness of different pseudonymization
    techniques, ranging from rule-based substitutions to using pre-trained Large Language
    Models (LLMs), on a variety of datasets and models used for two widely used NLP
    tasks: text classification and summarization. Our work provides crucial insights
    into the gaps between original and anonymized data (focusing on the pseudonymization
    technique) and model quality and fosters future research into higher-quality anonymization
    techniques better to balance the trade-offs between data protection and utility
    preservation. We make our code, pseudonymized datasets, and downstream models
    publicly available.'
  authors:
  - Oleksandr Yermilov
  - Vipul Raheja
  - Artem Chernodub
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - TrustNLP
  forum: ''
  id: TrustNLP_32
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Privacy- and Utility-Preserving NLP with Anonymized data: A case study of
    Pseudonymization'
  tldr: 'This work investigates the effectiveness of different pseudonymization techniques,
    ranging from rule-based substitutions to using pre-trained Large Language Models
    (LLMs), on a variety of datasets and models used for two widely used NLP tasks:
    text classification and summarization. Our work provides'
  track: The Third Workshop on Trustworthy Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Though state-of-the-art (SOTA) NLP systems have achieved remarkable performance
    on a variety of language understanding tasks, they primarily focus on questions
    that have a correct and a definitive answer. However, in real-world applications,
    users often ask questions that don't have a definitive answer such as questions
    about future events, questions lacking necessary details to find the answer, and
    questions that are ambiguous. Incorrectly answering such questions certainly hampers
    a system's reliability and trustworthiness. Can SOTA models accurately identify
    such questions and provide a reasonable response?To investigate the above question,
    we introduce QnotA, a dataset consisting of five different categories of questions
    that don't have definitive answers. Furthermore, for each QnotA instance, we also
    provide a corresponding 'QA' instance i.e. an alternate question that "can be"
    answered. With this data, we formulate three evaluation tasks that test a system's
    ability to 'identify', 'distinguish', and 'justify' QnotA questions. Through comprehensive
    experiments, we show that even SOTA models including GPT-3 and Flan T5 do not
    fare well on these tasks and lack considerably behind the human performance baseline.
    We conduct a thorough analysis which further leads to several interesting findings
    such as, despite not being able to accurately identify a QnotA question, GPT-3
    on being prompted to output a justification of why the given QnotA question doesn't
    have a definitive answer is able to provide a reasonable justification. Finally,
    we believe our work and findings will encourage and facilitate development of
    more robust NLP systems that can also reasonably respond to questions that don't
    have a definitive answer.
  authors:
  - Ayushi Agarwal
  - Nisarg Patel
  - Neeraj Varshney
  - Mihir Parmar
  - Pavan Mallina
  - Aryan Shah
  - Srihari Raju Sangaraju
  - Tirth Patel
  - Nihar Thakkar
  - Chitta Baral
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - TrustNLP
  forum: ''
  id: TrustNLP_35
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Can NLP Models 'Identify', 'Distinguish', and 'Justify' Questions that Don't
    have a Definitive Answer?
  tldr: Though state-of-the-art (SOTA) NLP systems have achieved remarkable performance
    on a variety of language understanding tasks, they primarily focus on questions
    that have a correct and a definitive answer. However, in real-world applications,
    users often ask questions that don't have a definitive ans
  track: The Third Workshop on Trustworthy Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Recent studies have shown that various biases exist in different NLP tasks,
    and over-reliance on these biases can result in poor generalization and low adversarial
    robustness in models. To address this issue, previous research has proposed several
    debiasing techniques that effectively mitigate specific biases, but are limited
    in their ability to address other biases. In this paper, we introduce a novel
    debiasing method, Sparse Mixture-of-Adapters (SMoA), which can effectively and
    efficiently mitigate multiple dataset biases. Our experiments on Natural Language
    Inference and Paraphrase Identification tasks demonstrate that SMoA outperforms
    both full-finetuning and adapter tuning baselines, as well as prior strong debiasing
    methods. Further analysis reveals that SMoA is interpretable, with each sub-adapter
    capable of capturing specific patterns from the training data and specializing
    in handling specific biases.
  authors:
  - Yanchen Liu
  - Jing Yan
  - Yan Chen
  - Jing Liu
  - Hua Wu
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - TrustNLP
  forum: ''
  id: TrustNLP_36
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'SMoA: Sparse Mixture of Adapters to Mitigate Multiple Dataset Biases'
  tldr: Recent studies have shown that various biases exist in different NLP tasks,
    and over-reliance on these biases can result in poor generalization and low adversarial
    robustness in models. To address this issue, previous research has proposed several
    debiasing techniques that effectively mitigate speci
  track: The Third Workshop on Trustworthy Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This work analyzes backdoor watermarks in an autoregressive transformer
    fine-tuned to perform a generative sequence-to-sequence task, specifically summarization.
    We propose and demonstrate an attack to identify trigger words or phrases by analyzing
    open ended generations from autoregressive models that have backdoor watermarks
    inserted. It is shown in our work that triggers based on random common words are
    easier to identify than those based on single, rare tokens. The attack proposed
    is easy to implement and only requires access to the model weights. Code used
    to create the backdoor watermarked models and analyze their outputs is shared
    at [github link to be inserted for camera ready version].
  authors:
  - Evan Lucas
  - Timothy Havens
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - TrustNLP
  forum: ''
  id: TrustNLP_37
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'GPTs Don''t Keep Secrets: Searching for Backdoor Watermark Triggers in Autoregressive
    Language Models'
  tldr: This work analyzes backdoor watermarks in an autoregressive transformer fine-tuned
    to perform a generative sequence-to-sequence task, specifically summarization.
    We propose and demonstrate an attack to identify trigger words or phrases by analyzing
    open ended generations from autoregressive models t
  track: The Third Workshop on Trustworthy Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper addresses the ethical concerns arising from the use of unauthorized
    public data in deep learning models and proposes a novel solution. Specifically,
    building on the work of Huang et al. (2021), we extend their bi-level optimization
    approach to generate unlearnable text using a gradient-based search technique.
    However, although effective, this approach faces practical limitations, including
    the requirement of batches of instances and model architecture knowledge that
    is not readily accessible to ordinary users with limited access to their own data.
    Furthermore, even with semantic-preserving constraints, unlearnable noise can
    alter the text's semantics. To address these challenges, we extract simple patterns
    from unlearnable text produced by bi-level optimization and demonstrate that the
    data remains unlearnable for unknown models.  Additionally, these patterns are
    not instance- or dataset-specific, allowing users to readily apply them to text
    classification and question-answering tasks, even if only a small proportion of
    users implement them on their public content.We also open-source codes to generate
    unlearnable text and assess unlearnable noise to benefit the public and future
    studies.
  authors:
  - Xinzhe Li
  - Ming Liu
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - TrustNLP
  forum: ''
  id: TrustNLP_38
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Make Text Unlearnable: Exploiting Effective Patterns to Protect Personal
    Data'
  tldr: This paper addresses the ethical concerns arising from the use of unauthorized
    public data in deep learning models and proposes a novel solution. Specifically,
    building on the work of Huang et al. (2021), we extend their bi-level optimization
    approach to generate unlearnable text using a gradient-ba
  track: The Third Workshop on Trustworthy Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Sentiment analysis (SA) systems are used in many products and hundreds
    of languages. Gender and racial biases are well-studied in English SA systems,
    but understudied in other languages, with few resources for such studies. To remedy
    this, we build a counterfactual evaluation corpus for gender and racial/migrant
    bias in four languages. We demonstrate its usefulness by answering a simple but
    important question that an engineer might need to answer when deploying a system:
    What biases do systems import from pre-trained models when compared to a baseline
    with no pre-training? Our evaluation corpus, by virtue of being counterfactual,
    not only reveals which models have less bias, but also pinpoints changes in model
    bias behaviour, which enables more targeted mitigation strategies. We release
    our code and evaluation corpora to facilitate future research.'
  authors:
  - Seraphina Goldfarb-tarrant
  - Adam Lopez
  - Roi Blanco
  - Diego Marcheggiani
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - TrustNLP
  forum: ''
  id: TrustNLP_41
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Bias Beyond English: Counterfactual Tests for Bias in Sentiment Analysis
    in Four Languages'
  tldr: Sentiment analysis (SA) systems are used in many products and hundreds of
    languages. Gender and racial biases are well-studied in English SA systems, but
    understudied in other languages, with few resources for such studies. To remedy
    this, we build a counterfactual evaluation corpus for gender and r
  track: The Third Workshop on Trustworthy Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: As the deployment of pre-trained language models (PLMs) expands, pressing
    security concerns have arisen regarding the potential for malicious extraction
    of training data, posing a threat to data privacy.This study is the first to provide
    a comprehensive survey of training data extraction from PLMs.Our review covers
    more than 100 key papers in fields such as natural language processing and security.First,
    preliminary knowledge is recapped and a taxonomy of various definitions of memorization
    is presented.The approaches for attack and defense are then systemized.Furthermore,
    the empirical findings of several quantitative studies are highlighted.Finally,
    future research directions based on this review are suggested.
  authors:
  - Shotaro Ishihara
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - TrustNLP
  forum: ''
  id: TrustNLP_42
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Training Data Extraction From Pre-trained Language Models: A Survey'
  tldr: As the deployment of pre-trained language models (PLMs) expands, pressing
    security concerns have arisen regarding the potential for malicious extraction
    of training data, posing a threat to data privacy.This study is the first to provide
    a comprehensive survey of training data extraction from PLMs.O
  track: The Third Workshop on Trustworthy Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Recent studies have revealed that NLP predictive models are vulnerable
    to adversarial attacks. Most existing studies focused on designing attacks to
    evaluate the robustness of NLP models in the English language alone. Literature
    has seen an increasing need for NLP solutions for other languages. We, therefore,
    ask one natural question whether state-of-the-art (SOTA) attack methods generalize
    to other languages. This paper investigates how to adapt SOTA adversarial attack
    algorithms in English to the Chinese language. Our experiments show that attack
    methods previously applied to English NLP can generate high-quality adversarial
    examples in Chinese when combined with proper text segmentation and linguistic
    constraints. In addition, we demonstrate that the generated adversarial examples
    can achieve high fluency and sentiment consistency by focusing on the Chinese
    language's morphology and phonology, which in turn can be used to improve the
    adversarial robustness of Chinese NLP models.
  authors:
  - Hanyu Liu
  - Chengyuan Cai
  - Yanjun Qi
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - TrustNLP
  forum: ''
  id: TrustNLP_43
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Expanding Scope: Adapting English Adversarial Attacks to Chinese'
  tldr: 'Recent studies have revealed that NLP predictive models are vulnerable to
    adversarial attacks. Most existing studies focused on designing attacks to evaluate
    the robustness of NLP models in the English language alone. Literature has seen
    an increasing need for NLP solutions for other languages. We, '
  track: The Third Workshop on Trustworthy Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Backdoor attacks are an insidious security threat against machine learning
    models. Adversaries can manipulate the predictions of compromised models by inserting
    triggers into the training phase. Various backdoor attacks have been devised which
    can achieve nearly perfect attack success without affecting model predictions
    for clean inputs. Means of mitigating such vulnerabilities are underdeveloped,
    especially in natural language processing. To fill this gap, we introduce IMBERT,
    which uses either gradients or self-attention scores derived from victim models
    to self-defend against backdoor attacks at inference time. Our empirical studies
    demonstrate that IMBERT can effectively identify up to 98.5\% of inserted triggers.
    Thus, it significantly reduces the attack success rate while attaining competitive
    accuracy on the clean dataset across widespread insertion-based attacks compared
    to two baselines. Finally, we show that our approach is model-agnostic, and can
    be easily ported to several pre-trained transformer models.
  authors:
  - Xuanli He
  - Jun Wang
  - Benjamin Rubinstein
  - Trevor Cohn
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - TrustNLP
  forum: ''
  id: TrustNLP_45
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'IMBERT: Making BERT Immune to Insertion-based Backdoor Attacks'
  tldr: Backdoor attacks are an insidious security threat against machine learning
    models. Adversaries can manipulate the predictions of compromised models by inserting
    triggers into the training phase. Various backdoor attacks have been devised which
    can achieve nearly perfect attack success without affect
  track: The Third Workshop on Trustworthy Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Large language models (LLMs) have led to a series of breakthroughs in
    natural language processing (NLP), owing to their excellent understanding and
    generation abilities. Remarkably, what further sets these models apart is the
    massive amounts of world knowledge they internalize during pretraining. While
    many downstream applications provide the model with an informational context to
    aid its performance on the underlying task, how the model's world knowledge interacts
    with the factual information presented in the context remains under explored.
    As a desirable behavior, an LLM should give precedence to the context whenever
    it contains task-relevant information that conflicts with the model's memorized
    knowledge. This enables model predictions to be grounded in the context, which
    can then be used to update or correct specific model predictions without frequent
    retraining. By contrast, when the context is irrelevant to the task, the model
    should ignore it and fall back on its internal knowledge. In this paper, we undertake
    a first joint study of the aforementioned two properties, namely controllability
    and robustness, in the context of LLMs. We demonstrate that state-of-the-art T5
    and PaLM (both pretrained and finetuned) could exhibit poor controllability and
    robustness, which do not scale with increasing model size. As a solution, we propose
    a novel method - Knowledge Aware FineTuning (KAFT) - to strengthen both controllability
    and robustness by incorporating counterfactual and irrelevant contexts to standard
    supervised datasets. Our comprehensive evaluation showcases the utility of KAFT
    across model architectures and sizes.
  authors:
  - Daliang Li
  - Ankit Singh Rawat
  - Manzil Zaheer
  - Xin Wang
  - Michal Lukasik
  - Andreas Veit
  - Felix Yu
  - Sanjiv Kumar
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - TrustNLP
  forum: ''
  id: TrustNLP_46
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Large Language Models with Controllable Working Memory
  tldr: Large language models (LLMs) have led to a series of breakthroughs in natural
    language processing (NLP), owing to their excellent understanding and generation
    abilities. Remarkably, what further sets these models apart is the massive amounts
    of world knowledge they internalize during pretraining. Wh
  track: The Third Workshop on Trustworthy Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Many social networking sites (SNS) offer machine translation of posts
    in an effort to increase understanding, engagement, and connectivity between users
    across language barriers. However, the translations of these posts are still not
    100\% accurate and can be a cause of misunderstandings that can harm post-authors'
    professional or personal relationships. An exacerbating factor is on most SNS,
    authors cannot view the translation of their own posts, nor make corrections to
    inaccurate translations. This paper reports findings from a survey (N = 189) and
    an interview (N = 15) to explore users' concerns regarding this automatic form
    of machine translation. Our findings show that users are concerned about potential
    inaccuracies in the meaning of the translations of their posts, and would thus
    appreciate being able to view and potentially correct such translations. Additionally,
    we found that when users write posts in their native language, they write them
    for specific audiences, so they do not always want them translated. This underscores
    the urgency of providing users with more control over the translation of their
    posts.
  authors:
  - Ananya Gupta
  - Jae Takeuchi
  - Bart Knijnenburg
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - TrustNLP
  forum: ''
  id: TrustNLP_47
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'On The Real-world Performance of Machine Translation: Exploring Social Media
    Post-authors'' Perspectives'
  tldr: Many social networking sites (SNS) offer machine translation of posts in an
    effort to increase understanding, engagement, and connectivity between users across
    language barriers. However, the translations of these posts are still not 100\%
    accurate and can be a cause of misunderstandings that can ha
  track: The Third Workshop on Trustworthy Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Manually annotated datasets are crucial for training and evaluating Natural
    Language Processing models. However, recent work has discovered that even widely-used
    benchmark datasets contain a substantial number of erroneous annotations. This
    problem has been addressed with Annotation Error Detection (AED) models, which
    can flag such errors for human re-annotation. However, even though many of these
    AED methods assume a final curation step in which a human annotator decides whether
    the annotation is erroneous, they have been developed as static models without
    any human-in-the-loop component. In this work, we propose ActiveAED, an AED method
    that can detect errors more accurately by repeatedly querying a human for error
    corrections in its prediction loop. We evaluate ActiveAED on eight datasets spanning
    five different tasks and find that it leads to improvements over the state of
    the art on seven of them, with gains of up to six percentage points in average
    precision. This work will be published in Findings of ACL 2023 and thus we would
    like to submit as non-archival. We are also interested in presenting this work
    at the LAW workshop, but will know whether this is possible only in a few weeks.
    We have attached to the appendix the reviews and the author response indicating
    our changes for the camera-ready version.
  authors:
  - Leon Weber
  - Barbara Plank
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - TrustNLP
  forum: ''
  id: TrustNLP_48
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'ActiveAED: A Human in the Loop Improves Annotation Error Detection'
  tldr: 'Manually annotated datasets are crucial for training and evaluating Natural
    Language Processing models. However, recent work has discovered that even widely-used
    benchmark datasets contain a substantial number of erroneous annotations. This
    problem has been addressed with Annotation Error Detection '
  track: The Third Workshop on Trustworthy Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper proposes a novel approach, called Iterative Gradient-Based
    Projection (IGBP), for removing non-linear encoded demographic information from
    neural representations. The method is evaluated on gender and race attributes
    using intrinsic and extrinsic metrics. The comprehensive results demonstrate the
    effectiveness of the proposed method.The paper got accepted to the Findings of
    ACL, reviews are included in the appendix.The parts of the paper which have been
    revised are colored in blue.
  authors:
  - Shadi Iskander
  - Kira Radinsky
  - Yonatan Belinkov
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - TrustNLP
  forum: ''
  id: TrustNLP_49
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Shielded Representations: Protecting Sensitive Attributes Through Iterative
    Gradient-Based Projection'
  tldr: This paper proposes a novel approach, called Iterative Gradient-Based Projection
    (IGBP), for removing non-linear encoded demographic information from neural representations.
    The method is evaluated on gender and race attributes using intrinsic and extrinsic
    metrics. The comprehensive results demonst
  track: The Third Workshop on Trustworthy Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Auditing unwanted social bias in language models (LMs) is inherently
    hard due to the multi-disciplinary nature of the work. In addition, the rapid
    evolution of LMs can make benchmarks irrelevant in no time. Bias auditing is further
    complicated by LM brittleness: when a presumably biased outcome is observed, is
    it due to model bias or model brittleness?We propose enlisting the models themselves
    to help construct bias auditing datasets that remain challenging, and introduce
    bias measures that distinguish between types of model errors. First, we extend
    an existing bias benchmark for NLI (BBNLI) using a combination of LM-generated
    lexical variations, adversarial filtering, and human validation.We demonstrate
    that the newly created dataset (BBNLI-next) is more challenging than BBNLI: on
    average, BBNLI-next reduces the accuracy of state-of-the-art NLI models from 95.3\%,
    as observed by BBNLI, to 58.6\%.Second, we employ BBNLI-next to showcase the interplay
    between robustness and bias, and the subtlety in differentiating between the two.
    Third, we point out shortcomings in current bias scores used in the literature
    and propose bias measures that take into account pro-/anti-stereotype bias and
    model brittleness.  We will publicly release the BBNLI-next dataset to inspire
    research on rapidly expanding benchmarks to keep up with model evolution, along
    with research on the robustness-bias interplay in bias auditing.Note: This paper
    contains offensive text examples.'
  authors:
  - Ioana Baldini
  - Chhavi Yadav
  - Payel Das
  - Kush Varshney
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - TrustNLP
  forum: ''
  id: TrustNLP_50
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Keeping Up with the Language Models: Robustness-Bias Interplay in NLI Data
    and Models'
  tldr: 'Auditing unwanted social bias in language models (LMs) is inherently hard
    due to the multi-disciplinary nature of the work. In addition, the rapid evolution
    of LMs can make benchmarks irrelevant in no time. Bias auditing is further complicated
    by LM brittleness: when a presumably biased outcome is o'
  track: The Third Workshop on Trustworthy Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Many NLP classification tasks, such as sexism/racism detection or toxicity
    detection, are based on human values. Yet, human values can vary under diverse
    cultural conditions. Therefore, we introduce a framework for value-aligned classification
    that performs prediction based on explicitly written human values in the command.
    Along with the task, we propose a practical approach that distills value-aligned
    knowledge from large-scale language models (LLMs) to construct value-aligned classifiers
    in two steps.First, we generate value-aligned training data from LLMs by prompt-based
    few-shot learning. Next, we fine-tune smaller classification models with the generated
    data for the task. Empirical results show that our VA-Models surpass multiple
    baselines by at least 15.56\% on the F1-score, including few-shot learning with
    OPT-175B and existing text augmentation methods. We suggest that using classifiers
    with explicit human value input improves both inclusivity \& explainability in
    AI.
  authors:
  - Yejin Bang
  - Tiezheng Yu
  - Andrea Madotto
  - Zhaojiang Lin
  - Mona Diab
  - Pascale Fung
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - TrustNLP
  forum: ''
  id: TrustNLP_51
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Enabling Classifiers to Make Judgements Explicitly Aligned with Human Values
  tldr: Many NLP classification tasks, such as sexism/racism detection or toxicity
    detection, are based on human values. Yet, human values can vary under diverse
    cultural conditions. Therefore, we introduce a framework for value-aligned classification
    that performs prediction based on explicitly written hum
  track: The Third Workshop on Trustworthy Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Bias research in NLP seeks to analyse models for social biases, thus helping
    NLP practitioners uncover, measure, and mitigate social harms. We analyse the
    body of work that uses prompts and templates to assess bias in language models.
    We draw on a measurement modelling framework to create a taxonomy of attributes
    that capture what a bias test aims to measure and how that measurement is carried
    out. By applying this taxonomy to 90 bias tests, we illustrate qualitatively and
    quantitatively that core aspects of bias test conceptualisations and operationalisations
    are frequently unstated or ambiguous, carry implicit assumptions, or be mismatched.
    Our analysis illuminates the scope of possible bias types the field is able to
    measure, and reveals types that are as yet under-researched. We offer guidance
    to enable the community to explore a wider section of the possible bias space,
    and to better close the gap between desired outcomes and experimental design,
    both for bias and for evaluating language models more broadly.
  authors:
  - Seraphina Goldfarb-tarrant
  - Eddie Ungless
  - Esma Balkir
  - Su Lin Blodgett
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - TrustNLP
  forum: ''
  id: TrustNLP_52
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'This Prompt is Measuring \textless{}MASK\textgreater{}: Evaluating Bias
    Evaluation in Language Models'
  tldr: Bias research in NLP seeks to analyse models for social biases, thus helping
    NLP practitioners uncover, measure, and mitigate social harms. We analyse the
    body of work that uses prompts and templates to assess bias in language models.
    We draw on a measurement modelling framework to create a taxonomy
  track: The Third Workshop on Trustworthy Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Transformer architectures are complex and their use in NLP, while it has
    engendered many successes, makes their interpretability or explainability challenging.  Recent
    debates have shown that attention maps and attribution methods are unreliable  (Pruthi
    et al., 2019; Brunner et al., 2019).  In this paper, we present some of their
    limitations and introduce COCKATIEL, which successfully addresses some of them.
    COCKATIEL is a novel, post-hoc, concept-based, model-agnostic XAI technique that
    generates meaningful explanations from the last layer of a neural net model trained
    on an NLP classification task by using Non-Negative Matrix Factorization (NMF)
    to discover the concepts the model leverages to make predictions and by exploiting
    a Sensitivity Analysis to estimate accurately the importance of each of these
    concepts for the model. It does so without compromising the accuracy of the underlying
    model or requiring a new one to be trained. We conduct experiments in single and
    multi-aspect sentiment analysis tasks and we show COCKATIEL's superior ability
    to discover concepts that align with humans' on Transformer models without any
    supervision, we objectively verify the faithfulness of its explanations through
    fidelity metrics, and we showcase its ability to provide meaningful explanations
    in two different datasets.
  authors:
  - Fanny Jourdan
  - Agustin Picard
  - Laurent Risser
  - Jean-michel Loubes
  - Nicholas Asher
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - TrustNLP
  forum: ''
  id: TrustNLP_53
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'COCKATIEL: COntinuous Concept ranKed ATtribution with Interpretable ELements
    for explaining neural net classifiers on NLP tasks'
  tldr: Transformer architectures are complex and their use in NLP, while it has engendered
    many successes, makes their interpretability or explainability challenging.  Recent
    debates have shown that attention maps and attribution methods are unreliable  (Pruthi
    et al., 2019; Brunner et al., 2019).  In this
  track: The Third Workshop on Trustworthy Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Large language models have achieved impressive few-shot performance on
    a wide variety of tasks. However, in many settings, users require confidence estimates
    for model predictions. While traditional classifiers produce scores for each label,
    language models instead produce scores for the generation which may not be well
    calibrated. We compare generations across diverse prompts and show that these
    can be used to create confidence scores. By utilizing more prompts we can get
    more precise confidence estimates and use response diversity as a proxy for confidence.
    We evaluate this approach across ten multiple-choice question-answering datasets
    using three models: T0, FLAN-T5, and GPT-3. In addition to analyzing multiple
    human written prompts, we automatically generate more prompts using a language
    model in order to produce finer-grained confidence estimates. Our method produces
    more calibrated confidence estimates compared to the log probability of the answer
    to a single prompt. These improvements could benefit users who rely on prediction
    confidence for integration into a larger system or in decision-making processes.'
  authors:
  - Gwenyth Portillo Wightman
  - Alexandra Delucia
  - Mark Dredze
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - TrustNLP
  forum: ''
  id: TrustNLP_54
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Strength in Numbers: Estimating Confidence of Large Language Models by Prompt
    Agreement'
  tldr: 'Large language models have achieved impressive few-shot performance on a
    wide variety of tasks. However, in many settings, users require confidence estimates
    for model predictions. While traditional classifiers produce scores for each label,
    language models instead produce scores for the generation '
  track: The Third Workshop on Trustworthy Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The problem of making Named Entity Recognition (NER) models robust to
    adversarial attacks has received widespread attention recently (Simoncini and
    Spanakis, 2021; Lin et al., 2021). The existing techniques for robustfying the
    NER models rely on exhaustive perturbation of the input training data to generate
    adversarial examples, often resulting in adversarial examples that are not semantically
    equivalent to the original. In this paper, we employ word attributions guided
    perturbations that generate adversarial examples with a comparable attack rates
    but at a lower modification rate. Our approach also uses disentanglement of entity
    and non-entity word representations as a mechanism to generate diverse and unbiased
    adversarial examples. Adversarial training results based on our method improves
    the F1 score over originally trained NER model by 8\% and 18\% on CoNLL-2003 and
    Ontonotes  5.0 datasets respectively.
  authors:
  - Xiaomeng Jin
  - Bhanukiran Vinzamuri
  - Sriram Venkatapathy
  - Heng Ji
  - Pradeep Natarajan
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - TrustNLP
  forum: ''
  id: TrustNLP_55
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Adversarial Named-Entity Recognition with Word Attributions and Disentanglement
  tldr: 'The problem of making Named Entity Recognition (NER) models robust to adversarial
    attacks has received widespread attention recently (Simoncini and Spanakis, 2021;
    Lin et al., 2021). The existing techniques for robustfying the NER models rely
    on exhaustive perturbation of the input training data to '
  track: The Third Workshop on Trustworthy Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Growing literature has shown that powerful NLP systems may encode social
    biases; however, the political bias of summarization models remains relatively
    unknown. In this work, we use an entity replacement method to investigate the
    portrayal of politicians in automatically generated summaries of news articles.
    We develop a computational framework based on political entities and lexical resources,
    and use it to assess biases about Donald Trump and Joe Biden in both extractive
    and abstractive summarization models. We find consistent differences, such as
    stronger associations of a collective US government (i.e., administration) with
    Biden than with Trump. These summary dissimilarities are most prominent when the
    entity is heavily featured in the source article. Our systematic characterization
    provides a framework for future studies of bias in summarization.
  authors:
  - Karen Zhou
  - Chenhao Tan
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - TrustNLP
  forum: ''
  id: TrustNLP_56
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Characterizing Political Bias in Automatic Summaries: A Case Study of Trump
    and Biden'
  tldr: 'Growing literature has shown that powerful NLP systems may encode social
    biases; however, the political bias of summarization models remains relatively
    unknown. In this work, we use an entity replacement method to investigate the
    portrayal of politicians in automatically generated summaries of news '
  track: The Third Workshop on Trustworthy Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In recent years,NLP practitioners have converged on the following practice:(i)
    import an off-the-shelf pretrained (masked) language model;(ii) append a multilayer
    perceptron atop the CLS token's hidden representation(with randomly initialized
    weights);and (iii) fine-tune the entire model on a downstream task (\textbackslash{}linearFTns).This
    procedure has produced massive gains on standard NLP benchmarks, but these models
    remain brittle, even to mild adversarial perturbations, such as word-level synonym
    substitutions. In this work, we demonstrate surprising gains in adversarial robustness
    enjoyed by Model-tuning Via Prompts (MVP),an alternative method of adapting to
    downstream tasks. Rather than modifying the model (by appending an MLP head),
    MVP instead modifies the input (by appending a prompt template). Across three
    classification datasets, MVP improves performance against adversarial word-level
    synonym substitutions by an average of 8\% over standard methods and even outperforms
    adversarial training-based state-of-art defenses by 3.5\%. By combining MVP with
    adversarial training, we achieve further improvements in robust accuracy while
    maintaining clean accuracy. Finally, we conduct ablations to investigate the mechanism
    underlying these gains. Notably, we find that the main causes of vulnerability
    of MLP can be attributed to the misalignment between pre-training and fine-tuning
    tasks, and the randomly initialized MLP parameters.
  authors:
  - Mrigank Raman
  - Pratyush Maini
  - Zico Kolter
  - Zachary C. Lipton
  - Danish Pruthi
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - TrustNLP
  forum: ''
  id: TrustNLP_57
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Model-tuning Via Prompts Makes NLP Models Adversarially Robust
  tldr: In recent years,NLP practitioners have converged on the following practice:(i)
    import an off-the-shelf pretrained (masked) language model;(ii) append a multilayer
    perceptron atop the CLS token's hidden representation(with randomly initialized
    weights);and (iii) fine-tune the entire model on a downst
  track: The Third Workshop on Trustworthy Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Models of various NLP tasks have been shown to exhibit stereotypes, and
    the bias in the question answering (QA) models is especially harmful as the output
    answers might be directly consumed by the end users. There have been datasets
    to evaluate bias in QA models, while bias mitigation technique for the QA models
    is still under-explored. In this work, we propose BMBI, an approach to mitigate
    the bias of multiple-choice QA models. Based on the intuition that a model would
    lean to be more biased if it learns from a biased example, we measure the bias
    level of a query instance by observing its influence on another instance. If the
    influenced instance is more biased, we derive that the query instance is biased.
    We then use the bias level detected as an optimization objective to form a multi-task
    learning setting in addition to the original QA task. We further introduce a new
    bias evaluation metric to quantify bias in a comprehensive and sensitive way.
    We show that our method could be applied to multiple QA formulations across multiple
    bias categories. It can significantly reduce the bias level in all 9 bias categories
    in the BBQ dataset while maintaining comparable QA accuracy.
  authors:
  - Mingyu Derek Ma
  - Jiun-yu Kao
  - Arpit Gupta
  - Yu-hsiang Lin
  - Wenbo Zhao
  - Tagyoung Chung
  - Kai-wei Chang
  - Nanyun Peng
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - TrustNLP
  forum: ''
  id: TrustNLP_59
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Mitigating Bias for Question Answering Models by Tracking Bias Influence
  tldr: 'Models of various NLP tasks have been shown to exhibit stereotypes, and the
    bias in the question answering (QA) models is especially harmful as the output
    answers might be directly consumed by the end users. There have been datasets
    to evaluate bias in QA models, while bias mitigation technique for '
  track: The Third Workshop on Trustworthy Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'A key component of modern conversational systems is the Dialogue State
    Tracker (or DST), which models a user''s goals and needs. Toward building more
    robust and reliable DSTs, we introduce a prompt-based learning approach to automatically
    generate effective adversarial examples to probe DST models. Two key characteristics
    of this approach are: (i) it only needs the output of the DST with no need for
    model parameters; and (ii) it can learn to generate natural language utterances
    that can target any DST. Through experiments over state-of-the-art DSTs, the proposed
    framework leads to the greatest reduction in accuracy and the best attack success
    rate while maintaining good fluency and low perturbation ratio. We also show how
    much the generated adversarial examples can bolster a DST through adversarial
    training. These results indicate the strength of prompt-based attacks on DSTs
    and leave open avenues for continued refinement.'
  authors:
  - Xiangjue Dong
  - Yun He
  - Ziwei Zhu
  - James Caverlee
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - TrustNLP
  forum: ''
  id: TrustNLP_60
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'PromptAttack: Probing Dialogue State Trackers with Adversarial Prompts'
  tldr: A key component of modern conversational systems is the Dialogue State Tracker
    (or DST), which models a user's goals and needs. Toward building more robust and
    reliable DSTs, we introduce a prompt-based learning approach to automatically
    generate effective adversarial examples to probe DST models. T
  track: The Third Workshop on Trustworthy Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Narratives include a rich source of events unfolding over time and context.
    Automatic understanding of these events provides a summarised comprehension of
    the narrative for further computation (such as reasoning). In this paper, we study
    the Information Status (IS) of the events and propose a novel challenging task:
    the automatic identification of new events in a narrative. We define an event
    as a triplet of subject, predicate, and object. The event is categorized as new
    with respect to the discourse context and whether it can be inferred through commonsense
    reasoning. We annotated a publicly available corpus of narratives with the new
    events at sentence level using human annotators. We present the annotation protocol
    and study the quality of the annotation and the difficulty of the task. We publish
    the annotated dataset, annotation materials, and machine learning baseline models
    for the task of new event extraction for narrative understanding.'
  authors:
  - Seyed Mahed Mousavi
  - Shohei Tanaka
  - Gabriel Roccabruna
  - Koichiro Yoshino
  - Satoshi Nakamura
  - Giuseppe Riccardi
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Narrative-Understanding
  forum: ''
  id: wnu2023_4
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: What's New? Identifying the Unfolding of New Events in a Narrative
  tldr: 'Narratives include a rich source of events unfolding over time and context.
    Automatic understanding of these events provides a summarised comprehension of
    the narrative for further computation (such as reasoning). In this paper, we study
    the Information Status (IS) of the events and propose a novel '
  track: The 5th Workshop on Narrative Understanding
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In recent years, there has been a growing scholarly interest in employing
    quantitative methods to analyze literary texts, as they offer unique insights,
    theories, and interpretations. In light of this, the current study employs quantitative
    analysis to examine the fiction written by the renowned British adventure novelist,
    Sir Henry Rider Haggard. Specifically, the study aims to investigate the affective
    content and prevalence of distinctive linguistic features in six of Haggard's
    most distinguished works. We evaluate dominant emotional states at the sentence
    level as well as investigate the deployment of specific linguistic features such
    as modifiers and deontic modals, and collocated terms. Through sentence-level
    emotion analysis the findings reveal a notable prevalence of "joy"-related emotions
    across the novels. Furthermore, the study observes that intensifiers are employed
    more commonly than the mitigators as modifiers and the collocated terms of modifiers
    exhibit high similarity across the novels. By integrating quantitative analyses
    with qualitative assessments, this study presents a novel perspective on the patterns
    of emotion and specialized grammatical features in some of Haggard's most celebrated
    literary works.
  authors:
  - Salim Sazzed
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Narrative-Understanding
  forum: ''
  id: wnu2023_5
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Emotion and Modifier in Henry Rider Haggard's Novels
  tldr: In recent years, there has been a growing scholarly interest in employing
    quantitative methods to analyze literary texts, as they offer unique insights,
    theories, and interpretations. In light of this, the current study employs quantitative
    analysis to examine the fiction written by the renowned Bri
  track: The 5th Workshop on Narrative Understanding
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Despite recent advances of AI, story understanding remains an open and
    under-investigated problem. We collect, preprocess, and publicly release a video-language
    story dataset, Synopses of Movie Narratives (SyMoN), containing 5,193 video summaries
    of popular movies and TV series with a total length of 869 hours. SyMoN captures
    naturalistic storytelling videos made by human creators and intended for a human
    audience. As a prototypical and naturalistic story dataset, SyMoN features high
    coverage of multimodal story events. Its use of storytelling techniques cause
    cross-domain semantic gaps that provide appropriate challenges to existing models.
    We establish benchmarks on video-text retrieval and zero-shot alignment on movie
    summary videos, which showcase the importance of in-domain data and long-term
    memory in story understanding. With SyMoN, we hope to lay the groundwork for progress
    in multimodal story understanding.
  authors:
  - Yidan Sun
  - Qin Chao
  - Yangfeng Ji
  - Boyang Li
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Narrative-Understanding
  forum: ''
  id: wnu2023_9
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Synopses of Movie Narratives: a Video-Language Dataset for Story Understanding'
  tldr: Despite recent advances of AI, story understanding remains an open and under-investigated
    problem. We collect, preprocess, and publicly release a video-language story dataset,
    Synopses of Movie Narratives (SyMoN), containing 5,193 video summaries of popular
    movies and TV series with a total length o
  track: The 5th Workshop on Narrative Understanding
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In this paper, we describe the problem of automatically evaluating quality
    of knowledge expressed in a non-fiction narrative text. We focus on a specific
    type of documents where each document describes a certain technical problem and
    its solution. The goal is not only to evaluate the quality of knowledge in such
    a document, but also to automatically suggest possible improvements to the writer
    so that a better knowledge-rich document is produced. We propose new evaluation
    metrics to evaluate quality of knowledge contents as well as flow of different
    types of sentences. The suggestions for improvement are generated based on these
    metrics. The proposed metrics are completely unsupervised in nature and they are
    derived from a set of simple corpus statistics. We demonstrate the effectiveness
    of the proposed metrics as compared to other existing baseline metrics in our
    experiments.
  authors:
  - Sachin Pawar
  - Girish Palshikar
  - Ankita Jain
  - Mahesh Singh
  - Mahesh Rangarajan
  - Aman Agarwal
  - Vishal Kumar
  - Karan Singh
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Narrative-Understanding
  forum: ''
  id: wnu2023_10
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Evaluation Metrics for Depth and Flow of Knowledge in Non-fiction Narrative
    Texts
  tldr: In this paper, we describe the problem of automatically evaluating quality
    of knowledge expressed in a non-fiction narrative text. We focus on a specific
    type of documents where each document describes a certain technical problem and
    its solution. The goal is not only to evaluate the quality of know
  track: The 5th Workshop on Narrative Understanding
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Predicting literary quality and reader appreciation of narrative texts
    are highly complex challenges in quantitative and computational literary studies
    due to the fluid definitions of quality and the vast feature space that can be
    considered when modeling a literary work. This paper investigates the potential
    of sentiment arcs combined with topical-semantic profiling of literary narratives
    as indicators for their literary quality. Our experiments focus on a large corpus
    of 19th and 20the century English language literary fiction, using GoodReads'
    ratings as an imperfect approximation of the diverse range of reader evaluations
    and preferences. By leveraging a stacked ensemble of regression models, we achieve
    a promising performance in predicting average readers' scores, indicating the
    potential of our approach in modeling literary quality.
  authors:
  - Pascale Moreira
  - Yuri Bizzoni
  - Kristoffer Nielbo
  - Ida Marie Lassen
  - Mads Thomsen
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Narrative-Understanding
  forum: ''
  id: wnu2023_11
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Modeling Readers' Appreciation of Literary Narratives Through Sentiment Arcs
    and Semantic Profiles
  tldr: Predicting literary quality and reader appreciation of narrative texts are
    highly complex challenges in quantitative and computational literary studies due
    to the fluid definitions of quality and the vast feature space that can be considered
    when modeling a literary work. This paper investigates the
  track: The 5th Workshop on Narrative Understanding
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Word category arcs measure the progression of word usage across a story.
    Previous work on arcs has explored structural and psycholinguistic arcs through
    the course of narratives, but so far it has been limited to \textbackslash{}textit\{English\}
    narratives and a narrow set of word categories covering binary emotions and cognitive
    processes. In this paper, we expand over previous work by (1) introducing a novel,
    general approach to quantitatively analyze word usage arcs for any word category
    through a combination of clustering and filtering; and (2) exploring narrative
    arcs in literature in eight different languages across multiple genres. Through
    multiple experiments and analyses, we quantify the nature of narratives across
    languages, corroborating existing work on monolingual narrative arcs as well as
    drawing new insights about the interpretation of arcs through correlation analyses.
  authors:
  - Winston Wu
  - Lu Wang
  - Rada Mihalcea
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Narrative-Understanding
  forum: ''
  id: wnu2023_12
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Word Category Arcs in Literature Across Languages and Genres
  tldr: Word category arcs measure the progression of word usage across a story. Previous
    work on arcs has explored structural and psycholinguistic arcs through the course
    of narratives, but so far it has been limited to \textbackslash{}textit\{English\}
    narratives and a narrow set of word categories coveri
  track: The 5th Workshop on Narrative Understanding
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper presents the Candide model as a computational architecture
    for modelling human-like, narrative-based language understanding. The model starts
    from the idea that narratives emerge through the process of interpreting novel
    linguistic observations, such as utterances, paragraphs and texts, with respect
    to previously acquired knowledge and beliefs. Narratives are personal, as they
    are rooted in past experiences, and constitute perspectives on the world that
    might motivate different interpretations of the same observations. Concretely,
    the Candide model operationalises this idea by dynamically modelling the belief
    systems and background knowledge of individual agents, updating these as new linguistic
    observations come in, and exposing them to a logic reasoning engine that reveals
    the possible sources of divergent interpretations. Apart from introducing the
    foundational ideas, we also present a proof-of-concept implementation that demonstrates
    the approach through a number of illustrative examples.
  authors:
  - Paul Van Eecke
  - Lara Verheyen
  - Tom Willaert
  - Katrien Beuls
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Narrative-Understanding
  forum: ''
  id: wnu2023_14
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'The Candide model: How narratives emerge where observations meet beliefs'
  tldr: This paper presents the Candide model as a computational architecture for
    modelling human-like, narrative-based language understanding. The model starts
    from the idea that narratives emerge through the process of interpreting novel
    linguistic observations, such as utterances, paragraphs and texts, w
  track: The 5th Workshop on Narrative Understanding
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In this position paper, we contend that advancing our understanding of
    narrative and the effective generation of longer, subjectively engaging texts
    is crucial for progress in modern Natural Language Processing (NLP) and potentially
    the broader field of Artificial Intelligence. We highlight the current lack of
    appropriate datasets, evaluation methods, and operational concepts necessary for
    initiating work on narrative processing.
  authors:
  - Ivan Yamshchikov
  - Alexey Tikhonov
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Narrative-Understanding
  forum: ''
  id: wnu2023_16
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: What is Wrong with Language Models that Can Not Tell a Story?
  tldr: In this position paper, we contend that advancing our understanding of narrative
    and the effective generation of longer, subjectively engaging texts is crucial
    for progress in modern Natural Language Processing (NLP) and potentially the broader
    field of Artificial Intelligence. We highlight the curr
  track: The 5th Workshop on Narrative Understanding
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Stories about everyday situations are an essential part of human communication,
    motivating the need to develop AI agents that can reliably understand these stories.
    Despite the long list of supervised methods for story completion and procedural
    understanding, current AI has no mechanisms to automatically track and explain
    procedures in unseen stories. To bridge this gap, we study the ability of AI models
    to transfer procedural knowledge to novel narrative tasks in a transparent manner.
    We design LEAP: a comprehensive framework that integrates state-of-the-art modeling
    architectures, training regimes, and augmentation strategies based on both natural
    and synthetic stories. To address the lack of densely annotated training data,
    we devise a robust automatic labeler based on few-shot prompting to enhance the
    augmented data. Our experiments with in- and out-of-domain tasks reveal insights
    into the interplay of different architectures, training regimes, and augmentation
    strategies. LEAP''s labeler has a clear positive impact on out-of-domain datasets,
    while the resulting dense annotation provides native explainability'
  authors:
  - Yifan Jiang
  - Filip Ilievski
  - Kaixin Ma
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Narrative-Understanding
  forum: ''
  id: wnu2023_17
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Transferring Procedural Knowledge across Commonsense Tasks
  tldr: Stories about everyday situations are an essential part of human communication,
    motivating the need to develop AI agents that can reliably understand these stories.
    Despite the long list of supervised methods for story completion and procedural
    understanding, current AI has no mechanisms to automati
  track: The 5th Workshop on Narrative Understanding
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This work explores the problem of generating task graphs of real-world
    activities. Different from prior formulations, we consider a setting where text
    transcripts of instructional videos performing a real-world activity (e.g., making
    coffee) are provided and the goal is to identify the key steps relevant to the
    task as well as the dependency relationship between these key steps. We propose
    a novel task graph generation approach that combines the reasoning capabilities
    of instruction-tuned language models along with clustering and ranking components
    to generate accurate task graphs in a completely unsupervised manner. We show
    that the proposed approach generates more accurate task graphs compared to a supervised
    learning approach on tasks from the Procel and CrossTask datasets.
  authors:
  - Lajanugen Logeswaran
  - Sungryull Sohn
  - Yunseok Jang
  - Moontae Lee
  - Honglak Lee
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Narrative-Understanding
  forum: ''
  id: wnu2023_18
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Unsupervised Task Graph Generation from Instructional Video Transcripts
  tldr: This work explores the problem of generating task graphs of real-world activities.
    Different from prior formulations, we consider a setting where text transcripts
    of instructional videos performing a real-world activity (e.g., making coffee)
    are provided and the goal is to identify the key steps rel
  track: The 5th Workshop on Narrative Understanding
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Sentence alignment -- establishing links between corresponding sentences
    in two relateddocuments -- is as important in paraphrase generation as it is in
    machine translation. Despite its applicability, its benefits are often overlooked
    in the context of narrative understanding. For instance, it can be leveraged in
    cross-lingual story analysis and cultural analytics. This includes identifying
    similarities and differences between narratives across languages, or understanding
    narrative structures in a more comprehensive way. To bridge this gap, we introduce
    a novel methodology for sentence alignment designed specifically for novels. In
    particular, we propose Novalign, an end-to-end, fully-neural architecture that
    maps source and target sentences based on their contextualized sentence embeddings.
    We extensively evaluate Novalign on a new, multilingual dataset derived from the
    Opus project consisting of 20 language pairs, and demonstrate that our model achieves
    state-of-the-art performance. To ensure reproducibility, we release our code and
    model checkpoints at omitted.link.
  authors:
  - Francesco Molfese
  - Andrei Stefan Bejgu
  - Simone Tedeschi
  - Roberto Navigli
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Narrative-Understanding
  forum: ''
  id: wnu2023_19
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Novalign: Neural Cross-Lingual Sentence Alignment for Novels'
  tldr: Sentence alignment -- establishing links between corresponding sentences in
    two relateddocuments -- is as important in paraphrase generation as it is in machine
    translation. Despite its applicability, its benefits are often overlooked in the
    context of narrative understanding. For instance, it can b
  track: The 5th Workshop on Narrative Understanding
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Understanding the settings of a given story has long been viewed as an
    essential component of understanding the story at large. This significance is
    not only underscored in academic literary analysis but also in kindergarten education.
    However, despite this significance, it has received relatively little attention
    regarding computational analyses of stories. This paper presents a dataset of
    2,302 time period setting labeled works and 6,991 location setting labeled works.
    This dataset aims to help with Cultural Analytics of literary works but may also
    aid in time-period-related questions within literary Q\textbackslash{}\&A systems.
  authors:
  - Kaley Rittichier
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Narrative-Understanding
  forum: ''
  id: wnu2023_22
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Story Settings: A Dataset'
  tldr: Understanding the settings of a given story has long been viewed as an essential
    component of understanding the story at large. This significance is not only underscored
    in academic literary analysis but also in kindergarten education. However, despite
    this significance, it has received relatively l
  track: The 5th Workshop on Narrative Understanding
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Capturing readers' engagement in fiction is a challenging but important
    aspect of narrative understanding. In this study, we collected 23 readers' reactions
    to 2 short stories through eye tracking, sentence-level annotations, and an overall
    engagement scale survey. We analyzed the significance of various qualities of
    the text in predicting how engaging a reader is likely to find it. As enjoyment
    of fiction is highly contextual, we also investigated individual differences in
    our data. Furthering our understanding of what captivates readers in fiction will
    help better inform models used in creative narrative generation and collaborative
    writing tools.
  authors:
  - Rose Neis
  - Karin De Langis
  - Zae Myung Kim
  - Dongyeop Kang
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Narrative-Understanding
  forum: ''
  id: wnu2023_23
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: An Analysis of Reader Engagement in Literary Fiction through Eye Tracking
    and Linguistic Features
  tldr: Capturing readers' engagement in fiction is a challenging but important aspect
    of narrative understanding. In this study, we collected 23 readers' reactions
    to 2 short stories through eye tracking, sentence-level annotations, and an overall
    engagement scale survey. We analyzed the significance of va
  track: The 5th Workshop on Narrative Understanding
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Animate entities in narrative comics stories are expressed through a
    number of visual representations across panels. Identifying these entities is
    necessary for recognizing characters and analysing narrative affordances unique
    to comics, and integrating these with linguistic reference annotation, however
    an annotation process for animate entity identification has not received adequate
    attention. This research explores methods for identifying animate entities visually
    in comics using annotation experiments. Two rounds of inter-annotator agreement
    experiments are run: the first asks annotators to outline areas on comic pages
    using a Polygon segmentation tool, and the second prompts annotators to assign
    each outlined entity''s animacy type to derive a quantitative measure of agreement.
    The first experiment results show that Polygon-based outlines successfully produce
    a qualitative measure of agreement; the second experiment supports that animacy
    status is best conceptualised as a graded, rather than binary, concept.'
  authors:
  - Lauren Edlin
  - Joshua Reiss
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Narrative-Understanding
  forum: ''
  id: wnu2023_25
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Identifying Visual Depictions of Animate Entities in Narrative Comics: An
    Annotation Study'
  tldr: Animate entities in narrative comics stories are expressed through a number
    of visual representations across panels. Identifying these entities is necessary
    for recognizing characters and analysing narrative affordances unique to comics,
    and integrating these with linguistic reference annotation, ho
  track: The 5th Workshop on Narrative Understanding
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper proposes a sentiment-centric pipeline to perform unsupervised
    plot extraction on non-linear novels like Virginia Woolf's Mrs. Dalloway, a novel
    widely considered to be "plotless. Combining transformer-based sentiment analysis
    models with statistical testing, we model sentiment's rate-of-change and correspondingly
    segment the novel into emotionally self-contained units qualitatively evaluated
    to be meaningful surrogate pseudo-chapters. We validate our findings by evaluating
    our pipeline as a fully unsupervised text segmentation model, achieving a F-1
    score of 0.643 (regional) and 0.214 (exact) in chapter break prediction on a validation
    set of linear novels with existing chapter structures. In addition, we observe
    notable differences between the distributions of predicted chapter lengths in
    linear and non-linear fictional narratives, with the latter exhibiting significantly
    greater variability. Our results hold significance for narrative researchers appraising
    methods for extracting plots from non-linear novels.
  authors:
  - Peiqi Sui
  - Lin Wang
  - Sil Hamilton
  - Thorsten Ries
  - Kelvin Wong
  - Stephen Wong
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Narrative-Understanding
  forum: ''
  id: wnu2023_26
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Mrs. Dalloway Said She Would Segment the Chapters Herself
  tldr: This paper proposes a sentiment-centric pipeline to perform unsupervised plot
    extraction on non-linear novels like Virginia Woolf's Mrs. Dalloway, a novel widely
    considered to be "plotless. Combining transformer-based sentiment analysis models
    with statistical testing, we model sentiment's rate-of-c
  track: The 5th Workshop on Narrative Understanding
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Although psycholinguists and psychologists have long studied the tendency
    of linguistic strings to evoke mental images in hearers or readers, most computational
    studies have applied this concept of imageability only to isolated words. Using
    recent developments in text-to-image generation models, such as DALLE mini, we
    propose computational methods that use generated images to measure the imageability
    of both single English words and connected text. We sample text prompts for image
    generation from three corpora: human-generated image captions, news article sentences,
    and poem lines. We subject these prompts to different deformances to examine the
    model''s ability to detect changes in imageability caused by compositional change.
    We find high correlation between the proposed computational measures of imageability
    and human judgments of individual words. We also find the proposed measures more
    consistently respond to changes in compositionality than baseline approaches.
    We discuss possible effects of model training and implications for the study of
    compositionality in text-to-image models.'
  authors:
  - Si Wu
  - David Smith
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Narrative-Understanding
  forum: ''
  id: wnu2023_27
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Composition and Deformance:  Measuring Imageability with a Text-to-Image
    Model'
  tldr: Although psycholinguists and psychologists have long studied the tendency
    of linguistic strings to evoke mental images in hearers or readers, most computational
    studies have applied this concept of imageability only to isolated words. Using
    recent developments in text-to-image generation models, suc
  track: The 5th Workshop on Narrative Understanding
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We present a new dataset for studying conversation disentanglement in
    movies and TV series. While previous work has focused on conversation disentanglement
    in IRC chatroom dialogues, movies and TV shows provide a space for studying complex
    pragmatic patterns of floor and topic change in face-to-face multi-party interactions.
    In this work, we draw on theoretical research in sociolinguistics, sociology,
    and film studies to operationalize a conversational thread (including the notion
    of a floor change) in dramatic texts, and use that definition to annotate a dataset
    of 10,033 dialogue turns (comprising 2,209 threads) from 831 movies. We compare
    the performance of several disentanglement models on this dramatic dataset, and
    apply the best-performing model to disentangle 808 movies. We see that, contrary
    to expectation, average thread lengths do not decrease significantly over the
    past 40 years, and characters portrayed by actors who are women, while underrepresented,
    initiate more new conversational threads relative to their speaking time.
  authors:
  - Kent Chang
  - Danica Chen
  - David Bamman
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Narrative-Understanding
  forum: ''
  id: wnu2023_29
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Dramatic Conversation Disentanglement
  tldr: We present a new dataset for studying conversation disentanglement in movies
    and TV series. While previous work has focused on conversation disentanglement
    in IRC chatroom dialogues, movies and TV shows provide a space for studying complex
    pragmatic patterns of floor and topic change in face-to-face
  track: The 5th Workshop on Narrative Understanding
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'In recent years, research in text summarization has mainly focused on
    the news domain,where texts are typically short and have strong layout features.
    The task of full-book summarization presents additional challenges which are hard
    to tackle with current resources, due to their limited size and availability in
    English only. To overcome these limitations, we present "Echoes from Alexandria,
    or in shortened form, "Echoes", a large resource for multilingual book summarization.
    Echoes features three novel datasets: i) Echo-Wiki, for multilingual book summarization,
    ii) Echo-XSum,for extremely-compressive multilingual book summarization, and iii)
    Echo-FairySum, for extractive book summarization. To the best of our knowledge,
    Echoes  with its thousands of books and summaries  is the largest resource, and
    the first to be multilingual, featuring 5 languages and 25 language pairs. In
    addition to Echoes, we also introduce a newextractive-then-abstractive baseline,
    and, supported by our experimental results and manual analysis of the summaries
    generated, we argue that this baseline is more suitable for book summarization
    than purely-abstractive approaches. We release our resource and software at https://github.com/Babelscape/echoes-from-alexandria
    in the hope of fostering innovative research in multilingual book summarization.'
  authors:
  - Alessandro Scir
  - Simone Conia
  - Simone Ciciliano
  - Roberto Navigli
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Narrative-Understanding
  forum: ''
  id: wnu2023_31
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Echoes from Alexandria: A Large Resource for Multilingual Book Summarization'
  tldr: In recent years, research in text summarization has mainly focused on the
    news domain,where texts are typically short and have strong layout features. The
    task of full-book summarization presents additional challenges which are hard
    to tackle with current resources, due to their limited size and ava
  track: The 5th Workshop on Narrative Understanding
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We present a novel approach to modeling narratives using narrative chain
    embeddings.A new dataset of narrative chains extracted from German news texts
    is presented.With neural methods, we produce models for both German and English
    that achieve state-of-the-art performance on the Multiple Choice Narrative Cloze
    task.Subsequently, we perform an extrinsic evaluation of the embeddings our models
    produce and show that they perform rather poorly in identifying narratively similar
    texts. We explore some of the reasons for this underperformance and discuss the
    upsides of our approach.We provide an outlook on alternative ways to model narratives,
    as well as techniques for evaluating such models.
  authors:
  - Hans Ole Hatzel
  - Chris Biemann
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - Narrative-Understanding
  forum: ''
  id: wnu2023_32
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Narrative Cloze as a Training Objective: Towards Modeling Stories Using
    Narrative Chain Embeddings'
  tldr: We present a novel approach to modeling narratives using narrative chain embeddings.A
    new dataset of narrative chains extracted from German news texts is presented.With
    neural methods, we produce models for both German and English that achieve state-of-the-art
    performance on the Multiple Choice Narr
  track: The 5th Workshop on Narrative Understanding
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Natural language processing is largely focused on written text processing.
    However, many computational linguists tacitly endorse myths about the nature of
    writing. We highlight two of these myths---the conflation of language and writing,
    and the notion that Chinese, Japanese, and Korean writing is ideographic---and
    suggest how the community can dispel them.
  authors:
  - Kyle Gorman
  - Richard Sproat
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - CAWL
  forum: ''
  id: CAWL_20
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Myths about Writing Systems in Speech & Language Technology
  tldr: Natural language processing is largely focused on written text processing.
    However, many computational linguists tacitly endorse myths about the nature of
    writing. We highlight two of these myths---the conflation of language and writing,
    and the notion that Chinese, Japanese, and Korean writing is i
  track: The Workshop on Computation and Written Language (CAWL)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: To gain a better understanding of the linguistic information encoded in
    character-based language models, we probe the multilingual contextual CANINE model.
    We design a range of phonetic probing tasks in six Nordic languages, including
    Faroese as an additional zero-shot instance. We observe that some phonetic information
    is indeed encoded in the character representations, as consonants and vowels can
    be well distinguished using a linear classifier. Furthermore, results for the
    Danish and Norwegian language seem to be worse for the consonant/vowel distinction
    in comparison to other languages. The information encoded in these representations
    can also be learned in a zero-shot scenario, as Faroese shows a reasonably good
    performance in the same vowel/consonant distinction task.
  authors:
  - Manex Agirrezabal
  - Sidsel Boldsen
  - Nora Hollenstein
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - CAWL
  forum: ''
  id: CAWL_8
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'The Hidden Folk: Linguistic Properties Encoded in Multilingual Contextual
    Character Representations'
  tldr: To gain a better understanding of the linguistic information encoded in character-based
    language models, we probe the multilingual contextual CANINE model. We design
    a range of phonetic probing tasks in six Nordic languages, including Faroese as
    an additional zero-shot instance. We observe that some
  track: The Workshop on Computation and Written Language (CAWL)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: "Handwritten texts produced by young learners often contain orthographic\
    \ features like spelling errors, capitalization errors, punctuation mistakes,\
    \ and impurities such as strikethrough, inserts, and smudges that are typically\
    \ normalized or ignored in existing transcriptions. For applications like handwriting\
    \ recognition with the goal of automatically analyzing a learner's language performance,\
    \ however, retaining such features would be necessary.\nTo address this, we present\
    \ transcription guidelines that retain the features addressed above. \nOur guidelines\
    \ were developed iteratively and include numerous example images to illustrate\
    \ the various issues.\nOn a subset of about 90 double-transcribed texts, we compute\
    \ inter-annotator agreement and show that our guidelines can be applied with high\
    \ levels of percentage agreement of about .98.\nOverall, we transcribed 1,350\
    \ learner texts, which is about the same size as the widely adopted handwriting\
    \ recognition datasets IAM (1,500 pages) and CVL (1,600 pages).\nOur final corpus\
    \ can be used to train a handwriting recognition system that transcribes closely\
    \ to the real productions by young learners.\nSuch a system is a prerequisite\
    \ for applying automatic orthography feedback systems to handwritten texts in\
    \ the future."
  authors:
  - Christian Gold
  - Ronja Laarmann-quante
  - Torsten Zesch
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - CAWL
  forum: ''
  id: CAWL_13
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Preserving the Authenticity of Handwritten Learner Language: Annotation
    Guidelines for Creating Transcripts Retaining Orthographic Features'
  tldr: Handwritten texts produced by young learners often contain orthographic features
    like spelling errors, capitalization errors, punctuation mistakes, and impurities
    such as strikethrough, inserts, and smudges that are typically normalized or ignored
    in existing transcriptions. For applications like ha
  track: The Workshop on Computation and Written Language (CAWL)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Maltese is a low-resource language of Arabic and Romance origins written
    in Latin script. We explore the impact of transliterating Maltese into Arabic
    script on a number of downstream tasks. We compare multiple transliteration pipelines
    ranging from simple one-to-one character maps to more sophisticated alternatives
    that explore multiple possibilities or make use of manual linguistic annotations.
    We show that the sophisticated systems are consistently better than simpler systems,
    quantitatively and qualitatively. We also show transliterating Maltese can be
    considered as an option to improve the cross-lingual transfer capabilities.
  authors:
  - Kurt Micallef
  - Fadhl Eryani
  - Nizar Habash
  - Houda Bouamor
  - Claudia Borg
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - CAWL
  forum: ''
  id: CAWL_14
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Exploring the Impact of Transliteration on NLP Performance for Low-Resource
    Languages: The Case of Maltese and Arabic'
  tldr: Maltese is a low-resource language of Arabic and Romance origins written in
    Latin script. We explore the impact of transliterating Maltese into Arabic script
    on a number of downstream tasks. We compare multiple transliteration pipelines
    ranging from simple one-to-one character maps to more sophistic
  track: The Workshop on Computation and Written Language (CAWL)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We examine the task of distinguishing between Hindi and Urdu when those
    languages are romanized, i.e., written in the Latin script.  Both languages are
    widely informally romanized, and to the extent that they are identified in the
    Latin script by language identification systems, they are typically conflated.  In
    the absence of large labeled collections of such text, we consider methods for
    generating training data. Beginning with a small set of seed words, each of which
    are strongly indicative of one of the languages versus the other, we prompt a
    pretrained large language model (LLM) to generate romanized text.  Treating text
    generated from an Urdu prompt as one class and text generated from a Hindi prompt
    as the other class, we build a binary language identification (LangID) classifier.  We
    demonstrate that the resulting classifier distinguishes manually romanized Urdu
    Wikipedia text from manually romanized Hindi Wikipedia text far better than chance.  We
    use this classifier to estimate the prevalence of Urdu in a large collection of
    text labeled as romanized Hindi that has been used to train large language models.
    These techniques can be applied to bootstrap classifiers in other cases where
    a dataset is known to contain multiple distinct but related classes, such as different
    dialects of the same language, but for which labels cannot easily be obtained.
  authors:
  - Elizabeth Nielsen
  - Christo Kirov
  - Brian Roark
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - CAWL
  forum: ''
  id: CAWL_9
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Distinguishing Romanized Hindi from Romanized Urdu
  tldr: We examine the task of distinguishing between Hindi and Urdu when those languages
    are romanized, i.e., written in the Latin script.  Both languages are widely informally
    romanized, and to the extent that they are identified in the Latin script by language
    identification systems, they are typically c
  track: The Workshop on Computation and Written Language (CAWL)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'We propose methods for transliterating English loanwords in Japanese
    from their Japanese written form (katakana/romaji) to their original English written
    form. Our data is a Japanese-English loanwords dictionary that we have created
    ourselves. We employ two approaches: the direct transliteration, which directly
    converts words from katakana to English, and the indirect transliteration, which
    utilizes the English pronunciation as an intermediate step. Additionally, we compare
    the effectiveness of using katakana versus romaji as input characters. We develop
    6 models of 2 types for our experiments: one with an English lexicon-filter, and
    the other without. For each type, we built 3 models, including a pair n-gram based
    on WFSTs and two sequence-to-sequence models leveraging LSTM and transformer.
    Our best performing model was the pair n-gram model with a lexicon-filter, directly
    transliterating from katakana to English.'
  authors:
  - Yuying Ren
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - CAWL
  forum: ''
  id: CAWL_10
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Back-Transliteration of English Loanwords in Japanese
  tldr: 'We propose methods for transliterating English loanwords in Japanese from
    their Japanese written form (katakana/romaji) to their original English written
    form. Our data is a Japanese-English loanwords dictionary that we have created
    ourselves. We employ two approaches: the direct transliteration, wh'
  track: The Workshop on Computation and Written Language (CAWL)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Japanese writing is a complex system, and a large part of the complexity
    resides in the use of kanji. A single kanji character in modern Japanese may have
    multiple pronunciations, either as native vocabulary or as words borrowed from
    Chinese. This causes a problem for text-to-speech synthesis (TTS) because the
    system has to predict which pronunciation of each kanji character is appropriate
    in the context. The problem is called homograph disambiguation. To solve the problem,
    this research provides a new annotated Japanese single kanji character pronunciation
    data set and describes an experiment using the logistic regression (LR) classifier.
    A baseline is computed to compare with the LR classifier accuracy. This experiment
    provides the first experimental research in Japanese single kanji homograph disambiguation.
    The annotated Japanese data is freely released to the public to support further
    work.
  authors:
  - Wen Zhang
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - CAWL
  forum: ''
  id: CAWL_12
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Pronunciation Ambiguities in Japanese Kanji
  tldr: 'Japanese writing is a complex system, and a large part of the complexity
    resides in the use of kanji. A single kanji character in modern Japanese may have
    multiple pronunciations, either as native vocabulary or as words borrowed from
    Chinese. This causes a problem for text-to-speech synthesis (TTS) '
  track: The Workshop on Computation and Written Language (CAWL)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: "Word error rate (WER) and character error rate (CER) are standard metrics\
    \ in\nSpeech Recognition (ASR), but one problem has always been alternative spellings:\
    \ If one's system transcribes adviser whereas the ground truth has advisor, this\
    \ will count as an error even though the two spellings really represent the same\
    \ word.\n\nJapanese is notorious for \"lacking orthography\u201D: most words can\
    \ be spelled in multiple ways, presenting a problem for accurate ASR evaluation.\
    \ In this paper we propose a new lenient evaluation metric as a more defensible\
    \ CER measure for Japanese ASR. We create a lattice of plausible respellings of\
    \ the reference transcription, using a combination of lexical resources, a Japanese\
    \ text-processing system, and a neural machine translation model for reconstructing\
    \ kanji from hiragana or katakana. In a\nmanual evaluation, raters rated 95.4\\\
    % of the proposed spelling variants as plausible. ASR results show that our method,\
    \ which does not penalize the system for choosing a valid alternate spelling of\
    \ a word, affords a 2.4\\%\u20133.1\\% absolute reduction in CER depending on\
    \ the task."
  authors:
  - Shigeki Karita
  - Richard Sproat
  - Haruko Ishikawa
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - CAWL
  forum: ''
  id: CAWL_4
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Lenient Evaluation of Japanese Speech Recognition: Modeling Naturally Occurring
    Spelling Inconsistency'
  tldr: 'Word error rate (WER) and character error rate (CER) are standard metrics
    in

    Speech Recognition (ASR), but one problem has always been alternative spellings:
    If one''s system transcribes adviser whereas the ground truth has advisor, this
    will count as an error even though the two spellings really rep'
  track: The Workshop on Computation and Written Language (CAWL)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: A numeration system encodes abstract numeric quantities as concrete strings
    of written characters. The numeration systems used by modern scripts tend to be
    precise and unambiguous, but this was not so for the ancient and partially-deciphered
    proto-Elamite (PE) script, where written numerals can have up to four distinct
    readings depending on the system that is used to read them. We consider the task
    of disambiguating between these readings in order to determine the values of the
    numeric quantities recorded in this corpus. We contribute an automated conversion
    from PE notation to modern Hindu-Arabic notation, as well as two disambiguation
    techniques based on structural properties of the original documents and classifiers
    learned with the bootstrapping algorithm. We also contribute a test set for evaluating
    disambiguation techniques, as well as a novel approach to cautious rule selection
    for bootstrapped classifiers. Our analysis confirms existing intuitions about
    this script and reveals previously-unknown correlations between tablet content
    and numeral magnitude. This work is crucial to understanding and deciphering PE,
    as the corpus is heavily accounting-focused and contains many more numeric tokens
    than tokens of text.
  authors:
  - Logan Born
  - M. Willis Monroe
  - Kathryn Kelley
  - Anoop Sarkar
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - CAWL
  forum: ''
  id: CAWL_7
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Disambiguating Numeral Sequences to Decipher Ancient Accounting Corpora
  tldr: A numeration system encodes abstract numeric quantities as concrete strings
    of written characters. The numeration systems used by modern scripts tend to be
    precise and unambiguous, but this was not so for the ancient and partially-deciphered
    proto-Elamite (PE) script, where written numerals can have
  track: The Workshop on Computation and Written Language (CAWL)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper presents a new approach to the ancient scripts decipherment
    problem based on combinatorial optimisation and coupled simulated annealing. The
    proposed system is able to produce enhanced results in cognate identification
    when compared to the state-of-the-art systems on standard evaluation benchmarks
    used in literature.
  authors:
  - Fabio Tamburini
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - CAWL
  forum: ''
  id: CAWL_5
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Decipherment of Lost Ancient Scripts as Combinatorial Optimisation Using
    Coupled Simulated Annealing
  tldr: This paper presents a new approach to the ancient scripts decipherment problem
    based on combinatorial optimisation and coupled simulated annealing. The proposed
    system is able to produce enhanced results in cognate identification when compared
    to the state-of-the-art systems on standard evaluation b
  track: The Workshop on Computation and Written Language (CAWL)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: A crucial step in deciphering a text is to identify what set of characters
    were used to write it. This requires grouping character tokens according to visual
    and contextual features, which can be challenging for human analysts when the
    number of tokens or underlying types is large. Prior work has shown that this
    process can be automated by clustering dense representations of character images,
    in a task which we call ``script clustering''. In this work, we present novel
    architectures which exploit varying degrees of contextual and visual information
    to learn representations for use in script clustering. We evaluate on a range
    of modern and ancient scripts, and find that our models produce representations
    which are more effective for script recovery than the current state-of-the-art,
    despite using just \textasciitilde{}2\textbackslash{}\% as many parameters. Our
    analysis fruitfully applies these models to assess hypotheses about the character
    inventory of the partially-deciphered proto-Elamite script.
  authors:
  - Logan Born
  - M. Willis Monroe
  - Kathryn Kelley
  - Anoop Sarkar
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - CAWL
  forum: ''
  id: CAWL_6
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Learning the Character Inventories of Undeciphered Scripts Using Unsupervised
    Deep Clustering
  tldr: A crucial step in deciphering a text is to identify what set of characters
    were used to write it. This requires grouping character tokens according to visual
    and contextual features, which can be challenging for human analysts when the
    number of tokens or underlying types is large. Prior work has sh
  track: The Workshop on Computation and Written Language (CAWL)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Writing systems have traditionally been classified by whether they prioritize
    encoding phonological information (phonographic) versus morphological or semantic
    information (logographic). Recent work has broached the question of how membership
    in these categories can be quantified. Sproat and Gutkin (2021) proposed a range
    of metrics by which degree of logography can be quantified, including mutual information
    and a metric based on contextual attention required by a sequence-to-sequence
    RNN that maps pronunciations to spellings. We aim to build on this work by treating
    a definition of logography which, in contrast to the definition used by Sproat
    and Gutkin, more directly incorporates morphological identity. We compare mutual
    information between graphic forms and phonological forms and between graphic forms
    and morphological identity for written Japanese and Sumerian. Our results suggest
    that our methods present a promising means of classifying the degree to which
    a writing system is logographic or phonographic.
  authors:
  - Noah Hermalin
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - CAWL
  forum: ''
  id: CAWL_18
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: A Mutual Information-based Approach to Quantifying Logography in Japanese
    and Sumerian
  tldr: 'Writing systems have traditionally been classified by whether they prioritize
    encoding phonological information (phonographic) versus morphological or semantic
    information (logographic). Recent work has broached the question of how membership
    in these categories can be quantified. Sproat and Gutkin '
  track: The Workshop on Computation and Written Language (CAWL)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Past research has identified a rich set of handcrafted linguistic features
    that can potentially assist various tasks. However, their extensive number makes
    it difficult to effectively select and utilize existing handcrafted features.
    Coupled with the problem of inconsistent implementation across research works,
    there has been no categorization scheme or generally-accepted feature names. This
    creates unwanted confusion. Also, no actively-maintained open-source library extracts
    a wide variety of handcrafted features. The current handcrafted feature extraction
    practices have several inefficiencies, and a researcher often has to build such
    an extraction system from the ground up. We collect and categorize more than 220
    popular handcrafted features grounded on past literature. Then, we conduct a correlation
    analysis study on several task-specific datasets and report the potential use
    cases of each feature. Lastly, we devise a multilingual handcrafted linguistic
    feature extraction system in a systematically expandable manner. We open-source
    our system to give the community a rich set of pre-implemented handcrafted features.
  authors:
  - Bruce W. Lee
  - Jason Lee
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_3
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'LFTK: Handcrafted Features in Computational Linguistics'
  tldr: Past research has identified a rich set of handcrafted linguistic features
    that can potentially assist various tasks. However, their extensive number makes
    it difficult to effectively select and utilize existing handcrafted features.
    Coupled with the problem of inconsistent implementation across res
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Large language models can solve reasoning tasks (like math problems) more
    effectively when they are allowed to generate rationales. However, a good tutoring
    system should not just generate solutions, but should also generate explanations
    and should be able to correct and guide students. We show that providing a code
    scratchpad improves performance on each tutoring step with a gradeschool mathematics
    dataset. On these tutoring tasks, GPT-3 models provided with a code scratchpad
    significantly outperform those given only a language scratchpad (77.7\%  vs 48.7\%
    cumulative accuracy).
  authors:
  - Shriyash Upadhyay
  - Etan Ginsberg
  - Chris Callison-Burch
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_4
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Improving Mathematics Tutoring With A Code Scratchpad
  tldr: Large language models can solve reasoning tasks (like math problems) more
    effectively when they are allowed to generate rationales. However, a good tutoring
    system should not just generate solutions, but should also generate explanations
    and should be able to correct and guide students. We show that
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Effective human learning depends on a wide selection of educational materials
    that align with the learner''s current understanding of the topic. While the Internet
    has revolutionized human learning or education, a substantial resource accessibility
    barrier still exists. Namely, the excess of online information can make it challenging
    to navigate and discover high-quality learning materials in a given subject area.
    In this paper, we propose an automatic pipeline for building an educational resource
    discovery system for new domains. The pipeline consists of three main steps: resource
    searching, feature extraction, and resource classification. We first collect frequent
    queries from a set of seed documents, and search the web with these queries to
    obtain candidate resources such as lecture slides and introductory blog posts.
    Then, we process these resources for BERT-based features and meta-features. Next,
    we train a tree-based classifier to decide whether they are suitable learning
    materials. The pipeline achieves F1 scores of 0.94 and 0.82 when evaluated on
    two similar but novel domains. Finally, we demonstrate how this pipeline can benefit
    two applications: prerequisite chain learning and leading paragraph generation
    for surveys. We also release a corpus of 39,728 manually labeled web resources
    and 659 queries from NLP, Computer Vision (CV), and Statistics (STATS).'
  authors:
  - Irene Li
  - Thomas George
  - Alex Fabbri
  - Tammy Liao
  - Benjamin Chen
  - Rina Kawamura
  - Richard Zhou
  - Vanessa Yan
  - Swapnil Hingmire
  - Dragomir Radev
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_6
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: A Transfer Learning Pipeline for Educational Resource Discovery with Application
    in Survey Generation
  tldr: Effective human learning depends on a wide selection of educational materials
    that align with the learner's current understanding of the topic. While the Internet
    has revolutionized human learning or education, a substantial resource accessibility
    barrier still exists. Namely, the excess of online i
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Single Choice exercises constitute a central exercise type for language
    learning in a learner's progression from mere implicit exposure through input
    enhancement to productive language use in open exercises. Distractors that support
    learning in the individual zone of proximal development should not be derived
    from static analyses of learner corpora, but rely on dynamic learning analytics
    based on half-open exercises. We demonstrate how a system's error diagnosis module
    can be re-used for automatic and dynamic generation and adaptation of distractors,
    as well as to inform exercise generation in terms of relevant learning goals and
    reasonable chunking in Jumbled Sentences exercises.
  authors:
  - Tanja Heck
  - Detmar Meurers
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_7
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Using Learning Analytics for Adaptive Exercise Generation
  tldr: 'Single Choice exercises constitute a central exercise type for language learning
    in a learner''s progression from mere implicit exposure through input enhancement
    to productive language use in open exercises. Distractors that support learning
    in the individual zone of proximal development should not '
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Large Language Models (LLMs) offer novel opportunities for educational
    applications that have the potential to transform traditional learning for students.
    Despite AI-enhanced applications having the potential to provide personalized
    learning experiences, more studies are needed on the design of generative AI systems
    and evidence for using them in real educational settings. In this paper, we design,
    implement and evaluate \textbackslash{}texttt\{Reviewriter\}, a novel tool to
    provide students with AI-generated instructions for writing peer reviews in German.
    Our study identifies three key aspects: a) we provide insights into student needs
    when writing peer reviews with generative models which we then use to develop
    a novel system to provide adaptive instructions b) we fine-tune three German language
    models on a selected corpus of 11,925 student-written peer review texts in German
    and choose German-GPT2 based on quantitative measures and human evaluation, and
    c) we evaluate our tool with fourteen students, revealing positive technology
    acceptance based on quantitative measures. Additionally, the qualitative feedback
    presents the benefits and limitations of generative AI in peer review writing.'
  authors:
  - Xiaotian Su
  - Thiemo Wambsganss
  - Roman Rietsche
  - Seyed Parsa Neshaei
  - Tanja Kser
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_8
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Reviewriter: AI-Generated Instructions For Peer Review Writing'
  tldr: Large Language Models (LLMs) offer novel opportunities for educational applications
    that have the potential to transform traditional learning for students. Despite
    AI-enhanced applications having the potential to provide personalized learning
    experiences, more studies are needed on the design of gen
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We introduce the Korean-Learner-Morpheme (KLM) corpus, a manually annotated
    dataset consisting of 129,784 morphemes from second language (L2) learners of
    Korean, featuring morpheme tokenization and part-of-speech (POS) tagging. We evaluate
    the performance of four Korean morphological analyzers in tokenization and POS
    tagging on the L2- Korean corpus. Results highlight the analyzers' reduced performance
    on L2 data, indicating the limitation of advanced deep-learning models when dealing
    with L2-Korean corpora. We further show that fine-tuning one of the models with
    the KLM corpus improves its accuracy of tokenization and POS tagging on L2-Korean
    dataset.
  authors:
  - Hakyung Sung
  - Gyu-Ho Shin
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_9
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Towards L2-friendly pipelines for learner corpora: A case of written production
    by L2-Korean learners'
  tldr: We introduce the Korean-Learner-Morpheme (KLM) corpus, a manually annotated
    dataset consisting of 129,784 morphemes from second language (L2) learners of
    Korean, featuring morpheme tokenization and part-of-speech (POS) tagging. We evaluate
    the performance of four Korean morphological analyzers in to
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The increasing use of AI chatbots as conversation partners for second-language
    learners highlights the importance of providing effective feedback. To ensure
    a successful learning experience, it is essential for researchers and practitioners
    to understand the optimal timing, methods of delivery, and types of feedback that
    are most beneficial to learners. Synchronous grammar corrective feedback (CF)
    has been shown to be more effective than asynchronous methods in online writing
    tasks. Additionally, self-correction by language learners has proven more beneficial
    than teacher-provided correction, particularly for spoken language skills and
    non-novice learners. However, existing language-learning AI chatbots often lack
    synchronous CF and self-correction capabilities. To address this, we propose a
    synchronous conversational corrective feedback (CCF) method, which allows self-correction
    and provides metalinguistic explanations (ME). Our study suggests that in chatbot-driven
    language-learning tools, corrective feedback is more effectively delivered through
    means other than the social chatbot, such as a GUI interface. Furthermore, we
    found that guided self-correction offers a superior learning experience compared
    to providing explicit corrections, particularly for learners with high learning
    motivation or lower linguistic ability.
  authors:
  - Kai-Hui Liang
  - Sam Davidson
  - Xun Yuan
  - Shehan Panditharatne
  - Chun-Yen Chen
  - Ryan Shea
  - Derek Pham
  - Yinghua Tan
  - Erik Voss
  - Luke Fryer
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_10
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'ChatBack: Investigating Methods of Providing Grammatical Error Feedback
    in a GUI-based Language Learning Chatbot'
  tldr: 'The increasing use of AI chatbots as conversation partners for second-language
    learners highlights the importance of providing effective feedback. To ensure
    a successful learning experience, it is essential for researchers and practitioners
    to understand the optimal timing, methods of delivery, and '
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: As the world regains its footing following the COVID-19 pandemic, academia
    is striving to consolidate the gains made in students' education experience. New
    technologies such as video-based learning have shown some early improvement in
    student learning and engagement. In this paper, we present ORBITS predictive engine
    at YOURIKA company, a video-based student support platform powered by knowledge
    tracing. In an exploratory case study of one master's level Speech Processing
    course at the Mohamed Bin Zayed University of Artificial Intelligence (MBZUAI)
    in Abu Dhabi, half the students used the system while the other half did not.
    Student qualitative feedback was universally positive and compared the system
    favorably against current available methods. These findings support the use of
    artificial intelligence techniques to improve the student learning experience.
  authors:
  - Shady Shehata
  - David Santandreu Calonge
  - Philip Purnell
  - Mark Thompson
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_11
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Enhancing Video-based Learning Using Knowledge Tracing: Personalizing Students''
    Learning Experience with ORBITS'
  tldr: As the world regains its footing following the COVID-19 pandemic, academia
    is striving to consolidate the gains made in students' education experience. New
    technologies such as video-based learning have shown some early improvement in
    student learning and engagement. In this paper, we present ORBITS
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We address the problem of generating high-quality question-answer pairs
    for educational materials. Previous work on this problem showed that using summaries
    as input improves the quality of question generation (QG) over original textbook
    text and that human-written summaries result in higher quality QG than automatic
    summaries. In this paper, a) we show that advances in Large Language Models (LLMs)
    are not yet sufficient to generate quality summaries for QG and b) we introduce
    a new methodology for enhancing bullet point student notes into fully fledged
    summaries and find that our methodology yields higher quality QG. We conducted
    a large-scale human annotation study of generated question-answer pairs for the
    evaluation of our methodology. In order to aid in future research, we release
    a  new dataset of 9.2K human annotations of generated questions.
  authors:
  - Hannah Gonzalez
  - Liam Dugan
  - Eleni Miltsakaki
  - Zhiqi Cui
  - Jiaxuan Ren
  - Bryan Li
  - Shriyash Upadhyay
  - Etan Ginsberg
  - Chris Callison-Burch
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_14
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Enhancing Human Summaries for Question-Answer Generation in Education
  tldr: We address the problem of generating high-quality question-answer pairs for
    educational materials. Previous work on this problem showed that using summaries
    as input improves the quality of question generation (QG) over original textbook
    text and that human-written summaries result in higher quality
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Question generation (QG) for reading comprehension, a technology for
    automatically generating questions related to given reading passages, has been
    used in various applications, including in education. Recently, QG methods based
    on deep neural networks have succeeded in generating fluent questions that are
    pertinent to given reading passages. One example of how QG can be applied in education
    is a reading tutor that automatically offers reading comprehension questions related
    to various reading materials. In such an application, QG methods should provide
    questions with difficulty levels appropriate for each learner''s reading ability
    in order to improve learning efficiency. Several difficulty-controllable QG methods
    have been proposed for doing so. However, conventional methods focus only on generating
    questions and cannot generate answers to them. Furthermore, they ignore the relation
    between question difficulty and learner ability, making it hard to determine an
    appropriate difficulty for each learner. To resolve these problems, we propose
    a new method for generating question--answer pairs that considers their difficulty,
    estimated using item response theory. The proposed difficulty-controllable generation
    is realized by extending two pre-trained transformer models: BERT and GPT-2.'
  authors:
  - Masaki Uto
  - Yuto Tomikawa
  - Ayaka Suzuki
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_16
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Difficulty-Controllable Neural Question Generation for Reading Comprehension
    using Item Response Theory
  tldr: Question generation (QG) for reading comprehension, a technology for automatically
    generating questions related to given reading passages, has been used in various
    applications, including in education. Recently, QG methods based on deep neural
    networks have succeeded in generating fluent questions t
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper presents Card-it, a web-based application for learning Italian
    verb conjugation. Card-it integrates a large-scale finite-state morphological\textasciitilde{}(FSM)
    analyzer and a flashcard application as a user-friendly way for learners to utilize
    the analyzer. While Card-it can be used by individual learners, to support classroom
    adoption, we implemented simple classroom management functionalities such as sharing
    flashcards to a class and tracking students' progression. We evaluated Card-it
    with teachers of Italian. Card-it was reported as engaging and supportive, especially
    by featuring two different quiz types combined with a verb form look-up feature.
    Teachers were optimistic about the potential of Card-it as a classroom supplementary
    tool for learners of Italian as L2. Future work includes sample sentences and
    a complete learners evaluation.
  authors:
  - Mariana Shimabukuro
  - Jessica Zipf
  - Shawn Yama
  - Christopher Collins
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_18
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Evaluating Classroom Potential for Card-it: Digital Flashcards for Studying
    and Learning Italian Morphology'
  tldr: This paper presents Card-it, a web-based application for learning Italian
    verb conjugation. Card-it integrates a large-scale finite-state morphological\textasciitilde{}(FSM)
    analyzer and a flashcard application as a user-friendly way for learners to utilize
    the analyzer. While Card-it can be used by
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Open-ended constructed response math word problems ("math plus text",
    or MPT) are a powerful tool in the assessment of students' abilities to engage
    in mathematical reasoning and creative thinking. Such problems ask the student
    to compute a value or construct an expression and then explain, potentially in
    prose, what steps they took and why they took them. MPT items can be scored against
    highly structured rubrics, and we develop a novel technique for the automated
    scoring of MPT items that leverages these rubrics to provide explainable scoring.
    We show that our approach can be trained automatically and performs well on a
    large dataset of 34,417 responses across 14 MPT items.
  authors:
  - Scott Hellman
  - Alejandro Andrade
  - Kyle Habermehl
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_19
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Scalable and Explainable Automated Scoring for Open-Ended Constructed Response
    Math Word Problems
  tldr: Open-ended constructed response math word problems ("math plus text", or MPT)
    are a powerful tool in the assessment of students' abilities to engage in mathematical
    reasoning and creative thinking. Such problems ask the student to compute a value
    or construct an expression and then explain, potentia
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In this paper we show that GEC systems display gender bias related to
    the use of masculine and feminine terms and the gender-neutral singular "they".
    We develop parallel datasets of texts with masculine and feminine terms, and singular
    "they", and use them to quantify gender bias in three competitive GEC systems.
    We contribute a novel data augmentation technique for singular "they" leveraging
    linguistic insights about its distribution relative to plural "they". We demonstrate
    that both this data augmentation technique and a refinement of a similar augmentation
    technique for masculine and feminine terms can generate training data that reduces
    bias in GEC systems, especially with respect to singular "they" while maintaining
    the same level of quality.
  authors:
  - Gunnar Lund
  - Kostiantyn Omelianchuk
  - Igor Samokhin
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_20
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Gender-Inclusive Grammatical Error Correction through Augmentation
  tldr: In this paper we show that GEC systems display gender bias related to the
    use of masculine and feminine terms and the gender-neutral singular "they". We
    develop parallel datasets of texts with masculine and feminine terms, and singular
    "they", and use them to quantify gender bias in three competitiv
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We develop an interactive web-based user interface for performing textspeech
    alignment and creating digital interactive "read-along audio books that highlight
    words as they are spoken and allow users to replay individual words when clicked.
    We build on an existing Python library for zero-shot multilingual textspeech alignment
    (Littell et al., 2022), extend it by exposing its functionality through a RESTful
    API, and rewrite the underlying speech recognition engine to run in the browser.
    The ReadAlong Studio Web App is open-source, user-friendly, prioritizes privacy
    and data sovereignty, allows for a variety of standard export formats, and is
    designed to work for the majority of the world's languages.
  authors:
  - Aidan Pine
  - David Huggins-Daines
  - Eric Joanis
  - Patrick Littell
  - Marc Tessier
  - Delasie Torkornoo
  - Rebecca Knowles
  - Roland Kuhn
  - Delaney Lothian
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_21
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: ReadAlong Studio Web Interface for Digital Interactive Storytelling
  tldr: We develop an interactive web-based user interface for performing textspeech
    alignment and creating digital interactive "read-along audio books that highlight
    words as they are spoken and allow users to replay individual words when clicked.
    We build on an existing Python library for zero-shot multil
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: A peer-assessment system allows students to provide feedback on each other's
    work. An effective peer assessment system urgently requires helpful reviews to
    facilitate students to make improvements and progress. Automated evaluation of
    review helpfulness, with the help of deep learning models and natural language
    processing techniques, gains much interest in the field of peer assessment. However,
    collecting labeled data with the "helpfulness" tag to build these prediction models
    remains challenging. A straightforward solution would be using a supervised learning
    algorithm to train a prediction model on a similar domain and apply it to our
    peer review domain for inference. But naively doing so can degrade the model performance
    in the presence of the distributional gap between domains. Such a distributional
    gap can be effectively addressed by Domain Adaptation (DA). Self-training has
    recently been shown as a powerful branch of DA to address the distributional gap.
    The first goal of this study is to evaluate the performance of self-training-based
    DA in predicting the helpfulness of peer reviews as well as the ability to overcome
    the distributional gap. Our second goal is to propose an advanced self-training
    framework to overcome the weakness of the existing self-training by tailoring
    knowledge distillation and noise injection, to further improve the model performance
    and better address the distributional gap.
  authors:
  - Chengyuan Liu
  - Divyang Doshi
  - Muskaan Bhargava
  - Ruixuan Shang
  - Jialin Cui
  - Dongkuan Xu
  - Edward Gehringer
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_24
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Labels are not necessary: Assessing peer-review helpfulness using domain
    adaptation based on self-training'
  tldr: A peer-assessment system allows students to provide feedback on each other's
    work. An effective peer assessment system urgently requires helpful reviews to
    facilitate students to make improvements and progress. Automated evaluation of
    review helpfulness, with the help of deep learning models and nat
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper proposes a new second language learning task of generating
    a response including specified grammatical items. We consider two approaches:
    1) fine-tuning a pre-trained language model (DialoGPT) by reinforcement learning
    and 2) providing a few-shot prompt to a large language model (GPT-3). For reinforcement
    learning, we examine combinations of three reward functions that consider grammatical
    items, diversity, and fluency. Our experiments confirm that both approaches can
    generate responses including the specified grammatical items and that it is crucial
    to consider fluency rather than diversity as the reward function.'
  authors:
  - Yuki Okano
  - Kotaro Funakoshi
  - Ryo Nagata
  - Manabu Okumura
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_25
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Generating Dialog Responses with Specified Grammatical Items for Second Language
    Learning
  tldr: 'This paper proposes a new second language learning task of generating a response
    including specified grammatical items. We consider two approaches: 1) fine-tuning
    a pre-trained language model (DialoGPT) by reinforcement learning and 2) providing
    a few-shot prompt to a large language model (GPT-3). F'
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The exponential growth of question answering (QA) has made it an indispensable
    topic in any Natural Language Processing (NLP) course. Additionally, the breadth
    of QA derived from this exponential growth makes it an ideal scenario for teaching
    related NLP topics such as information retrieval, explainability, and adversarial
    attacks among others. In this paper, we introduce UKP-SQuARE as a platform for
    QA education. This platform provides an interactive environment where students
    can run, compare, and analyze various QA models from different perspectives, such
    as general behavior, explainability, and robustness. Therefore, students can get
    a first-hand experience in different QA techniques during the class. Thanks to
    this, we propose a learner-centered approach for QA education in which students
    proactively learn theoretical concepts and acquire problem-solving skills through
    interactive exploration, experimentation, and practical assignments, rather than
    solely relying on traditional lectures. To evaluate the effectiveness of UKP-SQuARE
    in teaching scenarios, we adopted it in a postgraduate NLP course and surveyed
    the students after the course. Their positive feedback shows the platform's effectiveness
    in their course and invites a wider adoption.
  authors:
  - Haishuo Fang
  - Haritz Puerto
  - Iryna Gurevych
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_28
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'UKP-SQuARE: An Interactive Tool for Teaching Question Answering'
  tldr: The exponential growth of question answering (QA) has made it an indispensable
    topic in any Natural Language Processing (NLP) course. Additionally, the breadth
    of QA derived from this exponential growth makes it an ideal scenario for teaching
    related NLP topics such as information retrieval, explain
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Large-scale pre-trained language models such as GPT-3 have shown remarkable
    performance across various natural language processing tasks. However, applying
    prompt-based methods with GPT-3 for Grammatical Error Correction (GEC) tasks and
    their controllability remains underexplored. Controllability in GEC is crucial
    for real-world applications, particularly in educational settings, where the ability
    to tailor feedback according to learner levels and specific error types can significantly
    enhance the learning process.This paper investigates the performance and controllability
    of prompt-based methods with GPT-3 for GEC tasks using zero-shot and few-shot
    setting. We explore the impact of task instructions and examples on GPT-3's output,
    focusing on controlling aspects such as minimal edits, fluency edits, and learner
    levels. Our findings demonstrate that GPT-3 could effectively perform GEC tasks,
    outperforming existing supervised and unsupervised approaches. We also showed
    that GPT-3 could achieve controllability when appropriate task instructions and
    examples are given.
  authors:
  - Mengsay Loem
  - Masahiro Kaneko
  - Sho Takase
  - Naoaki Okazaki
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_30
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Exploring Effectiveness of GPT-3 in Grammatical Error Correction: A Study
    on Performance and Controllability in Prompt-Based Methods'
  tldr: Large-scale pre-trained language models such as GPT-3 have shown remarkable
    performance across various natural language processing tasks. However, applying
    prompt-based methods with GPT-3 for Grammatical Error Correction (GEC) tasks and
    their controllability remains underexplored. Controllability in
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In various natural language processing tasks, such as named entity recognition
    and machine translation, example-based approaches have been used to improve performance
    by leveraging existing knowledge. However, the effectiveness of this approach
    for Grammatical Error Correction (GEC) is unclear. In this work, we explore how
    an example-based approach affects the accuracy and interpretability of the output
    of GEC systems and the trade-offs involved. The approach we investigate has shown
    great promise in machine translation by using the \$k\$-nearest translation examples
    to improve the results of a pretrained Transformer model. We find that using this
    technique increases precision by reducing the number of false positives, but recall
    suffers as the model becomes more conservative overall. Increasing the number
    of example sentences in the datastore does lead to better performing systems,
    but with diminishing returns and a high decoding cost. Synthetic data can be used
    as examples, but the effectiveness varies depending on the base model. Finally,
    we find that finetuning on a set of data may be more effective than using that
    data during decoding as examples.
  authors:
  - Justin Vasselli
  - Taro Watanabe
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_32
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: A Closer Look at k-Nearest Neighbors Grammatical Error Correction
  tldr: In various natural language processing tasks, such as named entity recognition
    and machine translation, example-based approaches have been used to improve performance
    by leveraging existing knowledge. However, the effectiveness of this approach
    for Grammatical Error Correction (GEC) is unclear. In t
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: By aligning the functional components derived from the activations of
    transformer models trained for AES with external knowledge such as human-understandable
    feature groups, the proposed method improves the interpretability of a Longformer
    Automatic Essay Scoring (AES) system and provides tools for performing such analyses
    on further neural AES systems. The analysis focuses on models trained to score
    essays based on organization, main idea, support, and language. The findings provide
    insights into the models' decision-making processes, biases, and limitations,
    contributing to the development of more transparent and reliable AES systems.
  authors:
  - James Fiacco
  - David Adamson
  - Carolyn Ros
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_33
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Towards Extracting and Understanding the Implicit Rubrics of Transformer
    Based Automatic Essay Scoring Models
  tldr: 'By aligning the functional components derived from the activations of transformer
    models trained for AES with external knowledge such as human-understandable feature
    groups, the proposed method improves the interpretability of a Longformer Automatic
    Essay Scoring (AES) system and provides tools for '
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper analyzes winning solutions from the Feedback Prize competition
    series hosted from 2021-2022. The competition sought to improve Assisted Writing
    Feedback Tools (AWFTs) by crowdsourcing Large Language Model (LLM) solutions for
    evaluating student writing. The winning models are freely available for incorporation
    into educational applications, but the models need to be assessed for performance
    and other factors. This study reports the performance accuracy of Feedback Prize-winning
    models based on demographic factors such as student race/ethnicity, economic disadvantage,
    and English Language Learner status. Two competitions are analyzed. The first,
    which focused on identifying discourse elements, demonstrated minimal bias based
    on students' demographic factors. However, the second competition, which aimed
    to predict discourse effectiveness, exhibited moderate bias.
  authors:
  - Perpetual Baffour
  - Tor Saxberg
  - Scott Crossley
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_34
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Analyzing Bias in Large Language Model Solutions for Assisted Writing Feedback
    Tools: Lessons from the Feedback Prize Competition Series'
  tldr: This paper analyzes winning solutions from the Feedback Prize competition
    series hosted from 2021-2022. The competition sought to improve Assisted Writing
    Feedback Tools (AWFTs) by crowdsourcing Large Language Model (LLM) solutions for
    evaluating student writing. The winning models are freely availa
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Reading comprehension is a crucial skill in many aspects of education,
    including language learning, cognitive development, and fostering early literacy
    skills in children. Automated answer-aware reading comprehension question generation
    has significant potential to scale up learner support in educational activities.
    One key technical challenge in this setting is that there can be multiple questions,
    sometimes very different from each other, with the same answer; a trained question
    generation method may not necessarily know which question human educators would
    prefer. To address this challenge, we propose 1) a data augmentation method that
    enriches the training dataset with diverse questions given the same context and
    answer and 2) an overgenerate-and-rank method to select the best question from
    a pool of candidates. We evaluate our method on the FairytaleQA dataset, showing
    a 5\% absolute improvement in ROUGE-L over the best existing method. We also demonstrate
    the effectiveness of our method in generating harder, "implicit" questions, where
    the answers are not contained in the context as text spans.
  authors:
  - Nischal Ashok Kumar
  - Nigel Fernandez
  - Zichao Wang
  - Andrew Lan
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_37
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Improving Reading Comprehension Question Generation with Data Augmentation
    and Overgenerate-and-rank
  tldr: Reading comprehension is a crucial skill in many aspects of education, including
    language learning, cognitive development, and fostering early literacy skills
    in children. Automated answer-aware reading comprehension question generation
    has significant potential to scale up learner support in educat
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The standard definition generation task requires to automatically produce
    mono-lingual definitions (e.g., English definitions for English words), but ignores
    that the generated definitions may also consist of unfamiliar words for language
    learners. In this work, we propose a novel task of Trans-Lingual Definition Generation
    (TLDG), which aims to generate definitions in another language, i.e., the native
    speaker's language. Initially, we explore the unsupervised manner of this task
    and build up a simple implementation of fine-tuning the multi-lingual machine
    translation model. Then, we develop two novel methods, Prompt Combination and
    Contrastive Prompt Learning, for further enhancing the quality of the generation.
    Our methods are evaluated against the baseline Pipeline method in both rich- and
    low-resource settings, and we empirically establish its superiority in generating
    higher-quality trans-lingual definitions.
  authors:
  - Hengyuan Zhang
  - Dawei Li
  - Yanran Li
  - Chenming Shang
  - Chufan Shi
  - Yong Jiang
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_39
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Assisting Language Learners: Automated Trans-Lingual Definition Generation
    via Contrastive Prompt Learning'
  tldr: The standard definition generation task requires to automatically produce
    mono-lingual definitions (e.g., English definitions for English words), but ignores
    that the generated definitions may also consist of unfamiliar words for language
    learners. In this work, we propose a novel task of Trans-Ling
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The ability to revise in response to feedback is critical to students'
    writing success. In the case of argument writing in specific, identifying whether
    an argument revision (AR) is successful or not is a complex problem because AR
    quality is dependent on the overall content of an argument. For example, adding
    the same evidence sentence could strengthen or weaken existing claims in different
    argument contexts (ACs). To address this issue we developed Chain-of-Thought prompts
    to facilitate ChatGPT-generated ACs for AR quality predictions. The experiments
    on two corpora, our annotated elementary essays and existing college essays benchmark,
    demonstrate the superiority of the proposed ACs over baselines.
  authors:
  - Zhexiong Liu
  - Diane Litman
  - Elaine Wang
  - Lindsay Matsumura
  - Richard Correnti
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_40
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Predicting the Quality of Revisions in Argumentative Writing
  tldr: The ability to revise in response to feedback is critical to students' writing
    success. In the case of argument writing in specific, identifying whether an argument
    revision (AR) is successful or not is a complex problem because AR quality is
    dependent on the overall content of an argument. For exam
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In intelligent language tutoring systems, student dashboards should display
    the learning progress and performance and support the navigation through the learning
    content. Designing an interface that transparently offers information on students'
    learning in relation to specific learning targets while linking to the overarching
    functional goal, that motivates and organizes the practice in current foreign
    language teaching, is challenging.This becomes even more difficult in systems
    that adaptively expose students to different learning material and individualize
    system interactions. If such a system is used in an ecologically valid setting
    of blended learning, this generates additional requirements to incorporate the
    needs of students and teachers for control and customizability.We present the
    conceptual design of a student dashboard for a task-based, user-adaptive intelligent
    language tutoring system intended for use in real-life English classes in secondary
    schools. We highlight the key challenges and spell out open questions for future
    research.
  authors:
  - Leona Colling
  - Tanja Heck
  - Detmar Meurers
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_42
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Reconciling Adaptivity and Task Orientation in the Student Dashboard of an
    Intelligent Language Tutoring System
  tldr: In intelligent language tutoring systems, student dashboards should display
    the learning progress and performance and support the navigation through the learning
    content. Designing an interface that transparently offers information on students'
    learning in relation to specific learning targets while
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Improving conversational proficiency is a key target for students learning
    a new language. While acquiring conversational proficiency, students must learn
    the linguistic mechanisms of Repair and Grounding (R\textbackslash{}\&G) to  negotiate
    meaning and find common ground with their interlocutor so conversational breakdowns
    can be resolved. Task-oriented Spoken Dialogue Systems (SDS) have long been sought
    as a tool to hone conversational proficiency. However, the R\&G patterns for language
    learners interacting with a task-oriented spoken dialogue system are not reflected
    explicitly in any existing datasets. Therefore, to move the needle in Spoken Dialogue
    Systems for language learning we present GrounDialog: an annotated dataset of
    spoken conversations where we elicit a rich set of R\&G patterns.'
  authors:
  - Xuanming Zhang
  - Rahul Divekar
  - Rutuja Ubale
  - Zhou Yu
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_44
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'GrounDialog: A Dataset for Repair and Grounding in Task-oriented Spoken
    Dialogues for Language Learning'
  tldr: 'Improving conversational proficiency is a key target for students learning
    a new language. While acquiring conversational proficiency, students must learn
    the linguistic mechanisms of Repair and Grounding (R\textbackslash{}\&G) to  negotiate
    meaning and find common ground with their interlocutor so '
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Lectures are a learning experience for both students and teachers. Students
    learn from teachers about the subject material, while teachers learn from students
    about how to refine their instruction. Unfortunately, online student feedback
    is unstructured and abundant, making it challenging for teachers to learn and
    improve. We take a step towards tackling this challenge. First, we contribute
    a dataset for studying this problem: SIGHT is a large dataset of 288 math lecture
    transcripts and 15,784 comments collected from the Massachusetts Institute of
    Technology OpenCourseWare (MIT OCW) YouTube channel. Second, we develop a rubric
    for categorizing feedback types using qualitative analysis. Qualitative analysis
    methods are powerful in uncovering domain-specific insights, however they are
    costly to apply to large data sources. To overcome this challenge, we propose
    a set of best practices for using large language models (LLMs) to cheaply classify
    the comments at scale. We observe a striking correlation between the model''s
    and humans'' annotation: Categories with consistent human annotations (\textgreater{}0.9
    inter-rater reliability, IRR) also display higher human-model agreement (\textgreater{}0.7),
    while categories with less consistent human annotations (0.7-0.8 IRR) correspondingly
    demonstrate lower human-model agreement (0.3-0.5). These techniques uncover useful
    student feedback from thousands of comments, costing around \$0.002 per comment.
    We conclude by discussing exciting future directions on using online student feedback
    and improving automated annotation techniques for qualitative research.'
  authors:
  - Rose Wang
  - Pawan Wirawarn
  - Noah Goodman
  - Dorottya Demszky
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_45
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'SIGHT: A Large Annotated Dataset on Student Insights Gathered from Higher
    Education Transcripts'
  tldr: Lectures are a learning experience for both students and teachers. Students
    learn from teachers about the subject material, while teachers learn from students
    about how to refine their instruction. Unfortunately, online student feedback
    is unstructured and abundant, making it challenging for teacher
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper addresses the problem of providing automatic feedback on orthographic
    errors in handwritten text. Despite the availability of automatic error detection
    systems, the practical problem of digitizing the handwriting remains. Current
    handwriting recognition (HWR) systems produce highly accurate transcriptions but
    normalize away the very errors that are essential for providing useful feedback,
    e.g. orthographic errors. Our contribution is twofold:First, we create a comprehensive
    dataset of handwritten text with transcripts retaining orthographic errors by
    transcribing 1,350 pages from the German learner dataset FD-LEX. Second, we train
    a simple HWR system on our dataset, allowing it to transcribe words with orthographic
    errors.Thereby, we evaluate the effect of different dictionaries on recognition
    output, highlighting the importance of addressing spelling errors in these dictionaries.
  authors:
  - Christian Gold
  - Ronja Laarmann-Quante
  - Torsten Zesch
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_47
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Recognizing Learner Handwriting Retaining Orthographic Errors for Enabling
    Fine-Grained Error Feedback
  tldr: This paper addresses the problem of providing automatic feedback on orthographic
    errors in handwritten text. Despite the availability of automatic error detection
    systems, the practical problem of digitizing the handwriting remains. Current
    handwriting recognition (HWR) systems produce highly accura
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: As in other NLP tasks, Automatic Short Answer Grading (ASAG) systems have
    evolved from using rule-based and interpretable machine learning models to utilizing
    deep learning architectures to boost accuracy. Since proper feedback is critical
    to student assessment, explainability will be crucial for deploying ASAG in real-world
    applications. This paper proposes a framework to generate explainable outcomes
    for assessing question-answer pairs of a Data Mining course in a binary manner.
    Our framework utilizes a fine-tuned Transformer-based classifier and an explainability
    module using SHAP or Integrated Gradients to generate language explanations for
    each prediction. We assess the outcome of our framework by calculating accuracy-based
    metrics for classification performance. Furthermore, we evaluate the quality of
    the explanations by measuring their agreement with human-annotated justifications
    using Intersection-Over-Union at a token level to derive a plausibility score.Despite
    the relatively limited sample, results show that our framework derives explanations
    that are, to some degree, aligned with domain-expert judgment. Furthermore, both
    explainability methods perform similarly in their agreement with human-annotated
    explanations. A natural progression of our work is to analyze the use of our explainable
    ASAG framework on a larger sample to determine the feasibility of implementing
    a pilot study in a real-world setting.
  authors:
  - Maximilian Tornqvist
  - Mosleh Mahamud
  - Erick Mendez Guzman
  - Alexandra Farazouli
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_48
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'ExASAG: Explainable Framework for Automatic Short Answer Grading'
  tldr: As in other NLP tasks, Automatic Short Answer Grading (ASAG) systems have
    evolved from using rule-based and interpretable machine learning models to utilizing
    deep learning architectures to boost accuracy. Since proper feedback is critical
    to student assessment, explainability will be crucial for de
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Creating high-quality multiple-choice items requires careful attention
    to several factors, including ensuring that there is only one correct option,
    that options are independent of each other, that there is no overlap between options,
    and that each option is plausible. This attention is reflected in the explanations
    provided by human item-writers for each option. This study aimed to compare the
    creation of explanations of multiple-choice item options for reading comprehension
    by ChatGPT with those created by humans. We used two context-dependent multiple-choice
    item sets created based on EvidenceCentered Design. Results indicate that ChatGPT
    is capable of producing explanations with different type of information that are
    comparable to those created by humans. So that humans could benefit from additional
    information given to enhance their explanations. We conclude that ChatGPT ability
    to generate explanations for multiple-choice item options in reading comprehension
    tests is comparable to that of humans.
  authors:
  - George Duenas
  - Sergio Jimenez
  - Geral Mateus Ferro
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_49
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: You've Got a Friend in ... a Language Model? A Comparison of Explanations
    of Multiple-Choice Items of Reading Comprehension between ChatGPT and Humans
  tldr: Creating high-quality multiple-choice items requires careful attention to
    several factors, including ensuring that there is only one correct option, that
    options are independent of each other, that there is no overlap between options,
    and that each option is plausible. This attention is reflected in
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We introduce a novel technique for automatically summarizing lecture videos
    using large language models such as GPT-3 and we present a user study investigating
    the effects on the studying experience when automatic summaries are added to lecture
    videos. We test students under different conditions and find that the students
    who are shown a summary next to a lecture video perform better on quizzes designed
    to test the course materials than the students who have access only to the video
    or the summary. Our findings suggest that adding automatic summaries to lecture
    videos enhances the learning experience. Qualitatively, students preferred summaries
    when studying under time constraints.
  authors:
  - Hannah Gonzalez
  - Jiening Li
  - Helen Jin
  - Jiaxuan Ren
  - Hongyu Zhang
  - Ayotomiwa Akinyele
  - Adrian Wang
  - Eleni Miltsakaki
  - Ryan Baker
  - Chris Callison-Burch
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_50
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Automatically Generated Summaries of Video Lectures May Enhance Students'
    Learning Experience
  tldr: We introduce a novel technique for automatically summarizing lecture videos
    using large language models such as GPT-3 and we present a user study investigating
    the effects on the studying experience when automatic summaries are added to lecture
    videos. We test students under different conditions and
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The popularization of large language models (LLMs) such as OpenAI's GPT-3
    and GPT-4 have led to numerous innovations in the field of AI in education. With
    respect to automated writing evaluation (AWE), LLMs have reduced challenges associated
    with assessing writing quality characteristics that are difficult to identify
    automatically, such as discourse coherence. In addition, LLMs can provide rationales
    for their evaluations (ratings) which increases score interpretability and transparency.
    This paper investigates one approach to producing ratings by training GPT-4 to
    assess discourse coherence in a manner consistent with expert human raters. The
    findings of the study suggest that GPT-4 has strong potential to produce discourse
    coherence ratings that are comparable to human ratings, accompanied by clear rationales.
    Furthermore, the GPT-4 ratings outperform traditional NLP coherence metrics with
    respect to agreement with human ratings. These results have implications for advancing
    AWE technology for learning and assessment.
  authors:
  - Ben Naismith
  - Phoebe Mulcaire
  - Jill Burstein
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_55
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Automated evaluation of written discourse coherence using GPT-4
  tldr: The popularization of large language models (LLMs) such as OpenAI's GPT-3
    and GPT-4 have led to numerous innovations in the field of AI in education. With
    respect to automated writing evaluation (AWE), LLMs have reduced challenges associated
    with assessing writing quality characteristics that are di
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Lexical simplification (LS) automatically replaces words that are deemed
    difficult to understand for a given target population with simpler alternatives,
    whilst preserving the meaning of the original sentence. The TSAR-2022 shared task
    on LS provided participants with a multilingual lexical simplification test set.
    It contained nearly 1,200 complex words in English, Portuguese, and Spanish and
    presented multiple candidate substitutions for each complex word. The competition
    did not make training data available; therefore, teams had to use either off-the-shelf
    pre-trained large language models (LLMs) or out-domain data to develop their LS
    systems. As such, participants were unable to fully explore the capabilities of
    LLMs by re-training and/or fine-tuning them on in-domain data. To address this
    important limitation, we present ALEXSIS+, a multilingual dataset in the aforementioned
    three languages, and ALEXSIS++, an English monolingual dataset that together contains
    more than 50,000 unique sentences retrieved from news corpora and annotated with
    cosine similarities to the original complex word and sentence. Using these additional
    contexts, we are able to generate new high-quality candidate substitutions that
    improve LS performance on the TSAR-2022 test set regardless of the language or
    model.
  authors:
  - Kai North
  - Alphaeus Dmonte
  - Tharindu Ranasinghe
  - Matthew Shardlow
  - Marcos Zampieri
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_56
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'ALEXSIS+: Improving Substitute Generation and Selection for Lexical Simplification
    with Information Retrieval'
  tldr: Lexical simplification (LS) automatically replaces words that are deemed difficult
    to understand for a given target population with simpler alternatives, whilst
    preserving the meaning of the original sentence. The TSAR-2022 shared task on
    LS provided participants with a multilingual lexical simplifi
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Writing high-quality test questions (items) is critical to building educational
    measures but has traditionally also been a time-consuming process. One promising
    avenue for alleviating this is automated item generation, whereby methods from
    artificial intelligence (AI) are used to generate new items with minimal human
    intervention. Researchers have explored using large language models (LLMs) to
    generate new items with equivalent psychometric properties to human-written ones.
    But can LLMs generate items with improved psychometric properties, even when existing
    items have poor validity evidence? We investigate this using items from a natural
    language inference (NLI) dataset. We develop a novel prompting strategy based
    on selecting items with both the best and worst properties to use in the prompt
    and use GPT-3 to generate new NLI items. We find that the GPT-3 items show improved
    psychometric properties in many cases, whilst also possessing good content, convergent
    and discriminant validity evidence. Collectively, our results demonstrate the
    potential of employing LLMs to ease the item development process and suggest that
    the careful use of prompting may allow for iterative improvement of item quality.
  authors:
  - Antonio Laverghetta Jr.
  - John Licato
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_57
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Generating Better Items for Cognitive Assessments Using Large Language Models
  tldr: 'Writing high-quality test questions (items) is critical to building educational
    measures but has traditionally also been a time-consuming process. One promising
    avenue for alleviating this is automated item generation, whereby methods from
    artificial intelligence (AI) are used to generate new items '
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Responding to the increasing need for automated writing evaluation (AWE)
    systems to assess language use beyond lexis and grammar (Burstein et al., 2016),
    we introduce a new approach to identify rhetorical features of stance in academic
    English writing. Drawing on the discourse-analytic framework of engagement in
    the Appraisal analysis (Martin \& White, 2005), we manually annotated 4,688 sentences
    (126,411 tokens) for eight rhetorical stance categories (e.g., PROCLAIM, ATTRIBUTION)
    and additional discourse elements. We then report an experiment to train machine
    learning models to identify and categorize the spans of these stance expressions.
    The best-performing model (RoBERTa + LSTM) achieved macro-averaged F1 of .7208
    in the span identification of stance-taking expressions, slightly outperforming
    the intercoder reliability estimates before adjudication (F1 = .6629).
  authors:
  - Masaki Eguchi
  - Kristopher Kyle
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_58
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Span Identification of Epistemic Stance-Taking in Academic Written English
  tldr: 'Responding to the increasing need for automated writing evaluation (AWE)
    systems to assess language use beyond lexis and grammar (Burstein et al., 2016),
    we introduce a new approach to identify rhetorical features of stance in academic
    English writing. Drawing on the discourse-analytic framework of '
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper presents the ACTA system, which performs automated short-answer
    grading in the domain of high-stakes medical exams. The system builds upon previous
    work on neural similarity-based grading approaches by applying these to the medical
    domain and utilizing contrastive learning as a means to optimize the similarity
    metric. ACTA is evaluated against three strong baselines and is developed in alignment
    with operational needs, where low-confidence responses are flagged for human review.
    Learning curves are explored to understand the effects of training data on performance.
    The results demonstrate that ACTA leads to substantially lower number of responses
    being flagged for human review, while maintaining high classification accuracy.
  authors:
  - King Yiu Suen
  - Victoria Yaneva
  - Le An Ha
  - Janet Mee
  - Yiyun Zhou
  - Polina Harik
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_61
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'ACTA: Short-Answer Grading in High-Stakes Medical Exams'
  tldr: This paper presents the ACTA system, which performs automated short-answer
    grading in the domain of high-stakes medical exams. The system builds upon previous
    work on neural similarity-based grading approaches by applying these to the medical
    domain and utilizing contrastive learning as a means to o
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Automatic readability assessment (ARA) predicts how difficult it is for
    the reader to understand a text.  While ARA has traditionally been performed at
    the passage level, there has been increasing interest in ARA at the sentence level,
    given its applications in downstream tasks such as text simplification and language
    exercise generation. Recent research has suggested the effectiveness of hybrid
    approaches for ARA, but they have yet to be applied on the sentence level. We
    present the first study that compares neural and hybrid models for sentence-level
    ARA. We conducted experiments on graded sentences from the Wall Street Journal
    (WSJ) and a dataset derived from the OneStopEnglish corpus. Experimental results
    show that both neural and hybrid models outperform traditional classifiers trained
    on linguistic features. Hybrid models obtained the best accuracy on both datasets,
    surpassing the previous best result reported on the WSJ dataset by almost 13\%
    absolute.
  authors:
  - Fengkai Liu
  - John Lee
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_62
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Hybrid Models for Sentence Readability Assessment
  tldr: Automatic readability assessment (ARA) predicts how difficult it is for the
    reader to understand a text.  While ARA has traditionally been performed at the
    passage level, there has been increasing interest in ARA at the sentence level,
    given its applications in downstream tasks such as text simplifi
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Grammatical error correction (GEC) is a challenging task for non-native
    second language (L2) learners and learning machines. Data-driven GEC learning
    requires as much human-annotated genuine training data as possible. However, it
    is difficult to produce larger-scale human-annotated data, and synthetically generated
    large-scale parallel training data is valuable for GEC systems. In this paper,
    we propose a method for rebuilding a corpus of synthetic parallel data using target
    sentences predicted by a GEC model to improve performance. Experimental results
    show that our proposed pre-training outperforms that on the original synthetic
    datasets. Moreover, it is also shown that our proposed training without human-annotated
    L2 learners' corpora is as practical as conventional full pipeline training with
    both synthetic datasets and L2 learners' corpora in terms of accuracy.
  authors:
  - Mikio Oda
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_63
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Training for Grammatical Error Correction Without Human-Annotated L2 Learners'
    Corpora
  tldr: Grammatical error correction (GEC) is a challenging task for non-native second
    language (L2) learners and learning machines. Data-driven GEC learning requires
    as much human-annotated genuine training data as possible. However, it is difficult
    to produce larger-scale human-annotated data, and synthet
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper explores the use of L2-specific grammatical microsystems as
    elements of the domain knowledge of an Intelligent Computer-assisted Language
    Learning (ICALL) system. We report on the design of new grammatico-functional
    measures and their association with proficiency. We illustrate the approach with
    the design of the IT, THIS, THAT proform microsystem. The measures rely on the
    paradigmatic relations between words of the same linguistic functions. They are
    operationalised with one frequency-based and two probabilistic methods, i.e.,
    the relative proportions of the forms and their likelihood of occurrence. Ordinal
    regression models show that the measures are significant in terms of association
    with CEFR levels, paving the way for their introduction in a specific proform
    microsystem expert model.
  authors:
  - Cyriel Mallart
  - Andrew Simpkin
  - Rmi Venant
  - Nicolas Ballier
  - Bernardo Stearns
  - Jen Yu Li
  - Thomas Gaillat
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_65
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Exploring a New Grammatico-functional Type of Measure as Part of a Language
    Learning Expert System
  tldr: This paper explores the use of L2-specific grammatical microsystems as elements
    of the domain knowledge of an Intelligent Computer-assisted Language Learning
    (ICALL) system. We report on the design of new grammatico-functional measures
    and their association with proficiency. We illustrate the approa
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Lexical complexity prediction (LCP) is the task of predicting the complexity
    of words in a text on a continuous scale. It plays a vital role in simplifying
    or annotating complex words to assist readers.To study lexical complexity in Japanese,
    we construct the first Japanese LCP dataset. Our dataset provides separate complexity
    scores for Chinese/Korean annotators and others to address the readers' L1-specific
    needs. In the baseline experiment, we demonstrate the effectiveness of a BERT-based
    system for Japanese LCP.
  authors:
  - Yusuke Ide
  - Masato Mita
  - Adam Nohejl
  - Hiroki Ouchi
  - Taro Watanabe
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_66
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Japanese Lexical Complexity for Non-Native Readers: A New Dataset'
  tldr: 'Lexical complexity prediction (LCP) is the task of predicting the complexity
    of words in a text on a continuous scale. It plays a vital role in simplifying
    or annotating complex words to assist readers.To study lexical complexity in Japanese,
    we construct the first Japanese LCP dataset. Our dataset '
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'The paper presents experiments on using a Grammatical Error Correction
    (GEC) model to assess the correctness of answers that language learners give to
    grammar exercises. We explored whether a GEC model can be applied in the language
    learning context for a language with complex morphology. We empirically check
    a hypothesis that a GEC model corrects only errors and leaves correct answers
    unchanged. We perform a test on assessing learner answers in a real but constrained
    language-learning setup: the learners answer only fill-in-the-blank and multiple-choice
    exercises. For this purpose, we use ReLCo, a publicly available manually annotated
    learner dataset in Russian (Katinskaia et al., 2022). In this experiment, we fine-tune
    a large-scale T5 language model for the GEC task and estimate its performance
    on the RULEC-GEC dataset (Rozovskaya and Roth, 2019) to compare with top-performing
    models. We also release an updated version of the RULEC-GEC test set, manually
    checked by native speakers. Our analysis shows that the GEC model performs reasonably
    well in detecting erroneous answers to grammar exercises and potentially can be
    used for best-performing error types in a real learning setup. However, it struggles
    to assess answers which were tagged by human annotators as alternative-correct
    using the aforementioned hypothesis. This is in large part due to a still low
    recall in correcting errors, and the fact that the GEC model may modify even correct
    words---it may generate plausible alternatives, which are hard to evaluate against
    the gold-standard reference.'
  authors:
  - Anisia Katinskaia
  - Roman Yangarber
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_67
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Grammatical Error Correction for Sentence-level Assessment in Language Learning
  tldr: The paper presents experiments on using a Grammatical Error Correction (GEC)
    model to assess the correctness of answers that language learners give to grammar
    exercises. We explored whether a GEC model can be applied in the language learning
    context for a language with complex morphology. We empiric
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: An inclusive society needs to facilitate access to information for all
    of its members, including citizens with low literacy and with non-native language
    skills. We present an approach to assess Dutch text complexity on the sentence
    level and conduct an interpretability analysis to explore the link between neural
    models and linguistic complexity features. Building on these findings, we develop
    the first contextual lexical simplification model for Dutch and publish a pilot
    dataset for evaluation. We go beyondprevious work which primarily targeted lexical
    substitution and propose strategies for adjusting the model's linguistic register
    to generate simpler candidates. Our results indicate that continual pre-training
    and multi-task learning with conceptually related tasks are promising directions
    for ensuring the simplicity of the generated substitutions.
  authors:
  - Eliza Hobo
  - Charlotte Pouw
  - Lisa Beinborn
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_69
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: '"Geen makkie": Interpretable Classification and Simplification of Dutch
    Text Complexity'
  tldr: An inclusive society needs to facilitate access to information for all of
    its members, including citizens with low literacy and with non-native language
    skills. We present an approach to assess Dutch text complexity on the sentence
    level and conduct an interpretability analysis to explore the link b
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes a CEFR-based classifier of single-word and multi-word
    lexical complexity in context from a second language learner perspective in English
    and in French, developed as an analytical tool for the pedagogical team of the
    language learning application Mauril. We provide an overview of the required corpora
    and the way we transformed it into rich contextual representations that allow
    the disambiguation and accurate labelling in context of polysemous occurrences
    of a given lexical item. We report evaluation results for all models, including
    two multi-lingual lexical classifiers evaluated on novel French datasets created
    for this experiment. Finally, we share the perspective of Mauril's pedagogical
    team on the limitations of such systems.
  authors:
  - Desislava Aleksandrova
  - Vincent Pouliot
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_74
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: CEFR-based Contextual Lexical Complexity Classifier in English and French
  tldr: This paper describes a CEFR-based classifier of single-word and multi-word
    lexical complexity in context from a second language learner perspective in English
    and in French, developed as an analytical tool for the pedagogical team of the
    language learning application Mauril. We provide an overview o
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Classroom discourse is a core medium of instruction  analyzing it can
    provide a window into teaching and learning as well as driving the development
    of new tools for improving instruction. We introduce the largest dataset of mathematics
    classroom transcripts available to researchers, and demonstrate how this data
    can help improve instruction. The dataset consists of 1,660 45-60 minute long
    4th and 5th grade elementary mathematics observations collected by the National
    Center for Teacher Effectiveness (NCTE) between 2010-2013. The anonymized transcripts
    represent data from 317 teachers across 4 school districts that serve largely
    historically marginalized students. The transcripts come with rich metadata, including
    turn-level annotations for dialogic discourse moves, classroom observation scores,
    demographic information, survey responses and student test scores. We demonstrate
    that our natural language processing model, trained on our turn-level annotations,
    can learn to identify dialogic discourse moves and these moves are correlated
    with better classroom observation scores and learning outcomes. This dataset opens
    up several possibilities for researchers, educators and policymakers to learn
    about and improve K-12 instruction. The dataset can be found at https://github.com/ddemszky/classroom-transcript-analysis.
  authors:
  - Dorottya Demszky
  - Heather Hill
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_75
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'The NCTE Transcripts: A Dataset of Elementary Math Classroom Transcripts'
  tldr: Classroom discourse is a core medium of instruction  analyzing it can provide
    a window into teaching and learning as well as driving the development of new
    tools for improving instruction. We introduce the largest dataset of mathematics
    classroom transcripts available to researchers, and demonstrate
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Online learning platforms offer a wealth of educational material, but
    as the amount of content on these platforms grows, students may struggle to determine
    the most efficient order in which to cover the material to achieve a particular
    learning objective. In this paper, we propose a feature-based method for identifying
    pre-requisite dependencies between academic videos. Our approach involves using
    a transcript engine with a language model to transcribe domain-specific terms
    and then extracting novel similarity-based features to determine pre-requisite
    dependencies between video transcripts. This approach succeeds due to the development
    of a novel corpus of K-12 academic text, which was created using a proposed feature-based
    document parser. We evaluate our method on hand-annotated datasets for transcript
    extraction, video pre-requisites determination, and textbook parsing, which we
    have released. Our method for pre-requisite edge determination shows significant
    improvement (+4.7\%-10.24\% F1-score) compared to existing methods.
  authors:
  - Rushil Thareja
  - Ritik Garg
  - Shiva Baghel
  - Deep Dwivedi
  - Mukesh Mohania
  - Ritvik Kulshrestha
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_76
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Auto-req: Automatic detection of pre-requisite dependencies between academic
    videos'
  tldr: Online learning platforms offer a wealth of educational material, but as the
    amount of content on these platforms grows, students may struggle to determine
    the most efficient order in which to cover the material to achieve a particular
    learning objective. In this paper, we propose a feature-based me
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Pre-trained large language models (PLMs) are adaptable to a wide range
    of downstream tasks by fine-tuning their rich contextual embeddings to the task,
    often without requiring much task-specific data. In this paper, we explore the
    use of a recently developed Hebrew PLM  aleph-BERT  for automated short answer
    grading of high school biology items. We show that the alephBERT-based system
    outperforms a strong CNN-based baseline, and that it general-izes unexpectedly
    well in a zero-shot paradigm to items on an unseen topic that address the same
    underlying biological concepts, opening up the possibility of automatically assessing
    new items without item-specific fine-tuning.
  authors:
  - Abigail Gurin Schleifer
  - Beata Beigman Klebanov
  - Moriah Ariely
  - Giora Alexandron
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_77
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Transformer-based Hebrew NLP models for Short Answer Scoring in Biology
  tldr: Pre-trained large language models (PLMs) are adaptable to a wide range of
    downstream tasks by fine-tuning their rich contextual embeddings to the task,
    often without requiring much task-specific data. In this paper, we explore the
    use of a recently developed Hebrew PLM  aleph-BERT  for automated sho
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'In recent decades, there has been a significant push to leverage technology
    to aid both teachers and students in the classroom. Language processing advancements
    have been harnessed to provide better tutoring services, automated feedback to
    teachers, improved peer-to-peer feedback mechanisms, and measures of student comprehension
    for reading. Automated question generation systems have the potential to significantly
    reduce teachers'' workload in the latter. In this paper, we compare three differ-
    ent neural architectures for question generation across two types of reading material:
    narratives and textbooks. For each architecture, we explore the benefits of including
    question attributes in the input representation. Our models show that a T5 architecture
    has the best overall performance, with a RougeL score of 0.536 on a narrative
    corpus and 0.316 on a textbook corpus. We break down the results by attribute
    and discover that the attribute can improve the quality of some types of generated
    questions, including Action and Character, but this is not true for all models.'
  authors:
  - E. Margaret Perkoff
  - Abhidip Bhattacharyya
  - Jon Cai
  - Jie Cao
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_78
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Comparing Neural Question Generation Architectures for Reading Comprehension
  tldr: In recent decades, there has been a significant push to leverage technology
    to aid both teachers and students in the classroom. Language processing advancements
    have been harnessed to provide better tutoring services, automated feedback to
    teachers, improved peer-to-peer feedback mechanisms, and mea
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We present research aimed at solving a problem in  assessment of oral
    reading fluency using children's oral reading data from our online book reading
    app. It is known that properties of the passage being read aloud impact fluency
    estimates; therefore, passage-based measures are  used to remove passage-related
    variance when estimating growth in oral reading fluency. However, passage-based
    measures reported in the literature tend to treat passages as independent events,
    without explicitly modeling accumulation of lexical experience as one reads through
    a book. We propose such a model and show that it helps explain additional variance
    in the measurements of children's fluency as they read through a book, improving
    over a strong baseline. These results have implications for measuring growth in
    oral reading fluency.
  authors:
  - Beata Beigman Klebanov
  - Michael Suhan
  - Zuowei Wang
  - Tenaha O'reilly
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_81
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: A dynamic model of lexical experience for tracking of oral reading fluency
  tldr: We present research aimed at solving a problem in  assessment of oral reading
    fluency using children's oral reading data from our online book reading app. It
    is known that properties of the passage being read aloud impact fluency estimates;
    therefore, passage-based measures are  used to remove passa
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Essay scoring is a critical task used to evaluate second-language (L2)
    writing proficiency on high-stakes  language assessments.  While automated scoring
    approaches  are mature and have been around for decades,  human scoring is still
    considered the gold standard, despite its high costs and  well-known issues such
    as human rater fatigue and bias.  The recent introduction of large language models
    (LLMs) brings new opportunities for automated scoring.  In this paper, we evaluate
    how well GPT-3.5 and GPT-4 can rate short essay responses written by L2 English
    learners on a high-stakes language assessment, computing inter-rater agreement
    with human ratings. Results show that when calibration examples are provided,
    GPT-4 can perform almost as well as modern Automatic Writing Evaluation (AWE)
    methods, but agreement with human ratings can vary depending on the test-taker's
    first language (L1).
  authors:
  - Kevin P. Yancey
  - Geoffrey Laflair
  - Anthony Verardi
  - Jill Burstein
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_86
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Rating Short L2 Essays on the CEFR Scale with GPT-4
  tldr: Essay scoring is a critical task used to evaluate second-language (L2) writing
    proficiency on high-stakes  language assessments.  While automated scoring approaches  are
    mature and have been around for decades,  human scoring is still considered the
    gold standard, despite its high costs and  well-kn
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: L1-L2 parallel dependency treebanks are UD-annotated corpora of learner
    sentences paired with correction hypotheses. Automatic morphosyntactical annotation
    has the potential to remove the need for explicit manual error tagging and improve
    interoperability, but makes it more challenging to locate grammatical errors in
    the resulting datasets. We therefore propose a novel method for automatically
    extracting morphosyntactical error patterns and perform a preliminary bilingual
    evaluation of its first implementation through a similar example retrieval task.
    The resulting pipeline is also available as a prototype CALL application.
  authors:
  - Arianna Masciolini
  - Elena Volodina
  - Dana Dannlls
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_88
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Towards automatically extracting morphosyntactical error patterns from L1-L2
    parallel dependency treebanks
  tldr: L1-L2 parallel dependency treebanks are UD-annotated corpora of learner sentences
    paired with correction hypotheses. Automatic morphosyntactical annotation has
    the potential to remove the need for explicit manual error tagging and improve
    interoperability, but makes it more challenging to locate gra
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Since performing exercises (including, e.g.,practice tests) forms a crucial
    component oflearning, and creating such exercises requiresnon-trivial effort from
    the teacher. There is agreat value in automatic exercise generationin digital
    tools in education. In this paper, weparticularly focus on automatic creation
    of gap-filling exercises for language learning, specifi-cally grammar exercises.
    Since providing anyannotation in this domain requires human ex-pert effort, we
    aim to avoid it entirely and ex-plore the task of converting existing texts intonew
    gap-filling exercises, purely based on anexample exercise, without explicit instructionor
    detailed annotation of the intended gram-mar topics. We contribute (i) a novel
    neuralnetwork architecture specifically designed foraforementioned gap-filling
    exercise generationtask, and (ii) a real-world benchmark datasetfor French grammar.
    We show that our modelfor this French grammar gap-filling exercisegeneration outperforms
    a competitive baselineclassifier by 8\% in F1 percentage points, achiev-ing an
    average F1 score of 82\%. Our model im-plementation and the dataset are made publiclyavailable
    to foster future research, thus offeringa standardized evaluation and baseline
    solutionof the proposed partially annotated data predic-tion task in grammar exercise
    creation.
  authors:
  - Semere Kiros Bitew
  - Johannes Deleu
  - A. Seza Doruz
  - Chris Develder
  - Thomas Demeester
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_90
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Learning from Partially Annotated Data: Example-aware Creation of Gap-filling
    Exercises for Language Learning'
  tldr: Since performing exercises (including, e.g.,practice tests) forms a crucial
    component oflearning, and creating such exercises requiresnon-trivial effort from
    the teacher. There is agreat value in automatic exercise generationin digital
    tools in education. In this paper, weparticularly focus on autom
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The recent advancement of pre-trained Large Language Models (LLMs), such
    as OpenAI's ChatGPT, has led to transformative changes across fields. For example,
    developing intelligent systems in the educational sector that leverage the linguistic
    capabilities of LLMs demonstrates a visible potential. Though researchers have
    recently explored how ChatGPT could possibly assist in student learning, few studies
    have applied these techniques to real-world classroom settings involving teachers
    and students. In this study, we implement a reading comprehension exercise generation
    system that provides high-quality and personalized reading materials for middle
    school English learners in China. Extensive evaluations of the generated reading
    passages and corresponding exercise questions, conducted both automatically and
    manually, demonstrate that the system-generated materials are suitable for students
    and even surpass the quality of existing human-written ones. By incorporating
    first-hand feedback and suggestions from experienced educators, this study serves
    as a meaningful pioneering application of ChatGPT, shedding light on the future
    design and implementation of LLM-based systems in the educational context.
  authors:
  - Changrong Xiao
  - Sean Xin Xu
  - Kunpeng Zhang
  - Yufang Wang
  - Lei Xia
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_93
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Evaluating Reading Comprehension Exercises Generated by LLMs: A Showcase
    of ChatGPT in Education Applications'
  tldr: The recent advancement of pre-trained Large Language Models (LLMs), such as
    OpenAI's ChatGPT, has led to transformative changes across fields. For example,
    developing intelligent systems in the educational sector that leverage the linguistic
    capabilities of LLMs demonstrates a visible potential. Tho
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Coaching, which involves classroom observation and expert feedback, is
    a widespread and fundamental part of teacher training. However, the majority of
    teachers do not have access to consistent, high quality coaching due to limited
    resources and access to expertise. We explore whether generative AI could become
    a cost-effective complement to expert feedback by serving as an automated teacher
    coach. In doing so, we propose three teacher coaching tasks for generative AI:
    (A) scoring transcript segments based on classroom observation instruments, (B)identifying
    highlights and missed opportunities for good instructional strategies, and (C)
    providing actionable suggestions for eliciting more student reasoning. We recruit
    expert math teachers to evaluate the zero-shot performance of ChatGPT on each
    of these tasks for elementary math classroom transcripts. Our results reveal that
    ChatGPT generates responses that are relevant to improving instruction, but they
    are often not novel or insightful.  For example, 82\% of the model''s suggestions
    point to places in the transcript where the teacher is already implementing that
    suggestion. Our work highlights the challenges of producing insightful, novel
    and truthful feedback for teachers while paving the way for future research to
    address these obstacles and improve the capacity of generative AI to coach teachers.'
  authors:
  - Rose Wang
  - Dorottya Demszky
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_97
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Is ChatGPT a Good Teacher Coach? Measuring Zero-Shot Performance For Scoring
    and Providing Actionable Insights on Classroom Instruction
  tldr: Coaching, which involves classroom observation and expert feedback, is a widespread
    and fundamental part of teacher training. However, the majority of teachers do
    not have access to consistent, high quality coaching due to limited resources
    and access to expertise. We explore whether generative AI c
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In English speaking assessment, pretrained large language models (LLMs)
    such as BERT can score constructed response items as accurately as human raters.
    Less research has investigated whether LLMs perpetuate or exacerbate biases, which
    would pose problems for the fairness and validity of the test. This study examines
    gender and native language (L1) biases in human and automated scores, using an
    off-the-shelf (OOS) BERT model. Analyses focus on a specific type of bias known
    as differential item functioning (DIF), which compares examinees of similar English
    language proficiency. Results show that there is a moderate amount of DIF, based
    on examinees' L1 background in grade band 912. DIF is higher when scored by an
    OOS BERT model, indicating that BERT may exacerbate this bias; however, in practical
    terms, the degree to which BERT exacerbates DIF is very small. Additionally, there
    is more DIF for longer speaking items and for older examinees, but BERT does not
    exacerbate these patterns of DIF.
  authors:
  - Alexander Kwako
  - Yixin Wan
  - Jieyu Zhao
  - Mark Hansen
  - Kai-Wei Chang
  - Li Cai
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_99
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Does BERT Exacerbate Gender or L1 Biases in Automated English Speaking Assessment?
  tldr: In English speaking assessment, pretrained large language models (LLMs) such
    as BERT can score constructed response items as accurately as human raters. Less
    research has investigated whether LLMs perpetuate or exacerbate biases, which
    would pose problems for the fairness and validity of the test. T
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We study the new problem of automatic question generation (QG) from multi-modal
    sources containing images and texts, significantly expanding the scope of most
    of the existing work that focuses exclusively on QG from only textual sources.
    We propose a simple solution for our new problem, called MultiQG-TI, which enables
    a text-only question generator to process visual input in addition to textual
    input. Specifically, we leverage an image-to-text model and an optical character
    recognition model to obtain the textual description of the image and extract any
    texts in the image, respectively, and then feed them together with the input texts
    to the question generator. We only fine-tune the question generator while keeping
    the other components fixed. On the challenging ScienceQA dataset, we demonstrate
    that MultiQG-TI significantly outperforms ChatGPT with few-shot prompting, despite
    having hundred-times less trainable parameters. Additional analyses empirically
    confirm the necessity of both visual and textual signals for QG and show the impact
    of various modeling choices. Code is available at https://anonymous.4open.science/r/multimodal-QG-47F2/
  authors:
  - Zichao Wang
  - Richard Baraniuk
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_100
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'MultiQG-TI: Towards Question Generation from Multi-modal Sources'
  tldr: We study the new problem of automatic question generation (QG) from multi-modal
    sources containing images and texts, significantly expanding the scope of most
    of the existing work that focuses exclusively on QG from only textual sources.
    We propose a simple solution for our new problem, called Multi
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Enriching the quality of early childhood education with interactive math
    learning at home systems, empowered by recent advances in conversational AI technologies,
    is slowly becoming a reality. With this motivation, we implement a multimodal
    dialogue system to support play-based learning experiences at home, guiding kids
    to master basic math concepts. This work explores Spoken Language Understanding
    (SLU) pipeline within a task-oriented dialogue system developed for Kid Space,
    with cascading Automatic Speech Recognition (ASR) and Natural Language Understanding
    (NLU) components evaluated on our home deployment data with kids going through
    gamified math learning activities. We validate the advantages of a multi-task
    architecture for NLU and experiment with a diverse set of pretrained language
    representations for Intent Recognition and Entity Extraction tasks in the math
    learning domain. To recognize kids' speech in realistic home environments, we
    investigate several ASR systems, including the commercial Google Cloud and the
    latest open-source Whisper solutions with varying model sizes. We evaluate the
    SLU pipeline by testing our best-performing NLU models on noisy ASR output to
    inspect the challenges of understanding children for math learning in authentic
    homes.
  authors:
  - Eda Okur
  - Roddy Fuentes Alba
  - Saurav Sahay
  - Lama Nachman
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_102
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Inspecting Spoken Language Understanding from Kids for Basic Math Learning
    at Home
  tldr: 'Enriching the quality of early childhood education with interactive math
    learning at home systems, empowered by recent advances in conversational AI technologies,
    is slowly becoming a reality. With this motivation, we implement a multimodal
    dialogue system to support play-based learning experiences '
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Socratic questioning is a teaching strategy where the student is guided
    towards solving a problem on their own, instead of being given the solution directly.
    In this paper, we introduce a dataset of Socratic conversations where an instructor
    helps a novice programmer fix buggy solutions to simple computational problems.
    The dataset is then used for benchmarking the Socratic debugging abilities of
    GPT-based language models. While GPT-4 is observed to perform much better than
    GPT-3.5, its precision, and recall still fall short of human expert abilities,
    motivating further work in this area.
  authors:
  - Erfan Al-Hossami
  - Razvan Bunescu
  - Ryan Teehan
  - Laurel Powell
  - Khyati Mahajan
  - Mohsen Dorodchi
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_106
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Socratic Questioning of Novice Debuggers: A Benchmark Dataset and Preliminary
    Evaluations'
  tldr: Socratic questioning is a teaching strategy where the student is guided towards
    solving a problem on their own, instead of being given the solution directly.
    In this paper, we introduce a dataset of Socratic conversations where an instructor
    helps a novice programmer fix buggy solutions to simple co
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The increasing reliance on large language models (LLMs) in academic writing
    has led to a rise in plagiarism. Existing AI-generated text classifiers have limited
    accuracy and often produce false positives. We propose a novel approach using
    natural language processing (NLP) techniques, offering quantifiable metrics at
    both sentence and document levels for easier interpretation by human evaluators.
    Our method employs a multi-faceted approach, generating multiple paraphrased versions
    of a given question and inputting them into the LLM to generate answers. By using
    a contrastive loss function based on cosine similarity, we match generated sentences
    with those from the student's response. Our approach achieves up to 94\% accuracy
    in classifying human and AI text, providing a robust and adaptable solution for
    plagiarism detection in academic settings. This method improves with LLM advancements,
    reducing the need for new model training or reconfiguration, and offers a more
    transparent way of evaluating and detecting AI-generated text.
  authors:
  - Ali Quidwai
  - Chunhui Li
  - Parijat Dube
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_110
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Beyond Black Box AI generated Plagiarism Detection: From Sentence to Document
    Level'
  tldr: The increasing reliance on large language models (LLMs) in academic writing
    has led to a rise in plagiarism. Existing AI-generated text classifiers have limited
    accuracy and often produce false positives. We propose a novel approach using
    natural language processing (NLP) techniques, offering quanti
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Reinforcement Learning remains an underutilized method of training and
    fine-tuning Language Models (LMs) despite recent successes. This paper presents
    a simple approach of fine-tuning a language model with Reinforcement Learning
    to achieve competitive performance on the BEA 2023 Shared Task whose goal is to
    automatically generate teacher responses in educational dialogues. We utilized
    the novel NLPO algorithm that masks out tokens during generation to direct the
    model towards generations that maximize a reward function. We show results for
    both the t5-base model with 220 million parameters from the HuggingFace repository
    submitted to the leaderboard that, despite its comparatively small size, has achieved
    a good performance on both test and dev set, as well as GPT-2 with 124 million
    parameters. The presented results show that despite maximizing only one of the
    metrics used in the evaluation as a reward function our model scores highly in
    the other metrics as well.
  authors:
  - Thomas Huber
  - Christina Niklaus
  - Siegfried Handschuh
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_111
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Enhancing Educational Dialogues: A Reinforcement Learning Approach for Generating
    AI Teacher Responses'
  tldr: Reinforcement Learning remains an underutilized method of training and fine-tuning
    Language Models (LMs) despite recent successes. This paper presents a simple approach
    of fine-tuning a language model with Reinforcement Learning to achieve competitive
    performance on the BEA 2023 Shared Task whose go
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: (Tack et al., 2023) organized the shared task hosted by the 18th Workshop
    on Innovative Use of NLP for Building Educational Applications on generation of
    teacher language in educational dialogues. Following the structure of the shared
    task, in this study, we attempt to assess the generative abilities of large language
    models in providing informative and helpful insights to students, thereby simulating
    the role of a knowledgeable teacher. To this end, we present an extensive evaluation
    of several benchmarking generative models, including GPT-4 (few-shot, in-context
    learning), fine-tuned GPT-2, and fine-tuned DialoGPT. Additionally, to optimize
    for pedagogical quality, we fine-tuned the Flan-T5 model using reinforcement learning.
    Our experimental findings on the Teacher-Student Chatroom Corpus subset indicate
    the efficacy of GPT-4 over other fine-tuned models, measured using BERTScore and
    DialogRPT. We hypothesize that several dataset characteristics, including sampling,
    representativeness, and dialog completeness, pose significant challenges to fine-tuning,
    thus contributing to the poor generalizability of the fine-tuned models. Finally,
    we note the need for these generative models to be evaluated with a metric that
    relies not only on dialog coherence and matched language modeling distribution
    but also on the model's ability to showcase pedagogical skills.
  authors:
  - Yann Hicke
  - Abhishek Masand
  - Wentao Guo
  - Tushaar Gangavarapu
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_112
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Assessing the efficacy of large language models in generating accurate teacher
    responses
  tldr: (Tack et al., 2023) organized the shared task hosted by the 18th Workshop
    on Innovative Use of NLP for Building Educational Applications on generation of
    teacher language in educational dialogues. Following the structure of the shared
    task, in this study, we attempt to assess the generative abilitie
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper presents the results of our participation in the BEA 2023 shared
    task, which focuses on generating AI teacher responses in educational dialogues.
    We conducted experiments using several Open-Source Large Language Models (LLMs)
    and explored fine-tuning techniques along with prompting strategies, including
    Few-Shot and Chain-of-Thought approaches. Our best model was ranked 4.5 in the
    competition with a BertScore F1 of 0.71 and a DialogRPT final (avg) of 0.35. Nevertheless,
    our internal results did not exactly correlate with those obtained in the competition,
    which showed the difficulty in evaluating this task. Other challenges we faced
    were data leakage on the train set and the irregular format of the conversations.
  authors:
  - Alexis Baladn
  - Ignacio Sastre
  - Luis Chiruzzo
  - Aiala Ros
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_113
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'RETUYT-InCo at BEA 2023 Shared Task: Tuning Open-Source LLMs for Generating
    Teacher Responses'
  tldr: This paper presents the results of our participation in the BEA 2023 shared
    task, which focuses on generating AI teacher responses in educational dialogues.
    We conducted experiments using several Open-Source Large Language Models (LLMs)
    and explored fine-tuning techniques along with prompting strate
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Language models are one of the biggest game changers in downstream NLP
    applications, especially in conversational agents. In spite of their awesome capabilities
    to generated responses to solve the inquireis, there are still some big challenges
    to using them. One challenge is how to enable the LLMs to use the private internal
    data to solve inquires. And secondly, how to keep the LLMs updated with newly
    incoming data without the burden of fine-tuning as it is not only expensive but
    also not an available option for some commercial LLMs, such as ChatGPT. In this
    work, we propose Semantic In-Context Learning (S-ICL) to address the aforementioned
    challenges. Our approach was participated in the BEA 2023 shared task and ended
    up having the fourth place in both development and evaluation phases.
  authors:
  - Amin Omidvar
  - Aijun An
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_114
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Empowering Conversational Agents using Semantic In-Context Learning
  tldr: Language models are one of the biggest game changers in downstream NLP applications,
    especially in conversational agents. In spite of their awesome capabilities to
    generated responses to solve the inquireis, there are still some big challenges
    to using them. One challenge is how to enable the LLMs t
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper presents our approach to the BEA 2023 shared task of generating
    teacher responses in educational dialogues, using the Teacher-Student Chatroom
    Corpus. Our system prompts GPT-3.5-turbo to generate initial suggestions, which
    are then subjected to reranking. We explore multiple strategies for candidate
    generation, including prompting for multiple candidates and employing iterative
    few-shot prompts with negative examples. We aggregate all candidate responses
    and rerank them based on DialogRPT scores. To handle consecutive turns in the
    dialogue data, we divide the task of generating teacher utterances into two components:
    teacher replies to the student and teacher continuations of previously sent messages.
    Through our proposed methodology, our system achieved the top score on both automated
    metrics and human evaluation, surpassing the reference human teachers on the latter.'
  authors:
  - Justin Vasselli
  - Christopher Vasselli
  - Adam Nohejl
  - Taro Watanabe
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_115
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'NAISTeacher: A Prompt and Rerank Approach to Generating Teacher Utterances
    in Educational Dialogues'
  tldr: This paper presents our approach to the BEA 2023 shared task of generating
    teacher responses in educational dialogues, using the Teacher-Student Chatroom
    Corpus. Our system prompts GPT-3.5-turbo to generate initial suggestions, which
    are then subjected to reranking. We explore multiple strategies fo
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes the results of the first shared task on generation
    of teacher responses in educational dialogues. The goal of the task was to benchmark
    the ability of generative language models to act as AI teachers, replying to a
    student in a teacherstudent dialogue. Eight teams participated in the competition
    hosted on CodaLab and experimented with a wide variety of state-of-the-art models,
    including Alpaca, Bloom, DialoGPT, DistilGPT-2, Flan-T5, GPT- 2, GPT-3, GPT-4,
    LLaMA, OPT-2.7B, and T5- base. Their submissions were automatically scored using
    BERTScore and DialogRPT metrics, and the top three among them were further manually
    evaluated in terms of pedagogical ability based on Tack and Piech (2022). The
    NAISTeacher system, which ranked first in both automated and human evaluation,
    generated responses with GPT-3.5 Turbo using an ensemble of prompts and DialogRPT-based
    ranking of responses for given dialogue contexts. Despite promising achievements
    of the participating teams, the results also highlight the need for evaluation
    metrics better suited to educational contexts.
  authors:
  - Anas Tack
  - Ekaterina Kochmar
  - Zheng Yuan
  - Serge Bibauw
  - Chris Piech
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_116
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: The BEA 2023 Shared Task on Generating AI Teacher Responses in Educational
    Dialogues
  tldr: This paper describes the results of the first shared task on generation of
    teacher responses in educational dialogues. The goal of the task was to benchmark
    the ability of generative language models to act as AI teachers, replying to a
    student in a teacherstudent dialogue. Eight teams participated i
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper presents the ADAIO team's system entry in the Building Educational
    Applications (BEA) 2023 Shared Task on Generating AI Teacher Responses in Educational
    Dialogues. The task aims to assess the performance of state-of-the-art generative
    models as AI teachers in producing suitable responses within a student-teacher
    dialogue. Our system comprises evaluating various baseline models using OpenAI
    GPT-3 and designing diverse prompts to prompt the OpenAI models for teacher response
    generation. After the challenge, our system achieved second place by employing
    a few-shot prompt-based approach with the OpenAI text-davinci-003 model. The results
    highlight the few-shot learning capabilities of large-language models, particularly
    OpenAI's GPT-3, in the role of AI teachers.
  authors:
  - Adaeze Adigwe
  - Zheng Yuan
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - BEA
  forum: ''
  id: BEA_117
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'The ADAIO System at the BEA-2023 Shared Task: Shared Task Generating AI
    Teacher Responses in Educational Dialogues'
  tldr: 'This paper presents the ADAIO team''s system entry in the Building Educational
    Applications (BEA) 2023 Shared Task on Generating AI Teacher Responses in Educational
    Dialogues. The task aims to assess the performance of state-of-the-art generative
    models as AI teachers in producing suitable responses '
  track: 18th Workshop on Innovative Use of NLP for Building Educational Applications
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Clean-label (CL) attack is a form of data poisoning attack where an adversary  modifies
    only the textual input of the training data, without requiring access to the labeling
    function. CL attacks are relatively unexplored in NLP, as compared to label flipping
    (LF) attacks, where the latter additionally requires access to the labeling function
    as well. While CL attacks are more resilient to data sanitization and manual relabeling
    methods than LF attacks, they often demand as high as ten times the  poisoning
    budget than LF attacks.  In this work, we first  introduce an Adversarial Clean
    Label attack  which can adversarially perturb in-class training examples for poisoning
    the training set. We then show that an adversary can significantly bring down
    the data requirements for a CL attack, using the aforementioned approach, to as
    low as 20 \% of the data otherwise required.  We then systematically benchmark
    and analyze a number of defense methods, for both LF and CL attacks, some previously
    employed solely for LF attacks in the textual domain and others adapted from computer
    vision. We find that text-specific defenses greatly vary in their effectiveness
    depending on their properties.
  authors:
  - Ashim Gupta
  - Amrith Krishna
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - RepL4NLP
  forum: ''
  id: ACL_3
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Adversarial Clean Label Backdoor Attacks and Defenses on Text Classification
    Systems
  tldr: Clean-label (CL) attack is a form of data poisoning attack where an adversary  modifies
    only the textual input of the training data, without requiring access to the labeling
    function. CL attacks are relatively unexplored in NLP, as compared to label flipping
    (LF) attacks, where the latter additional
  track: The 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'We propose a novel task-agnostic in-domain pre-training method that sits
    between generic pre-training and fine-tuning. Our approach selectively masks in-domain
    keywords, i.e., words that provide a compact representation of the target domain.
    We identify such keywords using KeyBERT (Grootendorst, 2020). We evaluate our
    approach using six different settings: three datasets combined with two distinct
    pre-trained language models (PLMs). Our results reveal that the fine-tuned PLMs
    adapted using our in-domain pre-training strategy outperform PLMs that used in-domain
    pre-training with random masking as well as those that followed the common pre-train-then-fine-tune
    paradigm. Further, the overhead of identifying in-domain keywords is reasonable,
    e.g., 7-15% of the pre-training time (for two epochs) for BERT Large (Devlin et
    al., 2019).'
  authors:
  - Shahriar Golchin
  - Mihai Surdeanu
  - Nazgol Tavabi
  - Ata Kiapour
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - RepL4NLP
  forum: ''
  id: ACL_4
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Do not Mask Randomly: Effective Domain-adaptive Pre-training by Masking
    In-domain Keywords'
  tldr: We propose a novel task-agnostic in-domain pre-training method that sits between
    generic pre-training and fine-tuning. Our approach selectively masks in-domain
    keywords, i.e., words that provide a compact representation of the target domain.
    We identify such keywords using KeyBERT (Grootendorst, 202
  track: The 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Sentence embeddings induced with various transformer architectures encode
    much semantic and syntactic information in a distributed manner in a one-dimensional
    array. We investigate whether specific grammatical information can be accessed
    in these distributed representations. Using data from a task developed to test
    rule-like generalizations, our experiments on detecting subject-verb agreement
    yield several promising results. First, we show that while the usual sentence
    representations encoded as one-dimensional arrays do not easily support extraction
    of rule-like regularities, a two-dimensional  reshaping of these vectors allows
    various learning architectures to access such information. Next, we show that
    various architectures can detect patterns in these two-dimensional reshaped sentence
    embeddings and successfully learn a model based on smaller amounts of simpler
    training data, which performs well on more complex test data. This indicates that
    current sentence embeddings contain information that is regularly distributed,
    and which can be captured when the embeddings are reshaped into higher dimensional
    arrays. Our results cast light on representations produced by language models
    and help move towards developing few-shot learning approaches.
  authors:
  - Vivi Nastase
  - Paola Merlo
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - RepL4NLP
  forum: ''
  id: ACL_5
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Grammatical information in BERT sentence embeddings as two-dimensional arrays
  tldr: Sentence embeddings induced with various transformer architectures encode
    much semantic and syntactic information in a distributed manner in a one-dimensional
    array. We investigate whether specific grammatical information can be accessed
    in these distributed representations. Using data from a task d
  track: The 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Adversarial evaluations of language models typically focus on English
    alone. In this paper, we performed a multilingual evaluation of Named Entity Recognition
    (NER) in terms of its robustness to small perturbations in the input. Our results
    showed the NER models we explored across three languages (English, German and
    Hindi) are not very robust to such changes, as indicated by the fluctuations in
    the overall F1 score as well as in a more fine-grained evaluation. With that knowledge,
    we further explored whether it is possible to improve the existing NER models
    using a part of the generated adversarial data sets as augmented training data
    to train a new NER model or as fine-tuning data to adapt an existing NER model.
    Our results showed that both these approaches improve performance on the original
    as well as adversarial test sets. While there is no significant difference between
    the two approaches for English, re-training is significantly better than fine-tuning
    for German and Hindi.  '
  authors:
  - Akshay Srinivasan
  - Sowmya Vajjala
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - RepL4NLP
  forum: ''
  id: ACL_6
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: A Multilingual Evaluation of NER Robustness to Adversarial Inputs
  tldr: Adversarial evaluations of language models typically focus on English alone.
    In this paper, we performed a multilingual evaluation of Named Entity Recognition
    (NER) in terms of its robustness to small perturbations in the input. Our results
    showed the NER models we explored across three languages (E
  track: The 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: "Language models pretrained on general domain corpora usually exhibit\
    \ considerable degradation when generalizing to downstream tasks of specialized\
    \ domains. Existing approaches try to construct PLMs for each speci\uFB01c domains\
    \ either from scratch or through further pretraining, which not only costs substantial\
    \ resources, but also fails to cover all target domains at various granularity.\
    \ In this work, we propose RADA, a novel Retrieval-Augmented framework for Domain\
    \ Adaptation. We \uFB01rst construct a textual corpora that covers the downstream\
    \ task at \uFB02exible domain granularity and resource availability. We employ\
    \ it as a pluggable datastore to retrieve informative background knowledge, and\
    \ integrate them into the standard language model framework to augment representations.\
    \ We then propose a two-level selection scheme to integrate the most relevant\
    \ information while alleviating irrelevant noises. Speci\uFB01cally, we introduce\
    \ a differentiable sampling module as well as an attention mechanism to achieve\
    \ both passage-level and word-level selection. Such a retrieval-augmented framework\
    \ enables domain adaptation of language models with \uFB02exible domain coverage\
    \ and \uFB01ne-grained domain knowledge integration. We conduct comprehensive\
    \ experiments across biomedical, science and legal domains to demonstrate the\
    \ effectiveness of the overall framework, and its advantage over existing solutions."
  authors:
  - Benfeng Xu
  - Chunxu Zhao
  - Wenbin Jiang
  - PengFei Zhu
  - Songtai Dai
  - Chao Pang
  - Zhuo Sun
  - Shuohuan Wang
  - Yu Sun
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - RepL4NLP
  forum: ''
  id: ACL_7
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Retrieval-Augmented Domain Adaptation of Language Models
  tldr: "Language models pretrained on general domain corpora usually exhibit considerable\
    \ degradation when generalizing to downstream tasks of specialized domains. Existing\
    \ approaches try to construct PLMs for each speci\uFB01c domains either from scratch\
    \ or through further pretraining, which not only costs subs"
  track: The 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Diffusion probabilistic models have shown great success in generating
    high-quality images controllably, and researchers have tried to utilize this controllability
    into text generation domain. Previous works on diffusion-based language models
    have shown that they can be trained without external knowledge (such as pre-trained
    weights) and still achieve stable performance and controllability. In this paper,
    we trained a diffusion-based model on StylePTB dataset, the standard benchmark
    for fine-grained text style transfers. The tasks in StylePTB requires much more
    refined control over the output text compared to tasks evaluated in previous works,
    and our model was able to achieve state-of-the-art performance on StylePTB on
    both individual and compositional transfers. Moreover, our model, trained on limited
    data from StylePTB without external knowledge, outperforms previous works that
    utilized pretrained weights, embeddings, and external grammar parsers, and this
    may indicate that diffusion-based language models have great potential under low-resource
    settings.
  authors:
  - Yiwei Lyu
  - Tiange Luo
  - Jiacheng Shi
  - Todd Hollon
  - Honglak Lee
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - RepL4NLP
  forum: ''
  id: ACL_12
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Fine-grained Text Style Transfer with Diffusion-Based Language Models
  tldr: Diffusion probabilistic models have shown great success in generating high-quality
    images controllably, and researchers have tried to utilize this controllability
    into text generation domain. Previous works on diffusion-based language models
    have shown that they can be trained without external knowl
  track: The 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: "Although Question Answering (QA) have advanced to the human-level language\
    \ skills in NLP tasks, there is still a problem: the QA model gets confused when\
    \ there are similar sentences or paragraphs. Existing studies focus on enhancing\
    \ the text understanding of the candidate answers to improve the overall performance\
    \ of the QA models. However, since these methods focus on re-ranking queries or\
    \ candidate answers, they fail to resolve the confusion when many generated answers\
    \ are similar to the expected answer. To address these issues, we propose a novel\
    \ contrastive learning framework called ContrastiveQA that alleviates the confusion\
    \ problem in answer extraction. We propose a supervised method where we generate\
    \ positive and negative samples from the candidate answers and the given answer,\
    \ respectively. \nWe thus introduce ContrastiveQA, which uses contrastive learning\
    \ with sampling data to reduce incorrect answers. Experimental results on four\
    \ QA benchmarks show the effectiveness of the proposed method."
  authors:
  - Seungyeon Lee
  - Minho Lee
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - RepL4NLP
  forum: ''
  id: ACL_13
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Enhancing text comprehension for Question Answering with Contrastive Learning '
  tldr: 'Although Question Answering (QA) have advanced to the human-level language
    skills in NLP tasks, there is still a problem: the QA model gets confused when
    there are similar sentences or paragraphs. Existing studies focus on enhancing
    the text understanding of the candidate answers to improve the over'
  track: The 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Machine comprehension of procedural texts is essential for reasoning about
    the steps and automating the procedures. However, this requires identifying entities
    within a text and resolving the relationships between the entities. Previous work
    focused on the cooking domain and proposed a framework to convert a recipe text
    into a flow graph (FG) representation. In this work, we propose a framework based
    on the recipe FG for flow graph prediction of open-domain procedural texts. To
    investigate flow graph prediction performance in non-cooking domains, we introduce
    the wikiHow-FG corpus from articles on wikiHow, a website of how-to instruction
    articles. In experiments, we consider using the existing recipe corpus and performing
    domain adaptation from the cooking to the target domain. Experimental results
    show that the domain adaptation models achieve higher performance than those trained
    only on the cooking or target domain data.
  authors:
  - Keisuke Shirai
  - Hirotaka Kameko
  - Shinsuke Mori
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - RepL4NLP
  forum: ''
  id: ACL_14
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Towards Flow Graph Prediction of Open-Domain Procedural Texts
  tldr: 'Machine comprehension of procedural texts is essential for reasoning about
    the steps and automating the procedures. However, this requires identifying entities
    within a text and resolving the relationships between the entities. Previous work
    focused on the cooking domain and proposed a framework to '
  track: The 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Current multimodal models, aimed at solving Vision and Language (V+L)
    tasks, predominantly repurpose  Vision Encoders (VE) as feature extractors. While
    many VEs---of different architectures, trained on different data and objectives---are
    publicly available, they are not designed for the downstream  V+L tasks. Nonetheless,
    most current work assumes that a \textit{single} pre-trained VE can serve as a
    general-purpose encoder. In this work, we focus on analysis and aim to understand
    whether the information stored within different VEs is complementary, i.e. if
    providing the model with features from multiple VEs can improve the performance
    on a target task, and how they are combined. We exhaustively experiment with three
    popular VEs on six downstream V+L tasks and analyze the attention and VE-dropout
    patterns. Our analyses suggest that diverse VEs complement each other, resulting
    in improved downstream V+L task performance, where the improvements are not due
    to simple ensemble effects (i.e. the performance does not always improve when
    increasing the number of encoders).  We demonstrate that future VEs, which are
    not \textit{repurposed}, but explicitly \textit{designed} for V+L tasks, have
    the potential of improving performance on the target V+L tasks. '
  authors:
  - Gregor Geigle
  - Chen Liu
  - Jonas Pfeiffer
  - Iryna Gurevych
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - RepL4NLP
  forum: ''
  id: ACL_16
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: One does not fit all! On the Complementarity of Vision Encoders for Vision
    and Language Tasks
  tldr: 'Current multimodal models, aimed at solving Vision and Language (V+L) tasks,
    predominantly repurpose  Vision Encoders (VE) as feature extractors. While many
    VEs---of different architectures, trained on different data and objectives---are
    publicly available, they are not designed for the downstream  '
  track: The 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Recent advances in prompt tuning have proven effective as a new language
    modeling paradigm for various natural language understanding tasks. However, it
    is challenging to adapt the soft prompt embeddings to different domains or generalize
    to low-data settings when learning soft prompts itself is unstable, task-specific,
    and bias-prone. This paper proposes a principled learning framework---soft prompt
    construction (SPC)---to facilitate learning domain-adaptable soft prompts. Derived
    from the SPC framework is a simple loss that can plug into various models and
    tuning approaches to improve their cross-domain performance. We show SPC can improve
    upon SOTA for contextual query rewriting, summarization, and paraphrase detection
    by up to 5\%, 19\%, and 16\%, respectively.
  authors:
  - Wenbo Zhao
  - Arpit Gupta
  - Tagyoung Chung
  - Jing Huang
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - RepL4NLP
  forum: ''
  id: ACL_18
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'SPC: Soft Prompt Construction for Cross Domain Generalization'
  tldr: Recent advances in prompt tuning have proven effective as a new language modeling
    paradigm for various natural language understanding tasks. However, it is challenging
    to adapt the soft prompt embeddings to different domains or generalize to low-data
    settings when learning soft prompts itself is uns
  track: The 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: "We propose KGT5-context, a simple sequence-to-sequence model for link\
    \ prediction (LP) in knowledge graphs (KG). Our work expands on KGT5, a recent\
    \ LP model that exploits textual features of the KG, has small model size, and\
    \ is scalable. To reach good predictive performance, however, KGT5 relies on an\
    \ ensemble with a knowledge graph embedding model, which itself is excessively\
    \ large and costly to use. In this short paper, we show empirically that adding\
    \ contextual information \u2014 i.e., information about the direct neighborhood\
    \ of the query entity \u2014 alleviates the need for a separate KGE model to obtain\
    \ good performance. The resulting KGT5-context model is simple, reduces model\
    \ size significantly, and obtains state-of-the-art performance in our experimental\
    \ study."
  authors:
  - Adrian Kochsiek
  - Apoorv Saxena
  - Inderjeet Nair
  - Rainer Gemulla
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - RepL4NLP
  forum: ''
  id: ACL_23
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Friendly Neighbors: Contextualized Sequence-to-Sequence Link Prediction'
  tldr: We propose KGT5-context, a simple sequence-to-sequence model for link prediction
    (LP) in knowledge graphs (KG). Our work expands on KGT5, a recent LP model that
    exploits textual features of the KG, has small model size, and is scalable. To
    reach good predictive performance, however, KGT5 relies on a
  track: The 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The widespread usage of latent language representations via pre-trained
    language models (LMs) suggests that they are a promising source of structured
    knowledge. However, existing methods focus only on a single object per subject-relation
    pair, even though often multiple objects are correct. To overcome this limitation,
    we analyze these representations for their potential to yield materialized multi-object
    relational knowledge. We formulate the problem as a rank-then-select task. For
    ranking candidate objects, we evaluate existing prompting techniques and propose
    new ones incorporating domain knowledge.  Among the selection methods, we find
    that choosing objects with a likelihood above a learned relation-specific threshold
    gives a 49.5\% F1 score. Our results highlight the difficulty of employing LMs
    for the multi-valued slot-filling task, and pave the way for further research
    on extracting relational knowledge from latent language representations.
  authors:
  - Sneha Singhania
  - Simon Razniewski
  - Gerhard Weikum
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - RepL4NLP
  forum: ''
  id: ACL_26
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Extracting Multi-valued Relations from Language Models
  tldr: The widespread usage of latent language representations via pre-trained language
    models (LMs) suggests that they are a promising source of structured knowledge.
    However, existing methods focus only on a single object per subject-relation pair,
    even though often multiple objects are correct. To overc
  track: The 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Since the introduction of the SemEval $2020$ Task $11$ \citep{task},
    several approaches have been proposed in the literature for classifying propaganda

    based on the rhetorical techniques used to influence readers.

    These methods, however, classify one span at a time, ignoring dependencies from
    the labels of other spans within the same context.

    In this paper, we approach propaganda technique classification as a

    Multi-Instance Multi-Label (MIML) learning problem \citep{miml} and propose a
    simple RoBERTa-based model \citep{roberta} for classifying all spans in an article
    simultaneously. Further, we note that, due to the annotation process where

    annotators classified the spans by following a decision tree,

    there is an inherent hierarchical relationship among the different

    techniques, which existing approaches ignore. We incorporate these hierarchical
    label dependencies by adding an auxiliary classifier for each node in the decision
    tree to the training objective and ensembling the predictions from the original
    and auxiliary classifiers at test time. Overall, our model leads to an absolute
    improvement of $2.47\%$ micro-F1 over the model from the shared task winning team
    in a cross-validation setup and is the best performing non-ensemble model on the
    shared task leaderboard.'
  authors:
  - Anni Chen
  - Bhuwan Dhingra
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - RepL4NLP
  forum: ''
  id: ACL_29
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Hierarchical Multi-Instance Multi-Label Learning for Detecting Propaganda
    Techniques
  tldr: 'Since the introduction of the SemEval $2020$ Task $11$ \citep{task}, several
    approaches have been proposed in the literature for classifying propaganda

    based on the rhetorical techniques used to influence readers.

    These methods, however, classify one span at a time, ignoring dependencies from
    the la'
  track: The 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: While static word embedding models are known to represent linguistic analogies
    as parallel lines in high-dimensional space, the underlying mechanism as to why
    they result in such geometric structures remains obscure. We find that an elementary
    contrastive-style method employed over distributional information performs competitively
    with popular word embedding models on analogy recovery tasks, while achieving
    dramatic speedups in training time. Further, we demonstrate that a contrastive
    loss is sufficient to create these parallel structures in word embeddings, and
    establish a precise relationship between the co-occurrence statistics and the
    geometric structure of the resulting word embeddings.
  authors:
  - Narutatsu Ri
  - Fei-Tzin Lee
  - Nakul Verma
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - RepL4NLP
  forum: ''
  id: ACL_34
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Contrastive Loss is All You Need to Recover Analogies as Parallel Lines
  tldr: While static word embedding models are known to represent linguistic analogies
    as parallel lines in high-dimensional space, the underlying mechanism as to why
    they result in such geometric structures remains obscure. We find that an elementary
    contrastive-style method employed over distributional in
  track: The 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Recent models have shown that incorporating syntactic knowledge into the
    semantic role labelling (SRL) task leads to a significant improvement. In this
    paper, we propose Syntax-aware Graph-to-Graph Transformer (SynG2G-Tr) model, which
    encodes the syntactic structure using a novel way to input graph relations as
    embeddings, directly into the self-attention mechanism of Transformer.  This approach
    adds a soft bias towards attention patterns that follow the syntactic structure
    but also allows the model to use this information to learn alternative patterns.  We
    evaluate our model on both span-based and dependency-based SRL datasets, and outperform
    previous alternative methods in both in-domain and out-of-domain settings, on
    CoNLL 2005 and CoNLL 2009 datasets.
  authors:
  - Alireza Mohammadshahi
  - James Henderson
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - RepL4NLP
  forum: ''
  id: ACL_39
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Syntax-Aware Graph-to-Graph Transformer for Semantic Role Labelling
  tldr: 'Recent models have shown that incorporating syntactic knowledge into the
    semantic role labelling (SRL) task leads to a significant improvement. In this
    paper, we propose Syntax-aware Graph-to-Graph Transformer (SynG2G-Tr) model, which
    encodes the syntactic structure using a novel way to input graph '
  track: The 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: "While fully supervised relation classification (RC) models perform well\
    \ on large-scale datasets, their performance drops drastically in low-resource\
    \ settings. As generating annotated examples are expensive, recent zero-shot methods\
    \ have been proposed that reformulate RC into other NLP tasks for which supervision\
    \ exists such as textual entailment. However, these methods rely on templates\
    \ that are manually created which is costly and requires domain expertise. In\
    \ this paper, we present a novel strategy for template generation for relation\
    \ classification, which is based on adapting Harris\u2019 distributional similarity\
    \ principle to templates encoded using contextualized representations. Further,\
    \ we perform empirical evaluation of different strategies for combining the automatically\
    \ acquired templates with manual templates. The experimental results on TACRED\
    \ show that our approach not only performs better than the zero-shot RC methods\
    \ that only use manual templates, but also that it achieves state-of-the-art performance\
    \ for zero-shot TACRED at 64.3 F1 score."
  authors:
  - Mahdi Rahimi
  - Mihai Surdeanu
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - RepL4NLP
  forum: ''
  id: ACL_42
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Improving Zero-shot Relation Classification via Automatically-acquired Entailment
    Templates
  tldr: While fully supervised relation classification (RC) models perform well on
    large-scale datasets, their performance drops drastically in low-resource settings.
    As generating annotated examples are expensive, recent zero-shot methods have
    been proposed that reformulate RC into other NLP tasks for whic
  track: The 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: "The widespread adoption of large language models such as ChatGPT and\
    \ Bard has led to unprecedented demand for these technologies. The burgeoning\
    \ cost of inference for ever-increasing model sizes coupled with hardware shortages\
    \ has limited affordable access and poses a pressing need for efficiency approaches\
    \ geared towards high throughput and performance. Multi-input multi-output (MIMO)\
    \ algorithms such as data multiplexing, offer a promising solution with a many-fold\
    \ increase in throughput by performing inference for multiple inputs at the cost\
    \ of a single input. Yet these approaches are not currently performant enough\
    \ to be deployed in modern systems. We change that by developing MUX-PLMs, a class\
    \ of high throughput pre-trained language models (PLMs) trained with data multiplexing,\
    \ that can be fine-tuned for any downstream task to yield high-throughput high-performance.\
    \ Our novel multiplexing and demultiplexing modules proficiently entangle and\
    \ disentangle inputs, and enable high-performance high throughput \\muxplms{}\
    \ that are competitive with vanilla PLMs while achieving 2x/5x inference speedup\
    \ with only a 1\u22124% drop on a broad suite of tasks.\n"
  authors:
  - Vishvak Murahari
  - Ameet Deshpande
  - Carlos Jimenez
  - Izhak Shafran
  - Mingqiu Wang
  - Yuan Cao
  - Karthik Narasimhan
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - RepL4NLP
  forum: ''
  id: ACL_43
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'MUX-PLMs: Pre-training Language Models with Data Multiplexing'
  tldr: The widespread adoption of large language models such as ChatGPT and Bard
    has led to unprecedented demand for these technologies. The burgeoning cost of
    inference for ever-increasing model sizes coupled with hardware shortages has
    limited affordable access and poses a pressing need for efficiency ap
  track: The 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: "Speech language pathologists rely on information spanning the layers\
    \ of language, often drawing from multiple layers (e.g. phonology & semantics)\
    \ at once. Recent innovations in large language models (LLMs) have been shown\
    \ to build powerful representations for many complex language structures, especially\
    \ syntax and semantics, unlocking the potential of large datasets through self-supervised\
    \ learning techniques. However, these datasets are overwhelmingly orthographic,\
    \ favoring writing systems like the English alphabet, a natural but phonetically\
    \ imprecise choice. Meanwhile, LLM support for the international phonetic alphabet\
    \ (IPA) ranges from poor to absent. Further, LLMs encode text at a word- or near-word\
    \ level, and pre-training tasks have little to gain from phonetic/phonemic representations.\
    \ In this paper, we introduce BORT, an LLM for mixed orthography/IPA meant to\
    \ overcome these limitations. To this end, we extend the pre-training of an existing\
    \ LLM with our own self-supervised pronunciation tasks. We then fine-tune for\
    \ a clinical task that requires simultaneous phonological and semantic analysis.\
    \ For an \u201Ceasy\u201D and \u201Chard\u201D version of these tasks, we show\
    \ that fine-tuning from our models is more accurate by a relative 24% and 29%,\
    \ and improved on character error rates by a relative 75% and 31%, respectively,\
    \ than those starting from the original model."
  authors:
  - Robert Gale
  - Alexandra Salem
  - Gerasimos Fergadiotis
  - Steven Bedrick
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - RepL4NLP
  forum: ''
  id: ACL_46
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Mixed Orthographic/Phonemic Language Modeling: Beyond Orthographically Restricted
    Transformers (BORT)'
  tldr: Speech language pathologists rely on information spanning the layers of language,
    often drawing from multiple layers (e.g. phonology & semantics) at once. Recent
    innovations in large language models (LLMs) have been shown to build powerful
    representations for many complex language structures, especi
  track: The 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Recent work has demonstrated that using parameter efficient tuning techniques
    such as prefix tuning (or P-tuning) on pretrained language models can yield performance
    that is comparable or superior to fine-tuning  while dramatically reducing trainable
    parameters. Nevertheless, the effectiveness of such methods under the context
    of data augmentation, a common strategy to improve learning under  low data regimes,
    has not been fully explored. In this paper, we examine the effectiveness of several
    popular task-agnostic data augmentation techniques, i.e., EDA, Back Translation,
    and Mixup, when using two general parameter efficient tuning methods, P-tuning
    v2 and LoRA, under data scarcity.  We  show that  data augmentation can be used
    to boost the performance of P-tuning and LoRA models, but  the effectiveness of
    each technique varies and certain methods can lead to a notable degradation in
    performance, particularly when using larger models and on harder tasks. We further
    analyze the sentence representations of P-tuning compared to fine-tuning to help
    understand the above behaviour, and reveal how P-tuning generally presents a more
    limited ability to separate the sentence embeddings from different classes of
    augmented data. In addition, it displays poorer performance on heavily altered
    data. However, we demonstrate that by adding a simple contrastive loss function
    it can help mitigate such issues for prefix tuning, resulting in sizable improvements
    to augmented data performance. '
  authors:
  - Stephen Obadinma
  - Hongyu Guo
  - Xiaodan Zhu
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - RepL4NLP
  forum: ''
  id: ACL_47
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: N/A
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Effectiveness of Data Augmentation for Parameter Efficient Tuning with Limited
    Data
  tldr: Recent work has demonstrated that using parameter efficient tuning techniques
    such as prefix tuning (or P-tuning) on pretrained language models can yield performance
    that is comparable or superior to fine-tuning  while dramatically reducing trainable
    parameters. Nevertheless, the effectiveness of su
  track: The 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Bin Wang
  - Haizhou Li
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - RepL4NLP
  forum: ''
  id: ACL_4560
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Relational Sentence Embedding for Flexible Semantic Matching
  tldr: ''
  track: The 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Likang Xiao
  - Richong Zhang
  - Zijie Chen
  - Junfan Chen
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - RepL4NLP
  forum: ''
  id: ACL_3114
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Tucker Decomposition with Frequency Attention for Temporal Knowledge Graph
    Completion
  tldr: ''
  track: The 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Romain Bielawski
  - Rufin VanRullen
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - RepL4NLP
  forum: ''
  id: ACL_3566
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: CLIP-based image captioning via unsupervised cycle-consistency in the latent
    space
  tldr: ''
  track: The 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Guangsheng Bao
  - Zhiyang Teng
  - Yue Zhang
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - RepL4NLP
  forum: ''
  id: ACL_1454
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Token-level Fitting Issues of Seq2seq Models
  tldr: ''
  track: The 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Cheng-Han Chiang
  - Hung-yi Lee
  - Yung-Sung Chuang
  - James Glass
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - RepL4NLP
  forum: ''
  id: ACL_1380
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Revealing the Blind Spot of Sentence Encoder Evaluation by HEROS
  tldr: ''
  track: The 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - John Harvill
  - Mark Hasegawa-Johnson
  - Hee Suk Yoon
  - Chang D. Yoo
  - Eunseop Yoon
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - RepL4NLP
  forum: ''
  id: ACL_2575
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: One-Shot Exemplification Modeling via Latent Sense Representations
  tldr: ''
  track: The 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Lingfeng Shen
  - Haiyun Jiang
  - Lemao Liu
  - Shuming Shi
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - RepL4NLP
  forum: ''
  id: ACL_1282
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Sen2Pro: A Probabilistic Perspective to Sentence Embedding from Pre-trained
    Language Model'
  tldr: ''
  track: The 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Xudong Hong
  - Vera Demberg
  - Asad Sayeed
  - Qiankun Zheng
  - Bernt Schiele
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - RepL4NLP
  forum: ''
  id: ACL_4158
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Visual Coherence Loss for Coherent and Visually Grounded Story Generation
  tldr: ''
  track: The 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Online communities of involuntary celibates (incels) are a prominent source
    of misogynist hate speech. In this paper, we use quantitative text and network
    analysis approaches to examine how identity groups are discussed on incels.is,
    the largest black-pilled incels forum. We find that this community produces a
    wide range of novel identity terms and, while terms for women are most common,
    mentions of other minoritized identities are increasing. An analysis of the associations
    made with identity groups suggests an essentialist ideology where physical appearance,
    as well as gender and racial hierarchies, determine human value. We discuss implications
    for research into automated misogynist hate speech detection.
  authors:
  - Michael Yoder
  - Chloe Perry
  - David Brown
  - Kathleen Carley
  - Meredith Pruden
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WOAH
  forum: ''
  id: ACL_7
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Identity Construction in a Misogynist Incels Forum
  tldr: Online communities of involuntary celibates (incels) are a prominent source
    of misogynist hate speech. In this paper, we use quantitative text and network
    analysis approaches to examine how identity groups are discussed on incels.is,
    the largest black-pilled incels forum. We find that this community
  track: The 7th Workshop on Online Abuse and Harms (WOAH)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Over the past few years, much research has been conducted to identify
    and regulate toxic language. However, few studies have addressed a broader range
    of sensitive texts that are not necessarily overtly toxic. In this paper, we introduce
    and define a new category of sensitive text called "delicate text." We provide
    the taxonomy of delicate text and present a detailed annotation scheme. We annotate
    DeTexD, the first benchmark dataset for delicate text detection. The significance
    of the difference in the definitions is highlighted by the relative performance
    deltas between models trained each definitions and corpora and evaluated on the
    other. We make publicly available the DeTexD Benchmark dataset, annotation guidelines,
    and baseline model for delicate text detection.
  authors:
  - Artem Chernodub
  - Serhii Yavnyi
  - Oleksii Sliusarenko
  - Jade Razzaghi
  - Yichen Mo
  - Knar Hovakimyan
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WOAH
  forum: ''
  id: ACL_14
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'DeTexD: A Benchmark Dataset for Delicate Text Detection'
  tldr: Over the past few years, much research has been conducted to identify and
    regulate toxic language. However, few studies have addressed a broader range of
    sensitive texts that are not necessarily overtly toxic. In this paper, we introduce
    and define a new category of sensitive text called "delicate t
  track: The 7th Workshop on Online Abuse and Harms (WOAH)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Cyberbullying is a serious societal issue widespread on various channels
    and platforms, particularly social networking sites. Such platforms have proven
    to be exceptionally fertile grounds for such behavior. The dearth of high-quality
    training data for multilingual and low-resource scenarios, data that can accurately
    capture the nuances of social media conversations, often poses a roadblock to
    this task. This paper attempts to tackle cyberbullying, specifically its two most
    common manifestations - aggression and offensiveness. We present a novel, manually
    annotated dataset of a total of 10,000 English and Hindi-English code-mixed tweets,
    manually annotated for aggression detection and offensive language detection tasks.
    Our annotations are supported by inter-annotator agreement scores of 0.67 and
    0.74 for the two tasks, indicating substantial agreement. We perform comprehensive
    fine-tuning of pre-trained language models (PTLMs) using this dataset to check
    its efficacy. Our challenging test sets show that the best models achieve macro
    F1-scores of 67.87 and 65.45 on the two tasks, respectively. Further, we perform
    cross-dataset transfer learning to benchmark our dataset against existing aggression
    and offensive language datasets. We also present a detailed quantitative and qualitative
    analysis of errors in prediction, and with this paper, we publicly release the
    novel dataset, code, and models.
  authors:
  - Nazia Nafis
  - Diptesh Kanojia
  - Naveen Saini
  - Rudra Murthy
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WOAH
  forum: ''
  id: ACL_15
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Towards Safer Communities: Detecting Aggression and Offensive Language in
    Code-Mixed Tweets to Combat Cyberbullying'
  tldr: Cyberbullying is a serious societal issue widespread on various channels and
    platforms, particularly social networking sites. Such platforms have proven to
    be exceptionally fertile grounds for such behavior. The dearth of high-quality
    training data for multilingual and low-resource scenarios, data t
  track: The 7th Workshop on Online Abuse and Harms (WOAH)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: As pointed out by several scholars, current research on hate speech (HS)
    recognition is characterized by unsystematic data creation strategies and diverging
    annotation schemata. Subsequently, supervised-learning models tend to generalize
    poorly to datasets they were not trained on, and the performance of the models
    trained on datasets labeled using different HS taxonomies cannot be compared.
    To ease this problem, we propose applying extremely weak supervision that only
    relies on the class name rather than on class samples from the annotated data.
    We demonstrate the effectiveness of a state-of-the-art weakly-supervised text
    classification model in various in-dataset and cross-dataset settings. Furthermore,
    we conduct an in-depth quantitative and qualitative analysis of the source of
    poor generalizability of HS classification models.
  authors:
  - Yiping Jin
  - Leo Wanner
  - Vishakha Kadam
  - Alexander Shvets
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WOAH
  forum: ''
  id: ACL_16
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Towards Weakly-Supervised Hate Speech Classification Across Datasets
  tldr: As pointed out by several scholars, current research on hate speech (HS) recognition
    is characterized by unsystematic data creation strategies and diverging annotation
    schemata. Subsequently, supervised-learning models tend to generalize poorly to
    datasets they were not trained on, and the performan
  track: The 7th Workshop on Online Abuse and Harms (WOAH)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The ground truth in classification tasks is often approximated by the
    fraction of annotators who classified an item as belonging to the positive class.
    Instances for which this fraction is equal to or above 50\textbackslash{}\% are
    considered positive, including however ones that receive polarized opinions. This
    is a problematic encoding convention that disregards the potentially polarized
    nature of opinions and which is often employed to estimate abusive language. We
    present the distance from unimodality (DFU), a measure that estimates the extent
    of polarization on the distribution of opinions and which correlates well with
    human judgment. By applying DFU to posts crowd-annotated for toxicity, we found
    that polarized opinions are more likely by annotators originating from different
    countries. Also, we show that DFU can be exploited as an objective function to
    train models to predict whether a post will provoke polarized opinions in the
    future.
  authors:
  - John Pavlopoulos
  - Aristidis Likas
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WOAH
  forum: ''
  id: ACL_19
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Non-Archival
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Distance from Unimodality: Assessing Polarized Opinions in Abusive Language
    Detection'
  tldr: The ground truth in classification tasks is often approximated by the fraction
    of annotators who classified an item as belonging to the positive class. Instances
    for which this fraction is equal to or above 50\textbackslash{}\% are considered
    positive, including however ones that receive polarized o
  track: The 7th Workshop on Online Abuse and Harms (WOAH)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: "Hate speech detection faces two significant challenges: 1) the limited\
    \ availability of labeled data and 2) the high variability of hate speech across\
    \ different contexts and languages. Prompting brings a ray of hope to these challenges.\
    \ It allows injecting a model with task-specific knowledge without relying on\
    \ labeled data. \nThis paper explores zero-shot learning with prompting for hate\
    \ speech detection. We investigate how well zero-shot learning can detect hate\
    \ speech in 3 languages with limited labeled data. We experiment with various\
    \ large language models and verbalizers on 8 benchmark datasets. Our findings\
    \ highlight the impact of prompt selection on the results. They also suggest that\
    \ prompting, specifically with recent large language models, can achieve performance\
    \ comparable to and surpass fine-tuned models, making it a promising alternative\
    \ for under-resourced languages. Our findings highlight the potential of prompting\
    \ for hate speech detection and show how both the prompt and the model have a\
    \ significant impact on achieving more accurate predictions in this task."
  authors:
  - Flor Miriam Plaza-del-arco
  - Debora Nozza
  - Dirk Hovy
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WOAH
  forum: ''
  id: ACL_25
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Short paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Respectful or Toxic? Using Zero-Shot Learning with Language Models to Detect
    Hate Speech
  tldr: 'Hate speech detection faces two significant challenges: 1) the limited availability
    of labeled data and 2) the high variability of hate speech across different contexts
    and languages. Prompting brings a ray of hope to these challenges. It allows injecting
    a model with task-specific knowledge without'
  track: The 7th Workshop on Online Abuse and Harms (WOAH)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'We present an extensive evaluation of different fine-tuned models to
    detect instances of offensive and abusive language in Dutch across three benchmarks:
    a standard held-out test, a task- agnostic functional benchmark, and a dynamic
    test set. We also investigate the use of data cartography to identify high quality
    training data. Our results show a relatively good quality of the manually annotated
    data used to train the models while highlighting some critical weakness. We have
    also found a good portability of trained models along the same language phenomena.
    As for the data cartography, we have found a positive impact only on the functional
    benchmark and when selecting data per annotated dimension rather than using the
    entire training material.'
  authors:
  - Tommaso Caselli
  - Hylke Van Der Veen
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WOAH
  forum: ''
  id: ACL_26
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Benchmarking Offensive and Abusive Language in Dutch Tweets
  tldr: 'We present an extensive evaluation of different fine-tuned models to detect
    instances of offensive and abusive language in Dutch across three benchmarks:
    a standard held-out test, a task- agnostic functional benchmark, and a dynamic
    test set. We also investigate the use of data cartography to identi'
  track: The 7th Workshop on Online Abuse and Harms (WOAH)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: "We draw from the framework of relationality as a pathway for modeling\
    \ social relations to address gaps in text classification, generally, and offensive\
    \ language classification, specifically. We use minoritized language, such as\
    \ queer speech, to motivate a need for understanding and modeling social relations\u2013\
    both among individuals and among their social communities. We then point to socio-ethical\
    \ style as a research area for inferring and measuring social relations as well\
    \ as propose additional questions to structure future research on operationalizing\
    \ social context."
  authors:
  - Razvan Amironesei
  - Mark Diaz
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WOAH
  forum: ''
  id: ACL_27
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Relationality and Offensive Speech: A Research Agenda'
  tldr: We draw from the framework of relationality as a pathway for modeling social
    relations to address gaps in text classification, generally, and offensive language
    classification, specifically. We use minoritized language, such as queer speech,
    to motivate a need for understanding and modeling social r
  track: The 7th Workshop on Online Abuse and Harms (WOAH)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The prevalence of abusive language on different online platforms has been
    a major concern that raises the need for automated cross-platform abusive language
    detection. However, prior works focus on concatenating data from multiple platforms,
    inherently adopting Empirical Risk Minimization (ERM) method. In this work, we
    address this challenge from the perspective of domain generalization objective.
    We design SCL-Fish, a supervised contrastive learning integrated meta-learning
    algorithm to detect abusive language on unseen platforms. Our experimental analysis
    shows that SCL-Fish achieves better performance over ERM and the existing state-of-the-art
    models. We also show that SCL-Fish is data-efficient and achieves comparable performance
    with the large-scale pre-trained models upon finetuning for the abusive language
    detection task.
  authors:
  - Md Tawkat Islam Khondaker
  - Muhammad Abdul-mageed
  - Laks Lakshmanan, V.s.
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WOAH
  forum: ''
  id: ACL_29
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Cross-Platform and Cross-Domain Abusive Language Detection with Supervised
    Contrastive Learning
  tldr: The prevalence of abusive language on different online platforms has been
    a major concern that raises the need for automated cross-platform abusive language
    detection. However, prior works focus on concatenating data from multiple platforms,
    inherently adopting Empirical Risk Minimization (ERM) meth
  track: The 7th Workshop on Online Abuse and Harms (WOAH)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Warning: this paper contains content that some may find disturbing.


    While there has been increasing attention paid to the potential harms perpetuated
    by online platforms, most academic work on the subject centers on one narrow context:
    Western communities in primarily English language settings. Yet, social media
    platforms like YouTube support users globally and provide content in several languages,
    including low-resourced languages. In this study, we investigate this context
    via a mixed methods approach: collecting and analysing search and recommendation
    data from YouTube in low-resource language settings and conducting semi-structured
    interviews with YouTube users who speak low-resourced languages in Ethiopia. Our
    early findings indicate the failure of current content moderation schemes for
    low-resource languages and the further infliction and distribution of harm to
    marginalized communities through recommendation systems.'
  authors:
  - Hellina Hailu Nigatu
  - Inioluwa Raji
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WOAH
  forum: ''
  id: ACL_30
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Non-Archival
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Auditing YouTube Content Moderation in Low Resource Language Settings
  tldr: 'Warning: this paper contains content that some may find disturbing.


    While there has been increasing attention paid to the potential harms perpetuated
    by online platforms, most academic work on the subject centers on one narrow context:
    Western communities in primarily English language settings. Yet'
  track: The 7th Workshop on Online Abuse and Harms (WOAH)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'We introduce ExtremeBB, a textual database of over 53.5M posts made by
    38.5k users on 12 extremist bulletin board forums promoting online hate, harassment,
    the manosphere and other forms of extremism. It enables large-scale analyses of
    qualitative and quantitative historical trends going back two decades: measuring
    hate speech and toxicity; tracing the evolution of different strands of extremist
    ideology; tracking the relationships between online subcultures, extremist behaviours,
    and real-world violence; and monitoring extremist communities in near real time.
    This can shed light not only on the spread of problematic ideologies but also
    the effectiveness of interventions. ExtremeBB comes with a robust ethical data-sharing
    regime that allows us to share data with academics worldwide. Since 2020, access
    has been granted to 49 licensees in 16 research groups from 12 institutions.'
  authors:
  - Anh V. Vu
  - Lydia Wilson
  - Yi Ting Chua
  - Ilia Shumailov
  - Ross Anderson
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WOAH
  forum: ''
  id: ACL_33
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Non-Archival
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'ExtremeBB: A Database for Large-Scale Research into Online Hate, Harassment,
    the Manosphere and Extremism'
  tldr: We introduce ExtremeBB, a textual database of over 53.5M posts made by 38.5k
    users on 12 extremist bulletin board forums promoting online hate, harassment,
    the manosphere and other forms of extremism. It enables large-scale analyses of
    qualitative and quantitative historical trends going back two de
  track: The 7th Workshop on Online Abuse and Harms (WOAH)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: While many types of hate speech and online toxicity have been the focus
    of extensive research in NLP, toxic language stigmatizing poor people has been
    mostly disregarded. Yet, aporophobia, a social bias against the poor, is a common
    phenomenon online, which can be psychologically damaging as well as hindering
    poverty reduction policy measures. We demonstrate that aporophobic attitudes are
    indeed present in social media and argue that the existing NLP datasets and models
    are inadequate to effectively address this problem. Efforts toward designing specialized
    resources and novel socio-technical mechanisms for confronting aporophobia are
    needed.
  authors:
  - Svetlana Kiritchenko
  - Georgina Curto Rex
  - Isar Nejadgholi
  - Kathleen C. Fraser
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WOAH
  forum: ''
  id: ACL_34
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Aporophobia: An Overlooked Type of Toxic Language Targeting the Poor'
  tldr: While many types of hate speech and online toxicity have been the focus of
    extensive research in NLP, toxic language stigmatizing poor people has been mostly
    disregarded. Yet, aporophobia, a social bias against the poor, is a common phenomenon
    online, which can be psychologically damaging as well as
  track: The 7th Workshop on Online Abuse and Harms (WOAH)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In this paper, we introduce a fine-tuned transformer-based model focused
    on problematic webpage classification to identify webpages promoting hate and
    violence of various forms. Due to the unavailability of labelled problematic webpage
    data, first we propose a novel webpage data collection strategy which leverages
    well-studied short-text hate speech datasets. We have introduced a custom GPT-4
    few-shot prompt annotation scheme taking various webpage features to label the
    prohibitively expensive webpage annotation task. The resulting annotated data
    is used to build our problematic webpage classification model. We report the accuracy
    (87.6\% F1-score) of our webpage classification model and conduct a detailed comparison
    of it against other state-of-the-art hate speech classification model on problematic
    webpage identification task. Finally, we have showcased the importance of various
    webpage features in identifying a problematic webpage.
  authors:
  - Ojasvin Sood
  - Sandipan Dandapat
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WOAH
  forum: ''
  id: ACL_35
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Problematic Webpage Identification: A Trilogy of Hatespeech, Search Engines
    and GPT'
  tldr: 'In this paper, we introduce a fine-tuned transformer-based model focused
    on problematic webpage classification to identify webpages promoting hate and
    violence of various forms. Due to the unavailability of labelled problematic webpage
    data, first we propose a novel webpage data collection strategy '
  track: The 7th Workshop on Online Abuse and Harms (WOAH)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Classifiers tend to learn a false causal relationship between an over-represented
    concept and a label, which can result in over-reliance on the concept and compromised
    classification accuracy. It is imperative to have methods in place that can compare
    different models and identify over-reliances on specific concepts. We consider
    three well-known abusive language classifiers trained on large English datasets
    and focus on the concept of negative emotions, which is an important signal but
    should not be learned as a sufficient feature for the label of abuse. Motivated
    by the definition of global sufficiency, we first examine the unwanted dependencies
    learned by the classifiers by assessing their accuracy on a challenge set across
    all decision thresholds. Further, recognizing that a challenge set might not always
    be available, we introduce concept-based explanation metrics to assess the influence
    of the concept on the labels. These explanations allow us to compare classifiers
    regarding the degree of false global sufficiency they have learned between a concept
    and a label.
  authors:
  - Isar Nejadgholi
  - Svetlana Kiritchenko
  - Kathleen C. Fraser
  - Esma Balkir
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WOAH
  forum: ''
  id: ACL_38
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Concept-Based Explanations to Test for False Causal Relationships Learned
    by Abusive Language Classifiers
  tldr: 'Classifiers tend to learn a false causal relationship between an over-represented
    concept and a label, which can result in over-reliance on the concept and compromised
    classification accuracy. It is imperative to have methods in place that can compare
    different models and identify over-reliances on '
  track: The 7th Workshop on Online Abuse and Harms (WOAH)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: A rise in the circulation of memes has led to the spread of a new form
    of multimodal hateful content. Unfortunately, the degree of hate women receive
    on the internet is disproportionately skewed against them. This, combined with
    the fact that multimodal misogyny is more challenging to detect as opposed to
    traditional text-based misogyny, signifies that the task of identifying misogynistic
    memes online is one of utmost importance. To this end, the MAMI dataset was released,
    consisting of 12000 memes annotated for misogyny and four sub-classes of misogyny
    - shame, objectification, violence and stereotype. While this balanced dataset
    is widely cited, we find that the task itself remains largely unsolved. Thus,
    in our work, we investigate the performance of multiple models in an effort to
    analyse whether domain specific pretraining helps model performance. We also investigate
    why even state of the art models find this task so challenging, and whether domain-specific
    pretraining can help. Our results show that pretraining BERT on hateful memes
    and leveraging an attention based approach with ViT outperforms state of the art
    models by more than 10\%. Further, we provide insight into why these models may
    be struggling with this task with an extensive qualitative analysis of random
    samples from the test set.
  authors:
  - Smriti Singh
  - Amritha Haridasan
  - Raymond Mooney
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WOAH
  forum: ''
  id: ACL_39
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: "\"Female Astronaut: Because sandwiches won't make themselves up there\u201D\
    : Towards Multimodal misogyny detection in memes"
  tldr: A rise in the circulation of memes has led to the spread of a new form of
    multimodal hateful content. Unfortunately, the degree of hate women receive on
    the internet is disproportionately skewed against them. This, combined with the
    fact that multimodal misogyny is more challenging to detect as oppo
  track: The 7th Workshop on Online Abuse and Harms (WOAH)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Online conversations are particularly susceptible to derailment, which
    can manifest itself in the form of toxic communication patterns like disrespectful
    comments or verbal abuse. Forecasting conversation derailment  predicts signs
    of derailment in advance enabling proactive moderation of conversations. Current
    state-of-the-art approaches to address this problem rely on sequence models that
    treat dialogues as text streams. We propose a novel model based on a graph convolutional
    neural network that considers dialogue user dynamics and the influence of public
    perception on conversation utterances. Through empirical evaluation, we show that
    our model effectively captures conversation dynamics and outperforms the state-of-the-art
    models on the CGA and CMV benchmark datasets by 1.5\textbackslash{}\% and 1.7\textbackslash{}\%,
    respectively.
  authors:
  - Enas Altarawneh
  - Ameeta Agrawal
  - Michael Jenkin
  - Manos Papagelis
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WOAH
  forum: ''
  id: ACL_40
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Conversation Derailment Forecasting with Graph Convolutional Networks
  tldr: Online conversations are particularly susceptible to derailment, which can
    manifest itself in the form of toxic communication patterns like disrespectful
    comments or verbal abuse. Forecasting conversation derailment  predicts signs
    of derailment in advance enabling proactive moderation of conversati
  track: The 7th Workshop on Online Abuse and Harms (WOAH)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Online Gender-Based Violence (GBV), such as misogynistic abuse is an
    increasingly prevalent problem that technological approaches have struggled to
    address.Through the lens of the GBV framework, which is rooted in social science
    and policy, we systematically review 63 available resources for automated identification
    of such language. We find the datasets are limited in a number of important ways,
    such as their lack of theoretical grounding and stakeholder input, static nature,
    and focus on certain media platforms. Based on this review, we recommend development
    of future resources rooted in sociological expertise and

    centering stakeholder voices, namely GBV experts and people with lived experience
    of GBV.'
  authors:
  - Gavin Abercrombie
  - Aiqi Jiang
  - Poppy Gerrard-abbott
  - Ioannis Konstas
  - Verena Rieser
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WOAH
  forum: ''
  id: ACL_42
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Resources for Automated Identification of Online Gender-Based Violence:
    A Systematic Review'
  tldr: Online Gender-Based Violence (GBV), such as misogynistic abuse is an increasingly
    prevalent problem that technological approaches have struggled to address.Through
    the lens of the GBV framework, which is rooted in social science and policy, we
    systematically review 63 available resources for automat
  track: The 7th Workshop on Online Abuse and Harms (WOAH)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: "What is deemed offensive depends inherently on the socio-cultural contexts\
    \ within which those assessments are made. However, the subjective nature of this\
    \ task is often overlooked in traditional NLP studies. While there has been recent\
    \ work on socio-demographic correlates of offensiveness, the contribution of socio-cultural\
    \ factors to perceiving offensiveness, when considered as a moral judgement, across\
    \ the globe is still under-explored. \nWe summarize the findings from a cross-cultural\
    \ study of offensiveness annotations by 4295 participants from 21 different countries\
    \ across 8 geo-cultural regions. Our results show that (1) perceptions of offensiveness\
    \ vary significantly across geo-cultural regions, despite controlling for gender,\
    \ age, and socio-economic status, and (2) these differences are significantly\
    \ mediated by annotators' individual moral concerns that vary across cultures."
  authors:
  - Aida Mostafazadeh Davani
  - Mark Diaz
  - Dylan Baker
  - Vinodkumar Prabhakaran
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WOAH
  forum: ''
  id: ACL_45
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Non-Archival
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Disentangling Disagreements on Offensiveness: A Cross-Cultural Study'
  tldr: What is deemed offensive depends inherently on the socio-cultural contexts
    within which those assessments are made. However, the subjective nature of this
    task is often overlooked in traditional NLP studies. While there has been recent
    work on socio-demographic correlates of offensiveness, the contr
  track: The 7th Workshop on Online Abuse and Harms (WOAH)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Most research on hate speech detection has focused on English where a
    sizeable amount of labeled training data is available. However, to expand hate
    speech detection into more languages, approaches that require minimal training
    data are needed. In this paper, we test whether natural language inference (NLI)
    models which perform well in zero- and few-shot settings can benefit hate speech
    detection performance in scenarios where only a limited amount of labeled data
    is available in the target language. Our evaluation on five languages demonstrates
    large performance improvements of NLI fine-tuning over direct fine-tuning in the
    target language. However, the effectiveness of previous work that proposed intermediate
    fine-tuning on English data is hard to match. Only in settings where the English
    training data does not match the test domain, can our customised NLI-formulation
    outperform intermediate fine-tuning on English. Based on our extensive experiments,
    we propose a set of recommendations for hate speech detection in languages where
    minimal labeled training data is available.
  authors:
  - Janis Goldzycher
  - Moritz Preisig
  - Chantal Amrhein
  - Gerold Schneider
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WOAH
  forum: ''
  id: ACL_46
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Evaluating the Effectiveness of Natural Language Inference for Hate Speech
    Detection in Languages with Limited Labeled Data
  tldr: Most research on hate speech detection has focused on English where a sizeable
    amount of labeled training data is available. However, to expand hate speech detection
    into more languages, approaches that require minimal training data are needed.
    In this paper, we test whether natural language inferen
  track: The 7th Workshop on Online Abuse and Harms (WOAH)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In the past few years, the NLP community has actively worked on detecting
    LGBT+Phobia in online spaces, using textual data publicly available Most of these
    are for the English language and its variants since it is the most studied language
    by the NLP community. Nevertheless, efforts towards creating corpora in other
    languages are active worldwide. Despite this, the Spanish language is an understudied
    language regarding digital LGBT+Phobia. The only corpus we found in the literature
    was for the Peninsular Spanish dialects, which use LGBT+phobic terms different
    than those in the Mexican dialect. For this reason, we present Homo-MEX, a novel
    corpus for detecting LGBT+Phobia in Mexican Spanish. In this paper, we describe
    our data-gathering and annotation process. Also, we present a classification benchmark
    using various traditional machine learning algorithms and two pre-trained deep
    learning models to showcase our corpus classification potential.
  authors:
  - "Juan V\xE1squez"
  - Scott Andersen
  - Gemma Bel-enguix
  - "Helena G\xF3mez-adorno"
  - Sergio-luis Ojeda-trueba
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WOAH
  forum: ''
  id: ACL_50
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'HOMO-MEX: A Mexican Spanish Annotated Corpus for LGBT+phobia Detection on
    Twitter'
  tldr: In the past few years, the NLP community has actively worked on detecting
    LGBT+Phobia in online spaces, using textual data publicly available Most of these
    are for the English language and its variants since it is the most studied language
    by the NLP community. Nevertheless, efforts towards creating
  track: The 7th Workshop on Online Abuse and Harms (WOAH)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In this work we propose a novel annotation scheme which factors hate speech
    into five separate discursive categories. To evaluate our scheme, we construct
    a corpus of over 2.9M Twitter posts containing hateful expressions directed at
    Jews, and annotate a sample dataset of 1,050 tweets. We present a statistical
    analysis of the annotated dataset as well as discuss annotation examples, and
    conclude by discussing promising directions for future work.
  authors:
  - Gal Ron
  - Effi Levi
  - Odelia Oshri
  - Shaul Shenhav
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WOAH
  forum: ''
  id: ACL_51
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Short paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Factoring Hate Speech: A New Annotation Framework to Study Hate Speech in
    Social Media'
  tldr: 'In this work we propose a novel annotation scheme which factors hate speech
    into five separate discursive categories. To evaluate our scheme, we construct
    a corpus of over 2.9M Twitter posts containing hateful expressions directed at
    Jews, and annotate a sample dataset of 1,050 tweets. We present a '
  track: The 7th Workshop on Online Abuse and Harms (WOAH)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: "The definitions of abusive, offensive, toxic and uncivil comments used\
    \ for annotating corpora for automated content moderation are highly intersected\
    \ and researchers call for their disambiguation. We summarize the definitions\
    \ of these terms as they appear in 23 papers across different fields. \nWe compare\
    \ examples given for uncivil, offensive, and toxic comments, attempting to foster\
    \ more unified scientific resources. Additionally, we stress that the term incivility\
    \ that frequently appears in social science literature has hardly been mentioned\
    \ in the literature we analyzed that focuses on computational linguistics and\
    \ natural language processing."
  authors:
  - Pia Pachinger
  - Julia Neidhardt
  - Allan Hanbury
  - Anna Planitzer
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WOAH
  forum: ''
  id: ACL_53
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Non-Archival
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Toward Disambiguating the Definitions of Abusive, Offensive, Toxic, and Uncivil
    Comments
  tldr: "The definitions of abusive, offensive, toxic and uncivil comments used for\
    \ annotating corpora for automated content moderation are highly intersected and\
    \ researchers call for their disambiguation. We summarize the definitions of these\
    \ terms as they appear in 23 papers across different fields. \nWe co"
  track: The 7th Workshop on Online Abuse and Harms (WOAH)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We analyze homotransphobic content on Twitter across seven different languages,
    highlighting how this pervasive issue manifests in distinct cultural contexts
    worldwide. By developing a comprehensive taxonomy to classify homotransphobic
    speech, our study establishes a crucial foundation for building effective models
    to identify and counter such harmful speech on online platforms.
  authors:
  - Davide Locatelli
  - Greta Damo
  - Debora Nozza
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WOAH
  forum: ''
  id: ACL_54
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Non-Archival
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: A Cross-Lingual Study of Homotransphobia on Twitter
  tldr: 'We analyze homotransphobic content on Twitter across seven different languages,
    highlighting how this pervasive issue manifests in distinct cultural contexts
    worldwide. By developing a comprehensive taxonomy to classify homotransphobic
    speech, our study establishes a crucial foundation for building '
  track: The 7th Workshop on Online Abuse and Harms (WOAH)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The automated detection of harmful language has been of great importance
    for the online world, especially with the growing importance of social media and,
    consequently, polarisation. There are many open challenges to high quality detection
    of harmful text, from dataset creation to generalisable application, thus calling
    for more systematic studies. In this paper, we explore re-annotation as a means
    of examining the robustness of already existing labelled datasets, showing that,
    despite using alternative definitions, the inter-annotator agreement remains very
    inconsistent, highlighting the intrinsically subjective and variable nature of
    the task. In addition, we build automatic toxicity detectors using the existing
    datasets, with their original labels, and we evaluate them on our multi-definition
    and multi-source datasets. Surprisingly, while other studies show that hate speech
    detection models perform better on data that are derived from the same distribution
    as the training set, our analysis demonstrates this is not necessarily true.
  authors:
  - Katerina Korre
  - John Pavlopoulos
  - Jeffrey Sorensen
  - "L\xE9o Laugier"
  - Ion Androutsopoulos
  - Lucas Dixon
  - "Alberto Barr\xF3n-cede\xF1o"
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WOAH
  forum: ''
  id: ACL_55
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Harmful Language Datasets: An Assessment of Robustness'
  tldr: The automated detection of harmful language has been of great importance for
    the online world, especially with the growing importance of social media and,
    consequently, polarisation. There are many open challenges to high quality detection
    of harmful text, from dataset creation to generalisable appl
  track: The 7th Workshop on Online Abuse and Harms (WOAH)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The automatic detection of hate speech online is an active research area
    in NLP. Most of the studies to date are based on social media datasets that contribute
    to the creation of hate speech detection models trained on them. However, data
    creation processes contain their own biases, and models inherently learn from
    these dataset-specific biases. In this paper, we perform a large-scale cross-dataset
    comparison where we fine-tune language models on different hate speech detection
    datasets. This analysis shows how some datasets are more generalizable than others
    when used as training data. Crucially, our experiments show how combining hate
    speech detection datasets can contribute to the development of robust hate speech
    detection models. This robustness holds even when controlling by data size and
    compared with the best individual datasets.
  authors:
  - Dimosthenis Antypas
  - Jose Camacho-Collados
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WOAH
  forum: ''
  id: ACL_57
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long paper
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Robust Hate Speech Detection in Social Media: A Cross-Dataset Empirical
    Evaluation'
  tldr: The automatic detection of hate speech online is an active research area in
    NLP. Most of the studies to date are based on social media datasets that contribute
    to the creation of hate speech detection models trained on them. However, data
    creation processes contain their own biases, and models inher
  track: The 7th Workshop on Online Abuse and Harms (WOAH)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Different ways of linguistically expressing the same real-world event
    can lead to different perceptions of what happened. Previous work has shown that
    different descriptions of gender-based violence (GBV) influence the reader's perception
    of who is to blame for the violence, possibly reinforcing stereotypes which see
    the victim as partly responsible, too. As a contribution to raise awareness on
    perspective-based writing, and to facilitate access to alternative perspectives,
    we introduce the novel task of automatically rewriting GBV descriptions as a means
    to alter the perceived level of blame on the perpetrator. We present a quasi-parallel
    dataset of sentences with low and high perceived responsibility levels for the
    perpetrator, and experiment with unsupervised (mBART-based), zero-shot and few-shot
    (GPT3-based) methods for rewriting sentences. We evaluate our models using a questionnaire
    study and a suite of automatic metrics.
  authors:
  - Gosse Minnema
  - Huiyuan Lai
  - Benedetta Muscato
  - Malvina Nissim
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WOAH
  forum: ''
  id: ACL_F1
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Findings
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: '[Findings] Responsibility Perspective Transfer for Italian Femicide News'
  tldr: Different ways of linguistically expressing the same real-world event can
    lead to different perceptions of what happened. Previous work has shown that different
    descriptions of gender-based violence (GBV) influence the reader's perception
    of who is to blame for the violence, possibly reinforcing ste
  track: The 7th Workshop on Online Abuse and Harms (WOAH)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The task of fact-checking deals with assessing the veracity of factual
    claims based on credible evidence and background knowledge. In particular, scientific
    fact-checking is the variation of the task concerned with verifying claims rooted
    in scientific knowledge. This task has received significant attention due to the
    growing importance of scientific and health discussions on online platforms. Automated
    scientific fact-checking methods based on NLP can help combat the spread of misinformation,
    assist researchers in knowledge discovery, and help individuals understand new
    scientific breakthroughs. In this paper, we present a comprehensive survey of
    existing research in this emerging field and its related tasks. We provide a task
    description, discuss the construction process of existing datasets, and analyze
    proposed models and approaches. Based on our findings, we identify intriguing
    challenges and outline potential future directions to advance the field.
  authors:
  - Juraj Vladika
  - Florian Matthes
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WOAH
  forum: ''
  id: ACL_F2
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Findings
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: '[Findings] Scientific Fact-Checking: A Survey of Resources and Approaches'
  tldr: The task of fact-checking deals with assessing the veracity of factual claims
    based on credible evidence and background knowledge. In particular, scientific
    fact-checking is the variation of the task concerned with verifying claims rooted
    in scientific knowledge. This task has received significant a
  track: The 7th Workshop on Online Abuse and Harms (WOAH)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The ability to conduct retrospective analyses of attacks on human rights
    defenders over time and by location is important for humanitarian organizations
    to better understand historical or ongoing human rights violations and thus better
    manage the global impact of such events. We hypothesize that NLP can support such
    efforts by quickly processing large collections of news articles to detect and
    summarize the characteristics of attacks on human rights defenders. To that end,
    we propose a new dataset for detecting Attacks on Human Rights Defenders (HRDsAttack)
    consisting of crowdsourced annotations on 500 online news articles. The annotations
    include fine-grained information about the type and location of the attacks, as
    well as information about the victim(s). We demonstrate the usefulness of the
    dataset by using it to train and evaluate baseline models on several sub-tasks
    to predict the annotated characteristics.
  authors:
  - Shihao Ran
  - Di Lu
  - Aoife Cahill
  - Joel Tetreault
  - Alejandro Jaimes
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WOAH
  forum: ''
  id: ACL_F3
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Findings
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: '[Findings] A New Task and Dataset on Detecting Attacks on Human Rights Defenders'
  tldr: The ability to conduct retrospective analyses of attacks on human rights defenders
    over time and by location is important for humanitarian organizations to better
    understand historical or ongoing human rights violations and thus better manage
    the global impact of such events. We hypothesize that NLP
  track: The 7th Workshop on Online Abuse and Harms (WOAH)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: With the growing importance of detecting misinformation, many studies
    have focused on verifying factual claims by retrieving evidence. However, canonical
    fact verification tasks do not apply to catching subtle differences in factually
    consistent claims, which might still bias the readers, especially on contentious
    political or economic issues. Our underlying assumption is that among the trusted
    sources, one's argument is not necessarily more true than the other, requiring
    comparison rather than verification. In this study, we propose ClaimDIff, a novel
    dataset that primarily focuses on comparing the nuance between claim pairs. In
    ClaimDiff, we provide human-labeled 2,941 claim pairs from 268 news articles.
    We observe that while humans are capable of detecting the nuances between claims,
    strong baselines struggle to detect them, showing over a 19% absolute gap with
    the humans. We hope this initial study could help readers to gain an unbiased
    grasp of contentious issues through machine-aided comparison.
  authors:
  - Miyoung Ko
  - Ingyu Seong
  - Hwaran Lee
  - Joonsuk Park
  - Minsuk Chang
  - Minjoon Seo
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WOAH
  forum: ''
  id: ACL_F4
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Findings
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: '[Findings] ClaimDiff: Comparing and Contrasting Claims on Contentious Issues'
  tldr: With the growing importance of detecting misinformation, many studies have
    focused on verifying factual claims by retrieving evidence. However, canonical
    fact verification tasks do not apply to catching subtle differences in factually
    consistent claims, which might still bias the readers, especially
  track: The 7th Workshop on Online Abuse and Harms (WOAH)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Connor Baumler
  - Anna Sotnikova
  - "Hal Daum\xE9 III"
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WOAH
  forum: ''
  id: ACL_F5
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Findings
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: '[Findings] Which Examples Should be Multiply Annotated? Active Learning
    When Annotators May Disagree'
  tldr: ''
  track: The 7th Workshop on Online Abuse and Harms (WOAH)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Research on abusive content detection on social media has primarily focused
    on explicit forms of hate speech (HS), that are often identifiable by recognizing
    hateful words and expressions. Messages containing linguistically subtle and implicit
    forms of hate speech still constitute an open challenge for automatic hate speech
    detection. In this paper, we propose a new framework for generating adversarial
    implicit HS short-text messages using Auto-regressive Language Models. Moreover,
    we propose a strategy to group the generated implicit messages in complexity levels
    (EASY, MEDIUM, and HARD categories) characterizing how challenging these messages
    are for supervised classifiers. Finally, relying on (Dinan et al., 2019; Vidgen
    et al., 2021), we propose a ``build it, break it, fix it'', training scheme using
    HARD messages showing how iteratively retraining on HARD messages substantially
    leverages SOTA models' performances on implicit HS benchmarks.
  authors:
  - Nicolas Ocampo
  - Elena Cabrio
  - Serena Villata
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WOAH
  forum: ''
  id: ACL_F6
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Findings
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: '[Findings] Playing the Part of the Sharp Bully: Generating Adversarial Examples
    for Implicit Hate Speech Detection'
  tldr: 'Research on abusive content detection on social media has primarily focused
    on explicit forms of hate speech (HS), that are often identifiable by recognizing
    hateful words and expressions. Messages containing linguistically subtle and implicit
    forms of hate speech still constitute an open challenge '
  track: The 7th Workshop on Online Abuse and Harms (WOAH)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Annotator disagreement is common whenever human judgment is needed for
    supervised learning. It is conventional to assume that one label per item represents
    ground truth. However, this obscures minority opinions, if present. We regard
    ``ground truth'' as the distribution of all labels that a population of annotators
    could produce, if asked (and of which we only have a small sample). We next introduce
    DisCo (Distribution from Context), a simple neural model that learns to predict
    this distribution. The model takes annotator-item pairs, rather than items alone,
    as input, and performs inference by aggregating over all annotators. Despite its
    simplicity, our experiments show that, on six benchmark datasets, our model is
    competitive with, and frequently outperforms, other, more complex models that
    either do not model specific annotators or were not designed for label distribution
    learning.
  authors:
  - Tharindu Cyril Weerasooriya
  - Alexander Ororbia
  - Raj Bhensadadia
  - Ashiqur KhudaBukhsh
  - Christopher Homan
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WOAH
  forum: ''
  id: ACL_F7
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Findings
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: '[Findings] Disagreement Matters: Preserving Label Diversity by Jointly Modeling
    Item and Annotator Label Distributions with DisCo'
  tldr: Annotator disagreement is common whenever human judgment is needed for supervised
    learning. It is conventional to assume that one label per item represents ground
    truth. However, this obscures minority opinions, if present. We regard ``ground
    truth'' as the distribution of all labels that a populati
  track: The 7th Workshop on Online Abuse and Harms (WOAH)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We introduce SexTok, a multi-modal dataset composed of TikTok videos labeled
    as sexually suggestive (from the annotator's point of view), sex-educational content,
    or neither. Such a dataset is necessary to address the challenge of distinguishing
    between sexually suggestive content and virtual sex education videos on TikTok.
    Children's exposure to sexually suggestive videos has been shown to have adversarial
    effects on their development (Collins et al. 2017). Meanwhile, virtual sex education,
    especially on subjects that are more relevant to the LGBTQIA+ community, is very
    valuable (Mitchell et al. 2014). The platform's current system removes/punishes
    some of both types of videos, even though they serve different purposes. Our dataset
    contains video URLs, and it is also audio transcribed. To validate its importance,
    we explore two transformer-based models for classifying the videos. Our preliminary
    results suggest that the task of distinguishing between these types of videos
    is learnable but challenging. These experiments suggest that this dataset is meaningful
    and invites further study on the subject.
  authors:
  - Enfa George
  - Mihai Surdeanu
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WOAH
  forum: ''
  id: ACL_F8
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Findings
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: "[Findings] It\u2019s not Sexually Suggestive; It\u2019s Educative | Separating\
    \ Sex Education from Suggestive Content on TikTok videos"
  tldr: We introduce SexTok, a multi-modal dataset composed of TikTok videos labeled
    as sexually suggestive (from the annotator's point of view), sex-educational content,
    or neither. Such a dataset is necessary to address the challenge of distinguishing
    between sexually suggestive content and virtual sex ed
  track: The 7th Workshop on Online Abuse and Harms (WOAH)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Debiasing methods that seek to mitigate the tendency of Language Models
    (LMs) to occasionally output toxic or inappropriate text have recently gained
    traction. In this paper, we propose a standardized protocol which distinguishes
    methods that yield not only desirable results, but are also consistent with their
    mechanisms and specifications. For example, we ask, given a debiasing method that
    is developed to reduce toxicity in LMs, if the definition of toxicity used by
    the debiasing method is reversed, would the debiasing results also be reversed?
    We used such considerations to devise three criteria for our new protocol: Specification
    Polarity, Specification Importance, and Domain Transferability. As a case study,
    we apply our protocol to a popular debiasing method, Self-Debiasing, and compare
    it to  one we propose, called Instructive Debiasing, and demonstrate that consistency
    is as important an aspect to debiasing viability as is simply a desirable result.
    We show that our protocol provides essential insights into the generalizability
    and interpretability of debiasing methods that may otherwise go overlooked.'
  authors:
  - Robert Morabito
  - Jad Kabbara
  - Ali Emami
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WOAH
  forum: ''
  id: ACL_F9
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Findings
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: '[Findings] Debiasing should be Good and Bad'
  tldr: Debiasing methods that seek to mitigate the tendency of Language Models (LMs)
    to occasionally output toxic or inappropriate text have recently gained traction.
    In this paper, we propose a standardized protocol which distinguishes methods
    that yield not only desirable results, but are also consistent
  track: The 7th Workshop on Online Abuse and Harms (WOAH)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Work on hate speech has made considering rude and harmful examples in
    scientific publications inevitable. This situation raises various problems, such
    as whether or not to obscure profanities. While science must accurately disclose
    what it does, the unwarranted spread of hate speech can harm readers and increases
    its internet frequency. While maintaining publications'''' professional appearance,
    obfuscating profanities makes it challenging to evaluate the content, especially
    for non-native speakers.

    Surveying 150 ACL papers, we discovered that obfuscation is usually used for English
    but not other languages, and even then, quite unevenly.

    We discuss the problems with obfuscation and suggest a multilingual community
    resource called PrOf with a Python module to standardize profanity obfuscation
    processes. We believe PrOf can help scientific publication policies to make hate
    speech work accessible and comparable, irrespective of language.'
  authors:
  - Debora Nozza
  - Dirk Hovy
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WOAH
  forum: ''
  id: ACL_F10
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Findings
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: '[Findings] The State of Profanity Obfuscation in Natural Language Processing
    Scientific Publications'
  tldr: Work on hate speech has made considering rude and harmful examples in scientific
    publications inevitable. This situation raises various problems, such as whether
    or not to obscure profanities. While science must accurately disclose what it
    does, the unwarranted spread of hate speech can harm readers
  track: The 7th Workshop on Online Abuse and Harms (WOAH)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: "Warning: This paper contains content that may be offensive or upsetting.\
    \ \nUnderstanding the harms and offensiveness of statements requires reasoning\
    \ about the social and situational context in which statements are made. For example,\
    \ the utterance \"your English is very good\u201D may implicitly signal an insult\
    \ when uttered by a white man to a non-white colleague, but uttered by an ESL\
    \ teacher to their student would be interpreted as a genuine compliment. Such\
    \ contextual factors have been largely ignored by previous approaches to toxic\
    \ language detection. \n\nWe introduce COBRA frames, the first context-aware formalism\
    \ for explaining the intents, reactions, and harms of offensive or biased statements\
    \ grounded in their social and situational context. We create COBRACORPUS, a dataset\
    \ of 33k potentially offensive statements paired with machine-generated contexts\
    \ and free-text explanations of offensiveness, implied biases, speaker intents,\
    \ and listener reactions. \n\nTo study the contextual dynamics of offensiveness,\
    \ we train models to generate COBRA explanations, with and without access to the\
    \ context. We find that explanations by context-agnostic models are significantly\
    \ worse than by context-aware ones, especially in situations where the context\
    \ inverts the statement's offensiveness (29% accuracy drop). Our work highlights\
    \ the importance and feasibility of contextualized NLP by modeling social factors."
  authors:
  - Xuhui Zhou
  - Hao Zhu
  - Akhila Yerukola
  - Thomas Davidson
  - Jena D. Hwang
  - Swabha Swayamdipta
  - Maarten Sap
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WOAH
  forum: ''
  id: ACL_F11
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Findings
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: '[Findings] COBRA Frames: Contextual Reasoning about Effects and Harms of
    Offensive Statements'
  tldr: "Warning: This paper contains content that may be offensive or upsetting.\
    \ \nUnderstanding the harms and offensiveness of statements requires reasoning\
    \ about the social and situational context in which statements are made. For example,\
    \ the utterance \"your English is very good\u201D may implicitly signal an "
  track: The 7th Workshop on Online Abuse and Harms (WOAH)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Cutting-edge image generation has been praised for producing high-quality
    images, suggesting a ubiquitous future in a variety of applications. However,
    initial studies have pointed to the potential for harm due to predictive bias,
    reflecting and potentially reinforcing cultural stereotypes. In this work, we
    are the first to investigate how multimodal models handle diverse gender identities.
    Concretely, we conduct a thorough analysis in which we compare the output of three
    image generation models for prompts containing cisgender vs. non-cisgender identity
    terms. Our findings demonstrate that certain non-cisgender identities are consistently
    (mis)represented as less human, more stereotyped and more sexualised. We complement
    our experimental analysis with (a) a survey among non-cisgender individuals and
    (b) a series of interviews, to establish which harms affected individuals anticipate,
    and how they would like to be represented. We find respondents are particularly
    concerned about misrepresentation, and the potential to drive harmful behaviours
    and beliefs. Simple heuristics to limit offensive content are widely rejected,
    and instead respondents call for community involvement, curated training data
    and the ability to customise. These improvements could pave the way for a future
    where change is led by the affected community, and technology is used to positively
    ''[portray] queerness in ways that we haven't even thought of''' rather than reproducing
    stale, offensive stereotypes.
  authors:
  - Eddie Ungless
  - Bjorn Ross
  - Anne Lauscher
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - WOAH
  forum: ''
  id: ACL_F12
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Findings
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: '[Findings] Stereotypes and Smut: The (Mis)representation of Non-cisgender
    Identities by Text-to-Image Models'
  tldr: 'Cutting-edge image generation has been praised for producing high-quality
    images, suggesting a ubiquitous future in a variety of applications. However,
    initial studies have pointed to the potential for harm due to predictive bias,
    reflecting and potentially reinforcing cultural stereotypes. In this '
  track: The 7th Workshop on Online Abuse and Harms (WOAH)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Sandeep Silwal
  - Sara Ahmadian
  - Andrew Nystrom
  - Andrew Mccallum
  - Deepak Ramachandran
  - Mehran Kazemi
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SustaiNLP
  forum: ''
  id: SustaiNLP_2
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: ''
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'KwikBucks: Correlation Clustering with Cheap-Weak and Expensive-Strong Signals'
  tldr: ''
  track: The Fourth Workshop on Simple and Efficient Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Yanchen Liu
  - Timo Schick
  - Hinrich Schtze
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SustaiNLP
  forum: ''
  id: SustaiNLP_3
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Semantic-Oriented Unlabeled Priming for Large-Scale Language Models
  tldr: ''
  track: The Fourth Workshop on Simple and Efficient Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Daniel Campos
  - Alexandre Marques
  - Mark Kurtz
  - Cheng Xiang Zhai
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SustaiNLP
  forum: ''
  id: SustaiNLP_4
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'oBERTa: Improving Sparse Transfer Learning via improved initialization,
    distillation, and pruning regimes'
  tldr: ''
  track: The Fourth Workshop on Simple and Efficient Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Daniel Campos
  - Alessandro Magnani
  - Chengxiang Zhai
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SustaiNLP
  forum: ''
  id: SustaiNLP_5
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Quick Dense Retrievers Consume KALE: Post Training KullbackLeibler Alignment
    of Embeddings for Asymmetrical dual encoders'
  tldr: ''
  track: The Fourth Workshop on Simple and Efficient Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Sho Takase
  - Shun Kiyono
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SustaiNLP
  forum: ''
  id: SustaiNLP_6
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Lessons on Parameter Sharing across Layers in Transformers
  tldr: ''
  track: The Fourth Workshop on Simple and Efficient Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Daniel Campos
  - Chengxiang Zhai
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SustaiNLP
  forum: ''
  id: SustaiNLP_7
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'To Asymmetry and Beyond: Structured Pruning of Sequence to Sequence Models
    for Improved Inference Efficiency'
  tldr: ''
  track: The Fourth Workshop on Simple and Efficient Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Dantong Liu
  - Kaushik Pavani
  - Sunny Dasgupta
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SustaiNLP
  forum: ''
  id: SustaiNLP_9
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Small is the New Big: Pre-finetuned compact models are better for Asynchronous
    Active Learning'
  tldr: ''
  track: The Fourth Workshop on Simple and Efficient Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Aditya Shah
  - Surendrabikram Thapa
  - Aneesh Jain
  - Lifu Huang
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SustaiNLP
  forum: ''
  id: SustaiNLP_10
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'ADEPT: Adapter-based Efficient Prompt Tuning Approach for Language Models'
  tldr: ''
  track: The Fourth Workshop on Simple and Efficient Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Jean-michel Attendu
  - Jean-philippe Corbeil
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SustaiNLP
  forum: ''
  id: SustaiNLP_14
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'NLU on Data Diets: Dynamic Data Subset Selection for NLP Classification
    Tasks'
  tldr: ''
  track: The Fourth Workshop on Simple and Efficient Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Zhisong Zhang
  - Emma Strubell
  - Eduard Hovy
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SustaiNLP
  forum: ''
  id: SustaiNLP_15
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: On the Interactions of Structural Constraints and Data Resources for Structured
    Prediction
  tldr: ''
  track: The Fourth Workshop on Simple and Efficient Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Joel Niklaus
  - Daniele Giofre
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SustaiNLP
  forum: ''
  id: SustaiNLP_18
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Can we Pretrain a SotA Legal Language Model on a Budget From Scratch?
  tldr: ''
  track: The Fourth Workshop on Simple and Efficient Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Chenyang Lyu
  - Tianbo Ji
  - Yvette Graham
  - Jennifer Foster
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SustaiNLP
  forum: ''
  id: SustaiNLP_19
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Is a Video worth n  n Images? A Highly Efficient Approach to Transformer-based
    Video Question Answering
  tldr: ''
  track: The Fourth Workshop on Simple and Efficient Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Xin Xu
  - Yuqi Zhu
  - Xiaohan Wang
  - Ningyu Zhang
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SustaiNLP
  forum: ''
  id: SustaiNLP_21
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: How to Unleash the Power of Large Language Models for Few-shot Relation Extraction?
  tldr: ''
  track: The Fourth Workshop on Simple and Efficient Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Jay Mohta
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SustaiNLP
  forum: ''
  id: SustaiNLP_23
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Prompting language models improves performance in imbalanced setting
  tldr: ''
  track: The Fourth Workshop on Simple and Efficient Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Nick Mckenna
  - Priyanka Sen
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SustaiNLP
  forum: ''
  id: SustaiNLP_25
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: KGQA Without Retraining
  tldr: ''
  track: The Fourth Workshop on Simple and Efficient Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Shashank Sonkar
  - Zichao Wang
  - Richard Baraniuk
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SustaiNLP
  forum: ''
  id: SustaiNLP_26
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'MANER: Mask Augmented Named Entity Recognition for Extreme Low-Resource
    Languages'
  tldr: ''
  track: The Fourth Workshop on Simple and Efficient Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Peggy Tang
  - Junbin Gao
  - Lei Zhang
  - Zhiyong Wang
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SustaiNLP
  forum: ''
  id: SustaiNLP_27
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Efficient and Interpretable Compressive Text Summarisation with Unsupervised
    Dual-Agent Reinforcement Learning
  tldr: ''
  track: The Fourth Workshop on Simple and Efficient Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Jianbang Ding
  - Suiyun Zhang
  - Linlin Li
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SustaiNLP
  forum: ''
  id: SustaiNLP_28
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Adder Encoder for Pre-trained Language Model
  tldr: ''
  track: The Fourth Workshop on Simple and Efficient Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Gregory Szumel
  - Ghazal Khalighinejad
  - Rickard Stureborg
  - Sam Wiseman
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SustaiNLP
  forum: ''
  id: SustaiNLP_33
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Exploring the Effect of Frequency Resolution in FNet
  tldr: ''
  track: The Fourth Workshop on Simple and Efficient Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Aliki Anagnostopoulou
  - Mareike Hartmann
  - Daniel Sonntag
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SustaiNLP
  forum: ''
  id: SustaiNLP_34
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Towards Adaptable and Interactive Image Captioning with Data Augmentation
    and Episodic Memory
  tldr: ''
  track: The Fourth Workshop on Simple and Efficient Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Raghuveer Thirukovalluru
  - Bhuwan Dhingra
  - Sam Wiseman
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SustaiNLP
  forum: ''
  id: SustaiNLP_36
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Sequence Reducible Holdout Loss for Language Model Pretraining
  tldr: ''
  track: The Fourth Workshop on Simple and Efficient Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Ameeta Agrawal
  - Suresh Singh
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SustaiNLP
  forum: ''
  id: SustaiNLP_38
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Corpus Complexity Matters in Pretraining Language Models
  tldr: ''
  track: The Fourth Workshop on Simple and Efficient Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Xu Han
  - Bin Guo
  - Yoon Jung
  - Benjamin Yao
  - Yu Zhang
  - Xiaohu Liu
  - Chenlei Guo
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SustaiNLP
  forum: ''
  id: SustaiNLP_39
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'PersonaPKT: Building Personalized Dialogue Agents via Parameter-efficient
    Knowledge Transfer'
  tldr: ''
  track: The Fourth Workshop on Simple and Efficient Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Ganesh Jawahar
  - Subhabrata Mukherjee
  - Debadeepta Dey
  - Muhammad Abdul-mageed
  - Laks Lakshmanan, V.s.
  - Caio Mendes
  - Gustavo De Rosa
  - Shital Shah
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SustaiNLP
  forum: ''
  id: SustaiNLP_41
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Small Character Models Match Large Word Models for Autocomplete Under Memory
    Constraints
  tldr: ''
  track: The Fourth Workshop on Simple and Efficient Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Yuxuan Wang
  - Lyu Hong
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SustaiNLP
  forum: ''
  id: SustaiNLP_44
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Query Encoder Distillation via Embedding Alignment is a Strong Baseline Method
    to Boost Dense Retriever Online Efficiency
  tldr: ''
  track: The Fourth Workshop on Simple and Efficient Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: ''
  authors:
  - Benno Kruit
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SustaiNLP
  forum: ''
  id: SustaiNLP_45
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Minimalist Entity Disambiguation for Mid-Resource Languages
  tldr: ''
  track: The Fourth Workshop on Simple and Efficient Natural Language Processing
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper reports on the shared tasks organized by the 20th IWSLT Conference.
    The shared tasks address 9 scientific challenges in spoken language translation:
    simultaneous and offline translation, automatic subtitling and dubbing, speech-to-speech
    translation, multilingual, dialect and low-resource speech translation, and formality
    control. The shared tasks attracted a total of 38 submissions by 31 teams. The
    growing interest towards spoken language translation is also witnessed by the
    constantly increasing number of shared task organizers and contributors to the
    overview paper, almost evenly distributed across industry and academia.'
  authors:
  - Sweta Agrawal
  - Antonios Anastasopoulos
  - Luisa Bentivogli
  - "Ond\u0159ej Bojar"
  - Claudia Borg
  - Marine Carpuat
  - Roldano Cattoni
  - Mauro Cettolo
  - Mingda Chen
  - William Chen
  - Khalid Choukri
  - Alexandra Chronopoulou
  - Anna Currey
  - Thierry Declerck
  - Qianqian Dong
  - Kevin Duh
  - "Yannick Est\xE8ve"
  - Marcello Federico
  - Souhir Gahbiche
  - Barry Haddow
  - Benjamin Hsu
  - Phu Mon Htut
  - Hirofumi Inaguma
  - "D\xE1vid Javorsk\xFD"
  - John Judge
  - Yasumasa Kano
  - Tom Ko
  - Rishu Kumar
  - Pengwei Li
  - Xutai Ma
  - Prashant Mathur
  - Evgeny Matusov
  - Paul McNamee
  - John P. McCrae
  - Kenton Murray
  - Maria Nadejde
  - Satoshi Nakamura
  - Matteo Negri
  - Ha Nguyen
  - Jan Niehues
  - Xing Niu
  - Atul Kr. Ojha
  - John E. Ortega
  - Proyag Pal
  - Juan Pino
  - Lonneke van der Plas
  - "Peter Pol\xE1k"
  - Elijah Rippeth
  - Elizabeth Salesky
  - Jiatong Shi
  - Matthias Sperber
  - "Sebastian St\xFCker"
  - Katsuhito Sudoh
  - Yun Tang
  - Brian Thompson
  - Kevin Tran
  - Marco Turchi
  - Alex Waibel
  - Mingxuan Wang
  - Shinji Watanabe
  - Rodolfo Zevallos
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - IWSLT
  forum: ''
  id: IWSLT_1
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: other
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN
  tldr: 'This paper reports on the shared tasks organized by the 20th IWSLT Conference.
    The shared tasks address 9 scientific challenges in spoken language translation:
    simultaneous and offline translation, automatic subtitling and dubbing, speech-to-speech
    translation, multilingual, dialect and low-resource'
  track: The 20th International Conference on Spoken Language Translation
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We present the ACL 60/60 evaluation sets for multilingual translation
    of ACL 2022 technical presentations into 10 target languages. This dataset enables
    further research into multilingual speech translation under realistic recording
    conditions with unsegmented audio and domain-specific terminology, applying NLP
    tools to text and speech in the technical domain, and evaluating and improving
    model robustness to diverse speaker demographics.
  authors:
  - Elizabeth Salesky
  - Kareem Darwish
  - Mohamed Al-Badrashiny
  - Mona Diab
  - Jan Niehues
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - IWSLT
  forum: ''
  id: IWSLT_2
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Evaluating Multilingual Speech Translation under Realistic Conditions with
    Resegmentation and Terminology
  tldr: 'We present the ACL 60/60 evaluation sets for multilingual translation of
    ACL 2022 technical presentations into 10 target languages. This dataset enables
    further research into multilingual speech translation under realistic recording
    conditions with unsegmented audio and domain-specific terminology, '
  track: The 20th International Conference on Spoken Language Translation
  underline_id: null
  underline_url: null
  video_url: null
- abstract: "This paper presents the \textsc{MineTrans} English-to-Chinese speech\
    \ translation systems developed for two challenge tracks of IWSLT 2023, i.e.,\
    \ Offline Speech Translation (S2T) and Speech-to-Speech Translation (S2ST).  For\
    \ the S2T track, \textsc{MineTrans} employs a practical cascaded system to explore\
    \ the limits of translation performance in both constrained and unconstrained\
    \ settings, where the whole system consists of automatic speech recognition (ASR),\
    \ punctuation recognition (PC), and machine translation (MT) modules.  We also\
    \ investigate the effectiveness of multiple ASR architectures and explore two\
    \ MT strategies: supervised in-domain fine-tuning and prompt-guided translation\
    \ using a large language model.  For the S2ST track, we explore a speech-to-unit\
    \ (S2U) framework to build an end-to-end S2ST system. This system encodes the\
    \ target speech as discrete units via our trained HuBERT. Then it leverages the\
    \ standard sequence-to-sequence model to directly learn the mapping between source\
    \ speech and discrete units without any auxiliary recognition tasks (i.e., ASR\
    \ and MT tasks). Various efforts are made to improve the \textsc{MineTrans}'s\
    \ performance, such as acoustic model pre-training on large-scale data, data filtering,\
    \ data augmentation, speech segmentation, knowledge distillation, consistency\
    \ training, model ensembles, etc."
  authors:
  - Yichao Du
  - Guo Zhengsheng
  - Jinchuan Tian
  - Zhirui Zhang
  - Xing Wang
  - Jianwei Yu
  - Zhaopeng Tu
  - Tong Xu
  - Enhong Chen
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - IWSLT
  forum: ''
  id: IWSLT_3
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: The MineTrans Systems for IWSLT 2023 Offline Speech Translation and Speech-to-Speech
    Translation Tasks
  tldr: "This paper presents the \textsc{MineTrans} English-to-Chinese speech translation\
    \ systems developed for two challenge tracks of IWSLT 2023, i.e., Offline Speech\
    \ Translation (S2T) and Speech-to-Speech Translation (S2ST).  For the S2T track,\
    \ \textsc{MineTrans} employs a practical cascaded system to explo"
  track: The 20th International Conference on Spoken Language Translation
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'End-to-end automatic speech translation (AST) relies on data that combines
    audio inputs with text translation outputs. Previous work used existing large
    parallel corpora of transcriptions and translations in a knowledge distillation
    (KD) setup to distill a neural machine translation (NMT) into an AST student model.
    While KD allows using larger pretrained models, the reliance of previous KD approaches
    on manual audio transcripts in the data pipeline restricts the applicability of
    this framework to AST. We present an imitation learning approach where a teacher
    NMT system corrects the errors of an AST student without relying on manual transcripts.
    We show that the NMT teacher can recover from errors in automatic transcriptions
    and is able to correct erroneous translations of the AST student, leading to improvements
    of about 4 BLEU points over the standard AST end-to-end baseline on the English-German
    CoVoST-2 and MuST-C datasets, respectively. Code and data are publicly available:
    https://github.com/HubReb/imitkd_ast/releases/tag/v1.1'
  authors:
  - Rebekka Hubert
  - Artem Sokolov
  - Stefan Riezler
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - IWSLT
  forum: ''
  id: IWSLT_4
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Improving End-to-End Speech Translation by Imitation-Based Knowledge Distillation
    with Synthetic Transcripts
  tldr: End-to-end automatic speech translation (AST) relies on data that combines
    audio inputs with text translation outputs. Previous work used existing large
    parallel corpora of transcriptions and translations in a knowledge distillation
    (KD) setup to distill a neural machine translation (NMT) into an AS
  track: The 20th International Conference on Spoken Language Translation
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper presents the USTC system for the IWSLT 2023 Dialectal and Low-resource
    shared task, which involves translation from Tunisian Arabic to English. We aim
    to investigate the mutual transfer between Tunisian Arabic and Modern Standard
    Arabic (MSA) to enhance the performance of speech translation (ST) by following
    standard pre-training and fine-tuning pipelines. We synthesize a substantial amount
    of pseudo Tunisian-English paired data using a multi-step pre-training approach.
    Integrating a Tunisian-MSA translation module into the end-to-end ST model enables
    the transfer from Tunisian to MSA and facilitates linguistic normalization of
    the dialect. To increase the robustness of the ST system, we optimize the model's
    ability to adapt to ASR errors and propose a model ensemble method. Results indicate
    that applying the dialect transfer method can increase the BLEU score of dialectal
    ST. It is shown that the optimal system ensembles both cascaded and end-to-end
    ST models, achieving BLEU improvements of 2.4 and 2.8 in test1 and test2 sets,
    respectively, compared to the best published system.
  authors:
  - Pan Deng
  - Shihao Chen
  - Weitai Zhang
  - Jie Zhang
  - Lirong Dai
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - IWSLT
  forum: ''
  id: IWSLT_5
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: The USTC's Dialect Speech Translation System for IWSLT 2023
  tldr: This paper presents the USTC system for the IWSLT 2023 Dialectal and Low-resource
    shared task, which involves translation from Tunisian Arabic to English. We aim
    to investigate the mutual transfer between Tunisian Arabic and Modern Standard
    Arabic (MSA) to enhance the performance of speech translati
  track: The 20th International Conference on Spoken Language Translation
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Many existing speech translation benchmarks focus on native-English speech
    in high-quality recording conditions, which often do not match the conditions
    in real-life use-cases. In this paper, we describe our speech translation system
    for the multilingual track of IWSLT 2023, which focuses on the translation of
    scientific conference talks. The test condition features accented input speech
    and terminology-dense contents. The tasks requires translation into 10 languages
    of varying amounts of resources. In absence of training data from the target domain,
    we use a retrieval-based approach ($k$NN-MT) for effective adaptation ($+0.8$
    BLEU for speech translation). We also use adapters to easily integrate incremental
    training data from data augmentation, and show that it matches the performance
    of re-training. We observe that cascaded systems are more easily adaptable towards
    specific target domains, due to their separate modules. Our cascaded speech system
    outperforms its end-to-end counterpart on scientific talk translation, although
    their performance remains similar on TED talks.
  authors:
  - Danni Liu
  - Thai Binh Nguyen
  - Sai Koneru
  - Enes Yavuz Ugan
  - Ngoc-Quan Pham
  - Tuan Nam Nguyen
  - Tu Anh Dinh
  - Carlos Mullov
  - Alexander Waibel
  - Jan Niehues
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - IWSLT
  forum: ''
  id: IWSLT_7
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: KIT's Multilingual Speech Translation System for IWSLT 2023
  tldr: Many existing speech translation benchmarks focus on native-English speech
    in high-quality recording conditions, which often do not match the conditions
    in real-life use-cases. In this paper, we describe our speech translation system
    for the multilingual track of IWSLT 2023, which focuses on the tra
  track: The 20th International Conference on Spoken Language Translation
  underline_id: null
  underline_url: null
  video_url: null
- abstract: "This paper describes the BIGAI's submission to IWSLT 2023 Offline Speech\
    \ Translation task on three language tracks from English to Chinese, German and\
    \ Japanese. The end-to-end systems are built upon a Wav2Vec2 model for speech\
    \ recognition and mBART50 models for machine translation. An adapter module is\
    \ applied to bridge the speech module and the translation module. The CTC loss\
    \ between speech features and source token sequence is incorporated during training.\
    \ Experiments show that the systems can generate reasonable translations on three\
    \ languages. The proposed models achieve BLEU scores of 22.3 for en\u2192de, 10.7\
    \ for en\u2192ja and 33.0 for en\u2192zh on tst2023 TED datasets. However, the\
    \ performance is decreased by a significant margin on complex scenarios like persentations\
    \ and interview."
  authors:
  - Zhihang Xie
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - IWSLT
  forum: ''
  id: IWSLT_8
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: The BIGAI Offline Speech Translation Systems for IWSLT 2023 Evaluation
  tldr: This paper describes the BIGAI's submission to IWSLT 2023 Offline Speech Translation
    task on three language tracks from English to Chinese, German and Japanese. The
    end-to-end systems are built upon a Wav2Vec2 model for speech recognition and
    mBART50 models for machine translation. An adapter module
  track: The 20th International Conference on Spoken Language Translation
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We present a simple yet efficient method to enhance the quality of machine
    translation models trained on multimodal corpora by augmenting the training text
    with labels of detected objects in the corresponding video segments. We then test
    the effects of label augmentation in both baseline and two automatic speech recognition
    (ASR) conditions. In contrast with multimodal techniques that merge visual and
    textual features, our modular method is easy to implement and the results are
    more interpretable.  Comparisons are made with Transformer translation architectures
    trained with baseline and augmented labels, showing improvements of up to +1.0
    BLEU on the How2 dataset.
  authors:
  - Jeremy Gwinnup
  - Tim Anderson
  - Brian Ore
  - Eric Hansen
  - Kevin Duh
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - IWSLT
  forum: ''
  id: IWSLT_10
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Enhancing Video Translation Context with Object Labels
  tldr: We present a simple yet efficient method to enhance the quality of machine
    translation models trained on multimodal corpora by augmenting the training text
    with labels of detected objects in the corresponding video segments. We then test
    the effects of label augmentation in both baseline and two aut
  track: The 20th International Conference on Spoken Language Translation
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper presents the submission of Huawei Translation Services Center
    for the IWSLT 2023 dubbing task in the unconstrained setting. The proposed solution
    consists of a Transformer-based machine translation model and a phoneme duration
    predictor. The Transformer is deep and multiple target-to-source length-ratio
    class labels are used to control target lengths. The variation predictor in FastSpeech2
    is utilized to predict phoneme durations. To optimize the isochrony in dubbing,
    re-ranking and scaling are performed. The source audio duration is used as a reference
    to re-rank the translations of different length-ratio labels, and the one with
    minimum time deviation is preferred. Additionally, the phoneme duration outputs
    are scaled within a defined threshold to narrow the duration gap with the source
    audio.
  authors:
  - Zhiqiang Rao
  - Hengchao Shang
  - Jinlong Yang
  - Daimeng Wei
  - Zongyao Li
  - Jiaxin GUO
  - Shaojun Li
  - Zhengzhe Yu
  - Zhanglin Wu
  - Yuhao Xie
  - Bin Wei
  - Jiawei Zheng
  - Lizhi Lei
  - Hao Yang
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - IWSLT
  forum: ''
  id: IWSLT_11
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Length-Aware NMT and Adaptive Duration for Automatic Dubbing
  tldr: This paper presents the submission of Huawei Translation Services Center for
    the IWSLT 2023 dubbing task in the unconstrained setting. The proposed solution
    consists of a Transformer-based machine translation model and a phoneme duration
    predictor. The Transformer is deep and multiple target-to-sour
  track: The 20th International Conference on Spoken Language Translation
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper presents NAVER LABS Europe's systems for Tamasheq-French and
    Quechua-Spanish speech translation in the IWSLT 2023 Low-Resource track. Our work
    attempts to maximize translation quality in low-resource settings using multilingual
    parameter-efficient solutions that leverage strong pre-trained models. Our primary
    submission for Tamasheq outperforms the previous state of the art by 7.5 BLEU
    points on the IWSLT 2022 test set, and achieves 23.6 BLEU on this year's test
    set, outperforming the second best participant by 7.7 points. For Quechua, we
    also rank first and achieve 17.7 BLEU, despite having only two hours of translation
    data. Finally, we show that our proposed multilingual architecture is also competitive
    for high-resource languages, outperforming the best unconstrained submission to
    the IWSLT 2021 Multilingual track, despite using much less training data and compute.
  authors:
  - Edward Gow-Smith
  - Alexandre Berard
  - Marcely Zanon Boito
  - Ioan Calapodescu
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - IWSLT
  forum: ''
  id: IWSLT_12
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: NAVER LABS Europe's Multilingual Speech Translation Systems for the IWSLT
    2023 Low-Resource Track
  tldr: This paper presents NAVER LABS Europe's systems for Tamasheq-French and Quechua-Spanish
    speech translation in the IWSLT 2023 Low-Resource track. Our work attempts to
    maximize translation quality in low-resource settings using multilingual parameter-efficient
    solutions that leverage strong pre-traine
  track: The 20th International Conference on Spoken Language Translation
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper describes the FBK''s participation in the Simultaneous Translation
    and Automatic Subtitling tracks of the IWSLT 2023 Evaluation Campaign. Our submission
    focused on the use of direct architectures to perform both tasks: for the simultaneous
    one, we leveraged the knowledge already acquired by offline-trained models and
    directly applied a policy to obtain the real-time inference; for the subtitling
    one, we adapted the direct ST model to produce well-formed subtitles and exploited
    the same architecture to produce timestamps needed for the subtitle synchronization
    with audiovisual content. Our English-German SimulST system shows a reduced computational-aware
    latency compared to the one achieved by the top-ranked systems in the 2021 and
    2022 rounds of the task, with gains of up to 3.5 BLEU. Our automatic subtitling
    system outperforms the only-existing solution based on a direct system by 3.7
    and 1.7 SubER in English-German and English-Spanish respectively.'
  authors:
  - Sara Papi
  - Marco Gaido
  - Matteo Negri
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - IWSLT
  forum: ''
  id: IWSLT_13
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Direct Models for Simultaneous Translation and Automatic Subtitling: FBK@IWSLT2023'
  tldr: 'This paper describes the FBK''s participation in the Simultaneous Translation
    and Automatic Subtitling tracks of the IWSLT 2023 Evaluation Campaign. Our submission
    focused on the use of direct architectures to perform both tasks: for the simultaneous
    one, we leveraged the knowledge already acquired b'
  track: The 20th International Conference on Spoken Language Translation
  underline_id: null
  underline_url: null
  video_url: null
- abstract: There have been several meta-evaluation studies on the correlation between
    human ratings and offline machine translation (MT) evaluation metrics such as
    BLEU, chrF2,  BertScore and COMET. These metrics have been used to evaluate simultaneous
    speech translation (SST) but their correlations with human ratings of SST, which
    has been recently collected as Continuous Ratings (CR), are unclear. In this paper,
    we leverage the evaluations of candidate systems submitted to the English-German
    SST task at IWSLT 2022 and conduct an extensive correlation analysis of CR and
    the aforementioned metrics. Our study reveals that the offline metrics are well
    correlated with CR and can be reliably used for evaluating machine translation
    in simultaneous mode, with some limitations on the test set size. We conclude
    that given the current quality levels of SST, these metrics can be used as proxies
    for CR, alleviating the need for large scale human evaluation. Additionally, we
    observe that correlations of the metrics with translation as a reference is significantly
    higher than with simultaneous interpreting, and thus we recommend the former for
    reliable evaluation.
  authors:
  - "Dominik Mach\xE1\u010Dek"
  - "Ond\u0159ej Bojar"
  - Raj Dabre
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - IWSLT
  forum: ''
  id: IWSLT_14
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: MT Metrics Correlate with Human Ratings of Simultaneous Speech Translation
  tldr: There have been several meta-evaluation studies on the correlation between
    human ratings and offline machine translation (MT) evaluation metrics such as
    BLEU, chrF2,  BertScore and COMET. These metrics have been used to evaluate simultaneous
    speech translation (SST) but their correlations with human
  track: The 20th International Conference on Spoken Language Translation
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper presents Huawei Translation Service Center (HW-TSC)''s submission
    on the IWSLT 2023 formality control task, which provides two training scenarios:
    supervised and zero-shot, each containing two language pairs, and sets  constrained
    and unconstrained conditions. We train the formality control models for these
    four language pairs under these two conditions respectively, and submit the corresponding
    translation results. Our efforts are divided into two fronts: enhancing general
    translation quality and improving formality control capability. According to the
    different requirements of the formality control task, we use a multi-stage pre-training
    method to train a bilingual or multilingual neural machine translation (NMT) model
    as the basic model, which can improve the general translation quality of the base
    model to a relatively high level. Then, under the premise of affecting the general
    translation quality of the basic model as little as possible, we adopt domain
    adaptation and reranking-based transductive learning methods to improve the formality
    control capability of the model.'
  authors:
  - Zhanglin Wu
  - Zongyao Li
  - Daimeng Wei
  - Hengchao Shang
  - Jiaxin Guo
  - Xiaoyu Chen
  - Zhiqiang Rao
  - Zhengzhe YU
  - Jinlong Yang
  - Shaojun Li
  - Yuhao Xie
  - Bin Wei
  - Jiawei Zheng
  - Ming Zhu
  - Lizhi Lei
  - Hao Yang
  - Yanfei Jiang
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - IWSLT
  forum: ''
  id: IWSLT_16
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Improving Neural Machine Translation Formality Control with Domain Adaptation
    and Reranking-based Transductive Learning
  tldr: 'This paper presents Huawei Translation Service Center (HW-TSC)''s submission
    on the IWSLT 2023 formality control task, which provides two training scenarios:
    supervised and zero-shot, each containing two language pairs, and sets  constrained
    and unconstrained conditions. We train the formality contro'
  track: The 20th International Conference on Spoken Language Translation
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper presents HW-TSC's submissions to the IWSLT 2023 Offline Speech
    Translation task, including speech translation of talks from English to German,
    Chinese, and Japanese, respectively. We participate in all three conditions (constrained
    training, constrained with large language models training, and unconstrained training)
    with models of cascaded architectures. We use data enhancement, pre-training models
    and other means to improve the ASR quality, and R-Drop, deep model, domain data
    selection, etc. to improve the translation quality. Compared with last year's
    best results, we achieve 2.1 BLEU improvement on the MuST-C English-German test
    set.
  authors:
  - Zongyao Li
  - Zhanglin Wu
  - Zhiqiang Rao
  - Xie YuHao
  - Guo JiaXin
  - Daimeng Wei
  - Hengchao Shang
  - Wang Minghan
  - Xiaoyu Chen
  - Zhengzhe YU
  - Li ShaoJun
  - Lei LiZhi
  - Hao Yang
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - IWSLT
  forum: ''
  id: IWSLT_17
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'HW-TSC at IWSLT2023: Break the Quality Ceiling of Offline Track via Pre-Training
    and Domain Adaptation'
  tldr: This paper presents HW-TSC's submissions to the IWSLT 2023 Offline Speech
    Translation task, including speech translation of talks from English to German,
    Chinese, and Japanese, respectively. We participate in all three conditions (constrained
    training, constrained with large language models training
  track: The 20th International Conference on Spoken Language Translation
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes the submissions of the research group USTC-NELSLIP
    to the 2023 IWSLT Offline Speech Translation competition, which involves translating
    spoken English into written Chinese. We utilize both cascaded models and end-to-end
    models for this task. To improve the performance of the cascaded models, we introduce
    Whisper to reduce errors in the intermediate source language text, achieving a
    significant improvement in ASR recognition performance. For end-to-end models,
    we propose Stacked Acoustic-and-Textual En- coding extension (SATE-ex), which
    feeds the output of the acoustic decoder into the textual decoder for information
    fusion and to prevent error propagation. Additionally, we improve the performance
    of the end-to-end system in translating speech by combining the SATE-ex model
    with the encoder-decoder model through ensembling.
  authors:
  - Xinyuan Zhou
  - Jianwei Cui
  - Zhongyi Ye
  - Yichi Wang
  - Luzhen Xu
  - Hanyi Zhang
  - Weitai Zhang
  - Lirong Dai
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - IWSLT
  forum: ''
  id: IWSLT_21
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Submission of USTC's System for the IWSLT 2023 - Offline Speech Translation
    Track
  tldr: This paper describes the submissions of the research group USTC-NELSLIP to
    the 2023 IWSLT Offline Speech Translation competition, which involves translating
    spoken English into written Chinese. We utilize both cascaded models and end-to-end
    models for this task. To improve the performance of the cas
  track: The 20th International Conference on Spoken Language Translation
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes I2R's submission to the offline speech translation
    track for IWSLT 2023. We focus on an end-to-end approach for translation from
    English audio to German text, one of the three available language directions in
    this year's edition. The I2R system leverages on pretrained models that have been
    exposed to large-scale audio and text data for our base model. We introduce several
    stages of additional pretraining followed by fine-tuning to adapt the system for
    the downstream speech translation task. The strategy is supplemented by other
    techniques such as data augmentation, domain tagging, knowledge distillation,
    and model ensemble, among others. We evaluate the system on several publicly available
    test sets for comparison.
  authors:
  - Muhammad Huzaifah
  - Kye Min Tan
  - Richeng Duan
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - IWSLT
  forum: ''
  id: IWSLT_22
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: I2R's End-to-End Speech Translation System for IWSLT 2023 Offline Shared
    Task
  tldr: This paper describes I2R's submission to the offline speech translation track
    for IWSLT 2023. We focus on an end-to-end approach for translation from English
    audio to German text, one of the three available language directions in this year's
    edition. The I2R system leverages on pretrained models tha
  track: The 20th International Conference on Spoken Language Translation
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes the NiuTrans end-to-end speech translation system
    submitted for the IWSLT 2023 English-to-Chinese offline task. Our speech translation
    models are composed of pre-trained ASR and MT models under the SATE framework.
    Several pre-trained models with diverse architectures and input representations
    (e.g., log Mel-filterbank and waveform) were utilized. We proposed an IDA method
    to iteratively improve the performance of the MT models and generate the pseudo
    ST data through MT systems. We then trained ST models with different structures
    and data settings to enhance ensemble performance. Experimental results demonstrate
    that our NiuTrans system achieved a BLEU score of 29.22 on the MuST-C En-Zh tst-COMMON
    set, outperforming the previous year's submission by 0.12 BLEU despite using less
    MT training data.
  authors:
  - Yuchen Han
  - Xiaoqian Liu
  - Hao Chen
  - Yuhao Zhang
  - Chen Xu
  - Tong Xiao
  - Jingbo Zhu
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - IWSLT
  forum: ''
  id: IWSLT_23
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: The NiuTrans End-to-End Speech Translation System for IWSLT23 English-to-Chinese
    Offline Task
  tldr: This paper describes the NiuTrans end-to-end speech translation system submitted
    for the IWSLT 2023 English-to-Chinese offline task. Our speech translation models
    are composed of pre-trained ASR and MT models under the SATE framework. Several
    pre-trained models with diverse architectures and input r
  track: The 20th International Conference on Spoken Language Translation
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes the ON-TRAC consortium speech translation systems
    developed for IWSLT 2023 evaluation campaign. Overall, we participated in three
    speech translation tracks featured in the low-resource and dialect speech translation
    shared tasks, namely; i) spoken Tamasheq to written French, ii) spoken Pashto
    to written French, and  iii) spoken Tunisian to written English. All our primary
    submissions are based on the end-to-end speech-to-text neural architecture using
    a pretrained SAMU-XLSR model as a speech encoder and a mbart model as a decoder.
    The SAMU-XLSR model is built from the XLS-R~128 in order to generate language
    agnostic sentence-level embeddings. This building is driven by the LaBSE model
    trained on multilingual text dataset. This architecture allows us to improve the
    input speech representations and achieve significant improvements compared to
    conventional end-to-end speech translation systems.
  authors:
  - Antoine Laurent
  - Souhir Gahbiche
  - Ha Nguyen
  - Haroun Elleuch
  - Fethi Bougares
  - Antoine Thiol
  - Hugo Riguidel
  - Salima Mdhaffar
  - "Ga\xEBlle Laperri\xE8re"
  - Lucas Maison
  - Sameer Khurana
  - "Yannick Est\xE8ve"
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - IWSLT
  forum: ''
  id: IWSLT_24
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: ON-TRAC Consortium Systems for the IWSLT 2023 Dialectal and Low-resource
    Speech Translation Tasks
  tldr: This paper describes the ON-TRAC consortium speech translation systems developed
    for IWSLT 2023 evaluation campaign. Overall, we participated in three speech translation
    tracks featured in the low-resource and dialect speech translation shared tasks,
    namely; i) spoken Tamasheq to written French, ii)
  track: The 20th International Conference on Spoken Language Translation
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes the systems submitted for Marathi to Hindi low-resource
    speech translation task. Our primary submission is based on an end-to-end direct
    speech translation system, whereas the contrastive one is a cascaded system. The
    backbone of both the systems is a Hindi-Marathi bilingual ASR system trained on
    2790 hours of imperfect transcribed speech. The end-to-end speech translation
    system was directly initialized from the ASR, and then fine-tuned for direct speech
    translation with an auxiliary CTC loss for translation. The MT model for the cascaded
    system is initialized from a cross-lingual language model, which was then fine-tuned
    using 1.6 M parallel sentences. All our systems were trained from scratch on publicly
    available datasets. In the end, we use a language model to re-score the n-best
    hypotheses. Our primary submission achieved 30.5 and 39.6 BLEU whereas the contrastive
    system obtained 21.7 and 28.6 BLEU on official dev and test sets respectively.
    The paper also presents the analysis on several experiments that were conducted
    and outlines the strategies for improving speech translation in low-resource scenarios.
  authors:
  - Santosh Kesiraju
  - "Karel Bene\u0161"
  - Maksim Tikhonov
  - "Jan \u010Cernock\xFD"
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - IWSLT
  forum: ''
  id: IWSLT_25
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: BUT Systems for IWSLT 2023 Marathi - Hindi Low Resource Speech Translation
    Task
  tldr: This paper describes the systems submitted for Marathi to Hindi low-resource
    speech translation task. Our primary submission is based on an end-to-end direct
    speech translation system, whereas the contrastive one is a cascaded system. The
    backbone of both the systems is a Hindi-Marathi bilingual ASR
  track: The 20th International Conference on Spoken Language Translation
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes CMU's submission to the IWSLT 2023 simultaneous speech
    translation shared task for translating English speech to both German text and
    speech in a streaming fashion. We first build offline speech-to-text (ST) models
    using the joint CTC/attention framework. These models also use WavLM front-end
    features and mBART decoder initialization. We adapt our offline ST models for
    simultaneous speech-to-text translation (SST) by 1) incrementally encoding chunks
    of input speech, re-computing encoder states for each new chunk and 2) incrementally
    decoding output text, pruning beam search hypotheses to 1-best after processing
    each chunk. We then build text-to-speech (TTS) models using the VITS framework
    and achieve simultaneous speech-to-speech translation (SS2ST) by cascading our
    SST and TTS models.
  authors:
  - Brian Yan
  - Jiatong Shi
  - Soumi Maiti
  - William Chen
  - Xinjian Li
  - Yifan Peng
  - Siddhant Arora
  - Shinji Watanabe
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - IWSLT
  forum: ''
  id: IWSLT_26
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: CMU's IWSLT 2023 Simultaneous Speech Translation System
  tldr: This paper describes CMU's submission to the IWSLT 2023 simultaneous speech
    translation shared task for translating English speech to both German text and
    speech in a streaming fashion. We first build offline speech-to-text (ST) models
    using the joint CTC/attention framework. These models also use W
  track: The 20th International Conference on Spoken Language Translation
  underline_id: null
  underline_url: null
  video_url: null
- abstract: "This paper describes the speech translation system submitted as part\
    \ of the IWSLT 2023 shared task on low resource speech translation. The low resource\
    \ task aids in building models for language pairs where the training corpus is\
    \ limited. In this paper, we focus on two language pairs, namely, Tamasheq-French\
    \ (Tmh\u2192Fra) and Marathi-Hindi (Mr\u2192Hi) and implement a speech translation\
    \ system that is unconstrained. We evaluate three strategies in our system: (a)\
    \ Data augmentation where we perform different operations on audio as well as\
    \ text samples, (b) an ensemble model that integrates a set of models trained\
    \ using a combination of augmentation strategies, and (c) post-processing techniques\
    \ where we explore the use of large language models (LLMs) to improve the quality\
    \ of sentences that are generated. Experiments show how data augmentation can\
    \ relatively improve the BLEU score by 5.2% over the baseline system for Tmh\u2192\
    Fra while an ensemble model further improves performance by 17% for Tmh\u2192\
    Fra and 23% for Mr\u2192Hi task."
  authors:
  - Akshaya Vishnu Kudlu Shanbhogue
  - Ran Xue
  - Soumya Saha
  - Daniel Zhang
  - Ashwinkumar Ganesan
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - IWSLT
  forum: ''
  id: IWSLT_28
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Improving Low Resource Speech Translation with Data Augmentation and Ensemble
    Strategies
  tldr: This paper describes the speech translation system submitted as part of the
    IWSLT 2023 shared task on low resource speech translation. The low resource task
    aids in building models for language pairs where the training corpus is limited.
    In this paper, we focus on two language pairs, namely, Tamashe
  track: The 20th International Conference on Spoken Language Translation
  underline_id: null
  underline_url: null
  video_url: null
- abstract: AppTek participated in the subtitling and formality tracks of the IWSLT
    2023 evaluation. This paper describes the details of our subtitling pipeline -
    speech segmentation, speech recognition, punctuation prediction and inverse text
    normalization, text machine translation and direct speech-to-text translation,
    intelligent line segmentation - and how we make use of the provided subtitling-specific
    data in training and fine-tuning. The evaluation results show that our final submissions
    are competitive, in particular outperforming the submissions by other participants
    by 5% absolute as measured by the SubER subtitle quality metric. For the formality
    track, we participate with our En-Ru and En-Pt production models, which support
    formality control via prefix tokens. Except for informal Portuguese, we achieve
    near perfect formality level accuracy while at the same time offering high general
    translation quality.
  authors:
  - Parnia Bahar
  - Patrick Wilken
  - "Javier Iranzo-S\xE1nchez"
  - Mattia Di Gangi
  - Evgeny Matusov
  - "Zolt\xE1n T\xFCske"
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - IWSLT
  forum: ''
  id: IWSLT_29
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Speech Translation with Style: AppTek''s Submissions to the IWSLT Subtitling
    and Formality Tracks in 2023'
  tldr: AppTek participated in the subtitling and formality tracks of the IWSLT 2023
    evaluation. This paper describes the details of our subtitling pipeline - speech
    segmentation, speech recognition, punctuation prediction and inverse text normalization,
    text machine translation and direct speech-to-text tr
  track: The 20th International Conference on Spoken Language Translation
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This article describes the QUESPA team speech translation (ST) submissions
    for the Quechua to Spanish (QUE--SPA) track featured in the Evaluation Campaign
    of IWSLT 2023: low-resource and dialect speech translation. Two main submission
    types were supported in the campaign: constrained and unconstrained. We submitted
    six total systems of which our best (primary) constrained system consisted of
    an ST model based on the Fairseq S2T framework where the audio representations
    were created using log mel-scale filter banks as features and the translations
    were performed using a transformer. The best (primary) unconstrained system used
    a pipeline approach which combined automatic speech recognition (ASR) with machine
    translation (MT). The ASR transcriptions for the best unconstrained system were
    computed using a pre-trained XLS-R-based model along with a fine-tuned language
    model. Transcriptions were translated using a MT system based on a fine-tuned,
    pre-trained language model (PLM). The four other submissions are presented in
    this article (2 constrained and 2 unconstrained) for comparison because they consist
    of various architectures. Our results show that direct ST (ASR and MT combined
    together) can be more effective than a PLM in a low-resource (constrained) setting
    for Quechua to Spanish. On the other hand, we show that fine-tuning of any type
    on both the ASR and MT system is worthwhile, resulting in nearly 16 BLEU for the
    unconstrained task.'
  authors:
  - John E. Ortega
  - Rodolfo Zevallos
  - William Chen
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - IWSLT
  forum: ''
  id: IWSLT_30
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: QUESPA Submission for the IWSLT 2023 Dialect and Low-resource Speech Translation
    Tasks
  tldr: 'This article describes the QUESPA team speech translation (ST) submissions
    for the Quechua to Spanish (QUE--SPA) track featured in the Evaluation Campaign
    of IWSLT 2023: low-resource and dialect speech translation. Two main submission
    types were supported in the campaign: constrained and unconstrain'
  track: The 20th International Conference on Spoken Language Translation
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes the GMU Systems for the IWSLT 2023 Dialect and Low-resource
    Speech Translation Tasks. We submitted systems for five low-resource tasks and
    the dialectal task. In this work, we explored self-supervised pre-trained speech
    models and finetuned them on speech translation downstream tasks. We use the Wav2vec
    2.0, XLSR-53, and Hubert as self-supervised models. Unlike Hubert, Wav2vec 2.0
    and XLSR-53 achieve the best results when we remove the top three layers. Our
    results show that Wav2vec 2.0 and Hubert perform similarly with their relative
    best configuration. In addition, we found that Wav2vec 2.0 pre-trained on audio
    data of the same language as the source language of a speech translation model
    achieves better results. For the low-resource setting, the best results are achieved
    using either the Wav2vec 2.0 or Hubert models, while XLSR-53 achieves the best
    results for the dialectal transfer task. We find that XLSR-53 does not perform
    well for low-resource tasks. Using Wav2vec 2.0, we report close to~2 BLEU point
    improvements on the test set for the Tamasheq-French compared to the baseline
    system at the IWSLT 2022.
  authors:
  - Jonathan Mbuya
  - Antonios Anastasopoulos
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - IWSLT
  forum: ''
  id: IWSLT_31
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: GMU Systems for the IWSLT 2023 Dialect and Low-resource Speech Translation
    Tasks
  tldr: 'This paper describes the GMU Systems for the IWSLT 2023 Dialect and Low-resource
    Speech Translation Tasks. We submitted systems for five low-resource tasks and
    the dialectal task. In this work, we explored self-supervised pre-trained speech
    models and finetuned them on speech translation downstream '
  track: The 20th International Conference on Spoken Language Translation
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes our work on the IWSLT2023 Speech-to-Speech task.
    Our proposed cascaded system consists of an ensemble of Conformer and S2T-Transformer-based
    ASR models, a Transformer-based MT model, and a Diffusion-based TTS model. Our
    primary focus in this competition was to investigate the modeling ability of the
    Diffusion model for TTS tasks in high-resource scenarios and the role of TTS in
    the overall S2S task. To this end, we proposed DTS, an end-to-end diffusion-based
    TTS model that takes raw text as input and generates waveform by iteratively denoising
    on pure Gaussian noise. Compared to previous TTS models, the speech generated
    by DTS is more natural and performs better in code-switching scenarios. As the
    training process is end-to-end, it is relatively straightforward. Our experiments
    demonstrate that DTS outperforms other TTS models on the GigaS2S benchmark, and
    also brings positive gains for the entire S2S system.
  authors:
  - Minghan Wang
  - Yinglu Li
  - Jiaxin GUO
  - Zongyao Li
  - Hengchao Shang
  - Daimeng Wei
  - Min Zhang
  - Shimin Tao
  - Hao Yang
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - IWSLT
  forum: ''
  id: IWSLT_32
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: The HW-TSC's Speech-to-Speech Translation System for IWSLT 2023
  tldr: This paper describes our work on the IWSLT2023 Speech-to-Speech task. Our
    proposed cascaded system consists of an ensemble of Conformer and S2T-Transformer-based
    ASR models, a Transformer-based MT model, and a Diffusion-based TTS model. Our
    primary focus in this competition was to investigate the mo
  track: The 20th International Conference on Spoken Language Translation
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper presents JHU's submissions to the IWSLT 2023 dialectal and
    low-resource track of Tunisian Arabic to English speech translation. The Tunisian
    dialect lacks formal orthography and abundant training data, making it challenging
    to develop effective speech translation (ST) systems. To address these challenges,
    we explore the integration of large pre-trained machine translation (MT) models,
    such as mBART and NLLB-200 in both end-to-end (E2E) and cascaded speech translation
    (ST) systems. We also improve the performance of automatic speech recognition
    (ASR) through the use of pseudo-labeling data augmentation and channel matching
    on telephone data. Finally, we combine our E2E and cascaded ST systems with Minimum
    Bayes-Risk decoding. Our combined system achieves a BLEU score of 21.6 and 19.1
    on test2 and test3, respectively.
  authors:
  - Amir Hussein
  - Cihan Xiao
  - Neha Verma
  - Thomas Thebaud
  - Matthew Wiesner
  - Sanjeev Khudanpur
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - IWSLT
  forum: ''
  id: IWSLT_33
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: JHU IWSLT 2023 Dialect Speech Translation System Description
  tldr: 'This paper presents JHU''s submissions to the IWSLT 2023 dialectal and low-resource
    track of Tunisian Arabic to English speech translation. The Tunisian dialect lacks
    formal orthography and abundant training data, making it challenging to develop
    effective speech translation (ST) systems. To address '
  track: The 20th International Conference on Spoken Language Translation
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Multilingual neural translation models exploit cross-lingual transfer
    to perform zero-shot translation between unseen language pairs. Past efforts to
    improve cross-lingual transfer have focused on aligning contextual sentence-level
    representations. This paper introduces three novel contributions to allow exploiting
    nearest neighbours at the token level during training, including: (i) an efficient,
    gradient-friendly way to share representations between neighboring tokens; (ii)
    an attentional semantic layer which extracts latent features from shared embeddings;
    and (iii) an agreement loss to harmonize predictions across different sentence
    representations. Experiments on two multilingual datasets demonstrate consistent
    gains in zero shot translation over strong baselines.'
  authors:
  - Nishant Kambhatla
  - Logan Born
  - Anoop Sarkar
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - IWSLT
  forum: ''
  id: IWSLT_34
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Learning Nearest Neighbour Informed Latent Word Embeddings to Improve Zero-Shot
    Machine Translation
  tldr: 'Multilingual neural translation models exploit cross-lingual transfer to
    perform zero-shot translation between unseen language pairs. Past efforts to improve
    cross-lingual transfer have focused on aligning contextual sentence-level representations.
    This paper introduces three novel contributions to '
  track: The 20th International Conference on Spoken Language Translation
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We describe the Johns Hopkins ACL 60-60 Speech Translation systems submitted
    to the IWSLT 2023 Multilingual track, where we were tasked to translate ACL presentations
    from English into 10 languages. We developed cascaded speech translation systems
    for both the constrained and unconstrained subtracks. Our systems make use of
    pre-trained models as well as domain-specific corpora for this highly technical
    evaluation-only task. We find that the specific technical domain which ACL presentations
    fall into presents a unique challenge for both ASR and MT, and we present an error
    analysis and an ACL-specific corpus we produced to enable further work in this
    area.
  authors:
  - Henry Li Xinyuan
  - Neha Verma
  - Bismarck Bamfo Odoom
  - Ujvala Pradeep
  - Matthew Wiesner
  - Sanjeev Khudanpur
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - IWSLT
  forum: ''
  id: IWSLT_35
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: JHU IWSLT 2023 Multilingual Speech Translation System Description
  tldr: We describe the Johns Hopkins ACL 60-60 Speech Translation systems submitted
    to the IWSLT 2023 Multilingual track, where we were tasked to translate ACL presentations
    from English into 10 languages. We developed cascaded speech translation systems
    for both the constrained and unconstrained subtracks
  track: The 20th International Conference on Spoken Language Translation
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes the NPU-MSXF system for the IWSLT 2023 speech-to-speech
    translation (S2ST) task which aims to translate from English speech of multi-source
    to Chinese speech. The system is built in a cascaded manner consisting of automatic
    speech recognition (ASR), machine translation (MT), and text-to-speech (TTS).
    We make tremendous efforts to handle the challenging multi-source input. Specifically,
    to improve the robustness to multi-source speech input, we adopt various data
    augmentation strategies and a ROVER-based score fusion on multiple ASR model outputs.
    To better handle the noisy ASR transcripts, we introduce a three-stage fine-tuning
    strategy to improve translation accuracy. Finally, we build a TTS model with high
    naturalness and sound quality, which leverages a two-stage framework, using network
    bottleneck features as a robust intermediate representation for speaker timbre
    and linguistic content disentanglement. Based on the two-stage framework, pre-trained
    speaker embedding is leveraged as a condition to transfer the speaker timbre in
    the source English speech to the translated Chinese speech. Experimental results
    show that our system has high translation accuracy, speech naturalness, sound
    quality, and speaker similarity. Moreover, it shows good robustness to multi-source
    data.
  authors:
  - Kun Song
  - Yi Lei
  - Peikun Chen
  - Yiqing Cao
  - Kun Wei
  - Yongmao Zhang
  - Lei Xie
  - Ning Jiang
  - Guoqing Zhao
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - IWSLT
  forum: ''
  id: IWSLT_36
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: The NPU-MSXF Speech-to-Speech Translation System for IWSLT 2023 Speech-to-Speech
    Translation Task
  tldr: This paper describes the NPU-MSXF system for the IWSLT 2023 speech-to-speech
    translation (S2ST) task which aims to translate from English speech of multi-source
    to Chinese speech. The system is built in a cascaded manner consisting of automatic
    speech recognition (ASR), machine translation (MT), and
  track: The 20th International Conference on Spoken Language Translation
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes the UCSC's submission to the shared task on formality
    control for spoken language translation at IWSLT 2023. For this task, we explored
    the use of 'additive style intervention' using a pre-trained multilingual translation
    model, namely mBART. Compared to prior approaches where a single style-vector
    was added to all tokens in the encoder output, we explored an alternative approach
    in which we learn a unique style-vector for each input token. We believe this
    approach, which we call 'style embedding intervention,' is better suited for formality
    control as it can potentially learn which specific input tokens to modify during
    decoding. While the proposed approach obtained similar performance to 'additive
    style intervention' for the supervised English-to-Vietnamese task, it performed
    significantly better for English-to-Korean, in which it achieved an average matched
    accuracy of 90.6 compared to 85.2 for the baseline. When we constrained the model
    further to only perform style intervention on the <bos> (beginning of sentence)
    token, the average matched accuracy improved further to 92.0, indicating that
    the model could learn to control the formality of the translation output based
    solely on the embedding of the <bos> token.
  authors:
  - Priyesh Vakharia
  - Shree Vignesh S
  - Pranjali Basmatkar
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - IWSLT
  forum: ''
  id: IWSLT_37
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Low-Resource Formality Controlled NMT Using Pre-trained LM
  tldr: This paper describes the UCSC's submission to the shared task on formality
    control for spoken language translation at IWSLT 2023. For this task, we explored
    the use of 'additive style intervention' using a pre-trained multilingual translation
    model, namely mBART. Compared to prior approaches where a
  track: The 20th International Conference on Spoken Language Translation
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper describes NAIST''s submission to the IWSLT 2023 Simultaneous
    Speech Translation task: English-to-{German, Japanese, Chinese} speech-to-text
    translation and English-to-Japanese speech-to-speech translation. Our speech-to-text
    system uses an end-to-end multilingual speech translation model based on large-scale
    pre-trained speech and text models. We add Inter-connections into the model to
    incorporate the outputs from intermediate layers of the pre-trained speech model
    and augment prefix-to-prefix text data using Bilingual Prefix Alignment to enhance
    the simultaneity of the offline speech translation model. Our speech-to-speech
    system employs an incremental text-to-speech module that consists of a Japanese
    pronunciation estimation model, an acoustic model, and a neural vocoder.'
  authors:
  - Ryo Fukuda
  - Yuta Nishikawa
  - Yasumasa Kano
  - Yuka Ko
  - Tomoya Yanagita
  - Kosuke Doi
  - Mana Makinae
  - Sakriani Sakti
  - Katsuhito Sudoh
  - Satoshi Nakamura
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - IWSLT
  forum: ''
  id: IWSLT_38
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: NAIST Simultaneous Speech-to-speech Translation System for IWSLT 2023
  tldr: 'This paper describes NAIST''s submission to the IWSLT 2023 Simultaneous Speech
    Translation task: English-to-{German, Japanese, Chinese} speech-to-text translation
    and English-to-Japanese speech-to-speech translation. Our speech-to-text system
    uses an end-to-end multilingual speech translation model b'
  track: The 20th International Conference on Spoken Language Translation
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The decoder in simultaneous neural machine translation receives limited
    information from the source while having to balance the opposing requirements
    of latency versus translation quality. In this paper, we use an auxiliary target-side
    language model to augment the training of the decoder model. Under this notion
    of target adaptive training, generating rare or difficult tokens is rewarded which
    improves the translation quality while reducing latency. The predictions made
    by a language model in the decoder are combined with the traditional cross entropy
    loss which frees up the focus on the source side context. Our experimental results
    over multiple language pairs show that compared to previous state of the art methods
    in simultaneous translation, we can use an augmented target side context to improve
    BLEU scores significantly. We show improvements over the state of the art in the
    low latency range with lower average lagging values (faster output).
  authors:
  - Aditi Jain
  - Nishant Kambhatla
  - Anoop Sarkar
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - IWSLT
  forum: ''
  id: IWSLT_39
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Language Model Based Target Token Importance Rescaling for Simultaneous Neural
    Machine Translation
  tldr: The decoder in simultaneous neural machine translation receives limited information
    from the source while having to balance the opposing requirements of latency versus
    translation quality. In this paper, we use an auxiliary target-side language model
    to augment the training of the decoder model. Und
  track: The 20th International Conference on Spoken Language Translation
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes the Kyoto speech-to-speech translation system for
    IWSLT 2023. Our system is a combination of speech-to-text translation and text-to-speech
    synthesis. For the speech-to-text translation model, we used the dual-decoderTransformer
    model. For text-to-speech synthesis model, we took a cascade approach of an acoustic
    model and a vocoder.
  authors:
  - Zhengdong Yang
  - Shuichiro Shimizu
  - Wangjin Zhou
  - Sheng Li
  - Chenhui Chu
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - IWSLT
  forum: ''
  id: IWSLT_40
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: The Kyoto Speech-to-Speech Translation System for IWSLT 2023
  tldr: 'This paper describes the Kyoto speech-to-speech translation system for IWSLT
    2023. Our system is a combination of speech-to-text translation and text-to-speech
    synthesis. For the speech-to-text translation model, we used the dual-decoderTransformer
    model. For text-to-speech synthesis model, we took '
  track: The 20th International Conference on Spoken Language Translation
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Simultaneous speech translation (SimulST) translates partial speech inputs
    incrementally. Although the monotonic correspondence between input and output
    is preferable for smaller latency, it is not the case for distant language pairs
    such as English and Japanese. A prospective approach to this problem is to mimic
    simultaneous interpretation (SI) using SI data to train a SimulST model. However,
    the size of such SI data is limited, so the SI data should be used together with
    ordinary bilingual data whose translations are given in offline. In this paper,
    we propose an effective way to train a SimulST model using mixed data of SI and
    offline. The proposed method trains a single model using the mixed data with style
    tags that tell the model to generate SI- or offline-style outputs. Experiment
    results show improvements of BLEURT in different latency ranges, and our analyses
    revealed the proposed model generates SI-style outputs more than the baseline.
  authors:
  - Yuka Ko
  - Ryo Fukuda
  - Yuta Nishikawa
  - Yasumasa Kano
  - Katsuhito Sudoh
  - Satoshi Nakamura
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - IWSLT
  forum: ''
  id: IWSLT_41
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Tagged End-to-End Simultaneous Speech Translation Training Using Simultaneous
    Interpretation Data
  tldr: Simultaneous speech translation (SimulST) translates partial speech inputs
    incrementally. Although the monotonic correspondence between input and output
    is preferable for smaller latency, it is not the case for distant language pairs
    such as English and Japanese. A prospective approach to this probl
  track: The 20th International Conference on Spoken Language Translation
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'In this paper, we present our submission to the IWSLT 2023 Simultaneous
    Speech-to-Text Translation competition. Our participation involves three language
    directions: English-German, English-Chinese, and English-Japanese. Our proposed
    solution is a cascaded incremental decoding system that comprises an ASR model
    and an MT model. The ASR model is based on the U2++ architecture and can handle
    both streaming and offline speech scenarios with ease. Meanwhile, the MT model
    adopts the Deep-Transformer architecture. To improve performance, we explore methods
    to generate a confident partial target text output that guides the next MT incremental
    decoding process. In our experiments, we demonstrate that our simultaneous strategies
    achieve low latency while maintaining a loss of no more than 2 BLEU points when
    compared to offline systems.'
  authors:
  - Jiaxin GUO
  - Daimeng Wei
  - Zhanglin Wu
  - Zongyao Li
  - Zhiqiang Rao
  - Minghan Wang
  - Hengchao Shang
  - Xiaoyu Chen
  - Zhengzhe Yu
  - Shaojun Li
  - Yuhao Xie
  - Lizhi Lei
  - Hao Yang
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - IWSLT
  forum: ''
  id: IWSLT_42
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: The HW-TSC's Simultaneous Speech-to-Text Translation System for IWSLT 2023
    Evaluation
  tldr: 'In this paper, we present our submission to the IWSLT 2023 Simultaneous Speech-to-Text
    Translation competition. Our participation involves three language directions:
    English-German, English-Chinese, and English-Japanese. Our proposed solution is
    a cascaded incremental decoding system that comprises '
  track: The 20th International Conference on Spoken Language Translation
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'In this paper, we present our submission to the IWSLT 2023 Simultaneous
    Speech-to-Speech Translation competition. Our participation involves three language
    directions: English-German, English-Chinese, and English-Japanese. Our solution
    is a cascaded incremental decoding system, consisting of an ASR model, an MT model,
    and a TTS model. By adopting the strategies used in the Speech-to-Text track,
    we have managed to generate a more confident target text for each audio segment
    input, which can guide the next MT incremental decoding process. Additionally,
    we have integrated the TTS model to seamlessly reproduce audio files from the
    translation hypothesis. To enhance the effectiveness of our experiment, we have
    utilized a range of methods to reduce error conditions in the TTS input text and
    improve the smoothness of the TTS output audio.'
  authors:
  - Hengchao Shang
  - Zhiqiang Rao
  - Zongyao Li
  - Zhanglin Wu
  - Jiaxin GUO
  - Minghan Wang
  - Daimeng Wei
  - Shaojun Li
  - Zhengzhe YU
  - Xiaoyu Chen
  - Lizhi Lei
  - Hao Yang
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - IWSLT
  forum: ''
  id: IWSLT_43
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: The HW-TSC's Simultaneous Speech-to-Speech Translation System for IWSLT 2023
    Evaluation
  tldr: 'In this paper, we present our submission to the IWSLT 2023 Simultaneous Speech-to-Speech
    Translation competition. Our participation involves three language directions:
    English-German, English-Chinese, and English-Japanese. Our solution is a cascaded
    incremental decoding system, consisting of an ASR '
  track: The 20th International Conference on Spoken Language Translation
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In this paper, we describe our submission to the Simultaneous Track at
    IWSLT 2023. This year, we continue with the successful setup from the last year,
    however, we adopt the latest methods that further improve the translation quality.
    Additionally, we propose a novel online policy for attentional encoder-decoder
    models. The policy prevents the model to generate translation beyond the current
    speech input by using an auxiliary CTC output layer. We show that the proposed
    simultaneous policy can be applied to both streaming blockwise models and offline
    encoder-decoder models. We observe significant improvements in quality (up to
    1.1 BLEU) and the computational footprint (up to 45% relative RTF).
  authors:
  - Peter Polak
  - Danni Liu
  - Ngoc-Quan Pham
  - Jan Niehues
  - Alexander Waibel
  - "Ond\u0159ej Bojar"
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - IWSLT
  forum: ''
  id: IWSLT_44
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Towards Efficient Simultaneous Speech Translation: CUNI-KIT System for Simultaneous
    Track at IWSLT 2023'
  tldr: In this paper, we describe our submission to the Simultaneous Track at IWSLT
    2023. This year, we continue with the successful setup from the last year, however,
    we adopt the latest methods that further improve the translation quality. Additionally,
    we propose a novel online policy for attentional en
  track: The 20th International Conference on Spoken Language Translation
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes the submission of the UPC Machine Translation group
    to the IWSLT 2023 Offline Speech Translation task. Our Speech Translation systems
    utilize foundation models for speech (wav2vec 2.0) and text (mBART50). We incorporate
    a Siamese pretraining step of the speech and text encoders with CTC and Optimal
    Transport, to adapt the speech representations to the space of the text model,
    thus maximizing transfer learning from MT. After this pretraining, we fine-tune
    our system end-to-end on ST, with Cross Entropy and Knowledge Distillation. Apart
    from the available ST corpora, we create synthetic data with SegAugment to better
    adapt our models to the custom segmentations of the IWSLT test sets. Our best
    single model obtains 31.2 BLEU points on MuST-C tst-COMMON, 29.8 points on IWLST.tst2020
    and 33.4 points on the newly released IWSLT.ACLdev2023.
  authors:
  - Ioannis Tsiamas
  - "Gerard I. G\xE1llego"
  - Jose Fonollosa
  - "Marta R. Costa-juss\xE1"
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - IWSLT
  forum: ''
  id: IWSLT_45
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Speech Translation with Foundation Models and Optimal Transport: UPC at
    IWSLT23'
  tldr: 'This paper describes the submission of the UPC Machine Translation group
    to the IWSLT 2023 Offline Speech Translation task. Our Speech Translation systems
    utilize foundation models for speech (wav2vec 2.0) and text (mBART50). We incorporate
    a Siamese pretraining step of the speech and text encoders '
  track: The 20th International Conference on Spoken Language Translation
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This system description paper introduces the systems submitted by Xiaomi
    AI Lab to the three tracks of the IWSLT 2023 Evaluation Campaign, namely the offline
    speech translation (Offline-ST) track, the offline speech-to-speech translation
    (Offline-S2ST) track, and the simultaneous speech translation (Simul-ST) track.
    All our submissions for these three tracks only involve the English-Chinese language
    direction. Our English-Chinese speech translation systems are constructed using
    large-scale pre-trained models as the foundation. Specifically, we fine-tune these
    models' corresponding components for various downstream speech translation tasks.
    Moreover, we implement several popular techniques, such as data filtering, data
    augmentation, speech segmentation, and model ensemble, to improve the system's
    overall performance. Extensive experiments show that our systems achieve a significant
    improvement over the strong baseline systems in terms of the automatic evaluation
    metric.
  authors:
  - Wuwei Huang
  - Mengge Liu
  - Xiang Li
  - Yanzhi Tian
  - Fengyu Yang
  - Wen Zhang
  - Jian Luan
  - Bin Wang
  - Yuhang Guo
  - Jinsong Su
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - IWSLT
  forum: ''
  id: IWSLT_46
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: The Xiaomi AI Lab's Speech Translation Systems for IWSLT 2023 Offline Task,
    Simultaneous Task and Speech-to-Speech Task
  tldr: 'This system description paper introduces the systems submitted by Xiaomi
    AI Lab to the three tracks of the IWSLT 2023 Evaluation Campaign, namely the offline
    speech translation (Offline-ST) track, the offline speech-to-speech translation
    (Offline-S2ST) track, and the simultaneous speech translation '
  track: The 20th International Conference on Spoken Language Translation
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'In this paper, we present the KU x Upstage team''s submission for the
    Special Task on Formality Control on Spoken Language Translation, which involves
    translating English into four languages with diverse grammatical formality markers.  Our
    methodology comprises two primary components: 1) a language-specific data-driven
    approach, and 2) the generation of synthetic data through the employment of large-scale
    language models and empirically-grounded prompt engineering. By adapting methodologies
    and models to accommodate the unique linguistic properties of each language, we
    observe a notable enhancement in performance relative to the baseline, substantiating
    the heightened efficacy of data-driven approaches. Moreover, our devised prompt
    engineering strategy yields superior synthetic translation instances.'
  authors:
  - Seugnjun Lee
  - Hyeonseok Moon
  - Chanjun Park
  - Heuiseok Lim
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - IWSLT
  forum: ''
  id: IWSLT_47
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Improving Formality-Sensitive Machine Translation Using Data-Centric Approaches
    and Prompt Engineering
  tldr: 'In this paper, we present the KU x Upstage team''s submission for the Special
    Task on Formality Control on Spoken Language Translation, which involves translating
    English into four languages with diverse grammatical formality markers.  Our methodology
    comprises two primary components: 1) a language-s'
  track: The 20th International Conference on Spoken Language Translation
  underline_id: null
  underline_url: null
  video_url: null
- abstract: For the 2023 IWSLT Maltese Speech Translation Task, UM-DFKI jointly presents
    a cascade solution which achieves 0.6 BLEU. While this is the first time that
    a Maltese speech translation task has been released by IWSLT, this paper explores
    previous solutions for other speech translation tasks, focusing primarily on low-resource
    scenarios. Moreover, we present our method of fine-tuning XLS-R models for Maltese
    ASR using a collection of multi-lingual speech corpora as well as the fine-tuning
    of the mBART model for Maltese to English machine translation.
  authors:
  - Aiden Williams
  - Kurt Abela
  - Rishu Kumar
  - "Martin B\xE4r"
  - Hannah Billinghurst
  - Kurt Micallef
  - Ahnaf Mozib Samin
  - Andrea DeMarco
  - Lonneke van der Plas
  - Claudia Borg
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - IWSLT
  forum: ''
  id: IWSLT_48
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: UM-DFKI Maltese Speech Translation
  tldr: For the 2023 IWSLT Maltese Speech Translation Task, UM-DFKI jointly presents
    a cascade solution which achieves 0.6 BLEU. While this is the first time that
    a Maltese speech translation task has been released by IWSLT, this paper explores
    previous solutions for other speech translation tasks, focusing
  track: The 20th International Conference on Spoken Language Translation
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper provides an overview of NVIDIA NeMo's speech translation systems
    for the IWSLT 2023 Offline Speech Translation Task. This year, we focused on end-to-end
    system which capitalizes on pre-trained models and synthetic data to mitigate
    the problem of direct speech translation data scarcity. When trained on IWSLT
    2022 constrained data, our best En->De end-to-end model achieves the average score
    of 31 BLEU on 7 test sets from IWSLT 2010-2020 which improves over our last year
    cascade (28.4) and end-to-end (25.7) submissions. When trained on IWSLT 2023 constrained
    data, the average score drops to 29.5 BLEU.
  authors:
  - Oleksii Hrinchuk
  - Vladimir Bataev
  - Evelina Bakhturina
  - Boris Ginsburg
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - IWSLT
  forum: ''
  id: IWSLT_50
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: NVIDIA NeMo Offline Speech Translation Systems for IWSLT 2023
  tldr: This paper provides an overview of NVIDIA NeMo's speech translation systems
    for the IWSLT 2023 Offline Speech Translation Task. This year, we focused on end-to-end
    system which capitalizes on pre-trained models and synthetic data to mitigate
    the problem of direct speech translation data scarcity. Wh
  track: The 20th International Conference on Spoken Language Translation
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper describes the speech translation systems SRI-B developed for
    the IWSLT 2023 Evaluation Campaign Dialectal and Low-resource track: Marathi-Hindi
    Speech Translation. We propose systems for both the constrained (systems are trained
    only on the datasets provided by the organizers) and the unconstrained conditions
    (systems can be trained with any resource). For both the conditions, we build
    end-to-end speech translation networks comprising of a conformer encoder and a
    transformer decoder. Under both the conditions, we leverage Marathi Automatic
    Speech Recognition (ASR) data to pre-train the encoder and subsequently train
    the entire model on the speech translation data. Our results demonstrate that
    pre-training the encoder with ASR data is a key step in significantly improving
    the speech translation performance. We also show that conformer encoders are inherently
    superior to its transformer counterparts for speech translation tasks. Our primary
    submissions achieved a BLEU% score of 31.2 on the constrained condition and 32.4
    on the unconstrained condition. We secured the top position in the constrained
    condition and second position in the unconstrained condition.'
  authors:
  - Balaji Radhakrishnan
  - Saurabh Agrawal
  - Raj Prakash Gohil
  - Kiran Praveen
  - Advait Vinay Dhopeshwarkar
  - Abhishek Pandey
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - IWSLT
  forum: ''
  id: IWSLT_51
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'SRI-B''s Systems for IWSLT 2023 Dialectal and Low-resource Track: Marathi-Hindi
    Speech Translation'
  tldr: 'This paper describes the speech translation systems SRI-B developed for the
    IWSLT 2023 Evaluation Campaign Dialectal and Low-resource track: Marathi-Hindi
    Speech Translation. We propose systems for both the constrained (systems are trained
    only on the datasets provided by the organizers) and the unc'
  track: The 20th International Conference on Spoken Language Translation
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes the system we submitted to the IWSLT 2023 multilingual
    speech translation track, with input being English speech and output being text
    in 10 target languages. Our system consists of CNN and Transformer, convolutional
    neural networks downsample speech features and extract local information, while
    transformer extract global features and output the final results. In our system,
    we use speech recognition tasks to pre-train encoder parameters, and then use
    speech translation corpus to train the multilingual speech translation model.
    We have also adopted other methods to optimize the model, such as data augmentation,
    model ensemble, etc. Our system can obtain satisfactory results on test sets of
    10 languages in the MUST-C corpus.
  authors:
  - Zhipeng Wang
  - Yuhang Guo
  - Shuoying Chen
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - IWSLT
  forum: ''
  id: IWSLT_52
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: BIT's System for Multilingual Track
  tldr: This paper describes the system we submitted to the IWSLT 2023 multilingual
    speech translation track, with input being English speech and output being text
    in 10 target languages. Our system consists of CNN and Transformer, convolutional
    neural networks downsample speech features and extract local i
  track: The 20th International Conference on Spoken Language Translation
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper briefly describes Matesub, the subtitling tool Translated used
    to participate in the Subtitling shared task at IWSLT 2023. Matesub is a professional
    web-based tool that combines state-of-the-art AI with a WYSIWYG editor. The automatic
    generation of subtitles in Matesub is based on a cascade architecture, composed
    of ASR, text segmenter and MT neural models, which allows covering any pair from
    about 70 languages and their variants.
  authors:
  - Simone Perone
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - IWSLT
  forum: ''
  id: IWSLT_53
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Matesub: The Translated Subtitling Tool at the IWSLT2023 Subtitling Task'
  tldr: This paper briefly describes Matesub, the subtitling tool Translated used
    to participate in the Subtitling shared task at IWSLT 2023. Matesub is a professional
    web-based tool that combines state-of-the-art AI with a WYSIWYG editor. The automatic
    generation of subtitles in Matesub is based on a casca
  track: The 20th International Conference on Spoken Language Translation
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Generative Spoken Language Modeling research focuses on optimizing speech
    Language Models (LMs) using raw audio recordings without accessing any textual
    supervision. Such speech LMs usually operate over discrete units obtained from
    quantizing internal representations of self-supervised models. Although such units
    show impressive modeling results, their robustness capabilities have not been
    extensively investigated. This work focuses on improving the robustness of discrete
    input representations for generative spoken language modeling. First, we formally
    define how to measure the robustness of such representations to various signal
    variations that do not alter the spoken information (e.g., time-stretch). Next,
    we empirically demonstrate how current state-of-the-art representation models
    lack robustness to such variations. To overcome this, we propose an effective
    and efficient method to learn robust discrete speech representation for generative
    spoken language modeling. The proposed approach is based on applying a set of
    signal transformations to the speech signal and optimizing the model using an
    iterative pseudo-labeling scheme. Our method significantly improves over the evaluated
    baselines when considering encoding and modeling metrics. We additionally evaluate
    our method on the speech-to-speech translation task, considering Spanish-English
    and French-English translations, and show the proposed approach outperforms the
    evaluated baselines.
  authors:
  - Itai Gat
  - Felix Kreuk
  - Tu Anh Nguyen
  - Ann Lee
  - Jade Copet
  - Gabriel Synnaeve
  - Emmanuel Dupoux
  - Yossi Adi
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - IWSLT
  forum: ''
  id: IWSLT_54
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Augmentation Invariant Discrete Representation for Generative Spoken Language
    Modeling
  tldr: Generative Spoken Language Modeling research focuses on optimizing speech
    Language Models (LMs) using raw audio recordings without accessing any textual
    supervision. Such speech LMs usually operate over discrete units obtained from
    quantizing internal representations of self-supervised models. Altho
  track: The 20th International Conference on Spoken Language Translation
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Non-autoregressive machine translation (NAT) models have lower translation
    quality than autoregressive translation (AT) models because NAT decoders do not
    depend on previous target tokens in the decoder input. We propose a novel and
    general Dependency-Aware Decoder (DePA) to enhance target dependency modeling
    in the decoder of fully NAT models from two perspectives: decoder self-attention
    and decoder input. First, we propose an autoregressive forward-backward pre-training
    phase before NAT training, which enables the NAT decoder to gradually learn bidirectional
    target dependencies for the final NAT training. Second, we transform the decoder
    input from the source language representation space to the target language representation
    space through a novel attentive transformation process, which enables the decoder
    to better capture target dependencies. DePA can be applied to any fully NAT models.
    Extensive experiments show that DePA consistently improves highly competitive
    and state-of-the-art fully NAT models on widely used WMT and IWSLT benchmarks
    by up to 1.88 BLEU gain, while maintaining the inference latency comparable to
    other fully NAT models.'
  authors:
  - Jiaao Zhan
  - Qian Chen
  - Boxing Chen
  - Wen Wang
  - Yu Bai
  - Yang Gao
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - IWSLT
  forum: ''
  id: IWSLT_55
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'DePA: Improving Non-autoregressive Translation with Dependency-Aware Decoder'
  tldr: Non-autoregressive machine translation (NAT) models have lower translation
    quality than autoregressive translation (AT) models because NAT decoders do not
    depend on previous target tokens in the decoder input. We propose a novel and
    general Dependency-Aware Decoder (DePA) to enhance target dependenc
  track: The 20th International Conference on Spoken Language Translation
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Although unsupervised neural machine translation (UNMT) has achieved success
    in many language pairs, the copying problem, i.e., directly copying some parts
    of the input sentence as the translation, is common among distant language pairs,
    especially when low-resource languages are involved. We find this issue is closely
    related to an unexpected copying behavior during online back-translation (BT).
    In this work, we propose a simple but effective training schedule that incorporates
    a language discriminator loss. The loss imposes constraints on the intermediate
    translation so that the translation is in the desired language. By conducting
    extensive experiments on different language pairs, including similar and distant,
    high and low-resource languages, we find that our method alleviates the copying
    problem, thus improving the translation performance on low-resource languages.
  authors:
  - Yihong Liu
  - Alexandra Chronopoulou
  - "Hinrich Sch\xFCtze"
  - Alexander Fraser
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - IWSLT
  forum: ''
  id: IWSLT_56
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'On the Copying Problem of Unsupervised NMT: A Training Schedule with a Language
    Discriminator Loss'
  tldr: Although unsupervised neural machine translation (UNMT) has achieved success
    in many language pairs, the copying problem, i.e., directly copying some parts
    of the input sentence as the translation, is common among distant language pairs,
    especially when low-resource languages are involved. We find t
  track: The 20th International Conference on Spoken Language Translation
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'In this paper, we present our system for the textual entailment identification
    task as a subtask of the SemEval-2023 Task 7: Multi-evidence Natural Language
    Inference for Clinical Trial Data.The entailment identification task aims to determine
    whether a medical statement affirms a valid entailment given a clinical trial
    premise or forms a contradiction with it.Since the task is inherently a text classification
    task, we propose a system that performs binary classification given a statement
    and its associated clinical trial.Our proposed system leverages a human-defined
    prompt to aggregate the information contained in the statement, section name,
    and clinical trials.Pre-trained language models are then finetuned on the prompted
    input sentences to learn to discriminate the inference relation between the statement
    and clinical trial.To validate our system, we conduct extensive experiments with
    a wide variety of pre-trained language models.Our best system is built on DeBERTa-v3-large,
    which achieves an F1 score of 0.764 and secures the fifth rank in the official
    leaderboard.Further analysis indicates that leveraging our designed prompt is
    effective, and our model suffers from a low recall.Our code and pre-trained models
    are available at [https://github.com/HKUST-KnowComp/NLI4CT](https://github.com/HKUST-KnowComp/NLI4CT).'
  authors:
  - Weiqi Wang
  - Baixuan Xu
  - Tianqing Fang
  - Lirong Zhang
  - Yangqiu Song
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_1
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 7: Multi-Evidence Natural Language Inference for Clinical Trial
    Data'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'KnowComp at SemEval-2023 Task 7: Fine-tuning Pre-trained Language Models
    for Clinical Trial Entailment Identification'
  tldr: 'In this paper, we present our system for the textual entailment identification
    task as a subtask of the SemEval-2023 Task 7: Multi-evidence Natural Language
    Inference for Clinical Trial Data.The entailment identification task aims to determine
    whether a medical statement affirms a valid entailment g'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Clinical Trials Reports (CTRs) contain highly valuable health information
    from which Natural Language Inference (NLI) techniques determine if a given hypothesis
    can be inferred from a given premise. CTRs are abundant with domain terminology
    with particular terms that are difficult to understand without prior knowledge.
    Thus, we proposed to use domain ontologies as a source of external knowledge that
    could help with the inference process in theSemEval-2023 Task 7: Multi-evidence
    Natural Language Inference for Clinical Trial Data (NLI4CT). This document describes
    our participation in subtask 1: Textual Entailment, where Ontologies, NLP techniques,
    such as tokenization and named-entity recognition, and rule-based approaches are
    all combined in our approach. We were able to show that inputting annotations
    from domain ontologies improved the baseline systems.'
  authors:
  - "Sofia I. R. Concei\xE7\xE3o"
  - Diana F. Sousa
  - Pedro Silvestre
  - Francisco M Couto
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_4
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 7: Multi-Evidence Natural Language Inference for Clinical Trial
    Data'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'lasigeBioTM at SemEval-2023 Task 7: Improving Natural Language Inference
    Baseline Systems with Domain Ontologies'
  tldr: Clinical Trials Reports (CTRs) contain highly valuable health information
    from which Natural Language Inference (NLI) techniques determine if a given hypothesis
    can be inferred from a given premise. CTRs are abundant with domain terminology
    with particular terms that are difficult to understand with
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'In SemEval-2023 Task 1, a task of applying Word Sense Disambiguation
    in an image retrieval system was introduced. To resolve this task, this work proposes
    three approaches: (1) an unsupervised approach considering similarities between
    word senses and image captions, (2) a supervised approach using a Siamese neural
    network, and (3) a self-supervised approach using a Bayesian personalized ranking
    framework. According to the results, both supervised and self-supervised approaches
    outperformed the unsupervised approach. They can effectively identify correct
    images of ambiguous words in the dataset provided in this task.'
  authors:
  - Thanet Markchom
  - Huizhi Liang
  - Joyce Gitau
  - Zehao Liu
  - Varun Ojha
  - Lee Taylor
  - Jake Bonnici
  - Abdullah Alshadadi
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_5
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Task-1 - Visual Word Sense Disambiguation (Visual-WSD)
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'UoR-NCL at SemEval-2023 Task 1: Learning Word-Sense and Image Embeddings
    for Word Sense Disambiguation'
  tldr: 'In SemEval-2023 Task 1, a task of applying Word Sense Disambiguation in an
    image retrieval system was introduced. To resolve this task, this work proposes
    three approaches: (1) an unsupervised approach considering similarities between
    word senses and image captions, (2) a supervised approach using a'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper presents our work on the SemEval-2023 Task 10 Explainable
    Detection of Online Sexism (EDOS) using lexicon-based models. Our approach consists
    of three main steps: lexicon construction based on Pointwise Mutual Information
    (PMI) and Shapley value, lexicon augmentation using an unannotated corpus and
    Large Language Models (LLMs), and, lastly, lexical incorporation for Bag-of-Word
    (BoW) logistic regression and fine-tuning LLMs. Our results demonstrate that our
    Shapley approach effectively produces a high-quality lexicon. We also show that
    by simply counting the presence of certain words in our lexicons and comparing
    the count can outperform a BoW logistic regression in task B/C and fine-tuning
    BERT in task C. In the end, our classifier achieved F1-scores of 53.34\textbackslash{}\%
    and 27.31\textbackslash{}\% on the official blind test sets for tasks B and C,
    respectively. We, additionally, provide in-depth analysis highlighting model limitation
    and bias. We also present our attempts to understand the model''s behaviour based
    on our constructed lexicons. Our code and the resulting lexicons are open-sourced
    in our GitHub repository https://github.com/SirBadr/SemEval2022-Task10.'
  authors:
  - Pakawat Nakwijit
  - Mahmoud Samir
  - Matthew Purver
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_6
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 10: Towards Explainable Detection of Online Sexism'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Lexicools at SemEval-2023 Task 10: Sexism Lexicon Construction via XAI'
  tldr: 'This paper presents our work on the SemEval-2023 Task 10 Explainable Detection
    of Online Sexism (EDOS) using lexicon-based models. Our approach consists of three
    main steps: lexicon construction based on Pointwise Mutual Information (PMI) and
    Shapley value, lexicon augmentation using an unannotated '
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes our zero-shot approachesfor the Visual Word Sense
    Disambiguation(VWSD) Task in English. Our preliminarystudy shows that the simple
    approach of match-ing candidate images with the phrase usingCLIP suffers from
    the many-to-many natureof image-text pairs. We find that the CLIP textencoder
    may have limited abilities in captur-ing the compositionality in natural language.Conversely,
    the descriptive focus of the phrasevaries from instance to instance. We addressthese
    issues in our two systems, Augment-CLIPand Stable Diffusion Sampling (SD Sampling).Augment-CLIP
    augments the text prompt bygenerating sentences that contain the contextphrase
    with the help of large language mod-els (LLMs). We further explore CLIP modelsin
    other languages, as the an ambiguous wordmay be translated into an unambiguous
    one inthe other language. SD Sampling uses text-to-image Stable Diffusion to generate
    multipleimages from the given phrase, increasing thelikelihood that a subset of
    images match theone that paired with the text.
  authors:
  - Jie Li
  - Yow-Ting Shiue
  - Yong-Siang Shih
  - Jonas Geiping
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_7
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Task-1 - Visual Word Sense Disambiguation (Visual-WSD)
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Augmenters at SemEval-2023 Task 1: Enhancing CLIP in Handling Compositionality
    and Ambiguity for Zero-Shot Visual WSD through Prompt Augmentation and Text-To-Image
    Diffusion'
  tldr: This paper describes our zero-shot approachesfor the Visual Word Sense Disambiguation(VWSD)
    Task in English. Our preliminarystudy shows that the simple approach of match-ing
    candidate images with the phrase usingCLIP suffers from the many-to-many natureof
    image-text pairs. We find that the CLIP text
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'We present the findings of SemEval-2023 Task 12, a shared task on sentiment
    analysis for low-resource African languages using Twitter dataset. The task featured
    three subtasks; subtask A is monolingual sentiment classification with 12 tracks
    which are all monolingual languages, subtask B is multilingual sentiment classification
    using the tracks in subtask A and subtask C is a zero-shot sentiment classification.
    We present the results and findings of subtask A, subtask B and subtask C. We
    also release the code on github. Our goal is to leverage low-resource tweet data
    using pre-trained Afro-xlmr-large, AfriBERTa-Large, Bert-base-arabic-camelbert-da-sentiment
    (Arabic-camelbert), Multilingual-BERT (mBERT) and BERT models for sentiment analysis
    of 14 African languages. The datasets for these subtasks consists of a gold standard
    multi-class labeled Twitter datasets from these languages. Our results demonstrate
    that Afro-xlmr-large model performed better compared to the other models in most
    of the languages datasets. Similarly, Nigerian languages: Hausa, Igbo, and Yoruba
    achieved better performance compared to other languages and this can be attributed
    to the higher volume of data present in the languages.'
  authors:
  - Saheed Abdullahi Salahudeen
  - Falalu Ibrahim Lawan
  - Ahmad Wali
  - Amina Abubakar Imam
  - Aliyu Rabiu Shuaibu
  - Aliyu Yusuf
  - Nur Bala Rabiu
  - Musa Bello
  - Shamsuddeen Umaru Adamu
  - Saminu Mohammad Aliyu
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_8
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 12: AfriSenti-SemEval: Sentiment Analysis for Low-resource African
    Languages using Twitter Dataset'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'HausaNLP at SemEval-2023 Task 12: Leveraging African Low Resource TweetData
    for Sentiment Analysis'
  tldr: We present the findings of SemEval-2023 Task 12, a shared task on sentiment
    analysis for low-resource African languages using Twitter dataset. The task featured
    three subtasks; subtask A is monolingual sentiment classification with 12 tracks
    which are all monolingual languages, subtask B is multilin
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The na\textbackslash{}"\{i\}ve approach for fine-tuning pretrained deep
    learning models on downstream tasks involves feeding them mini-batches of randomly
    sampled data.  In this paper, we propose a more elaborate method for fine-tuning
    Pretrained Multilingual Transformers (PMTs) on multilingual data. Inspired by
    the success of curriculum learning approaches, we investigate the significance
    of fine-tuning PMTs on multilingual data in a sequential fashion language by language.
    Unlike the curriculum learning paradigm where the model is presented with increasingly
    complex examples, we do not adopt a notion of ``easy'' and ``hard'' samples. Instead,
    our experiments draw insight from psychological findings on how the human brain
    processes new information and the persistence of newly learned concepts. We perform
    our experiments on a challenging news-framing dataset that contains texts in six
    languages. Our proposed method outperforms the na\textbackslash{}"\{i\}ve approach
    by achieving improvements of \$\textbackslash{}mathbf\{2.57\textbackslash{}\%\}\$
    in terms of F1 score. Even when we supplement the na\textbackslash{}"\{i\}ve approach
    with recency fine-tuning, we still achieve an improvement of \$\textbackslash{}mathbf\{1.34\textbackslash{}\%\}\$
    with a \$\textbackslash{}mathbf\{3.63\textbackslash{}\%\}\$ convergence speed-up.
    Moreover, we are the first to observe an interesting pattern in which deep learning
    models exhibit a human-like \textbackslash{}textit\{primacy-recency effect\}.
  authors:
  - Tarek Mahmoud
  - Preslav Nakov
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_9
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 3: Detecting the Category, the Framing, and the Persuasion Techniques
    in Online News in a Multi-lingual Setup'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'BERTastic at SemEval-2023 Task 3: Fine-Tuning Pretrained Multilingual Transformers  Does
    Order Matter?'
  tldr: The na\textbackslash{}"\{i\}ve approach for fine-tuning pretrained deep learning
    models on downstream tasks involves feeding them mini-batches of randomly sampled
    data.  In this paper, we propose a more elaborate method for fine-tuning Pretrained
    Multilingual Transformers (PMTs) on multilingual data
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'The task of clickbait spoiling is: generating a short text that satisfies
    the curiosity induced by a clickbait post. Clickbait links to a web page and advertises
    its contents by arousing curiosity instead of providing an informative summary.
    Previous studies on clickbait spoiling has shown the approach that classifing
    the type of spoilers is needed, then generating the appropriate spoilers is more
    effective on the Webis Clickbait Spoiling Corpus 2022 dataset. Our contribution
    focused on study of the three classes (phrase, passage and multi) and finding
    appropriate models to generate spoilers foreach class. Results were analysed in
    each type of spoilers, revealed some reasons of having diversed results in different
    spoiler types. "passage" type spoiler was identified as the most difficult and
    the most valuable type of spoiler.'
  authors:
  - Shirui Tang
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_11
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 5: Clickbait Spoiling'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Brooke-English at SemEval-2023 Task 5: Clickbait Spoiling'
  tldr: 'The task of clickbait spoiling is: generating a short text that satisfies
    the curiosity induced by a clickbait post. Clickbait links to a web page and advertises
    its contents by arousing curiosity instead of providing an informative summary.
    Previous studies on clickbait spoiling has shown the appro'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In Task 9, we are required to analyze the textual intimacy of tweets in
    10 languages.We fine-tune XLM-RoBERTa (XLM-R) pre-trained model to adapt to this
    multilingual regression task. After tentative experiments, severe class imbalance
    is observed in the official released dataset, which may compromise the convergence
    and weaken the model effect. To tackle such challenge, we take measures in two
    aspects. On the one hand, we implement data augmentation through machine translation
    to enlarge the scale of classes with fewer samples. On the other hand, we introduce
    focal mean square error (MSE) loss to emphasize the contributions of hard samples
    to total loss, thus further mitigating the impact of class imbalance on model
    effect.Extensive experiments demonstrate remarkable effectiveness of our strategies,
    and our model achieves high performance on the Pearson's correlation coefficient
    (CC) almost above 0.85 on validation dataset.
  authors:
  - Yuxi Chen
  - Yu Chang
  - Yanqing Tao
  - Yanru Zhang
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_12
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 9: Multilingual Tweet Intimacy Analysis'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Sea_and_Wine at SemEval-2023 Task 9: A Regression Model with Data Augmentation
    for Multilingual Intimacy Analysis'
  tldr: In Task 9, we are required to analyze the textual intimacy of tweets in 10
    languages.We fine-tune XLM-RoBERTa (XLM-R) pre-trained model to adapt to this
    multilingual regression task. After tentative experiments, severe class imbalance
    is observed in the official released dataset, which may compromis
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper describes our system for SemEval-2023 Task 3 Subtask 2 on
    Framing Detection. We used a multi-label contrastive loss for fine-tuning large
    pre-trained language models in a multi-lingual setting, achieving very competitive
    results: our system was ranked first on the official test set and on the official
    shared task leaderboard for five of the six languages for which we had training
    data and for which we could perform fine-tuning. Here, we describe our experimental
    setup, as well as various ablation studies. The code of our system is available
    at https://github.com/QishengL/SemEval2023.'
  authors:
  - Qisheng Liao
  - Meiting Lai
  - Preslav Nakov
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_13
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 3: Detecting the Category, the Framing, and the Persuasion Techniques
    in Online News in a Multi-lingual Setup'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'MarsEclipse at SemEval-2023 Task 3: Multi-lingual and Multi-label Framing
    Detection with Contrastive Learning'
  tldr: 'This paper describes our system for SemEval-2023 Task 3 Subtask 2 on Framing
    Detection. We used a multi-label contrastive loss for fine-tuning large pre-trained
    language models in a multi-lingual setting, achieving very competitive results:
    our system was ranked first on the official test set and on'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In supervised learning, a significant amount of data is essential. To
    achieve this, we generated and evaluated datasets based on a provided dataset
    using transformer and non-transformer models. By utilizing these generated datasets
    during the training of new models, we attain a higher balanced accuracy during
    validation compared to using only the original dataset.
  authors:
  - Christian Falkenberg
  - Erik Sch\"{o}nw\"{a}lder
  - Tom Rietzke
  - Chris-Andris G\"{o}rner
  - Robert Walther
  - Julius Gonsior
  - Anja Reusch
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_14
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 5: Clickbait Spoiling'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Mr-Fosdick at SemEval-2023 Task 5: Comparing Dataset Expansion Techniques
    for Non-Transformer and Transformer Models: Improving Model Performance through
    Data Augmentation'
  tldr: In supervised learning, a significant amount of data is essential. To achieve
    this, we generated and evaluated datasets based on a provided dataset using transformer
    and non-transformer models. By utilizing these generated datasets during the training
    of new models, we attain a higher balanced accur
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Subjectivity and difference of opinion are key social phenomena, and it
    is crucial to take these into account in the annotation and detection process
    of derogatory textual content. In this paper, we use four datasets provided by
    SemEval-2023 Task 11 and fine-tune a BERT model to capture the disagreement in
    the annotation. We find individual annotator modeling and aggregation lowers the
    Cross-Entropy score by an average of 0.21, compared to the direct training on
    the soft labels. Our findings further demonstrate that annotator metadata contributes
    to the average 0.029 reduction in the Cross-Entropy score.
  authors:
  - Sadat Shahriar
  - Thamar Solorio
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_15
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 11: Learning with Disagreements (Le-Wi-Di)'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'SafeWebUH at SemEval-2023 Task 11: Learning Annotator Disagreement in Derogatory
    Text: Comparison of Direct Training vs Aggregation'
  tldr: Subjectivity and difference of opinion are key social phenomena, and it is
    crucial to take these into account in the annotation and detection process of
    derogatory textual content. In this paper, we use four datasets provided by SemEval-2023
    Task 11 and fine-tune a BERT model to capture the disagree
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Our team focuses on the multimodal domain of images and texts, we propose
    a model that can learn the matching relationship between text-image pairs by contrastive
    learning. More specifically, We train the model from the labeled data provided
    by the official organizer, after pre-training, texts are used to reference learned
    visual concepts enabling visual word sense disambiguation tasks. In addition,
    the top results our teams get have been released showing the effectiveness of
    our solution.
  authors:
  - Zhenghui Li
  - Qi Zhang
  - Xueyin Xia
  - Yinxiang Ye
  - Qi Zhang
  - Cong Huang
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_16
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Task-1 - Visual Word Sense Disambiguation (Visual-WSD)
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'ECNU_MIV at SemEval-2023 Task 1: CTIM - Contrastive Text-Image Model for
    Multilingual Visual Word Sense Disambiguation'
  tldr: Our team focuses on the multimodal domain of images and texts, we propose
    a model that can learn the matching relationship between text-image pairs by contrastive
    learning. More specifically, We train the model from the labeled data provided
    by the official organizer, after pre-training, texts are u
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes our approach to Subtask 1 "News Genre Categorization"
    of SemEval-2023 Task 3 "Detecting the Category, the Framing, and the Persuasion
    Techniques in Online News in a Multi-lingual Setup", which aims to determine whether
    a given news article is an opinion piece, an objective report, or satirical. We
    fine-tuned the domain-specific language model POLITICS, which was pre-trained
    on a large-scale dataset of more than 3.6M English political news articles following
    ideology-driven pre-training objectives. In order to use it in the multilingual
    setup of the task, we added as a pre-processing step the translation of all documents
    into English. Our system ranked among the top systems overall in most language,
    and ranked 1st on the English dataset.
  authors:
  - Nicolas Devatine
  - Philippe Muller
  - Chlo\'{e} Braud
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_17
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 3: Detecting the Category, the Framing, and the Persuasion Techniques
    in Online News in a Multi-lingual Setup'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'MELODI at SemEval-2023 Task 3: In-domain Pre-training for Low-resource Classification
    of News Articles'
  tldr: This paper describes our approach to Subtask 1 "News Genre Categorization"
    of SemEval-2023 Task 3 "Detecting the Category, the Framing, and the Persuasion
    Techniques in Online News in a Multi-lingual Setup", which aims to determine whether
    a given news article is an opinion piece, an objective repor
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes our system for SemEval-2023 Task 2 Multilingual Complex
    Named EntityRecognition (MultiCoNER II). Our teamSamsung Research China - Beijing
    proposesan AL-R (Adjustable Loss RoBERTa) model toboost the performance of recognizing
    short andcomplex entities with the challenges of longtaildata distribution, out
    of knowledge base andnoise scenarios. We first employ an adjustabledice loss optimization
    objective to overcomethe issue of long-tail data distribution, which isalso proved
    to be noise-robusted, especially incombatting the issue of fine-grained label
    confusing.Besides, we develop our own knowledgeenhancement tool to provide related
    contextsfor the short context setting and addressthe issue of out of knowledge
    base. Experimentshave verified the validation of our approaches.
  authors:
  - Haojie Zhang
  - Xiao Li
  - Renhua Gu
  - Xiaoyan Qu
  - Xiangfeng Meng
  - Shuo Hu
  - Song Liu
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_18
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 2: MultiCoNER II Multilingual Complex Named Entity Recognition'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Samsung Research China - Beijing at SemEval-2023 Task 2: An AL-R Model for
    Multilingual Complex Named Entity Recognition'
  tldr: This paper describes our system for SemEval-2023 Task 2 Multilingual Complex
    Named EntityRecognition (MultiCoNER II). Our teamSamsung Research China - Beijing
    proposesan AL-R (Adjustable Loss RoBERTa) model toboost the performance of recognizing
    short andcomplex entities with the challenges of longt
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper presents our system and findings for SemEval 2023 Task 9 Tweet
    Intimacy Analysis. The main objective of this task was to predict the intimacy
    of tweets in 10 languages. Our submitted model (ranked 28/45) consists of a transformer-based
    approach with data augmentation via machine translation.
  authors:
  - Abdessamad Benlahbib
  - Hamza Alami
  - Achraf Boumhidi
  - Omar Benslimane
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_19
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 9: Multilingual Tweet Intimacy Analysis'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'NLP-LISAC at SemEval-2023 Task 9: Multilingual Tweet Intimacy Analysis via
    a Transformer-based Approach and Data Augmentation'
  tldr: This paper presents our system and findings for SemEval 2023 Task 9 Tweet
    Intimacy Analysis. The main objective of this task was to predict the intimacy
    of tweets in 10 languages. Our submitted model (ranked 28/45) consists of a transformer-based
    approach with data augmentation via machine translati
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'We describe our participation on the Multi-evidence Natural Language
    Inference for Clinical Trial Data (NLI4CT) of SemEval''23. The organizers provided
    a collection of clinical trials as training data and a set of statements, which
    can be related to either a single trial or to a comparison of two trials. The
    task consisted of two sub-tasks: (i) textual entailment (Task 1) for predicting
    whether the statement is supported (Entailment) or not (Contradiction) by the
    corresponding trial(s); and (ii) evidence retrieval (Task 2) for selecting the
    evidences (sentences in the trials) that support the decision made for Task 1.
    We built a model based on a sentence-based BERT similarity model which was pre-trained
    on ClinicalBERT embeddings. Our best results on the official test sets were f-scores
    of 0.64 and 0.67 for Tasks 1 and 2, respectively.'
  authors:
  - Mariana Neves
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_20
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 7: Multi-Evidence Natural Language Inference for Clinical Trial
    Data'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Bf3R at SemEval-2023 Task 7: a text similarity model for textual entailment
    and evidence retrieval in clinical trials and animal studies'
  tldr: We describe our participation on the Multi-evidence Natural Language Inference
    for Clinical Trial Data (NLI4CT) of SemEval'23. The organizers provided a collection
    of clinical trials as training data and a set of statements, which can be related
    to either a single trial or to a comparison of two tri
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Multimodal ambiguity is a challenge for understanding text and images.
    Large pre-trained models have reached a high level of quality already. This paper
    presents an implementation for solving a image disambiguation task relying solely
    on the knowledge captured in multimodal and language models. Within the task 1
    of SemEval 2023 (Visual Word Sense Disambiguation), this approach managed to achieve
    an MRR of 0.738 using CLIP-Large and the OPT model for generating text. Applying
    a generative model to create more text given a phrase with an ambiguous word leads
    to an improvement of our results. The performance gain from a bigger language
    model is larger than the performance gain from using the lager CLIP model.
  authors:
  - Sebastian Diem
  - Chan Jong Im
  - Thomas Mandl
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_21
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Task-1 - Visual Word Sense Disambiguation (Visual-WSD)
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'University of Hildesheim at SemEval-2023 Task 1: Combining Pre-trained Multimodal
    and Generative Models for Image Disambiguation'
  tldr: Multimodal ambiguity is a challenge for understanding text and images. Large
    pre-trained models have reached a high level of quality already. This paper presents
    an implementation for solving a image disambiguation task relying solely on the
    knowledge captured in multimodal and language models. With
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The task ValueEval aims at assigning a sub- set of possible human value
    categories under- lying a given argument. Values behind argu- ments are often
    determinants to evaluate the relevance and importance of decisions in eth- ical
    sense, thereby making them essential for argument mining. The work presented here
    proposes two systems for the same. Both sys- tems use RoBERTa to encode sentences
    in each document. System1 makes use of features ob- tained from training models
    for two auxiliary tasks, whereas System2 combines RoBERTa with topic modeling
    to get sentence represen- tation. These features are used by a classifi- cation
    head to generate predictions. System1 secured the rank 22 in the official task
    rank- ing, achieving the macro F1-score 0.46 on the main dataset. System2 was
    not a part of official evaluation. Subsequent experiments achieved highest (among
    the proposed systems) macro F1-scores of 0.48 (System2), 0.31 (ablation on System1)
    and 0.33 (ablation on System1) on the main dataset, the Nahj al-Balagha dataset,
    and the New York Times dataset.
  authors:
  - Kushagri Tandon
  - Niladri Chatterjee
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_22
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 4: ValueEval: Identification of Human Values behind Arguments'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'LRL_NC at SemEval-2023 Task 4: The Touche23-George-boole Approach for Multi-Label
    Classification of Human-Values behind Arguments'
  tldr: The task ValueEval aims at assigning a sub- set of possible human value categories
    under- lying a given argument. Values behind argu- ments are often determinants
    to evaluate the relevance and importance of decisions in eth- ical sense, thereby
    making them essential for argument mining. The work pre
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Natural Language Processing techniques can be leveraged to process legal
    proceedings for various downstream applications, such as sum- marization of a
    given judgement, prediction of the judgement for a given legal case, prece- dent
    search, among others. These applications will benefit from legal judgement documents
    already segmented into topically coherent units. The current task, namely, Rhetorical
    Role Pre- diction, aims at categorising each sentence in the sequence of sentences
    in a judgement document into different labels. The system proposed in this work
    combines topic mod- eling and RoBERTa to encode sentences in each document. A
    BiLSTM layer has been utilised to get contextualised sentence repre- sentations.
    The Rhetorical Role predictions for each sentence in each document are gen- erated
    by a final CRF layer of the proposed neuro-computing system. This system secured
    the rank 12 in the official task ranking, achiev- ing the micro-F1 score 0.7980.
    The code for the proposed systems has been made available at https://github.com/KushagriT/SemEval23\_
    LegalEval\_TeamLRL\_NC
  authors:
  - Kushagri Tandon
  - Niladri Chatterjee
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_23
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 6: LegalEval: Understanding Legal Texts'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'LRL_NC at SemEval-2023 Task 6: Sequential Sentence Classification for Legal
    Documents Using Topic Modeling Features'
  tldr: Natural Language Processing techniques can be leveraged to process legal proceedings
    for various downstream applications, such as sum- marization of a given judgement,
    prediction of the judgement for a given legal case, prece- dent search, among
    others. These applications will benefit from legal jud
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes our submission to the SemEval 2023 multilingual tweet
    intimacy analysis shared task. The goal of the task was to assess the level of
    intimacy of Twitter posts in ten languages. The proposed approach consists of
    several steps. First, we perform in-domain pre-training to create a language model
    adapted to Twitter data. In the next step, we train an ensemble of regression
    models to expand the training set with pseudo-labeled examples. The extended dataset
    is used to train the final solution. Our method was ranked first in five out of
    ten language subtasks, obtaining the highest average score across all languages.
  authors:
  - Slawomir Dadas
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_24
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 9: Multilingual Tweet Intimacy Analysis'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'OPI at SemEval-2023 Task 9: A Simple But Effective Approach to Multilingual
    Tweet Intimacy Analysis'
  tldr: 'This paper describes our submission to the SemEval 2023 multilingual tweet
    intimacy analysis shared task. The goal of the task was to assess the level of
    intimacy of Twitter posts in ten languages. The proposed approach consists of
    several steps. First, we perform in-domain pre-training to create a '
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The goal of visual word sense disambiguation is to find the image that
    best matches the provided description of the word's meaning. It is a challenging
    problem, requiring approaches that combine language and image understanding. In
    this paper, we present our submission to SemEval 2023 visual word sense disambiguation
    shared task. The proposed system integrates multimodal embeddings, learning to
    rank methods, and knowledge-based approaches. We build a classifier based on the
    CLIP model, whose results are enriched with additional information retrieved from
    Wikipedia and lexical databases. Our solution was ranked third in the multilingual
    task and won in the Persian track, one of the three language subtasks.
  authors:
  - Slawomir Dadas
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_25
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Task-1 - Visual Word Sense Disambiguation (Visual-WSD)
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'OPI at SemEval-2023 Task 1: Image-Text Embeddings and Multimodal Information
    Retrieval for Visual Word Sense Disambiguation'
  tldr: The goal of visual word sense disambiguation is to find the image that best
    matches the provided description of the word's meaning. It is a challenging problem,
    requiring approaches that combine language and image understanding. In this paper,
    we present our submission to SemEval 2023 visual word se
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'In this paper, we (team RGAT) describe our approach for the SemEval 2023
    Task 2: Multilingual Complex Named Entity Recognition (MultiCoNER II). The goal
    of this task is to locate and classify named entities in unstructured short complex
    texts in 12 different languages and one multilingual setup. We use the dependency
    tree of the input query as additional feature in a Graph Attention Network along
    with the token and part-of-speech features. We also experiment with additional
    layers like BiLSTM and Transformer in addition to the CRF layer. However, we have
    not included any external Knowledge base like Wikipedia to enrich our inputs.
    We evaluated our proposed approach on the English NER dataset that resulted in
    a clean-subset F1 of 61.29\textbackslash{}\% and overall F1 of 56.91\textbackslash{}\%.
    However, other approaches that used external knowledge base performed significantly
    better.'
  authors:
  - Abir Chakraborty
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_26
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 2: MultiCoNER II Multilingual Complex Named Entity Recognition'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'RGAT at SemEval-2023 Task 2: Named Entity Recognition Using Graph Attention
    Network'
  tldr: 'In this paper, we (team RGAT) describe our approach for the SemEval 2023
    Task 2: Multilingual Complex Named Entity Recognition (MultiCoNER II). The goal
    of this task is to locate and classify named entities in unstructured short complex
    texts in 12 different languages and one multilingual setup. We '
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: A standard majority-based approach to text classification is challenged
    with an individualised approach in the Semeval-2023 Task 11. Here, disagreements
    are treated as a useful source of information that could be utilised in the training
    pipeline. The team proposal makes use of partially disaggregated data and additional
    information about annotators provided by the organisers to train a BERT-based
    model for offensive text classification. The approach extends previous studies
    examining the impact of using raters' demographic features on classification performance
    (Hovy, 2015) or training machine learning models on disaggregated data (Davani
    et al., 2022). The proposed approach was ranked 11 across all 4 datasets, scoring
    best for cases with a large pool of annotators (6th place in the MD-Agreement
    dataset) utilising features based on raters' annotation behaviour.
  authors:
  - Ewelina Gajewska
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_27
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 11: Learning with Disagreements (Le-Wi-Di)'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'eevvgg at SemEval-2023 Task 11: Offensive Language Classification with Rater-based
    Information'
  tldr: A standard majority-based approach to text classification is challenged with
    an individualised approach in the Semeval-2023 Task 11. Here, disagreements are
    treated as a useful source of information that could be utilised in the training
    pipeline. The team proposal makes use of partially disaggregat
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes our participation in SemEval-2023 Task 9, Intimacy
    Analysis of Multilingual Tweets. We fine-tune some of the most popular transformer
    models with the training dataset and synthetic data generated by different data
    augmentation techniques. During the development phase, our best results were obtained
    by using XLM-T. Data augmentation techniques provide a very slight improvement
    in the results. Our system ranked in the 27th position out of the 45 participating
    systems. Despite its modest results, our system shows promising results in languages
    such as Portuguese, English, and Dutch. All our code is available in the repository
    https://github.com/isegura/hulat\_intimacy.
  authors:
  - Isabel Segura-Bedmar
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_28
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 9: Multilingual Tweet Intimacy Analysis'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'HULAT at SemEval-2023 Task 9: Data Augmentation for Pre-trained Transformers
    Applied to Multilingual Tweet Intimacy Analysis'
  tldr: This paper describes our participation in SemEval-2023 Task 9, Intimacy Analysis
    of Multilingual Tweets. We fine-tune some of the most popular transformer models
    with the training dataset and synthetic data generated by different data augmentation
    techniques. During the development phase, our best r
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes our participation in SemEval-2023 Task 10, whose
    goal is the detection of sexism in social media. We explore some of the most popular
    transformer models such as BERT, DistilBERT, RoBERTa, and XLNet. We also study
    different data augmentation techniques to increase the training dataset. During
    the development phase, our best results were obtained by using RoBERTa and data
    augmentation for tasks B and C. However, the use of synthetic data does not improve
    the results for task C. We participated in the three subtasks. Our approach still
    has much room for improvement, especially in the two fine-grained classifications.
    All our code is available in the repository https://github.com/isegura/hulat\_edos.
  authors:
  - Isabel Segura-Bedmar
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_29
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 10: Towards Explainable Detection of Online Sexism'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'HULAT at SemEval-2023 Task 10: Data Augmentation for Pre-trained Transformers
    Applied to the Detection of Sexism in Social Media'
  tldr: This paper describes our participation in SemEval-2023 Task 10, whose goal
    is the detection of sexism in social media. We explore some of the most popular
    transformer models such as BERT, DistilBERT, RoBERTa, and XLNet. We also study
    different data augmentation techniques to increase the training da
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Identifying expressions of human values in textual data is a crucial albeit
    complicated challenge, not least because ethics are highly variable, often implicit,
    and transcend circumstance. Opinions, arguments, and the like are generally founded
    upon more than one guiding principle, which are not necessarily independent. As
    such, little is known about how to classify and predict moral undertones in natural
    language sequences. Here, we describe and present a solution to ValueEval, our
    shared contribution to SemEval 2023 Task 4. Our research design focuses on investigating
    chain classifier architectures with pretrained contextualized embeddings to detect
    20 different human values in written arguments. We show that our best model substantially
    surpasses the classification performance of the baseline method established in
    prior work. We discuss limitations to our approach and outline promising directions
    for future work.
  authors:
  - Spencer Paulissen
  - Caroline Wendt
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_30
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 4: ValueEval: Identification of Human Values behind Arguments'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Lauri Ingman at SemEval-2023 Task 4: A Chain Classifier for Identifying
    Human Values behind Arguments'
  tldr: Identifying expressions of human values in textual data is a crucial albeit
    complicated challenge, not least because ethics are highly variable, often implicit,
    and transcend circumstance. Opinions, arguments, and the like are generally founded
    upon more than one guiding principle, which are not nec
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper presents our systems and findings for SemEval-2023 Task 12:
    AfriSenti-SemEval: Sentiment Analysis for Low-resource African Languages. The
    main objective of this task was to determine the polarity of a tweet (positive,
    negative, or neutral). Our submitted models (highest rank is 1 and lowest rank
    is 21 depending on the target Track) consist of various Transformer-based approaches.'
  authors:
  - Abdessamad Benlahbib
  - Achraf Boumhidi
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_31
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 12: AfriSenti-SemEval: Sentiment Analysis for Low-resource African
    Languages using Twitter Dataset'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'NLP-LISAC at SemEval-2023 Task 12: Sentiment Analysis for Tweets expressed
    in African languages via Transformer-based Models'
  tldr: 'This paper presents our systems and findings for SemEval-2023 Task 12: AfriSenti-SemEval:
    Sentiment Analysis for Low-resource African Languages. The main objective of this
    task was to determine the polarity of a tweet (positive, negative, or neutral).
    Our submitted models (highest rank is 1 and lowe'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'In this paper, we discuss our models applied to Task 4: Human Value Detection
    of SemEval 2023, which incorporated two different embedding techniques to interpret
    the data. Preliminary experiments were conducted to observe important word types.
    Subsequently, we explored an XGBoost model, an unsupervised learning model, and
    two Ensemble learning models were then explored. The best performing model, an
    ensemble model employing a soft voting technique, secured the 34th spot out of
    39 teams, on a class imbalanced dataset. We explored the inclusion of different
    parts of the provided knowledge resource and found that considering only specific
    parts assisted our models.'
  authors:
  - Ethan Heavey
  - Milton King
  - James Hughes
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_32
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 4: ValueEval: Identification of Human Values behind Arguments'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'StFX-NLP at SemEval-2023 Task 4: Unsupervised and Supervised Approaches
    to Detecting Human Values in Arguments'
  tldr: 'In this paper, we discuss our models applied to Task 4: Human Value Detection
    of SemEval 2023, which incorporated two different embedding techniques to interpret
    the data. Preliminary experiments were conducted to observe important word types.
    Subsequently, we explored an XGBoost model, an unsupervi'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The "Multi-evidence Natural Language Inference forClinical Trial Data"
    task at SemEval 2023competition focuses on extracting essentialinformation on
    clinical trial data, by posing twosubtasks on textual entailment and evidence
    retrieval.In the context of SemEval, we present a comparisonbetween a method based
    on the BioBERT model anda CNN model. The task is based on a collection ofbreast
    cancer Clinical Trial Reports (CTRs),statements, explanations, and labels annotated
    bydomain expert annotators. We achieved F1 scores of0.69 for determining the inference
    relation(entailment vs contradiction) between CTR -statement pairs. The implementation
    of our system ismade available via Github - https://github.com/volosincu/FII\_Smart\_\_Semeval2023.
  authors:
  - Mihai Volosincu
  - Cosmin Lupu
  - Diana Trandabat
  - Daniela Gifu
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_33
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 7: Multi-Evidence Natural Language Inference for Clinical Trial
    Data'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'FII SMART at SemEval 2023 Task7: Multi-evidence Natural Language Inference
    for Clinical Trial Data'
  tldr: The "Multi-evidence Natural Language Inference forClinical Trial Data" task
    at SemEval 2023competition focuses on extracting essentialinformation on clinical
    trial data, by posing twosubtasks on textual entailment and evidence retrieval.In
    the context of SemEval, we present a comparisonbetween a met
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We describe our experiments for SemEval-2023 Task 4 on the identification
    of human values behind arguments (ValueEval). Because human values are subjective
    concepts which require precise definitions, we hypothesize that incorporating
    the definitions of human values (in the form of annotation instructions and validated
    survey items) during model training can yield better prediction performance. We
    explore this idea and show that our proposed models perform better than the challenge
    organizers' baselines, with improvements in macro F1 scores of up to 18\%.
  authors:
  - Christian Fang
  - Qixiang Fang
  - Dong Nguyen
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_34
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 4: ValueEval: Identification of Human Values behind Arguments'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Epicurus at SemEval-2023 Task 4: Improving Prediction of Human Values behind
    Arguments by Leveraging Their Definitions'
  tldr: We describe our experiments for SemEval-2023 Task 4 on the identification
    of human values behind arguments (ValueEval). Because human values are subjective
    concepts which require precise definitions, we hypothesize that incorporating
    the definitions of human values (in the form of annotation instruc
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: To improve the ability of language models to handle Natural Language Processing(NLP)
    tasks and intermediate step of pre-training has recently beenintroduced. In this
    setup, one takes a pre-trained language model, trains it ona (set of) NLP dataset(s),
    and then finetunes it for a target task. It isknown that the selection of relevant
    transfer tasks is important, but recentlysome work has shown substantial performance
    gains by doing intermediatetraining on a very large set of datasets. Most previous
    work uses generativelanguage models or only focuses on one or a couple of tasks
    and uses acarefully curated setup. We compare intermediate training with one or
    manytasks in a setup where the choice of datasets is more arbitrary; we use allSemEval
    2023 text-based tasks. We reach performance improvements for most taskswhen using
    intermediate training. Gains are higher when doing intermediatetraining on single
    tasks than all tasks if the right transfer taskis identified. Dataset smoothing
    and heterogeneous batching did not lead torobust gains in our setup.
  authors:
  - Rob van der Goot
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_35
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 8: Causal medical claim identification and related PICO frame
    extraction from social media posts'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'MaChAmp at SemEval-2023 tasks 2, 3, 4, 5, 7, 8, 9, 10, 11, and 12: On the
    Effectiveness of Intermediate Training on an Uncurated Collection of Datasets.'
  tldr: To improve the ability of language models to handle Natural Language Processing(NLP)
    tasks and intermediate step of pre-training has recently beenintroduced. In this
    setup, one takes a pre-trained language model, trains it ona (set of) NLP dataset(s),
    and then finetunes it for a target task. It iskn
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We describe our contribution to the SemEVAl 2023 AfriSenti-SemEval shared
    task, where we tackle the task of sentiment analysis in 14 different African languages.
    We develop both monolingual and multilingual models under a full supervised setting
    (subtasks A and B). We also develop models for the zero-shot setting (subtask
    C). Our approach involves experimenting with transfer learning using six language
    models, including further pretraining of some of these models as well as a final
    finetuning stage. Our best performing models achieve an F1-score of 70.36 on development
    data and an F1-score of 66.13 on test data. Unsurprisingly, our results demonstrate
    the effectiveness of transfer learning and finetuning techniques for sentiment
    analysis across multiple languages. Our approach can be applied to other sentiment
    analysis tasks in different languages and domains.
  authors:
  - Gagan Bhatia
  - Ife Adebara
  - Abdelrahim Elmadany
  - Muhammad Abdul-mageed
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_36
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 12: AfriSenti-SemEval: Sentiment Analysis for Low-resource African
    Languages using Twitter Dataset'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'UBC-DLNLP at SemEval-2023 Task 12: Impact of Transfer Learning on African
    Sentiment Analysis'
  tldr: We describe our contribution to the SemEVAl 2023 AfriSenti-SemEval shared
    task, where we tackle the task of sentiment analysis in 14 different African languages.
    We develop both monolingual and multilingual models under a full supervised setting
    (subtasks A and B). We also develop models for the zer
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The Human Value Detection shared task\textbackslash{}cite\{kiesel:2023\}
    aims to classify whether or not the argument draws on a set of 20 value categories,
    given a textual argument. This is a difficult task as the discrimination of human
    values behind arguments is often implicit. Moreover, the number of label categories
    can be up to 20 and the distribution of data is highly imbalanced. To address
    these issues, we employ a multi-label classification model and utilize a class-balanced
    loss function. Our system wins 5 first places, 2 second places, and 6 third places
    out of 20 categories of the Human Value Detection shared task, and our overall
    average score of 0.54 also places third. The code is publicly available at \textbackslash{}url\{https://www.github.com/diqiuzhuanzhuan/semeval2023\}.
  authors:
  - Long Ma
  - Zeye Sun
  - Jiawei Jiang
  - Xuan Li
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_37
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 4: ValueEval: Identification of Human Values behind Arguments'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'PAI at SemEval-2023 Task 4: A General Multi-label Classification System
    with Class-balanced Loss Function and Ensemble Module'
  tldr: The Human Value Detection shared task\textbackslash{}cite\{kiesel:2023\} aims
    to classify whether or not the argument draws on a set of 20 value categories,
    given a textual argument. This is a difficult task as the discrimination of human
    values behind arguments is often implicit. Moreover, the numb
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper describes our system for SemEval-2023 Task 6: LegalEval: Understanding
    Legal Texts. We only participate in Sub-Task (A), Predicting Rhetorical Roles.
    Our final submission achieves 73.35 test set F1 score, ranking 17th of 27 participants.
    The proposed method combines global and local models of label distributions and
    transitions between labels. Through our analyses, we show that especially modelling
    the temporal distribution of labels contributes positively to performance.'
  authors:
  - Henrik Manegold
  - Leander Girrbach
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_38
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 6: LegalEval: Understanding Legal Texts'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: "T\xFCReuth Legal at SemEval-2023 Task 6: Modelling Local and Global Structure\
    \ of Judgements for Rhetorical Role Prediction"
  tldr: 'This paper describes our system for SemEval-2023 Task 6: LegalEval: Understanding
    Legal Texts. We only participate in Sub-Task (A), Predicting Rhetorical Roles.
    Our final submission achieves 73.35 test set F1 score, ranking 17th of 27 participants.
    The proposed method combines global and local model'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Legal documents tend to be large in size. In this paper, we provide an
    experiment with attention-based approaches complemented by certain document processing
    techniques for judgment prediction. For the prediction of explanation, we consider
    this as an extractive text summarization problem based on an output of (1) CNN
    with attention mechanism and (2) self-attention of language models. Our extensive
    experiments show that treating document endings at first results in a 2.1\% improvement
    in judgment prediction across all the models. Additional content peeling from
    non-informative sentences allows an improvement of explanation prediction performance
    by 4\% in the case of attention-based CNN models. The best submissions achieved
    8'th and 3'rd ranks on judgment prediction (C1) and prediction with explanation
    (C2) tasks respectively among 11 participating teams. The results of our experiments
    are published
  authors:
  - Nicolay Rusnachenko
  - Thanet Markchom
  - Huizhi Liang
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_39
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 6: LegalEval: Understanding Legal Texts'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'nclu_team at SemEval-2023 Task 6: Attention-based Approaches for Large Court
    Judgement Prediction with Explanation'
  tldr: Legal documents tend to be large in size. In this paper, we provide an experiment
    with attention-based approaches complemented by certain document processing techniques
    for judgment prediction. For the prediction of explanation, we consider this as
    an extractive text summarization problem based on a
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This study aims to tackle some challenges posed by legal texts in the
    field of NLP. The LegalEval challenge proposes three tasks, based on Indial Legal
    documents: Rhetorical Roles Prediction, Legal Named Entity Recognition, and Court
    Judgement Prediction with Explanation. Our work focuses on the first two tasks.
    For the first task we present a context-aware approach to enhance sentence information.
    With the help of this approach, the classification model utilizing InLegalBert
    as a transformer achieved 81.12\% Micro-F1. For the second task we present a NER
    approach to extract and classify entities like names of petitioner, respondent,
    court or statute of a given document. The model utilizing XLNet as transformer
    and a dependency parser on top achieved 87.43\% Macro-F1.'
  authors:
  - Yuri Noviello
  - Enrico Pallotta
  - Flavio Pinzarrone
  - Giuseppe Tanzi
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_40
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 6: LegalEval: Understanding Legal Texts'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'TeamUnibo at SemEval-2023 Task 6: A transformer based approach to Rhetorical
    Roles prediction and NER in Legal Texts'
  tldr: 'This study aims to tackle some challenges posed by legal texts in the field
    of NLP. The LegalEval challenge proposes three tasks, based on Indial Legal documents:
    Rhetorical Roles Prediction, Legal Named Entity Recognition, and Court Judgement
    Prediction with Explanation. Our work focuses on the fir'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'These working notes summarize the participation of the UMUTeam in the
    SemEval 2023 shared task: AfriSenti, focused on Sentiment Analysis in several
    African languages. Two subtasks are proposed, one in which each language is considered
    separately and another one in which all languages are merged. Our proposal to
    solve both subtasks is grounded on the combination of features extracted from
    several multilingual Large Language Models and a subset of language-independent
    linguistic features. Our best results are achieved with the African languages
    less represented in the training set: Xitsonga, a Mozambique dialect, with a weighted
    f1-score of 54.89\textbackslash{}\%; Algerian Arabic, with a weighted f1-score
    of 68.52\textbackslash{}\%; Swahili, with a weighted f1-score of 60.52\textbackslash{}\%;
    and Twi, with a weighted f1-score of 71.14\%.'
  authors:
  - "Jos\xE9 Antonio Garc\xEDa-D\xEDaz"
  - Camilo Caparros-laiz
  - "\xC1ngela Almela"
  - "Gema Alcar\xE1z-M\xE1rmol"
  - "Mar\xEDa Jos\xE9 Mar\xEDn-P\xE9rez"
  - "Rafael Valencia-Garc\xEDa"
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_41
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 12: AfriSenti-SemEval: Sentiment Analysis for Low-resource African
    Languages using Twitter Dataset'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'UMUTeam at SemEval-2023 Task 12: Ensemble Learning of LLMs applied to Sentiment
    Analysis for Low-resource African Languages'
  tldr: 'These working notes summarize the participation of the UMUTeam in the SemEval
    2023 shared task: AfriSenti, focused on Sentiment Analysis in several African
    languages. Two subtasks are proposed, one in which each language is considered
    separately and another one in which all languages are merged. Our'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This work presents the participation of the UMUTeam and the SINAI research
    groups in the SemEval-2023 Task 9: Multilingual Tweet Intimacy Analysis. The goal
    of this task is to predict the intimacy of a set of tweets in 10 languages: English,
    Spanish, Italian, Portuguese, French, Chinese, Hindi, Arabic, Dutch and Korean,
    of which, the last 4 are not in the training data. Our approach to address this
    task is based on data augmentation and the use of three multilingual Large Language
    Models (multilingual BERT, XLM and mDeBERTA) by ensemble learning. Our team ranked
    30th out of 45 participants. Our best results were achieved with two unseen languages:
    Korean (16th) and Hindi (19th).'
  authors:
  - "Jos\xE9 Antonio Garc\xEDa-D\xEDaz"
  - Ronghao Pan
  - "Salud Mar\xEDa Jim\xE9nez Zafra"
  - "Mar\xEDa-Teresa Martn-Valdivia"
  - "L. Alfonso Ure\xF1a-L\xF3pez"
  - "Rafael Valencia-Garc\xEDa"
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_42
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 9: Multilingual Tweet Intimacy Analysis'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'UMUTeam and SINAI at SemEval-2023 Task 9: Multilingual Tweet Intimacy Analysis
    using Multilingual Large Language Models and Data Augmentation'
  tldr: 'This work presents the participation of the UMUTeam and the SINAI research
    groups in the SemEval-2023 Task 9: Multilingual Tweet Intimacy Analysis. The goal
    of this task is to predict the intimacy of a set of tweets in 10 languages: English,
    Spanish, Italian, Portuguese, French, Chinese, Hindi, Arab'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes the participation of team QUST in the SemEval2023
    task3. The monolingual models are first evaluated with the under-sampling of the
    majority classes in the early stage of the task. Then, the pre-trained multilingual
    model is fine-tuned with a combination of the class weights and the sample weights.
    Two different fine-tuning strategies, the task-agnostic and the task-dependent,
    are further investigated. All experiments are conducted under the 10-fold cross-validation,
    the multilingual approaches are superior to the monolingual ones. The submitted
    system achieves the second best in Italian and Spanish (zero-shot) in subtask-1.
  authors:
  - Ye Jiang
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_43
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 3: Detecting the Category, the Framing, and the Persuasion Techniques
    in Online News in a Multi-lingual Setup'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Team QUST at SemEval-2023 Task 3: A Comprehensive Study of Monolingual and
    Multilingual Approaches for Detecting Online News Genre, Framing and Persuasion
    Techniques'
  tldr: 'This paper describes the participation of team QUST in the SemEval2023 task3.
    The monolingual models are first evaluated with the under-sampling of the majority
    classes in the early stage of the task. Then, the pre-trained multilingual model
    is fine-tuned with a combination of the class weights and '
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Sexism is a growing online problem. It harms women who are targeted and
    makes online spaces inaccessible and unwelcoming. In this paper, we present our
    approach for Task A of SemEval-2023 Task 10: Explainable Detection of Online Sexism
    (EDOS), which aims to perform binary sexism detection on textual content. To solve
    this task, we fine-tune the pre-trained model based on several popular natural
    language processing methods to improve the generalization ability in the face
    of different data. According to the experimental results, the effective combination
    of multiple methods enables our approach to achieve excellent performance gains.'
  authors:
  - Yu Chang
  - Yuxi Chen
  - Yanru Zhang
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_44
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 10: Towards Explainable Detection of Online Sexism'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'niceNLP at SemEval-2023 Task 10: Dual Model Alternate Pseudo-labeling Improves
    Your Predictions'
  tldr: 'Sexism is a growing online problem. It harms women who are targeted and makes
    online spaces inaccessible and unwelcoming. In this paper, we present our approach
    for Task A of SemEval-2023 Task 10: Explainable Detection of Online Sexism (EDOS),
    which aims to perform binary sexism detection on textual'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This study describes the model design of the NCUEE-NLP system for the
    SemEval-2023 Task 8. We use the pre-trained transformer models and fine-tune the
    task datasets to identify medical causal claims and extract population, intervention,
    and outcome elements in a Reddit post when a claim is given. Our best system submission
    for the causal claim identification subtask achieved a F1-score of 70.15\%. Our
    best submission for the PIO frame extraction subtask achieved F1-scores of 37.78\%
    for Population class, 43.58\% for Intervention class, and 30.67\% for Outcome
    class, resulting in a macro-averaging F1-score of 37.34\%. Our system evaluation
    results ranked second position among all participating teams.
  authors:
  - Lung-Hao Lee
  - Yuan-Hao Cheng
  - Jen-Hao Yang
  - Kao-Yuan Tien
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_48
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 8: Causal medical claim identification and related PICO frame
    extraction from social media posts'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'NCUEE-NLP at SemEval-2023 Task 8: Identifying Medical Causal Claims and
    Extracting PIO Frames Using the Transformer Models'
  tldr: This study describes the model design of the NCUEE-NLP system for the SemEval-2023
    Task 8. We use the pre-trained transformer models and fine-tune the task datasets
    to identify medical causal claims and extract population, intervention, and outcome
    elements in a Reddit post when a claim is given. Ou
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We present the system description of our team Zhegu in SemEval-2023 Task
    9 Multilingual Tweet Intimacy Analysis. We propose \textbackslash{}textbf\{EPM\}
    (\textbackslash{}textbf\{E\}xponential \textbackslash{}textbf\{P\}enalty \textbackslash{}textbf\{M\}ean
    Squared Loss) for the purpose of enhancing the ability of learning difficult samples
    during the training process. Meanwhile, we also apply several methods (frozen
    Tuning \textbackslash{}\& contrastive learning based on Language) on the XLM-R
    multilingual language model for fine-tuning and model ensemble. The results in
    our experiments provide strong faithful evidence of the effectiveness of our methods.
    Eventually, we achieved a Pearson score of 0.567 on the test set.
  authors:
  - Pan He
  - Yanru Zhang
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_49
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 9: Multilingual Tweet Intimacy Analysis'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Zhegu at SemEval-2023 Task 9: Exponential Penalty Mean Squared Loss for
    Multilingual Tweet Intimacy Analysis'
  tldr: We present the system description of our team Zhegu in SemEval-2023 Task 9
    Multilingual Tweet Intimacy Analysis. We propose \textbackslash{}textbf\{EPM\}
    (\textbackslash{}textbf\{E\}xponential \textbackslash{}textbf\{P\}enalty \textbackslash{}textbf\{M\}ean
    Squared Loss) for the purpose of enhancing
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper describes the system of the ABCD team for three main tasks
    in the SemEval-2023 Task 12: AfriSenti-SemEval for Low-resource African Languages
    using Twitter Dataset. We focus on exploring the performance of ensemble architectures
    based on the soft voting technique and different pre-trained transformer-based
    language models. The experimental results show that our system has achieved competitive
    performance in some Tracks in Task A: Monolingual Sentiment Analysis, where we
    rank the Top 3, Top 2, and Top 4 for the Hause, Igbo and Moroccan languages. Besides,
    our model achieved competitive results and ranked \$14\^{}\{th\}\$ place in Task
    B (multilingual) setting and \$14\^{}\{th\}\$ and \$8\^{}\{th\}\$ place in Track
    17 and Track 18 of Task C (zero-shot) setting.'
  authors:
  - Dang Thin
  - Dai Nguyen
  - Dang Qui
  - Duong Hao
  - Ngan Nguyen
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_50
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 12: AfriSenti-SemEval: Sentiment Analysis for Low-resource African
    Languages using Twitter Dataset'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'ABCD Team at SemEval-2023 Task 12: An Ensemble Transformer-based System
    for African Sentiment Analysis'
  tldr: 'This paper describes the system of the ABCD team for three main tasks in
    the SemEval-2023 Task 12: AfriSenti-SemEval for Low-resource African Languages
    using Twitter Dataset. We focus on exploring the performance of ensemble architectures
    based on the soft voting technique and different pre-trained '
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'The following is a description of the RIGA team''s submissions for the
    English track of the SemEval-2023 Task 2: Multilingual Complex Named Entity Recognition
    (MultiCoNER) II. Our approach achieves 17\% boost in results by utilizing pre-existing
    Large-scale Language Models (LLMs), such as GPT-3, to gather additional contexts.
    We then fine-tune a pre-trained neural network utilizing these contexts. The final
    step of our approach involves meticulous model and compute resource scaling, which
    results in improved performance. Our results placed us 12th out of 34 teams in
    terms of overall ranking and 7th in terms of the noisy subset ranking. The code
    for our method is available on GitHub (https://github.com/emukans/multiconer2-riga).'
  authors:
  - Eduards Mukans
  - Guntis Barzdins
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_51
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 2: MultiCoNER II Multilingual Complex Named Entity Recognition'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'RIGA at SemEval-2023 Task 2: NER Enhanced with GPT-3'
  tldr: 'The following is a description of the RIGA team''s submissions for the English
    track of the SemEval-2023 Task 2: Multilingual Complex Named Entity Recognition
    (MultiCoNER) II. Our approach achieves 17\% boost in results by utilizing pre-existing
    Large-scale Language Models (LLMs), such as GPT-3, to g'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: When we interact with other humans, humanvalues guide us to consider the
    human element.As we shall see, value analysis in NLP hasbeen applied to personality
    profiling but not toargument mining. As part of SemEval-2023Shared Task 4, our
    system paper describes amulti-label classifier for identifying human val-ues.
    Human value detection requires multi-label classification since each argument
    maycontain multiple values. In this paper, we pro-pose an architecture called
    Label Graph Trans-former (LG-Transformer). LG-Transformeris a two-stage pipeline
    consisting of a trans-former jointly encoding argument and labelsand a graph module
    encoding and obtainingfurther interactions between labels. Using ad-versarial
    training, we can boost performanceeven further. Our best method scored 50.00 us-ing
    F1 score on the test set, which is 7.8 higherthan the best baseline method. Our
    code ispublicly available on Github.
  authors:
  - Hamed Hematian Hemati
  - Sayed Hesam Alavian
  - Hossein Sameti
  - Hamid Beigy
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_53
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 4: ValueEval: Identification of Human Values behind Arguments'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'SUTNLP at SemEval-2023 Task 4: LG-Transformer for Human Value Detection'
  tldr: When we interact with other humans, humanvalues guide us to consider the human
    element.As we shall see, value analysis in NLP hasbeen applied to personality
    profiling but not toargument mining. As part of SemEval-2023Shared Task 4, our
    system paper describes amulti-label classifier for identifying h
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: There is no simple definition of sexism, butit can be described as prejudice,
    stereotyping,or discrimination, especially against women,based on their gender.
    In online interactions,sexism is common. One out of ten Americanadults says that
    they have been harassed be-cause of their gender and have been the targetof sexism,
    so sexism is a growing issue. TheExplainable Detection of Online Sexism sharedtask
    in SemEval-2023 aims at building sexismdetection systems for the English language.
    Inorder to address the problem, we use largelanguage models such as RoBERTa and
    De-BERTa. In addition, we present Random LayerAdversarial Training (RLAT) for
    transformers,and show its significant impact on solving allsubtasks. Moreover,
    we use virtual adversar-ial training and contrastive learning to improveperformance
    on subtask A. Upon completionof subtask A, B, and C test sets, we obtainedmacro-F1
    of 84.45, 67.78, and 52.52, respec-tively outperforming proposed baselines on
    allsubtasks. Our code is publicly available onGithub.
  authors:
  - Hamed Hematian Hemati
  - Sayed Hesam Alavian
  - Hamid Beigy
  - Hossein Sameti
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_54
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 10: Towards Explainable Detection of Online Sexism'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'SUTNLP at SemEval-2023 Task 10: RLAT-Transformer for explainable online
    sexism detection'
  tldr: There is no simple definition of sexism, butit can be described as prejudice,
    stereotyping,or discrimination, especially against women,based on their gender.
    In online interactions,sexism is common. One out of ten Americanadults says that
    they have been harassed be-cause of their gender and have bee
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper describes our system submission for SemEval-2023 Task 12 AfriSenti-SemEval:
    Sentiment Analysis for African Languages. We propose an XGBoost-based ensemble
    model trained on emoticon frequency-based features and the predictions of several
    statistical models such as SVMs, Logistic Regression, Random Forests, and BERT-based
    pre-trained language models such as AfriBERTa and AfroXLMR. We also report results
    from additional experiments not in the system. Our system achieves a mixed bag
    of results, achieving a best rank of 7th in three of the languages - Igbo, Twi,
    and Yoruba.'
  authors:
  - Monil Gokani
  - K V Aditya Srivatsa
  - Radhika Mamidi
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_55
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 12: AfriSenti-SemEval: Sentiment Analysis for Low-resource African
    Languages using Twitter Dataset'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Witcherses at SemEval-2023 Task 12: Ensemble Learning for African Sentiment
    Analysis'
  tldr: 'This paper describes our system submission for SemEval-2023 Task 12 AfriSenti-SemEval:
    Sentiment Analysis for African Languages. We propose an XGBoost-based ensemble
    model trained on emoticon frequency-based features and the predictions of several
    statistical models such as SVMs, Logistic Regression'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'In this paper, we describe our submissions to the SemEval-2023 contest.
    We tackled subtask 12 - "AfriSenti-SemEval: Sentiment Analysis for Low-resource
    African Languages using Twitter Dataset". We developed different models for 12
    African languages and a 13th model for a multilingual dataset built from these
    12 languages. We applied a wide variety of word and char n-grams based on their
    tf-idf values, 4 classical machine learning methods, 2 deep learning methods,
    and 3 oversampling methods. We used 12 sentiment lexicons and applied extensive
    hyperparameter tuning.'
  authors:
  - Ron Keinan
  - Yaakov Hacohen-Kerner
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_57
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 12: AfriSenti-SemEval: Sentiment Analysis for Low-resource African
    Languages using Twitter Dataset'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'JCT at SemEval-2023 Tasks 12 A and 12B: Sentiment Analysis for Tweets Written
    in Low-resource African Languages using Various Machine Learning and Deep Learning
    Methods, Resampling, and HyperParameter Tuning'
  tldr: 'In this paper, we describe our submissions to the SemEval-2023 contest. We
    tackled subtask 12 - "AfriSenti-SemEval: Sentiment Analysis for Low-resource African
    Languages using Twitter Dataset". We developed different models for 12 African
    languages and a 13th model for a multilingual dataset built f'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'IXA proposes a Sequence labeling fine-tune approach, which consists of
    a lightweight few-shot baseline (10e), the system takes advantage of transfer
    learning from pre-trained Named Entity Recognition and cross-lingual knowledge
    from the LM checkpoint. This technique obtains a drastic reduction in the effective
    training costs that works as a perfect baseline, future improvements in the baseline
    approach could fit: 1) Domain adequation, 2) Data augmentation, and 3) Intermediate
    task learning.'
  authors:
  - Edgar Andres Santamaria
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_58
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 2: MultiCoNER II Multilingual Complex Named Entity Recognition'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'IXA at SemEval-2023 Task 2: Baseline Xlm-Roberta-base Approach'
  tldr: IXA proposes a Sequence labeling fine-tune approach, which consists of a lightweight
    few-shot baseline (10e), the system takes advantage of transfer learning from
    pre-trained Named Entity Recognition and cross-lingual knowledge from the LM checkpoint.
    This technique obtains a drastic reduction in th
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In this paper, we present our approach to the task of identification of
    persuasion techniques in text, which is a subtask of the SemEval-2023 Task 3 on
    the multilingual detection of genre, framing, and persuasion techniques in online
    news. The subtask is multi-label at the paragraph level and the inventory considered
    by the organizers covers 23 persuasion techniques. Our solution is based on an
    ensemble of a variety of pre-trained language models (PLMs) fine-tuned on the
    propaganda dataset. We first describe our system, the different experimental setups
    we considered, and then provide the results on the dev and test sets released
    by the organizers. The official evaluation shows our solution ranks 1st in English
    and attains high scores in all the other languages, i.e. French, German, Italian,
    Polish, and Russian. We also perform an extensive analysis of the data and the
    annotations to investigate how they can influence the quality of our systems.
  authors:
  - Antonio Purificato
  - Roberto Navigli
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_60
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 3: Detecting the Category, the Framing, and the Persuasion Techniques
    in Online News in a Multi-lingual Setup'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'APatt at SemEval-2023 Task 3: The Sapienza NLP System for Ensemble-based
    Multilingual Propaganda Detection'
  tldr: In this paper, we present our approach to the task of identification of persuasion
    techniques in text, which is a subtask of the SemEval-2023 Task 3 on the multilingual
    detection of genre, framing, and persuasion techniques in online news. The subtask
    is multi-label at the paragraph level and the in
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes the system we designed for our participation to SemEval2023
    Task 12 Track 6 about Algerian dialect sentiment analysis. We propose a transformer
    language model approach combined with a lexicon mixing terms and emojis which
    is used in a post-processing filtering stage. The Algerian sentiment lexicons
    was extracted manually from tweets. We report on our experiments on Algerian dialect,
    where we compare the performance of \textbackslash{}marbert to the one of \textbackslash{}arabicbert
    and \textbackslash{}camelbert on the training and development datasets of Task  12.
    We also analyse the contribution of our post processing lexical filtering for
    sentiment analysis. Our system obtained a F1 score equal to 70\textbackslash{}\%,
    ranking 9\textbackslash{}raise+.5ex\textbackslash{}hbox\{th\} among 30 participants.
  authors:
  - Faiza Belbachir
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_61
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 12: AfriSenti-SemEval: Sentiment Analysis for Low-resource African
    Languages using Twitter Dataset'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Foul at SemEval-2023 Task 12: MARBERT Language model and lexical filtering
    for sentiments analysis of tweets in Algerian Arabic'
  tldr: This paper describes the system we designed for our participation to SemEval2023
    Task 12 Track 6 about Algerian dialect sentiment analysis. We propose a transformer
    language model approach combined with a lexicon mixing terms and emojis which
    is used in a post-processing filtering stage. The Algeria
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes our system submitted for SemEval Task 7, Multi-Evidence
    Natural Language Inference for Clinical Trial Data. The task consists of 2 subtasks.
    Subtask 1 is to determine the relationships between clinical trial data (CTR)
    and statements. Subtask 2 is to output a set of supporting facts extracted from
    the premises with the input of CTR premises and statements. Through experiments,
    we found that our GPT2-based pre-trained models can obtain good results in Subtask
    2. Therefore, we use the GPT2-based pre-trained model to fine-tune Subtask 2.
    We transform the evidence retrieval task into a binary class task by combining
    premises and statements as input, and the output is whether the premises and statements
    match. We obtain a top-5 score in the evaluation phase of Subtask 2.
  authors:
  - Mingtong Huang
  - Junxiang Ren
  - Lang Liu
  - Ruilin Song
  - Wenbo Yin
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_62
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 7: Multi-Evidence Natural Language Inference for Clinical Trial
    Data'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'CPIC at SemEval-2023 Task 7: GPT2-Based Model for Multi-evidence Natural
    Language Inference for Clinical Trial Data'
  tldr: This paper describes our system submitted for SemEval Task 7, Multi-Evidence
    Natural Language Inference for Clinical Trial Data. The task consists of 2 subtasks.
    Subtask 1 is to determine the relationships between clinical trial data (CTR)
    and statements. Subtask 2 is to output a set of supporting f
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The objective of this shared task is to gain an understanding of legal
    texts, and it is beset with difficulties such as the comprehension of lengthy
    noisy legal documents, domain specificity as well as the scarcity of annotated
    data. To address these challenges, we propose a system that employs a hierarchical
    model and integrates domain-adaptive pretraining, data augmentation, and auxiliary-task
    learning techniques.  Moreover, to enhance generalization and robustness, we ensemble
    the models that utilize these diverse techniques. Our system ranked first on the
    RR sub-task and in the middle for the other two sub-tasks.
  authors:
  - Jingjing Huo
  - Kezun Zhang
  - Zhengyong Liu
  - Xuan Lin
  - Wenqiang Xu
  - Maozong Zheng
  - Zhaoguo Wang
  - Song Li
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_63
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 6: LegalEval: Understanding Legal Texts'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'AntContentTech at SemEval-2023 Task 6: Domain-adaptive Pretraining and Auxiliary-task
    Learning for Understanding Indian Legal Texts'
  tldr: The objective of this shared task is to gain an understanding of legal texts,
    and it is beset with difficulties such as the comprehension of lengthy noisy legal
    documents, domain specificity as well as the scarcity of annotated data. To address
    these challenges, we propose a system that employs a hi
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: SemEval-2023's Task 1, Visual Word Sense Disambiguation, a task about
    text semantics and visual semantics, selecting an image from a list of candidates,
    that best exhibits a given target word in a small context. We tried several methods,
    including the image captioning method and CLIP methods, and submitted our predictions
    in the competition for this task. This paper describes the methods we used and
    their performance and provides an analysis and discussion of the performance.
  authors:
  - Yuchen Wei
  - Milton King
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_64
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Task-1 - Visual Word Sense Disambiguation (Visual-WSD)
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'StFX NLP at SemEval-2023 Task 1: Multimodal Encoding-based Methods for Visual
    Word Sense Disambiguation'
  tldr: SemEval-2023's Task 1, Visual Word Sense Disambiguation, a task about text
    semantics and visual semantics, selecting an image from a list of candidates,
    that best exhibits a given target word in a small context. We tried several methods,
    including the image captioning method and CLIP methods, and su
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We propose an ensemble method that combines several pre-trained language
    models to enhance entity recognition in legal text. Our approach achieved a 90.9873\%
    F1 score on the private test set, ranking 2nd on the leaderboard for SemEval 2023
    Task 6, Subtask B - Legal Named Entities Extraction.
  authors:
  - Quang-Minh Tran
  - Xuan-Dung Doan
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_65
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 6: LegalEval: Understanding Legal Texts'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'VTCC-NER at SemEval-2023 Task 6: An Ensemble Pre-trained Language Models
    for Named Entity Recognition'
  tldr: We propose an ensemble method that combines several pre-trained language models
    to enhance entity recognition in legal text. Our approach achieved a 90.9873\%
    F1 score on the private test set, ranking 2nd on the leaderboard for SemEval 2023
    Task 6, Subtask B - Legal Named Entities Extraction.
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes our submission to SemEval-2023 Task 6, Subtask B,
    a shared task on performing Named Entity Recognition in legal documents for specific
    legal entity types. Documents are divided into the preamble and judgement texts,
    and certain entity types should only be tagged in one of the two text sections.
    To address this challenge, our team proposes a token classification model that
    is augmented with information about the document type, which achieves greater
    performance than the non-augmented system.
  authors:
  - Michael Ginn
  - Roman Khamov
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_66
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 6: LegalEval: Understanding Legal Texts'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Ginn-Khamov at SemEval-2023 Task 6, Subtask B: Legal Named Entities Extraction
    for Heterogenous Documents'
  tldr: This paper describes our submission to SemEval-2023 Task 6, Subtask B, a shared
    task on performing Named Entity Recognition in legal documents for specific legal
    entity types. Documents are divided into the preamble and judgement texts, and
    certain entity types should only be tagged in one of the tw
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This is our system description paper for ValueEval task.The title is:Mao-Zedong
    At SemEval-2023 Task 4: Label Represention Multi-Head Attention Model With Contrastive
    Learning-Enhanced Nearest Neighbor Mechanism For Multi-Label Text Classification,and
    the author is Che Zhang and Pingan Liu and ZhenyangXiao and HaojunFei. In this
    paper, we propose a model that combinesthe label-specific attention network with
    the contrastive learning-enhanced nearest neighbor mechanism.'
  authors:
  - Che Zhang
  - Ping'an Liu
  - Zhenyang Xiao
  - Haojun Fei
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_67
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 4: ValueEval: Identification of Human Values behind Arguments'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Mao-Zedong at SemEval-2023 Task 4: Label Represention Multi-Head Attention
    Model with Contrastive Learning-Enhanced Nearest Neighbor Mechanism for Multi-Label
    Text Classification'
  tldr: 'This is our system description paper for ValueEval task.The title is:Mao-Zedong
    At SemEval-2023 Task 4: Label Represention Multi-Head Attention Model With Contrastive
    Learning-Enhanced Nearest Neighbor Mechanism For Multi-Label Text Classification,and
    the author is Che Zhang and Pingan Liu and Zheny'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes the system and the resulting model submitted by our
    team "PCJ" to the SemEval-2023 Task 10 sub-task A contest. In this task, we need
    to test the English text content in the posts to determine whether there is sexism,
    which involves emotional text classification. Our submission system utilizes methods
    based on RoBERTa, SimCSE-RoBERTa pre-training models, and model ensemble to classify
    and train on datasets provided by the organizers. In the final assessment, our
    submission achieved a macro average F1 score of 0.8449, ranking 28th out of 84
    teams in Task A.
  authors:
  - Chujun Pu
  - Xiaobing Zhou
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_68
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 10: Towards Explainable Detection of Online Sexism'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'PCJ at SemEval-2023 Task 10: A Ensemble Model Based on Pre-trained Model
    for Sexism Detection and Classification in English'
  tldr: This paper describes the system and the resulting model submitted by our team
    "PCJ" to the SemEval-2023 Task 10 sub-task A contest. In this task, we need to
    test the English text content in the posts to determine whether there is sexism,
    which involves emotional text classification. Our submission s
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The Visual Word Sense Disambiguation (VWSD) shared task aims at selecting
    the image among candidates that best interprets the semantics of a target word
    with a short-length phrase for English, Italian, and Farsi. The limited phrase
    context, which only contains 2-3 words, challenges the model's understanding ability,
    and the visual label requires image-text matching performance across different
    modalities. In this paper, we propose a prompt based and multimodal retrieval
    enhanced VWSD system, which uses the rich potential knowledge of large-scale pretrained
    models by prompting and additional text-image information from knowledge bases
    and open datasets. Under the English situation and given an input phrase, (1)
    the context retrieval module predicts the correct definition from sense inventory
    by matching phrase and context through a biencoder architecture. (2) The image
    retrieval module retrieves the relevant images from an image dataset.(3) The matching
    module decides that either text or image is used to pair with image labels by
    a rule-based strategy, then ranks the candidate images according to the similarity
    score.Our system ranks first in the English track and second in the average of
    all languages (English, Italian, and Farsi).
  authors:
  - Xudong Zhang
  - Tiange Zhen
  - Jing Zhang
  - Yujin Wang
  - Song Liu
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_69
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Task-1 - Visual Word Sense Disambiguation (Visual-WSD)
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'SRCB at SemEval-2023 Task 1: Prompt Based and Cross-Modal Retrieval Enhanced
    Visual Word Sense Disambiguation'
  tldr: The Visual Word Sense Disambiguation (VWSD) shared task aims at selecting
    the image among candidates that best interprets the semantics of a target word
    with a short-length phrase for English, Italian, and Farsi. The limited phrase
    context, which only contains 2-3 words, challenges the model's under
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In recent years, there has been a vast increase in the available clinical
    data. Variant Deep learning techniques are used to enhance the retrieval and interpretation
    of these data. This task deployed  Natural language inference (NLI)  in  Clinical
    Trial Reports (CTRs) to provide individualized care that is supported by evidence.
    A collection of breast cancer clinical trial records, statements, annotations,
    and labels from experienced domain experts. NLI  presents a chance to advance
    the widespread understanding and retrieval of medical evidence, leading to significant
    improvements in connecting the most recent evidence to personalized care. The
    primary objective is to identify the inference relationship (entailment or contradiction)
    between pairs of clinical trial records and statements. In this research, we used
    different transformer-based models, and The proposed model, "Role-based Double
    Roberta-Large," achieved  the best result on the testing dataset with  F1-score
    equal to 67.0\%
  authors:
  - Kefah Alissa
  - Malak Abdullah
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_70
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Task-1 - Visual Word Sense Disambiguation (Visual-WSD)
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'JUST-KM at SemEval-2023 Task 7: Multi-evidence Natural Language Inference
    using Role-based Double Roberta-Large'
  tldr: 'In recent years, there has been a vast increase in the available clinical
    data. Variant Deep learning techniques are used to enhance the retrieval and interpretation
    of these data. This task deployed  Natural language inference (NLI)  in  Clinical
    Trial Reports (CTRs) to provide individualized care '
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Named Entity Recognition(NER) is a task ofrecognizing entities at a token
    level in a sen-tence. This paper focuses on solving NER tasksin a multilingual
    setting for complex named en-tities.Our team, LLM-RM participated in therecently
    organized SemEval 2023 task, Task 2:MultiCoNER II,Multilingual Complex NamedEntity
    Recognition. We approach the problemby leveraging cross-lingual representation
    pro-vided by fine-tuning XLM-Roberta base modelon datasets of all of the 12 languages
    provided - Bangla, Chinese, English, Farsi, French,German, Hindi, Italian, Portuguese,
    Spanish,Swedish and Ukrainian.
  authors:
  - Rahul Mehta
  - Vasudeva Varma
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_71
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 2: MultiCoNER II Multilingual Complex Named Entity Recognition'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'LLM-RM at SemEval-2023 Task 2: Multilingual Complex NER Using XLM-RoBERTa'
  tldr: Named Entity Recognition(NER) is a task ofrecognizing entities at a token
    level in a sen-tence. This paper focuses on solving NER tasksin a multilingual
    setting for complex named en-tities.Our team, LLM-RM participated in therecently
    organized SemEval 2023 task, Task 2:MultiCoNER II,Multilingual Com
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Visual Word Sense Disambiguation shared task at SemEval-2023 aims to identify
    an image corresponding to the intended meaning of a given ambiguous word (with
    related context) from a set of candidate images. The lack of textual description
    for the candidate image and the corresponding word's ambiguity makes it a challenging
    problem. This paper describes teamPN's multi-modal and modular approach to solving
    this in English track of the task. We efficiently used recent multi-modal pre-trained
    models backed by real-time multi-modal knowledge graphs to augment textual knowledge
    for the images and select the best matching image accordingly. We outperformed
    the baseline model by \textasciitilde{}5 points and proposed a unique approach
    that can further work as a framework for other modular and knowledge-backed solutions.
  authors:
  - Nikita Katyal
  - Pawan Rajpoot
  - Subhanandh Tamilarasu
  - Joy Mustafi
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_72
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Task-1 - Visual Word Sense Disambiguation (Visual-WSD)
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'teamPN at SemEval-2023 Task 1: Visual Word Sense Disambiguation Using Zero-Shot
    MultiModal Approach'
  tldr: Visual Word Sense Disambiguation shared task at SemEval-2023 aims to identify
    an image corresponding to the intended meaning of a given ambiguous word (with
    related context) from a set of candidate images. The lack of textual description
    for the candidate image and the corresponding word's ambiguity
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'The objective of the SemEval-2023 Task 1: Visual Word Sense Disambiguation
    (VWSD) is to identify the image illustrating the indented meaning of a target
    word and some minimal additional context. The omnipresence of textual and visual
    data in the task strongly suggests the utilization of the recent advances in multi-modal
    machine learning, i.e., pretrained visiolinguistic models (VLMs). Often referred
    to as foundation models due to their strong performance on many vision-language
    downstream tasks, these models further demonstrate powerful zero-shot capabilities.
    In this work, we utilize various pertained VLMs in a zero-shot fashion for multiple
    approaches using external knowledge sources to enrich the contextual information.
    Further, we evaluate our methods on the final test data and extensively analyze
    the suitability of different knowledge sources, the influence of training data,
    model sizes, multi-linguality, and different textual prompting strategies. Although
    we are not among the best-performing systems (rank 20 of 56), our experiments
    described in this work prove competitive results. Moreover, we aim to contribute
    meaningful insights and propel multi-modal machine learning tasks like VWSD.'
  authors:
  - Florian Schneider
  - Chris Biemann
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_73
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Task-1 - Visual Word Sense Disambiguation (Visual-WSD)
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'LT at SemEval-2023 Task 1: Effective Zero-Shot Visual Word Sense Disambiguation
    Approaches using External Knowledge Sources'
  tldr: 'The objective of the SemEval-2023 Task 1: Visual Word Sense Disambiguation
    (VWSD) is to identify the image illustrating the indented meaning of a target
    word and some minimal additional context. The omnipresence of textual and visual
    data in the task strongly suggests the utilization of the recent a'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Sexism has become a growing concern on social media platforms as it impacts
    the health of the internet and can have negative impacts on society.This paper
    describes the coco system that participated in SemEval-2023 Task 10, Explainable
    Detection of Online Sexism (EDOS), which aims at sexism detection in various settings
    of natural language understanding. We develop a novel neural framework for sexism
    detection and misogyny that can combine text representations obtained using pre-trained
    language model models such as Bidirectional Encoder Representations from Transformers
    and using BiLSTM architecture to obtain the local and global semantic information.Further,
    considering that the EDOS dataset is relatively small and extremely unbalanced,
    we conducted data augmentation and introduced two datasets in the field of sexism
    detection. Moreover, we introduced Focal Loss which is a loss function in order
    to improve the performance of processing imbalanced data classification. Our system
    achieved an F1 score of 78.95\textbackslash{}\% on Task A - binary sexism.
  authors:
  - Kangshuai Guo
  - Ruipeng Ma
  - Shichao Luo
  - Yan Wang
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_75
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 10: Towards Explainable Detection of Online Sexism'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Coco at SemEval-2023 Task 10: Explainable Detection of Online Sexism'
  tldr: Sexism has become a growing concern on social media platforms as it impacts
    the health of the internet and can have negative impacts on society.This paper
    describes the coco system that participated in SemEval-2023 Task 10, Explainable
    Detection of Online Sexism (EDOS), which aims at sexism detectio
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'In this paper, we present a possible solution to the SemEval23 shared
    task of generating spoilers for clickbait headlines. Using a Zero-Shot approach
    with two different Transformer architectures, BLOOM and RoBERTa, we generate three
    different types of spoilers: phrase, passage and multi. We found, RoBERTa pretrained
    for Question-Answering to perform better than BLOOM for causal language modelling,
    however both architectures proved promising for future attempts at such tasks.'
  authors:
  - Niels Krog
  - Manex Agirrezabal
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_76
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 5: Clickbait Spoiling'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Diane Simmons at SemEval-2023 Task 5: Is it possible to make good clickbait
    spoilers using a Zero-Shot approach? Check it out!'
  tldr: 'In this paper, we present a possible solution to the SemEval23 shared task
    of generating spoilers for clickbait headlines. Using a Zero-Shot approach with
    two different Transformer architectures, BLOOM and RoBERTa, we generate three
    different types of spoilers: phrase, passage and multi. We found, R'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This article presents our solution for SemEval-2023 Task 1: Visual Word
    Sense Disambiguation. The aim of the task was to select the most suitable from
    a list of ten images for a given word, extended by a small textual context. Our
    solution comprises two parts. The first focuses on an attempt to further extend
    the textual context, based on word definitions contained in WordNet and in Open
    English WordNet. The second focuses on selecting the most suitable image using
    the CLIP model with previously developed word context and additional information
    obtained from the BEiT image classification model. Our solution allowed us to
    achieve a result of 70.84\% on the official test dataset for the English language.'
  authors:
  - Magorzata Grbowiec
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_77
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Task-1 - Visual Word Sense Disambiguation (Visual-WSD)
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'OPI PIB at SemEval-2023 Task 1: A CLIP-based Solution Paired with an Additional
    Word Context Extension'
  tldr: 'This article presents our solution for SemEval-2023 Task 1: Visual Word Sense
    Disambiguation. The aim of the task was to select the most suitable from a list
    of ten images for a given word, extended by a small textual context. Our solution
    comprises two parts. The first focuses on an attempt to furt'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper describes our system developed for the SemEval-2023 Task 12
    ``Sentiment Analysis for Low-resource African Languages using Twitter Dataset''''.
    Sentiment analysis is one of the most widely studied applications in natural language
    processing. However, most prior work still focuses on a small number of high-resource
    languages. Building reliable sentiment analysis systems for low-resource languages
    remains challenging, due to the limited training data in this task. In this work,
    we propose to leverage language-adaptive and task-adaptive pretraining on African
    texts and study transfer learning with source language selection on top of an
    African language-centric pretrained language model.Our key findings are: (1) Adapting
    the pretrained model to the target language and task using a small yet relevant
    corpus improves performance remarkably by more than 10 F1 score points. (2) Selecting
    source languages with positive transfer gains during training can avoid harmful
    interference from dissimilar languages, leading to better results in multilingual
    and cross-lingual settings. In the shared task, our system wins 8 out of 15 tracks
    and, in particular, performs best in the multilingual evaluation.'
  authors:
  - Mingyang Wang
  - Heike Adel
  - Lukas Lange
  - "Jannik Str\xF6tgen"
  - "Hinrich Sch\xFCtze"
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_78
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 12: AfriSenti-SemEval: Sentiment Analysis for Low-resource African
    Languages using Twitter Dataset'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'NLNDE at SemEval-2023 Task 12: Adaptive Pretraining and Source Language
    Selection for Low-Resource Multilingual Sentiment Analysis'
  tldr: 'This paper describes our system developed for the SemEval-2023 Task 12 ``Sentiment
    Analysis for Low-resource African Languages using Twitter Dataset''''. Sentiment
    analysis is one of the most widely studied applications in natural language processing.
    However, most prior work still focuses on a small '
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper describes our system on SemEval-2023 Task 10: Explainable
    Detection of Online Sexism (EDOS). This work aims to design an automatic system
    for detecting and classifying sexist content in online spaces. We propose a set
    of transformer-based pre-trained models with task-adaptive pretraining and ensemble
    learning. The main contributions of our system include analyzing the performance
    of different transformer-based pre-trained models and combining these models,
    as well as providing an efficient method using large amounts of unlabeled data
    for model adaptive pretraining. We have also explored several other strategies.
    On the test dataset, our system achieves F1-scores of 83\%, 64\%, and 47\% on
    subtasks A, B, and C, respectively.'
  authors:
  - Hadiseh Mahmoudi
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_79
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 10: Towards Explainable Detection of Online Sexism'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'IUST_NLP at SemEval-2023 Task 10: Explainable Detecting Sexism with Transformers
    and Task-adaptive Pretraining'
  tldr: 'This paper describes our system on SemEval-2023 Task 10: Explainable Detection
    of Online Sexism (EDOS). This work aims to design an automatic system for detecting
    and classifying sexist content in online spaces. We propose a set of transformer-based
    pre-trained models with task-adaptive pretraining '
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Visual Word Sense Disambiguation (WSD), as a fine-grained image-text retrieval
    task, aims to identify the images that are relevant to ambiguous target words
    or phrases. However, the difficulties of limited contextual information and cross-linguistic
    background knowledge in text processing make this task challenging. To alleviate
    this issue, we propose a Fine-grained Contrastive Language-Image Learning (FCLL)
    model, which learns fine-grained image-text knowledge by employing a new fine-grained
    contrastive learning mechanism and enriches contextual information by establishing
    relationship between concepts and sentences. In addition, a new multimodal-multilingual
    knowledge base involving ambiguous target words is constructed for visual WSD.
    Experiment results on the benchmark datasets from SemEval-2023 Task 1 show that
    our FCLL ranks at the first in overall evaluation with an average H@1 of 72.56\%
    and an average MRR of 82.22\%. The results demonstrate that FCLL is effective
    in inference on fine-grained language-vision knowledge. Source codes and the knowledge
    base are publicly available at https://github.com/CharlesYang030/FCLL.
  authors:
  - Qihao Yang
  - Yong Li
  - Xuelin Wang
  - Shunhao Li
  - Tianyong Hao
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_80
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Task-1 - Visual Word Sense Disambiguation (Visual-WSD)
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'TAM of SCNU at SemEval-2023 Task 1: FCLL: A Fine-grained Contrastive Language-Image
    Learning Model for Cross-language Visual Word Sense Disambiguation'
  tldr: 'Visual Word Sense Disambiguation (WSD), as a fine-grained image-text retrieval
    task, aims to identify the images that are relevant to ambiguous target words
    or phrases. However, the difficulties of limited contextual information and cross-linguistic
    background knowledge in text processing make this '
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper describes our contribution to SemEval-23 Shared Task 12: ArfiSenti.
    The task consists of several sentiment classification subtasks for rarely studied
    African languages to predict positive, negative, or neutral classes of a given
    Twitter dataset. In our system we utilized three different models; FastText, MultiLang
    Transformers, and Language-Specific Transformers to find the best working model
    for the classification challenge. We experimented with mentioned models and mostly
    reached the best prediction scores using the Language Specific Transformers. Our
    best-submitted result was ranked 3rd among submissions for the Amharic language,
    obtaining an F1 score of 0.702 behind the second-ranked system.'
  authors:
  - Selman Delil
  - Birol Kuyumcu
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_81
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 12: AfriSenti-SemEval: Sentiment Analysis for Low-resource African
    Languages using Twitter Dataset'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Sefamerve at SemEval-2023 Task 12: Semantic Evaluation of Rarely Studied
    Languages'
  tldr: 'This paper describes our contribution to SemEval-23 Shared Task 12: ArfiSenti.
    The task consists of several sentiment classification subtasks for rarely studied
    African languages to predict positive, negative, or neutral classes of a given
    Twitter dataset. In our system we utilized three different m'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'The growth of pending legal cases in populouscountries, such as India,
    has become a major is-sue. Developing effective techniques to processand understand
    legal documents is extremelyuseful in resolving this problem. In this pa-per,
    we present our systems for SemEval-2023Task 6: understanding legal texts (Modi
    et al., 2023). Specifically, we first develop the Legal-BERT-HSLN model that considers
    the com-prehensive context information in both intra-and inter-sentence levels
    to predict rhetoricalroles (subtask A) and then train a Legal-LUKEmodel, which
    is legal-contextualized and entity-aware, to recognize legal entities (subtask
    B).Our evaluations demonstrate that our designedmodels are more accurate than
    baselines, e.g.,with an up to 15.0\% better F1 score in subtaskB. We achieved
    notable performance in the taskleaderboard, e.g., 0.834 micro F1 score, andranked
    No.5 out of 27 teams in subtask A.'
  authors:
  - Xin Jin
  - Yuchen Wang
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_82
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 6: LegalEval: Understanding Legal Texts'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'TeamShakespeare at SemEval-2023 Task 6: Understand Legal Documents with
    Contextualized Large Language Models'
  tldr: 'The growth of pending legal cases in populouscountries, such as India, has
    become a major is-sue. Developing effective techniques to processand understand
    legal documents is extremelyuseful in resolving this problem. In this pa-per,
    we present our systems for SemEval-2023Task 6: understanding legal '
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'The problem of online sexism, which refers to offensive content targeting
    women based on their gender or the intersection of their gender with one or more
    additional identity characteristics, such as race or religion, has become a widespread
    phenomenon on social media. This can include sexist comments and memes. To address
    this issue, the SemEval-2023 international workshop introduced the "Explainable
    Detection of Online Sexism Challenge", which aims to explain the classifications
    given by AI models for detecting sexism. In this paper, we present the contributions
    of our team, JUST\textbackslash{}\_ONE, to all three sub-tasks of the challenge:
    subtask A, a binary classification task; subtask B, a four-class classification
    task; and subtask C, a fine-grained classification task. To accomplish this, we
    utilized pre-trained language models, specifically BERT and RoBERTa from Hugging
    Face, and a selective ensemble method in task 10 of the SemEval 2023 competition.
    As a result, our team achieved the following rankings and scores in different
    tasks: 19th out of 84 with a Macro-F1 score of 0.8538 in task A, 22nd out of 69
    with a Macro-F1 score of 0.6417 in task B, and 14th out of 63 with a Macro-F1
    score of 0.4774 in task C.'
  authors:
  - Doaa Obeidat
  - Wala'a Shnaigat
  - Heba Nammas
  - Malak Abdullah
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_83
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 10: Towards Explainable Detection of Online Sexism'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'JUST_ONE at SemEval-2023 Task 10: Explainable Detection of Online Sexism
    (EDOS)'
  tldr: The problem of online sexism, which refers to offensive content targeting
    women based on their gender or the intersection of their gender with one or more
    additional identity characteristics, such as race or religion, has become a widespread
    phenomenon on social media. This can include sexist commen
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper presents the best-performing approach alias "Adam Smith" for
    the SemEval-2023 Task 4: "Identification of Human Values behind Arguments". The
    goal of the task was to create systems that automatically identify the values
    within textual arguments. We train transformer-based models until they reach their
    loss minimum or f1-score maximum. Ensembling the models by selecting one global
    decision threshold that maximizes the f1-score leads to the best-performing system
    in the competition. Ensembling based on stacking with logistic regressions shows
    the best performance on an additional dataset provided to evaluate the robustness
    ("Nahj al-Balagha"). Apart from outlining the submitted system, we demonstrate
    that the use of the large ensemble model is not necessary and that the system
    size can be significantly reduced.'
  authors:
  - Daniel Schroter
  - Daryna Dementieva
  - Georg Groh
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_84
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 4: ValueEval: Identification of Human Values behind Arguments'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Adam-Smith at SemEval-2023 Task 4: Discovering Human Values in Arguments
    with Ensembles of Transformer-based Models'
  tldr: 'This paper presents the best-performing approach alias "Adam Smith" for the
    SemEval-2023 Task 4: "Identification of Human Values behind Arguments". The goal
    of the task was to create systems that automatically identify the values within
    textual arguments. We train transformer-based models until they'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper presents our participation to the "Human Value Detection shared
    task (Kiesel et al., 2023), as "Andronicus of Rhodes. We describe the approaches
    behind each entry in the official evaluation, along with the motivation behind
    each approach. Our best-performing approach has been based on BERT large, with
    4 classification heads, implementing two different classification approaches (with
    different activation and loss functions), and two different partitioning of the
    training data, to handle class imbalance. Classification is performed through
    majority voting. The proposed approach outperforms the BERT baseline, ranking
    in the upper half of the competition.
  authors:
  - Georgios Papadopoulos
  - Marko Kokol
  - Maria Dagioglou
  - Georgios Petasis
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_85
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 4: ValueEval: Identification of Human Values behind Arguments'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Andronicus of Rhodes at SemEval-2023 Task 4: Transformer-Based Human Value
    Detection Using Four Different Neural Network Architectures'
  tldr: This paper presents our participation to the "Human Value Detection shared
    task (Kiesel et al., 2023), as "Andronicus of Rhodes. We describe the approaches
    behind each entry in the official evaluation, along with the motivation behind
    each approach. Our best-performing approach has been based on BER
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We report our participation in the SemEval-2023 shared task on propaganda
    detection and describe our solutions with pre-trained models and their ensembles.
    For Subtask 1 (News Genre Categorisation), we report the impact of several settings,
    such as the choice of the classification models (monolingual or multilingual or
    their ensembles), the choice of the training sets (base or additional sources),
    the impact of detection certainty in making a classification decision as well
    as the impact of other hyper-parameters.In particular, we fine-tune models on
    additional data for other genre classification tasks, such as FTD. We also try
    adding texts from genre-homogenous corpora, such as Panorama, Babylon Bee for
    satire and Giganews for for reporting texts.We also make prepared models for Subtasks
    2 and 3 with finetuning the corresponding models first for Subtask 1.The code
    needed to reproduce the experiments is available.
  authors:
  - Mikhail Lepekhin
  - Serge Sharoff
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_87
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 3: Detecting the Category, the Framing, and the Persuasion Techniques
    in Online News in a Multi-lingual Setup'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'FTD at SemEval-2023 Task 3: News Genre and Propaganda Detection by Comparing
    Mono- and Multilingual Models with Fine-tuning on Additional Data'
  tldr: We report our participation in the SemEval-2023 shared task on propaganda
    detection and describe our solutions with pre-trained models and their ensembles.
    For Subtask 1 (News Genre Categorisation), we report the impact of several settings,
    such as the choice of the classification models (monolingua
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes the participation of the research laboratory MIND,
    at the University of Milano-Bicocca, in the SemEval 2023 task related to Learning
    With Disagreements (Le-Wi-Di). The main goal is to identify the level of agreement/disagreement
    from a collection of textual datasets with different characteristics in terms
    of style, language and task.The proposed approach is grounded on the hypothesis
    that the disagreement between annotators could be grasped by the uncertainty that
    a model, based on several linguistic characteristics, could have on the prediction
    of a given gold label.
  authors:
  - Giulia Rizzi
  - Alessandro Astorino
  - Daniel Scalena
  - Paolo Rosso
  - Elisabetta Fersini
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_88
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 11: Learning with Disagreements (Le-Wi-Di)'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'MIND at SemEval-2023 Task 11: From Uncertain Predictions to Subjective Disagreement'
  tldr: This paper describes the participation of the research laboratory MIND, at
    the University of Milano-Bicocca, in the SemEval 2023 task related to Learning
    With Disagreements (Le-Wi-Di). The main goal is to identify the level of agreement/disagreement
    from a collection of textual datasets with differe
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper presents the system developed by the Sartipi-Sedighin team
    for SemEval 2023 Task 2, which is a shared task focused on multilingual complex
    named entity recognition (NER), or MultiCoNER II. The goal of this task is to
    identify and classify complex named entities (NEs) in text across multiple languages.
    To tackle the MultiCoNER II task, we leveraged pre-trained language models (PLMs)
    fine-tuned for each language included in the dataset. In addition, we also applied
    a data augmentation technique to increase the amount of training data available
    to our models. Specifically, we searched for relevant NEs that already existed
    in the training data within Wikipedia, and we added new instances of these entities
    to our training corpus.Our team achieved an overall F1 score of 61.25\% in the
    English track and 71.79\% in the multilingual track across all 13 tracks of the
    shared task that we submitted to.
  authors:
  - Amir Sartipi
  - Amirreza Sedighin
  - Afsaneh Fatemi
  - Hamidreza Baradaran Kashani
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_89
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 2: MultiCoNER II Multilingual Complex Named Entity Recognition'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Sartipi-Sedighin at SemEval-2023 Task 2: Fine-grained Named Entity Recognition
    with Pre-trained Contextual Language Models and Data Augmentation from Wikipedia'
  tldr: This paper presents the system developed by the Sartipi-Sedighin team for
    SemEval 2023 Task 2, which is a shared task focused on multilingual complex named
    entity recognition (NER), or MultiCoNER II. The goal of this task is to identify
    and classify complex named entities (NEs) in text across multip
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'We describe the methods we used for legal text understanding, specifically
    Task 6 Legal-Eval at SemEval 2023. The outcomes  could assist law practitioners
    and help automate the working process of judicial systems. The shared task defined
    three main sub-tasks: sub-task A, Rhetorical Roles Prediction (RR); sub-task B,
    Legal Named Entities Extraction (L-NER); and sub-task C, Court Judgement Prediction
    with Explanation (CJPE). Our team addressed all three sub-tasks by exploring various
    Deep Learning (DL) based models. Overall, our team''s approaches achieved promising
    results on all three sub-tasks, demonstrating the potential of deep learning-based
    models in the judicial domain.'
  authors:
  - Intisar Almuslim
  - Sean Stilwell
  - Surya Kiran Suresh
  - Diana Inkpen
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_90
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 6: LegalEval: Understanding Legal Texts'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'uOttawa at SemEval-2023 Task 6: Deep Learning for Legal Text Understanding'
  tldr: 'We describe the methods we used for legal text understanding, specifically
    Task 6 Legal-Eval at SemEval 2023. The outcomes  could assist law practitioners
    and help automate the working process of judicial systems. The shared task defined
    three main sub-tasks: sub-task A, Rhetorical Roles Prediction '
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In this manuscript, we describe the participation of UMUTeam in the Explainable
    Detection of Online Sexism shared task proposed at SemEval 2023. This task concerns
    the precise and explainable detection of sexist content on Gab and Reddit, i.e.,
    developing detailed classifiers that not only identify what is sexist, but also
    explain why it is sexism. Our participation in the three EDOS subtasks is based
    on extending new unlabeled sexism data in the Masked Language Model task of a
    pre-trained model, such as RoBERTa-large to improve its generalization capacity
    and its performance on classification tasks. Once the model has been pre-trained
    with the new data, fine-tuning of this model is performed for different specific
    sexism classification tasks. Our system has achieved excellent results in this
    competitive task, reaching top 24 (84) in Task A, top 23 (69) in Task B, and top
    13 (63) in Task C.
  authors:
  - Ronghao Pan
  - "Jos\xE9 Antonio Garc\xEDa-D\xEDaz"
  - "Salud Mar\xEDa Jim\xE9nez Zafra"
  - "Rafael Valencia-Garc\xEDa"
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_91
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 10: Towards Explainable Detection of Online Sexism'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'UMUTeam at SemEval-2023 Task 10: Fine-grained detection of sexism in English'
  tldr: 'In this manuscript, we describe the participation of UMUTeam in the Explainable
    Detection of Online Sexism shared task proposed at SemEval 2023. This task concerns
    the precise and explainable detection of sexist content on Gab and Reddit, i.e.,
    developing detailed classifiers that not only identify '
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'The paper describes the SemEval-2023 Task 10: "Explainable Detection
    of Online Sexism (EDOS)", which investigates the detection of sexism on two social
    media sites, Gab and Reddit, by encouraging the development of machine learning
    models that perform binary and multi-class classification on English texts. The
    EDOS Task consisted of three hierarchical sub-tasks: binary sexism detection in
    sub-task A, category of sexism detection in sub-task B and fine-grained vector
    of sexism detection in sub-task C. My participation in EDOS comprised fine-tuning
    of different layer representations of Transformer-based pre-trained language models,
    namely BERT, AlBERT and RoBERTa, and ensemble learning via majority voting of
    the best performing models. Despite the low rank mainly due to a submission error,
    the system employed the largest version of the aforementioned Transformer models
    (BERT-Large, ALBERT-XXLarge-v1, ALBERT-XXLarge-v2, RoBERTa-Large), experimented
    with their multi-layer structure and aggregated their predictions so as to get
    the final result. My predictions on the test sets achieved 82.88\%, 63.77\% and
    43.08\% Macro-F1 score in sub-tasks A, B and C respectively.'
  authors:
  - Christina Christodoulou
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_92
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 10: Towards Explainable Detection of Online Sexism'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'NLP_CHRISTINE at SemEval-2023 Task 10: Utilizing Transformer Contextual
    Representations and Ensemble Learning for Sexism Detection on Social Media Texts'
  tldr: 'The paper describes the SemEval-2023 Task 10: "Explainable Detection of Online
    Sexism (EDOS)", which investigates the detection of sexism on two social media
    sites, Gab and Reddit, by encouraging the development of machine learning models
    that perform binary and multi-class classification on English'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Human values are of great concern to social sciences which refer to when
    people have different beliefs and priorities of what is generally worth striving
    for and how to do so. This paper presents an approach for human value argument
    mining using contrastive learning to leverage the isotropy of language models.
    We fine-tuned DeBERTa-Large in a multi-label classification fashion and achieved
    an F1 score of 49\% for the task, resulting in a rank of 11. Our proposed model
    provides a valuable tool for analyzing arguments related to human values and highlights
    the significance of leveraging the isotropy of large language models for identifying
    human values.
  authors:
  - Milad Molazadeh Oskuee
  - Mostafa Rahgouy
  - Hamed Babaei Giglou
  - Cheryl D Seals
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_93
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 4: ValueEval: Identification of Human Values behind Arguments'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'T.M. Scanlon at SemEval-2023 Task 4: Leveraging Pretrained Language Models
    for Human Value Argument Mining with Contrastive Learning'
  tldr: Human values are of great concern to social sciences which refer to when people
    have different beliefs and priorities of what is generally worth striving for
    and how to do so. This paper presents an approach for human value argument mining
    using contrastive learning to leverage the isotropy of langu
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In this manuscript, we describe the participation of the UMUTeam in SemEval-2023
    Task 3, a shared task on detecting different aspects of news articles and other
    web documents, such as document category, framing dimensions, and persuasion technique
    in a multilingual setup. The task has been organized into three related subtasks,
    and we have been involved in the first two. Our approach is based on a fine-tuned
    multilingual transformer-based model that uses the dataset of all languages at
    once and a sentence transformer model to extract the most relevant chunk of a
    text for subtasks 1 and 2. The input data was truncated to 200 tokens with 50
    overlaps using the sentence-transformer model to obtain the subset of text most
    related to the articles' titles. Our system has performed good results in subtask
    1 in most languages, and in some cases, such as French and German, we have archived
    first place in the official leader board. As for task 2, our system has also performed
    very well in all languages, ranking in all the top 10.
  authors:
  - Ronghao Pan
  - "Jos\xE9 Antonio Garc\xEDa-D\xEDaz"
  - "Miguel \xC1ngel Rodr\xEDguez-Garc\xEDa"
  - "Rafael Valencia-Garc\xEDa"
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_94
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 3: Detecting the Category, the Framing, and the Persuasion Techniques
    in Online News in a Multi-lingual Setup'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'UMUTeam at SemEval-2023 Task 3: Multilingual transformer-based model for
    detecting the Genre, the Framing, and the Persuasion Techniques in Online News'
  tldr: In this manuscript, we describe the participation of the UMUTeam in SemEval-2023
    Task 3, a shared task on detecting different aspects of news articles and other
    web documents, such as document category, framing dimensions, and persuasion technique
    in a multilingual setup. The task has been organized
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'In this paper, we proposed and explored the impact of four different
    dataset augmentation andextension strategies that we used for solving the subtask
    3 of SemEval-2023 Task 3: multi-label persuasion techniques classification in
    a multi-lingual context. We consider two types of augmentation methods (one based
    on a modified version of synonym replacement and one based on translations) and
    two ways of extending the training dataset (using filtered data generated by GPT-3
    and using a dataset from a previous competition). We studied the effects of the
    aforementioned techniques by using theaugmented and/or extended training dataset
    to fine-tune a pretrained XLM-RoBERTa-Large model. Using the augmentation methods
    alone, we managed to obtain 3rd place for English, 13th place for Italian and
    between the 5th to 9th places for the other 7 languages during the competition.'
  authors:
  - Sergiu Amihaesei
  - Laura Cornei
  - George Stoica
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_95
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 3: Detecting the Category, the Framing, and the Persuasion Techniques
    in Online News in a Multi-lingual Setup'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Appeal for Attention at SemEval-2023 Task 3: Data augmentation extension
    strategies for detection of online news persuasion techniques'
  tldr: 'In this paper, we proposed and explored the impact of four different dataset
    augmentation andextension strategies that we used for solving the subtask 3 of
    SemEval-2023 Task 3: multi-label persuasion techniques classification in a multi-lingual
    context. We consider two types of augmentation methods '
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'In this manuscript, we describe the participation of the UMUTeam in SemEval-2023
    Task 5, namely, Clickbait Spoiling, a shared task on identifying spoiler type
    (i.e., a phrase or a passage) and generating short texts that satisfy curiosity
    induced by a clickbait post, i.e. generating spoilers for the clickbait post.
    Our participation in Task 1 is based on fine-tuning pre-trained models, which
    consists in taking a pre-trained model and tuning it to fit the spoiler classification
    task. Our system has obtained excellent results in Task 1: we outperformed all
    proposed baselines, being within the Top 10 for most measures. Foremost, we reached
    Top 3 in F1 score in the passage spoiler ranking.'
  authors:
  - Ronghao Pan
  - "Jos\xE9 Antonio Garc\xEDa-D\xEDaz"
  - "Franciso Garc\xEDa-S\xE1nchez"
  - "Rafael Valencia-Garc\xEDa"
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_96
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Task-1 - Visual Word Sense Disambiguation (Visual-WSD)
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Chick Adams at SemEval-2023 Task 5: Using RoBERTa and DeBERTa to Extract
    Post and Document-based Features for Clickbait Spoiling'
  tldr: In this manuscript, we describe the participation of the UMUTeam in SemEval-2023
    Task 5, namely, Clickbait Spoiling, a shared task on identifying spoiler type
    (i.e., a phrase or a passage) and generating short texts that satisfy curiosity
    induced by a clickbait post, i.e. generating spoilers for the
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper presents the best-performing solution to the SemEval 2023 Task
    3 on the subtask 3 dedicated to persuasion techniques detection. Due to a high
    multilingual character of the input data and a large number of 23 predicted labels
    (causing a lack of labelled data for some language-label combinations), we opted
    for fine-tuning pre-trained transformer-based language models. Conducting multiple
    experiments, we find the best configuration, which consists of large multilingual
    model (XLM-RoBERTa large) trained jointly on all input data, with carefully calibrated
    confidence thresholds for seen and surprise languages separately. Our final system
    performed the best on 6 out of 9 languages (including two surprise languages)
    and achieved highly competitive results on the remaining three languages.
  authors:
  - Timo Hromadka
  - Timotej Smolen
  - Tomas Remis
  - Branislav Pecher
  - Ivan Srba
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_97
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 3: Detecting the Category, the Framing, and the Persuasion Techniques
    in Online News in a Multi-lingual Setup'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'KInITVeraAI at SemEval-2023 Task 3: Simple yet Powerful Multilingual Fine-Tuning
    for Persuasion Techniques Detection'
  tldr: This paper presents the best-performing solution to the SemEval 2023 Task
    3 on the subtask 3 dedicated to persuasion techniques detection. Due to a high
    multilingual character of the input data and a large number of 23 predicted labels
    (causing a lack of labelled data for some language-label combina
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Intimacy is one of the fundamental aspects of our social life. It relates
    to intimate interactions with others, often including verbal self-disclosure.
    In this paper, we researched  machine learning algorithms for quantification of
    the intimacy in the tweets. A new multilingual textual intimacy dataset named
    MINT was used. It contains tweets in 10 languages, including English, Spanish,
    French, Portuguese, Italian, and Chinese in both training and test datasets, and
    Dutch, Korean, Hindi, and Arabic in test data only. In the first experiment, linear
    regression models combine with the features and word embedding, and XLM-T deep
    learning model were compared. In the second experiment, cross-lingual learning
    between languanges was tested. In the third experiments, data was clustered  using
    K-means. The results indicate that XLM-T pre-trained embedding  might be a good
    choice for an unsupervised learning algorithm for intimacy detection.
  authors:
  - "Jelena Lazi\u0107"
  - "Sanja Vujnovi\u0107"
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_99
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 9: Multilingual Tweet Intimacy Analysis'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'jelenasteam at SemEval-2023 Task 9: Quantification of Intimacy in Multilingual
    Tweets using Machine Learning Algorithms: A Comparative Study on the MINT Dataset'
  tldr: Intimacy is one of the fundamental aspects of our social life. It relates
    to intimate interactions with others, often including verbal self-disclosure.
    In this paper, we researched  machine learning algorithms for quantification of
    the intimacy in the tweets. A new multilingual textual intimacy data
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper introduces our participating system to the Explainable Detection
    of Online Sexism (EDOS) SemEval-2023 - Task 10: Explainable Detection of Online
    Sexism. The EDOS shared task covers three hierarchical sub-tasks for sexism detection,
    coarse-grained and fine-grained categorization. We have investigated both single-task
    and multi-task learning based on RoBERTa transformer-based language models. For
    improving the results, we have performed further pre-training of RoBERTa on the
    provided unlabeled data. Besides, we have employed a small sample of the unlabeled
    data for semi-supervised learning using the minimum class-confusion loss. Our
    system has achieved macro F1 scores of 82.25\textbackslash{}\%, 67.35\textbackslash{}\%,
    and 49.8\textbackslash{}\% on Tasks A, B, and C, respectively.'
  authors:
  - Salima Lamsiyah
  - Abdelkader El Mahdaouy
  - Hamza Alami
  - Ismail Berrada
  - Christoph Schommer
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_101
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 10: Towards Explainable Detection of Online Sexism'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'UL & UM6P at SemEval-2023 Task 10: Semi-Supervised Multi-task Learning for
    Explainable Detection of Online Sexism'
  tldr: 'This paper introduces our participating system to the Explainable Detection
    of Online Sexism (EDOS) SemEval-2023 - Task 10: Explainable Detection of Online
    Sexism. The EDOS shared task covers three hierarchical sub-tasks for sexism detection,
    coarse-grained and fine-grained categorization. We have i'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes the system developed by the USTC-NELSLIP team for
    SemEval-2023 Task 2 Multilingual Complex Named Entity Recognition (MultiCoNER
    II). We propose a method named Statistical Construction and Dual Adaptation of
    Gazetteer (SCDAG) for Multilingual Complex NER. The method first utilizes a statistics-based
    approach to construct a gazetteer. Secondly, the representations of gazetteer
    networks and language models are adapted by minimizing the KL divergence between
    them at the sentence-level and entity-level. Finally, these two networks are then
    integrated for supervised named entity recognition (NER) training. The proposed
    method is applied to several state-of-the-art Transformer-based NER models with
    a gazetteer built from Wikidata, and shows great generalization ability across
    them. The final predictions are derived from an ensemble of these trained models.
    Experimental results and detailed analysis verify the effectiveness of the proposed
    method. The official results show that our system ranked 1st on one track (Hindi)
    in this task.
  authors:
  - Jun-Yu Ma
  - Jia-Chen Gu
  - Jiajun Qi
  - Zhenhua Ling
  - Quan Liu
  - Xiaoyi Zhao
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_102
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 2: MultiCoNER II Multilingual Complex Named Entity Recognition'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'USTC-NELSLIP at SemEval-2023 Task 2: Statistical Construction and Dual Adaptation
    of Gazetteer for Multilingual Complex NER'
  tldr: This paper describes the system developed by the USTC-NELSLIP team for SemEval-2023
    Task 2 Multilingual Complex Named Entity Recognition (MultiCoNER II). We propose
    a method named Statistical Construction and Dual Adaptation of Gazetteer (SCDAG)
    for Multilingual Complex NER. The method first utilize
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'The subtle human values we acquire through life experiences govern our
    thoughts and gets reflected in our speech. It plays an integral part in capturing
    the essence of our individuality and making it imperative to identify such values
    in computational systems that mimic human actions. Computational argumentation
    is a field that deals with the argumentation capabilities of humans and can benefit
    from identifying such values. Motivated by that, we present an ensemble approach
    for detecting human values from argument text. Our ensemble comprises three models:
    (i) An entailment-based model for determining the human values based on their
    descriptions, (ii) A Roberta-based classifier that predicts the set of human values
    from an argument. (iii) A Roberta-based classifier to predict a reduced set of
    human values from an argument. We experiment with different ways of combining
    the models and report our results. Furthermore, our best combination achieves
    an overall F1 score of 0.48 on the main test set.'
  authors:
  - Sougata Saha
  - Rohini Srihari
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_103
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 4: ValueEval: Identification of Human Values behind Arguments'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Rudolf Christoph Eucken at SemEval-2023 Task 4: An Ensemble Approach for
    Identifying Human Values from Arguments'
  tldr: 'The subtle human values we acquire through life experiences govern our thoughts
    and gets reflected in our speech. It plays an integral part in capturing the essence
    of our individuality and making it imperative to identify such values in computational
    systems that mimic human actions. Computational '
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper describes the system for the YNU-HPCC team in subtask 1 of
    the SemEval-2023 Task 7: Multi-evidence Natural Language Inference for Clinical
    Trial Data (NLI4CT). This task requires judging the textual entailment relationship
    between the given CTR and the statement annotated by the expert annotator. This
    system is based on the fine-tuned Bi-directional Encoder Representation from Transformers
    for Biomedical Text Mining (BioBERT) model with supervised contrastive learning
    and back translation. Supervised contrastive learning is to enhance the classification,
    and back translation is to enhance the training data. Our system achieved relatively
    good results on the competition''s official leaderboard. The code of this paper
    is available at https://github.com/facanhe/SemEval-2023-Task7.'
  authors:
  - Chao Feng
  - Jin Wang
  - Xuejie Zhang
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_104
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 7: Multi-Evidence Natural Language Inference for Clinical Trial
    Data'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'YNU-HPCC at SemEval-2023 Task7: Multi-evidence Natural Language Inference
    for Clinical Trial Data Based a BioBERT Model'
  tldr: 'This paper describes the system for the YNU-HPCC team in subtask 1 of the
    SemEval-2023 Task 7: Multi-evidence Natural Language Inference for Clinical Trial
    Data (NLI4CT). This task requires judging the textual entailment relationship
    between the given CTR and the statement annotated by the expert an'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The MultiCoNER II shared task aims at detecting semantically ambiguous
    and complex named entities in short and low-context settings for multiple languages.
    The lack of context makes the recognition of ambiguous named entities challenging.
    To alleviate this issue, our team SRCB proposes an external knowledge based system,
    where we utilize 3 different types of external knowledge retrieved in different
    ways. Given an original text, our system retrieves the possible labels and the
    descriptions for each potential entity detected by a mention detection model.
    And we also retrieve a related document as extra context from Wikipedia for each
    original text. We concatenate the original text with the external knowledge as
    the input of NER models. The informative contextual representations with external
    knowledge significantly improve the NER performance in both Chinese and English
    tracks. Our system win the 3rd place in the Chinese track and the 6th place in
    the English track.
  authors:
  - Yuming Zhang
  - Hongyu Li
  - Yongwei Zhang
  - Shanshan Jiang
  - Bin Dong
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_105
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 2: MultiCoNER II Multilingual Complex Named Entity Recognition'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'SRCB at SemEval-2023 Task 2: A System of Complex Named Entity Recognition
    with External Knowledge'
  tldr: The MultiCoNER II shared task aims at detecting semantically ambiguous and
    complex named entities in short and low-context settings for multiple languages.
    The lack of context makes the recognition of ambiguous named entities challenging.
    To alleviate this issue, our team SRCB proposes an external k
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper describes our system used in the SemEval-2023 Task12: Sentiment
    Analysis for Low-resource African Languages using Twit- ter Dataset (Muhammad
    et al., 2023c). The AfriSenti-SemEval Shared Task 12 is based on a collection
    of Twitter datasets in 14 African languages for sentiment classification. It con-
    sists of three sub-tasks. Task A is a monolin- gual sentiment classification which
    covered 12 African languages. Task B is a multilingual sen- timent classification
    which combined training data from Task A (12 African languages). Task C is a zero-shot
    sentiment classification. We uti- lized various strategies, including monolingual
    training, multilingual mixed training, and trans- lation technology, and proposed
    a weighted vot- ing method that combined the results of differ- ent strategies.
    Substantially, in the monolingual subtask, our system achieved Top-1 in two lan-
    guages (Yoruba and Twi) and Top-2 in four languages (Nigerian Pidgin, Algerian
    Arabic, and Swahili, Multilingual). In the multilingual subtask, Our system achived
    Top-2 in publish leaderBoard.'
  authors:
  - Meizhi Jin
  - Cheng Chen
  - Mengyuan Zhou
  - Mengfei Yuan
  - Xiaolong Hou
  - Xiyang Du
  - Lianxin Jiang
  - Jianyu Li
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_106
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 12: AfriSenti-SemEval: Sentiment Analysis for Low-resource African
    Languages using Twitter Dataset'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'PingAnLifeInsurance at SemEval-2023 Task 12: Sentiment Analysis for Low-resource
    African Languages with Multi-Model Fusion'
  tldr: 'This paper describes our system used in the SemEval-2023 Task12: Sentiment
    Analysis for Low-resource African Languages using Twit- ter Dataset (Muhammad
    et al., 2023c). The AfriSenti-SemEval Shared Task 12 is based on a collection
    of Twitter datasets in 14 African languages for sentiment classificat'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper describes our system used for sub-task C (1 \& 2) in Task
    6: LegalEval: Understanding Legal Texts. We propose a three-level encoder-based
    classification architecture that works by fine-tuning a BERT-based pre-trained
    encoder, and post-processing the embeddings extracted from its last layers, using
    transformer encoder layers and RNNs. We run ablation studies on the same and analyze
    itsperformance. To extract the explanations for the predicted class we develop
    an explanation extraction algorithm, exploiting the idea of a model''s occlusion
    sensitivity. We explored some training strategies with a detailed analysis of
    the dataset. Our system ranks 2nd (macro-F1 metric) for its sub-task C-1 and 7th
    (ROUGE-2 metric) for sub-task C-2.'
  authors:
  - Nishchal Prasad
  - Mohand Boughanem
  - Taoufiq Dkaki
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_107
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 6: LegalEval: Understanding Legal Texts'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'IRIT_IRIS_C at SemEval-2023 Task 6: A Multi-level Encoder-based Architecture
    for Judgement Prediction of Legal Cases and their Explanation'
  tldr: 'This paper describes our system used for sub-task C (1 \& 2) in Task 6: LegalEval:
    Understanding Legal Texts. We propose a three-level encoder-based classification
    architecture that works by fine-tuning a BERT-based pre-trained encoder, and post-processing
    the embeddings extracted from its last laye'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes our participation in the Clickbait challenge at SemEval
    2023. In this work, we address the Clickbait classification task using transformers
    models in an ensemble configuration. We tackle the Spoiler Generation task using
    a two-level ensemble strategy of models trained for extractive QA, and selecting
    the best K candidates for multi-part spoilers. In the test partitions, our approaches
    obtained a classification accuracy of 0.716 for classification and a BLEU-4 score
    of 0.439 for spoiler generation.
  authors:
  - Emilio Villa Cueva
  - Daniel Vallejo Aldana
  - "Fernando S\xE1nchez Vega"
  - "Adri\xE1n Pastor L\xF3pez Monroy"
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_108
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 5: Clickbait Spoiling'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Walter Burns at SemEval-2023 Task 5: NLP-CIMAT - Leveraging Model Ensembles
    for Clickbait Spoiling'
  tldr: This paper describes our participation in the Clickbait challenge at SemEval
    2023. In this work, we address the Clickbait classification task using transformers
    models in an ensemble configuration. We tackle the Spoiler Generation task using
    a two-level ensemble strategy of models trained for extrac
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper describes the EvidenceSCL system submitted by our team (INF-UFRGS)
    to SemEval-2023 Task 7: Multi-Evidence Natural Language Inference for Clinical
    Trial Data (NLI4CT). NLI4CT is divided into two tasks, one for determining the
    inference relation between a pair of statements in clinical trials and a second
    for retrieving a set of supporting facts from the premises necessary to justify
    the label predicted in the first task. Our approach uses pair-level supervised
    contrastive learning to classify pairs of sentences. We trained EvidenceSCL on
    two datasets created from NLI4CT and additional data from other NLI datasets.
    We show that our approach can address both goals of NLI4CT, and although it reached
    an intermediate position, there is room for improvement in the technique.'
  authors:
  - "Abel Corr\xEAa Dias"
  - Filipe Dias
  - Higor Moreira
  - Viviane Moreira
  - "Jo\xE3o Luiz Comba"
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_109
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 7: Multi-Evidence Natural Language Inference for Clinical Trial
    Data'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Team INF-UFRGS at SemEval-2023 Task 7: Supervised Contrastive Learning for
    Pair-level Sentence Classification and Evidence Retrieval'
  tldr: 'This paper describes the EvidenceSCL system submitted by our team (INF-UFRGS)
    to SemEval-2023 Task 7: Multi-Evidence Natural Language Inference for Clinical
    Trial Data (NLI4CT). NLI4CT is divided into two tasks, one for determining the
    inference relation between a pair of statements in clinical tria'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Social media is a concept developed to link people and make the globe
    smaller. But it has recently developed into a center for sexist memes that target
    especially women. As a result, there are more events of hostile actions and harassing
    remarks present online. In this paper, we introduce our system for the task of
    online sexism detection, a part of SemEval 2023 task 10. We introduce fine-tuned
    RoBERTa model to address this specific problem. The efficiency of the proposed
    strategy is demonstrated by the experimental results reported in this research.
  authors:
  - Amit Das
  - Nilanjana Raychawdhary
  - Tathagata Bhattacharya
  - Gerry Dozier
  - Cheryl D. Seals
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_110
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 10: Towards Explainable Detection of Online Sexism'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'AU_NLP at SemEval-2023 Task 10: Explainable Detection of Online Sexism Using
    Fine-tuned RoBERTa'
  tldr: Social media is a concept developed to link people and make the globe smaller.
    But it has recently developed into a center for sexist memes that target especially
    women. As a result, there are more events of hostile actions and harassing remarks
    present online. In this paper, we introduce our system
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper describes the system entered by the author to the SemEval-2023
    Task 12: Sentiment analysis for African languages. The system focuses on the Kinyarwanda
    language and uses a language-specific model. Kinyarwanda morphology is modeled
    in a two tier transformer architecture and the transformer model is pre-trained
    on a large text corpus using multi-task masked morphology prediction. The model
    is deployed on an experimental platform that allows users to experiment with the
    pre-trained language model fine-tuning without the need to write machine learning
    code. Our final submission to the shared task achieves second ranking out of 34
    teams in the competition, achieving 72.50\% weighted F1 score. Our analysis of
    the evaluation results highlights challenges in achieving high accuracy on the
    task and identifies areas for improvement.'
  authors:
  - Antoine Nzeyimana
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_111
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 12: AfriSenti-SemEval: Sentiment Analysis for Low-resource African
    Languages using Twitter Dataset'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'KINLP at SemEval-2023 Task 12: Kinyarwanda Tweet Sentiment Analysis'
  tldr: 'This paper describes the system entered by the author to the SemEval-2023
    Task 12: Sentiment analysis for African languages. The system focuses on the Kinyarwanda
    language and uses a language-specific model. Kinyarwanda morphology is modeled
    in a two tier transformer architecture and the transformer'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: People are expressing their opinions online for a lot of years now. Although
    these opinions and comments provide people an opportunity of expressing their
    views, there is a lot of hate speech that can be found online. More specifically,
    sexist comments are very popular affecting and creating a negative impact on a
    lot of women and girls online. This paper describes the approaches of the SemEval-2023
    Task 10 competition for Explainable Online Sexism Detection (EDOS). The task has
    been divided into 3 subtasks, introducing different classes of sexist comments.
    We have approached these tasks using the bert-cased and uncased models which are
    trained on the annotated dataset that has been provided in the competition. Task
    A provided the best F1 score of 80\% on the test set, and tasks B and C provided
    58\% and 40\% respectively.
  authors:
  - Rakib Hossain Rifat
  - Abanti Shruti
  - Marufa Kamal
  - Farig Sadeque
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_112
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 10: Towards Explainable Detection of Online Sexism'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'ACSMKRHR at SemEval-2023 Task 10: Explainable Online Sexism Detection(EDOS)'
  tldr: People are expressing their opinions online for a lot of years now. Although
    these opinions and comments provide people an opportunity of expressing their
    views, there is a lot of hate speech that can be found online. More specifically,
    sexist comments are very popular affecting and creating a negat
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes our fine-tuned pretrained language model for task
    9 (Multilingual Tweet Intimacy Analysis, MTIA) of the SemEval 2023 competition.
    MTIA aims to quantitatively analyze tweets in 6 languages for intimacy, giving
    a score from 1 to 5. The challenge of MTIA is in semantically extracting information
    from code-mixed texts. To alleviate this difficulty, we suggested a solution that
    combines attention and memory mechanisms. The preprocessed tweets are input to
    the XLM-T layer to get sentence embeddings and subsequently to the bidirectional
    GRU layer to obtain intimacy ratings. Experimental results show an improvement
    in the overall performance of our model in both seen and unseen languages.
  authors:
  - Qisheng Cai
  - Jin Wang
  - Xuejie Zhang
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_113
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 9: Multilingual Tweet Intimacy Analysis'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'YNU-HPCC at SemEval-2023 Task 9: Pretrained Language Model for Multilingual
    Tweet Intimacy Analysis'
  tldr: This paper describes our fine-tuned pretrained language model for task 9 (Multilingual
    Tweet Intimacy Analysis, MTIA) of the SemEval 2023 competition. MTIA aims to quantitatively
    analyze tweets in 6 languages for intimacy, giving a score from 1 to 5. The challenge
    of MTIA is in semantically extracti
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper presents the experimentation of systems for detecting online
    sexism relying on classical models, deep learning models, and transformer-based
    models. The systems aim to provide a comprehensive approach to handling the intricacies
    of online language, including slang and neologisms. The dataset consists of labeled
    and unlabeled data from Gab and Reddit, which allows for the development of unsupervised
    or semi-supervised models. The system utilizes TF-IDF with classical models, bidirectional
    models with embedding, and pre-trained transformer models. The paper discusses
    the experimental setup and results, demonstrating the effectiveness of the system
    in detecting online sexism.
  authors:
  - Efrat Luzzon
  - Chaya Liebeskind
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_114
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 10: Towards Explainable Detection of Online Sexism'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'JCT_DM at SemEval-2023 Task 10: Detection of Online Sexism: from Classical
    Models to Transformers'
  tldr: This paper presents the experimentation of systems for detecting online sexism
    relying on classical models, deep learning models, and transformer-based models.
    The systems aim to provide a comprehensive approach to handling the intricacies
    of online language, including slang and neologisms. The data
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The MultiCoNER II task aims to detect complex, ambiguous, and fine-grained
    named entities in low-context situations and noisy scenarios like the presence
    of spelling mistakes and typos for multiple languages. The task poses significant
    challenges due to the scarcity of contextual information, the high granularity
    of the entities(up to 33 classes), and the interference of noisy data. To address
    these issues, our team \{\textbackslash{}bf PAI\} proposes a universal Named Entity
    Recognition (NER) system that integrates external entity information to improve
    performance. Specifically, our system retrieves entities with properties from
    the knowledge base (i.e. Wikipedia) for a given text, then concatenates entity
    information with the input sentence and feeds it into Transformer-based models.
    Finally, our system wins 2 first places, 4 second places, and 1 third place out
    of 13 tracks. The code is publicly available at \textbackslash{}url\{https://github.com/diqiuzhuanzhuan/semeval-2023\}.
  authors:
  - Long Ma
  - Kai Lu
  - Tianbo Che
  - Hailong Huang
  - Weiguo Gao
  - Xuan Li
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_115
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 2: MultiCoNER II Multilingual Complex Named Entity Recognition'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'PAI at SemEval-2023 Task 2: A Universal System for Named Entity Recognition
    with External Entity Information'
  tldr: The MultiCoNER II task aims to detect complex, ambiguous, and fine-grained
    named entities in low-context situations and noisy scenarios like the presence
    of spelling mistakes and typos for multiple languages. The task poses significant
    challenges due to the scarcity of contextual information, the hi
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Legal documents are notorious for their complexity and domain-specific
    language, making them challenging for legal practitioners as well as non-experts
    to comprehend. To address this issue, the LegalEval 2023 track proposed several
    shared tasks, including the task of Rhetorical Roles Prediction (Task A). We participated
    as NITS\_Legal team in Task A and conducted exploratory experiments to improve
    our understanding of the task. Our results suggest that sequence context is crucial
    in performing rhetorical roles prediction. Given the lengthy nature of legal documents,
    we propose a BiLSTM-based sentence sequence labeling approach that uses a local
    context-incorporated dataset created from the original dataset. To better represent
    the sentences during training, we extract legal domain-specific sentence embeddings
    from a Legal BERT model. Our experimental findings emphasize the importance of
    considering local context instead of treating each sentence independently to achieve
    better performance in this task. Our approach has the potential to improve the
    accessibility and usability of legal documents.
  authors:
  - Deepali Jain
  - Malaya Borah
  - Anupam Biswas
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_116
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 6: LegalEval: Understanding Legal Texts'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'NITS_Legal at SemEval-2023 Task 6: Rhetorical Roles Prediction of Indian
    Legal Documents via Sentence Sequence Labeling Approach'
  tldr: Legal documents are notorious for their complexity and domain-specific language,
    making them challenging for legal practitioners as well as non-experts to comprehend.
    To address this issue, the LegalEval 2023 track proposed several shared tasks,
    including the task of Rhetorical Roles Prediction (Tas
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Nowadays, intimacy is a fundamental aspect of how we relate to other
    people in social settings. The most frequent way in which we can determine a high
    level of intimacy is in the use of certain emoticons, curse words, verbs, etc.
    This paper presents the approach developed to solve SemEval 2023 task 9: Multiligual
    Tweet Intimacy Analysis. To address the task, a transfer learning approach was
    conducted by fine tuning various pre-trained languagemodels. Since the dataset
    supplied by the organizer was highly imbalanced, our main strategy to obtain high
    prediction values was the implementation of different oversampling and undersampling
    techniques on the training set. Our final submission achieved an overall Pearson''s
    r of 0.497.'
  authors:
  - Abel Pichardo Estevez
  - "Jacinto Mata V\xE1zquez"
  - "Victoria Pach\xF3n \xC1lvarez"
  - Nordin El Balima Cordero
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_117
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 9: Multilingual Tweet Intimacy Analysis'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'I2C-Huelva at SemEval-2023 Task 9: Analysis of Intimacy in Multilingual
    Tweets Using Resampling Methods and Transformers'
  tldr: 'Nowadays, intimacy is a fundamental aspect of how we relate to other people
    in social settings. The most frequent way in which we can determine a high level
    of intimacy is in the use of certain emoticons, curse words, verbs, etc. This
    paper presents the approach developed to solve SemEval 2023 task '
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This work details our approach for addressing Tasks A and B of the Semeval
    2023 Task 10: Explainable Detection of Online Sexism (EDOS). For Task A a simple
    ensemble based of majority vote system was presented.  To build our proposal,
    first a review of transformers was carried out and the 3 best performing models
    were selected to be part of the ensemble. Next, for these models, the best hyperpameters
    were searched using a reduced data set. Finally, we trained these models using
    more data. During the development phase, our ensemble system achieved an f1-score
    of 0.8403. For task B, we developed a model based on the deBERTa transformer,
    utilizing the hyperparameters identified for task A. During the development phase,
    our proposed model attained an f1-score of 0.6467. Overall, our methodology demonstrates
    an effective approach to the tasks, leveraging advanced machine learning techniques
    and hyperparameters searches to achieve high performance in detecting and classifying
    instances of sexism in online text.'
  authors:
  - Lavinia Felicia Fudulu
  - Alberto Rodriguez Tenorio
  - "Victoria Pach\xF3n \xC1lvarez"
  - "Jacinto Mata V\xE1zquez"
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_118
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 10: Towards Explainable Detection of Online Sexism'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'I2C-Huelva at SemEval-2023 Task 10: Ensembling Transformers Models for the
    Detection of Online Sexism'
  tldr: 'This work details our approach for addressing Tasks A and B of the Semeval
    2023 Task 10: Explainable Detection of Online Sexism (EDOS). For Task A a simple
    ensemble based of majority vote system was presented.  To build our proposal,
    first a review of transformers was carried out and the 3 best perf'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper describes our system used in the SemEval-2023 Task 9 Multilingual
    Tweet Intimacy Analysis. There are two key challenges in this task: the complexity
    of multilingual and zero-shot cross-lingual learning, and the difficulty of semantic
    mining of tweet intimacy. To solve the above problems, our system extracts contextual
    representations from the pretrained language models, XLM-T, and employs various
    optimization methods, including adversarial training, data augmentation, ordinal
    regression loss and special training strategy. Our system ranked 14th out of 54
    participating teams on the leaderboard and ranked 10th on predicting languages
    not in the training data. Our code is available on Github.'
  authors:
  - Hao Zhang
  - Youlin Wu
  - Junyu Lu
  - Zewen Bai
  - Jiangming Wu
  - Hongfei Lin
  - Shaowu Zhang
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_119
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 9: Multilingual Tweet Intimacy Analysis'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'ZBL2W at SemEval-2023 Task 9: A Multilingual Fine-tuning Model with Data
    Augmentation for Tweet Intimacy Analysis'
  tldr: 'This paper describes our system used in the SemEval-2023 Task 9 Multilingual
    Tweet Intimacy Analysis. There are two key challenges in this task: the complexity
    of multilingual and zero-shot cross-lingual learning, and the difficulty of semantic
    mining of tweet intimacy. To solve the above problems, '
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This study describes the model design of the NCUEE-NLP system for the
    SemEval-2023 NLI4CT task that focuses on multi-evidence natural language inference
    for clinical trial data. We use the LinkBERT transformer in the biomedical domain
    (denoted as BioLinkBERT) as our main system architecture. First, a set of sentences
    in clinical trial reports is extracted as evidence for premise-statement inference.
    This identified evidence is then used to determine the inference relation (i.e.,
    entailment or contradiction). Finally, a soft voting ensemble mechanism is applied
    to enhance the system performance. For Subtask 1 on textual entailment, our best
    submission had an F1-score of 0.7091, ranking sixth among all 30 participating
    teams. For Subtask 2 on evidence retrieval, our best result obtained an F1-score
    of 0.7940, ranking ninth of 19 submissions.
  authors:
  - Chao-Yi Chen
  - Kao-Yuan Tien
  - Yuan-Hao Cheng
  - Lung-Hao Lee
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_120
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 7: Multi-Evidence Natural Language Inference for Clinical Trial
    Data'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'NCUEE-NLP at SemEval-2023 Task 7: Ensemble Biomedical LinkBERT Transformers
    in Multi-evidence Natural Language Inference for Clinical Trial Data'
  tldr: 'This study describes the model design of the NCUEE-NLP system for the SemEval-2023
    NLI4CT task that focuses on multi-evidence natural language inference for clinical
    trial data. We use the LinkBERT transformer in the biomedical domain (denoted
    as BioLinkBERT) as our main system architecture. First, '
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Semi-supervised learning has promising performance in deep learning,
    one of the approaches is consistency training on a large amount of unlabeled data
    to constrain model predictions to be invariant to input noise. However, The degree
    of correlation between unlabeled data and task objective directly affects model
    prediction performance. This paper describes our system designed for SemEval-2023
    Task 10: Explainable Detection of Online Sexism. We utilize a consistency training
    framework and data augmentation as the main strategy to train a model. The score
    obtained by our method is 0.8180 in subtask A, ranking 57 in all the teams.'
  authors:
  - Yehui Xu
  - Haiyan Ding
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_121
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 10: Towards Explainable Detection of Online Sexism'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Tsingriver at SemEval-2023 Task 10: Labeled Data Augmentation in Consistency
    Training'
  tldr: 'Semi-supervised learning has promising performance in deep learning, one
    of the approaches is consistency training on a large amount of unlabeled data
    to constrain model predictions to be invariant to input noise. However, The degree
    of correlation between unlabeled data and task objective directly '
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'How similar is the detection of media bias to the detection of persuasive
    techniques? We have explored how transferring knowledge from one task to the other
    may help to improve the performance. This paper presents the systems developed
    for participating in the SemEval-2023 Task 3: Detecting the Genre, the Framing,
    and the Persuasion Techniques in Online News in a Multi-lingual Setup. We have
    participated in both the subtask 1: News Genre Categorisation, and the subtask
    3: Persuasion Techniques Detection. Our solutions are based on two-stage fine-tuned
    multilingual models. We evaluated our approach on the 9 languages provided in
    the task. Our results show that the use of transfer learning from media bias detection
    to persuasion techniques detection is beneficial for the subtask of detecting
    the genre (macro F1-score of 0.523 in the English test set) as it improves previous
    results, but not for the detection of persuasive techniques (micro F1-score of
    0.24 in the English test set).'
  authors:
  - "Francisco-Javier Rodrigo-Gin\xE9s"
  - Laura Plaza
  - Jorge Carrillo-de-Albornoz
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_122
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 3: Detecting the Category, the Framing, and the Persuasion Techniques
    in Online News in a Multi-lingual Setup'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'UnedMediaBiasTeam @ SemEval-2023 Task 3: Can We Detect Persuasive Techniques
    Transferring Knowledge From Media Bias Detection?'
  tldr: 'How similar is the detection of media bias to the detection of persuasive
    techniques? We have explored how transferring knowledge from one task to the other
    may help to improve the performance. This paper presents the systems developed
    for participating in the SemEval-2023 Task 3: Detecting the Genr'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The following system description presents our approach to the detection
    of persuasion techniques in online news.  The given task has been framed as a
    multi-label classification problem. In a multi-label classification problem, each
    input chunkin this case paragraphis assigned one of several class labels. Span
    level annotations were also provided. In order to assign class labels to the given
    documents, we opted for RoBERTa (A Robustly Optimized BERT Pretraining Approach)
    for both approachessequence and token classification.Starting off with a pre-trained
    model for language representation, we fine-tuned this model on the given classification
    task with the provided annotated data in supervised training steps.
  authors:
  - Albert Pritzkau
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_125
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 3: Detecting the Category, the Framing, and the Persuasion Techniques
    in Online News in a Multi-lingual Setup'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'NL4IA at SemEval-2023 Task 3: A Comparison of Sequence Classification and
    Token Classification to Detect Persuasive Techniques'
  tldr: The following system description presents our approach to the detection of
    persuasion techniques in online news.  The given task has been framed as a multi-label
    classification problem. In a multi-label classification problem, each input chunkin
    this case paragraphis assigned one of several class la
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: MultiCoNER-II is a fine-grained Named Entity Recognition (NER) task that
    aims to identify ambiguous and complex named entities in multiple languages, with
    a small amount of contextual information available. To address this task, we propose
    a multi-stage information retrieval (IR) pipeline that improves the performance
    of language models for fine-grained NER. Our approach involves leveraging a combination
    of a BM25-based IR model and a language model to retrieve relevant passages from
    a corpus. These passages are then used to train a model that utilizes a weighted
    average of losses. The prediction is generated by a decoder stack that includes
    a projection layer and conditional random field. To demonstrate the effectiveness
    of our approach, we participated in the English track of the MultiCoNER-II competition.
    Our approach yielded promising results, which we validated through detailed analysis.
  authors:
  - Shivani Choudhary
  - Niladri Chatterjee
  - Subir Saha
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_126
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 2: MultiCoNER II Multilingual Complex Named Entity Recognition'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'IITD at SemEval-2023 Task 2: A Multi-Stage Information Retrieval Approach
    for Fine-Grained Named Entity Recognition'
  tldr: MultiCoNER-II is a fine-grained Named Entity Recognition (NER) task that aims
    to identify ambiguous and complex named entities in multiple languages, with a
    small amount of contextual information available. To address this task, we propose
    a multi-stage information retrieval (IR) pipeline that impro
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper summarizes the participation of the L3i laboratory of the University
    of La Rochelle in the SemEval-2023 Task 2, Multilingual Complex Named Entity Recognition
    (MultiCoNER II). Similar to MultiCoNER I, the task seeks to develop methods to
    detect semantic ambiguous and complex entities in short and low-context settings.
    However, MultiCoNER II adds a fine-grained entity taxonomy with over 30 entity
    types and corrupted data  on the test partitions. We approach these complications
    following prompt-based learning as (1) a ranking problem using a seq2seq framework,
    and (2) an extractive question-answering task. Our findings show that even if
    prompting techniques have a similar recall to fine-tuned hierarchical language
    model-based encoder methods, precision tends to be more affected.
  authors:
  - Carlos-Emiliano Gonzalez-Gallardo
  - Thi Hong Hanh Tran
  - Nancy Girdhar
  - Emanuela Boros
  - Jose G. Moreno
  - Antoine Doucet
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_127
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 2: MultiCoNER II Multilingual Complex Named Entity Recognition'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'L3I++ at SemEval-2023 Task 2: Prompting for Multilingual Complex Named Entity
    Recognition'
  tldr: This paper summarizes the participation of the L3i laboratory of the University
    of La Rochelle in the SemEval-2023 Task 2, Multilingual Complex Named Entity Recognition
    (MultiCoNER II). Similar to MultiCoNER I, the task seeks to develop methods to
    detect semantic ambiguous and complex entities in sh
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Online sexism is a rising issue that threatens women''s safety, fosters
    hostile situations, and upholds social inequities. We describe a task SemEval-2023
    Task 10 for creating English-language models that can precisely identify and categorize
    sexist content on internet forums and social platforms like Gab and Reddit as
    well to provide an explainability in order to address this problem. The problem
    is divided into three hierarchically organized subtasks: binary sexism detection,
    sexism by category, and sexism by fine-grained vector. The dataset consists of
    20,000 labelled entries. For Task A, pertained models like Convolutional Neural
    Network (CNN) and Bidirectional Long Short-Term Memory (BiLSTM), which is called
    CNN-BiLSTM and Generative Pretrained Transformer 2 (GPT-2) models were used, as
    well as the GPT-2 model for Task B and C, and have provided experimental configurations.
    According to our findings, the GPT-2 model performs better than the CNN-BiLSTM
    model for Task A, while GPT-2 is highly accurate for Tasks B and C on the training,
    validation and testing splits of the training data provided in the task. Our proposed
    models allow researchers to create more precise and understandable models for
    identifying and categorizing sexist content in online forums, thereby empowering
    users and moderators.'
  authors:
  - Advaitha Vetagiri
  - Prottay Adhikary
  - Partha Pakray
  - Amitava Das
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_128
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 10: Towards Explainable Detection of Online Sexism'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'CNLP-NITS at SemEval-2023 Task 10: Online sexism prediction, PREDHATE!'
  tldr: Online sexism is a rising issue that threatens women's safety, fosters hostile
    situations, and upholds social inequities. We describe a task SemEval-2023 Task
    10 for creating English-language models that can precisely identify and categorize
    sexist content on internet forums and social platforms lik
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper presents our solution, garNER, to the SemEval-2023 MultiConer
    task. We propose a knowledge augmentation approach by directly querying entities
    from the Wikipedia API and appending the summaries of the entities to the input
    sentence. These entities are either retrieved from the labeled training set (Gold
    Entity) or from off-the-shelf entity taggers (Entity Extractor). Ensemble methods
    are then applied across multiple models to get the final prediction. Our analysis
    shows that the added contexts are beneficial only when such contexts are relevant
    to the target-named entities, but detrimental when the contexts are irrelevant.
  authors:
  - Md Zobaer Hossain
  - Averie Ho Zoen So
  - Silviya Silwal
  - H. Andres Gonzalez Gongora
  - Ahnaf Mozib Samin
  - Jahedul Alam Junaed
  - Aritra Mazumder
  - Sourav Saha
  - Sabiha Tahsin Soha
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_129
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 2: MultiCoNER II Multilingual Complex Named Entity Recognition'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'garNER at SemEval-2023: Simplified Knowledge Augmentation for Multilingual
    Complex Named Entity Recognition'
  tldr: This paper presents our solution, garNER, to the SemEval-2023 MultiConer task.
    We propose a knowledge augmentation approach by directly querying entities from
    the Wikipedia API and appending the summaries of the entities to the input sentence.
    These entities are either retrieved from the labeled tra
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper presents D2KLab's system used for the shared task of "Multilingual
    Complex Named Entity Recognition (MultiCoNER II)", as part of SemEval 2023 Task
    2. The system relies on a fine-tuned transformer based language model for extracting
    named entities. In addition to the architecture of the system, we discuss our
    results and observations.
  authors:
  - Thibault Ehrhart
  - Julien Plu
  - Raphael Troncy
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_130
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 2: MultiCoNER II Multilingual Complex Named Entity Recognition'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'D2KLab at SemEval-2023 Task 2: Leveraging T-NER to Develop a Fine-Tuned
    Multilingual Model for Complex Named Entity Recognition'
  tldr: This paper presents D2KLab's system used for the shared task of "Multilingual
    Complex Named Entity Recognition (MultiCoNER II)", as part of SemEval 2023 Task
    2. The system relies on a fine-tuned transformer based language model for extracting
    named entities. In addition to the architecture of the sy
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'In this paper, we present our team''s involvement in Task 6: LegalEval:
    Understanding Legal Texts. The task comprised three subtasks, and we focus on
    subtask A: Rhetorical Roles prediction. Our approach included experimenting with
    pre-trained embeddings and refining them with statistical and neural classifiers.
    We provide a thorough examination ofour experiments, solutions, and analysis,
    culminating in our best-performing model and current progress. We achieved a micro
    F1 score of 0.6133 on the test data using fine-tuned LegalBERT embeddings.'
  authors:
  - Pavan Baswani
  - Hiranmai Sri Adibhatla
  - Manish Shrivastava
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_131
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 6: LegalEval: Understanding Legal Texts'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'LTRC at SemEval-2023 Task 6: Experiments with Ensemble Embeddings'
  tldr: 'In this paper, we present our team''s involvement in Task 6: LegalEval: Understanding
    Legal Texts. The task comprised three subtasks, and we focus on subtask A: Rhetorical
    Roles prediction. Our approach included experimenting with pre-trained embeddings
    and refining them with statistical and neural c'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper describes our submission to theSemEval 2023 Task 3 on two
    subtasks: detectingpersuasion techniques and framing. Bothsubtasks are multi-label
    classification problems.We present a set of experiments, exploring howto get robust
    performance across languages usingpre-trained RoBERTa models. We test differentoversampling
    strategies, a strategy ofadding textual features from predictions obtainedwith
    related models, and present bothinconclusive and negative results. We achievea
    robust ranking across languages and subtaskswith our best ranking being nr. 1
    for Subtask 3on Spanish.'
  authors:
  - Amalie Pauli
  - Rafael Sarabia
  - Leon Derczynski
  - Ira Assent
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_132
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 3: Detecting the Category, the Framing, and the Persuasion Techniques
    in Online News in a Multi-lingual Setup'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'TeamAmpa at SemEval-2023 Task 3: Exploring Multilabel and Multilingual RoBERTa
    Models for Persuasion and Framing Detection'
  tldr: 'This paper describes our submission to theSemEval 2023 Task 3 on two subtasks:
    detectingpersuasion techniques and framing. Bothsubtasks are multi-label classification
    problems.We present a set of experiments, exploring howto get robust performance
    across languages usingpre-trained RoBERTa models. We'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper presents our proposed method for english documents genre classification
    in the context of SemEval 2023 task 3, subtask 1. Our method use ensemble technique
    to combine four distinct models predictions: Longformer, RoBERTa, GCN, and a sentences
    number-based model. Each model is optimized on simple objectives and easy to grasp.
    We provide snippets of code that define each model to make the reading experience
    better. Our method ranked 12th in documents genre classification for english texts.'
  authors:
  - Hamza Alami
  - Abdessamad Benlahbib
  - Abdelkader El Mahdaouy
  - Ismail Berrada
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_133
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 3: Detecting the Category, the Framing, and the Persuasion Techniques
    in Online News in a Multi-lingual Setup'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'UM6P at SemEval-2023 Task 3: News genre classification based on transformers,
    graph convolution networks and number of sentences'
  tldr: 'This paper presents our proposed method for english documents genre classification
    in the context of SemEval 2023 task 3, subtask 1. Our method use ensemble technique
    to combine four distinct models predictions: Longformer, RoBERTa, GCN, and a sentences
    number-based model. Each model is optimized on'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Court Judgement Prediction with Explanation (CJPE) is a task in the field
    of legal analysis and evaluation, which involves predicting the outcome of a court
    case based on the available legal text and providing a detailed explanation of
    the prediction. This is an important task in the legal system as it can aid in
    decision-making and improve the efficiency of the court process. In this paper,
    we present a new approach to understanding legal texts, which are normally long
    documents, based on data-oriented methods. Specifically, we first try to exploit
    the characteristic of data to understand the legal texts. The output is then used
    to train the model using the Longformer architecture. Regarding the experiment,
    the proposed method is evaluated on the sub-task CJPE of the SemEval-2023 Task
    6. Accordingly, our method achieves top 1 and top 2 on the classification task
    and explanation task, respectively. Furthermore, we present several open research
    issues for further investigations in order to improve the performance in this
    research field.
  authors:
  - Thanh Dat Hoang
  - Chi Minh Bui
  - Nam Bui
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_134
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 6: LegalEval: Understanding Legal Texts'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Viettel-AI at SemEval-2023 Task 6: Legal Document Understanding with Longformer
    for Court Judgment Prediction with Explanation'
  tldr: Court Judgement Prediction with Explanation (CJPE) is a task in the field
    of legal analysis and evaluation, which involves predicting the outcome of a court
    case based on the available legal text and providing a detailed explanation of
    the prediction. This is an important task in the legal system as
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper describes our participation in Task 12: AfriSenti-SemEval
    2023, i.e., track 12 of subtask A, track 16 of subtask B, and track 18 of subtask
    C. To deal with these three tracks, we utilize Support Vector Machine (SVM) +
    One vs Rest, SVM + One vs Rest with SMOTE, and AfriBERTa-large models. In particular,
    our SVM + One vs Rest with SMOTE model could obtain the highest weighted F1-Score
    for tracks 16 and 18 in the evaluation phase, that is, 65.14\% and 33.49\%, respectively.
    Meanwhile, our SVM + One vs Rest model could perform better than other models
    for track 12 in the evaluation phase.'
  authors:
  - Novitasari Arlim
  - Slamet Riyanto
  - Rodiah Rodiah
  - Al Hafiz Akbar Maulana Siagian
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_135
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 12: AfriSenti-SemEval: Sentiment Analysis for Low-resource African
    Languages using Twitter Dataset'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'GunadarmaXBRIN at SemEval-2023 Task 12: Utilization of SVM and AfriBERTa
    for Monolingual, Multilingual, and Zero-shot Sentiment Analysis in African Languages'
  tldr: 'This paper describes our participation in Task 12: AfriSenti-SemEval 2023,
    i.e., track 12 of subtask A, track 16 of subtask B, and track 18 of subtask C.
    To deal with these three tracks, we utilize Support Vector Machine (SVM) + One
    vs Rest, SVM + One vs Rest with SMOTE, and AfriBERTa-large models. '
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes the system we submitted to the SemEval 2023 Task
    2 Multilingual Complex Named Entity Recognition (MultiCoNER II) in four monolingual
    tracks (English, Spanish, French, and Portuguese). Considering the low context
    setting and the fine-grained taxonomy presented in this task, we propose a system
    that leverages the language model representations using hand-crafted tag descriptors.
    We explored how integrating the contextualized representations of tag descriptors
    with a language model can help improve the model performance for this task. We
    performed our evaluations on the development and test sets used in the task for
    the Practice Phase and the Evaluation Phase respectively.
  authors:
  - Jesus Lovon-Melgarejo
  - Jose G. Moreno
  - "Romaric Besan\xE7on"
  - Olivier Ferret
  - Lynda Lechani
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_136
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 2: MultiCoNER II Multilingual Complex Named Entity Recognition'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'MEERQAT-IRIT at SemEval-2023 Task 2: Leveraging Contextualized Tag Descriptors
    for Multilingual Named Entity Recognition'
  tldr: This paper describes the system we submitted to the SemEval 2023 Task 2 Multilingual
    Complex Named Entity Recognition (MultiCoNER II) in four monolingual tracks (English,
    Spanish, French, and Portuguese). Considering the low context setting and the
    fine-grained taxonomy presented in this task, we pr
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper presents proposed solutions for addressing two subtasks in
    SemEval-2023 Task 3: "Detecting the Genre, the Framing, and the Persuasion techniques
    in online news in a multi-lingual setup. In subtask 1, "News Genre Categorisation,
    the goal is to classify a news article as an opinion, a report, or a satire. In
    subtask 3, "Detection of Persuasion Technique, the system must reveal persuasion
    techniques used in each news article paragraph choosing among23 defined methods.
    Solutions leverage the application of the eXplainable Artificial Intelligence
    (XAI) method, Shapley Additive Explanations (SHAP). In subtask 1, SHAP was used
    to understand what was driving the model to fail so that it could be improved
    accordingly. In contrast, in subtask 3, a re-calibration of the Attention Mechanism
    was realized by extracting critical tokens for each persuasion technique. The
    underlying idea is the exploitation of XAI for countering the overfitting of the
    resulting model and attempting to improve the performance when there are few samples
    in the training data. The achieved performance on English for subtask 1 ranked
    6th with an F1-score of 58.6\% (despite 78.4\% of the 1st) and for subtask 3 ranked
    12th with a micro-averaged F1-score of 29.8\% (despite 37.6\% of the 1st).'
  authors:
  - Micaela Bangerter
  - Giuseppe Fenza
  - Mariacristina Gallo
  - Vincenzo Loia
  - Alberto Volpe
  - Carmen De Maio
  - Claudio Stanzione
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_137
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 3: Detecting the Category, the Framing, and the Persuasion Techniques
    in Online News in a Multi-lingual Setup'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Unisa at SemEval-2023 Task 3: A SHAP-based method for Propaganda Detection'
  tldr: 'This paper presents proposed solutions for addressing two subtasks in SemEval-2023
    Task 3: "Detecting the Genre, the Framing, and the Persuasion techniques in online
    news in a multi-lingual setup. In subtask 1, "News Genre Categorisation, the goal
    is to classify a news article as an opinion, a repor'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Sexism is an injustice afflicting women and has become a common form of
    oppression in social media.In recent years, the automatic detection of sexist
    instances has been utilized to combat this oppression.The Subtask A of SemEval-2023
    Task 10, Explainable Detection of Online Sexism, aims to detect whether an English-language
    post is sexist. In this paper, we describe our system for the competition.The
    structure of the classification model is based on RoBERTa, and we further pre-train
    it on the domain corpus. For fine-tuning, we adopt Unsupervised Data Augmentation
    (UDA), a semi-supervised learning approach, to improve the robustness of the system.
    Specifically, we employ Easy Data Augmentation (EDA) method as the noising operation
    for consistency training. We train multiple models based on different hyperparameter
    settings and adopt the majority voting method to predict the labels of test entries.
    Our proposed system achieves a Macro-F1 score of 0.8352 and a ranking of 41/84
    on the leaderboard of Subtask A.
  authors:
  - Bingjie Yu
  - Zewen Bai
  - Haoran Ji
  - Shiyi Li
  - Hao Zhang
  - Hongfei Lin
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_138
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 10: Towards Explainable Detection of Online Sexism'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'DUTIR at SemEval-2023 Task 10: Semi-supervised Learning for Sexism Detection
    in English'
  tldr: Sexism is an injustice afflicting women and has become a common form of oppression
    in social media.In recent years, the automatic detection of sexist instances has
    been utilized to combat this oppression.The Subtask A of SemEval-2023 Task 10,
    Explainable Detection of Online Sexism, aims to detect wh
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Complex named entities (NE), like the titles of creative works, are not
    simple nouns and pose challenges for NER systems. In the SemEval 2023, Task 2:
    MultiCoNER II was proposed, whose goal is to recognize complex entities against
    out of knowledge-base entities and noisy scenarios. To address the challenges
    posed by MultiCoNER II, our team NetEase.AI proposed an entity recognition system
    that integrates text error correction system and external knowledge, which can
    recognize entities in scenes that contain entities out of knowledge base and text
    with noise. Upon receiving an input sentence, our systems will correct the sentence,
    extract the entities in the sentence as candidate set using the entity recognition
    model that incorporates the gazetteer information, and then use the external knowledge
    to classify the candidate entities to obtain entity type features. Finally, our
    system fused the multi-dimensional features of the candidate entities into a stacking
    model, which was used to select the correct entities from the candidate set as
    the final output. Our system exhibited good noise resistance and excellent entity
    recognition performance, resulting in our team''s first place victory in the Chinese
    track of MultiCoNER II.'
  authors:
  - Ruixuan Lu
  - Zihang Tang
  - Guanglong Hu
  - Dong Liu
  - Jiacheng Li
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_139
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 2: MultiCoNER II Multilingual Complex Named Entity Recognition'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'NetEase.AI at SemEval-2023 Task 2: Enhancing Complex Named Entities Recognition
    in Noisy Scenarios via Text Error Correction and External Knowledge'
  tldr: 'Complex named entities (NE), like the titles of creative works, are not simple
    nouns and pose challenges for NER systems. In the SemEval 2023, Task 2: MultiCoNER
    II was proposed, whose goal is to recognize complex entities against out of knowledge-base
    entities and noisy scenarios. To address the ch'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This work presents and evaluates an approach to efficiently leverage the
    context exploitation ability of pre-trained Transformer models as a way of boosting
    the performance of models tackling the Legal Rhetorical Role Labeling task. The
    core idea is to feed the model with sentence chunks that are assembled in a way
    that avoids the insertion of padding tokens and the truncation of sentences and,
    hence, obtain better sentence embeddings. The achieved results show that our proposal
    is efficient, despite its simplicity, since models based on it overcome strong
    baselines by 3.76\% in the worst case and by 8.71\% in the best case.
  authors:
  - Alexandre Gomes de Lima
  - Jose G. Moreno
  - Eduardo H. da S. Aranha
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_141
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 6: LegalEval: Understanding Legal Texts'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'IRIT_IRIS_A at SemEval-2023 Task 6: Legal Rhetorical Role Labeling Supported
    by Dynamic-Filled Contextualized Sentence Chunks'
  tldr: This work presents and evaluates an approach to efficiently leverage the context
    exploitation ability of pre-trained Transformer models as a way of boosting the
    performance of models tackling the Legal Rhetorical Role Labeling task. The core
    idea is to feed the model with sentence chunks that are as
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The "Causal Medical Claim Identification and Extraction from Social Media
    Posts task at SemEval 2023 competition focuses on identifying and validating medical
    claims in English, by posing two subtasks on causal claim identification and PIO
    (Population, Intervention, Outcome) frame extraction. In the context of SemEval,
    we present a method for sentence classification in four categories (claim, experience,
    experience\_based\_claim or a question) based on BioBERT model with a MLP layer.  The
    website from which the dataset was gathered, Reddit, is a social news and content
    discussion site. The evaluation results show the effectiveness of the solution
    of this study (83.68\%).
  authors:
  - Andra Oica
  - Daniela Gifu
  - Diana Trandabat
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_142
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 8: Causal medical claim identification and related PICO frame
    extraction from social media posts'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Togedemaru at SemEval-2023 Task 8: Causal Medical Claim Identification and
    Extraction from Social Media Posts'
  tldr: The "Causal Medical Claim Identification and Extraction from Social Media
    Posts task at SemEval 2023 competition focuses on identifying and validating medical
    claims in English, by posing two subtasks on causal claim identification and PIO
    (Population, Intervention, Outcome) frame extraction. In the
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes our participation as team FramingFreaks in the SemEval-2023
    task 3 ``Category and Framing Predictions in online news in a multi-lingual setup.''
    We participated in subtasks 1 and 2. Our approach was to classify texts by splitting
    them into subwords to reduce the feature set size and then using these tokens
    as input in Support Vector Machine (SVM) or logistic regression classifiers. Our
    results are similar to the baseline results.
  authors:
  - Rosina Baumann
  - Sabrina Deisenhofer
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_143
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 3: Detecting the Category, the Framing, and the Persuasion Techniques
    in Online News in a Multi-lingual Setup'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'FramingFreaks at SemEval-2023 Task 3: Detecting the Category and the Framing
    of Texts as Subword Units with Traditional Machine Learning'
  tldr: This paper describes our participation as team FramingFreaks in the SemEval-2023
    task 3 ``Category and Framing Predictions in online news in a multi-lingual setup.''
    We participated in subtasks 1 and 2. Our approach was to classify texts by splitting
    them into subwords to reduce the feature set size
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This study presents a systematic method for analyzing the level of intimacy
    in tweets across ten different languages, using multi-task learning for SemEval
    2023 Task 9: Multilingual Tweet Intimacy Analysis. The system begins with the
    utilization of the official training data, and then we experiment with different
    fine-tuning tricks and effective strategies, such as data augmentation, multi-task
    learning, etc. Through additional experiments, the approach is shown to be effective
    for the task. To enhance the model''s robustness, different transformer-based
    language models and some widely-used plug-and-play priors are incorporated into
    our system. Our final submission achieved a Pearson R of 0.6160 for the intimacy
    score on the official test set, placing us at the top of the leader board among
    45 teams.'
  authors:
  - Mengfei Yuan
  - Cheng Chen
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_144
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 9: Multilingual Tweet Intimacy Analysis'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Lazybob at SemEval-2023 Task 9: Quantifying Intimacy of Multilingual Tweets
    with Multi-Task Learning'
  tldr: 'This study presents a systematic method for analyzing the level of intimacy
    in tweets across ten different languages, using multi-task learning for SemEval
    2023 Task 9: Multilingual Tweet Intimacy Analysis. The system begins with the
    utilization of the official training data, and then we experiment '
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper describes our system used in the SemEval-2023 \textbackslash{}textit\{Task
    10 Explainable Detection of Online Sexism (EDOS)\}. Specifically, we participated
    in subtask B: a 4-class sexism classification task, and subtask C: a more fine-grained
    (11-class) sexism classification task, where it is necessary to predict the category
    of sexism. We treat these two subtasks as one multi-label hierarchical text classification
    problem, and propose an integrated sexism detection model for improving the performance
    of the sexism detection task. More concretely, we use the pre-trained BERT model
    to encode the text and class label and a hierarchy-relevant structure encoder
    is employed to model the relationship between classes of subtasks B and C. Additionally,  a
    self-training strategy is designed to alleviate the imbalanced problem of distribution
    classes. Extensive experiments on subtasks B and C demonstrate the effectiveness
    of our proposed approach.'
  authors:
  - Ziyi Yao
  - Heyan Chai
  - Jinhao Cui
  - Siyu Tang
  - Qing Liao
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_145
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 10: Towards Explainable Detection of Online Sexism'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'HITSZQ at SemEval-2023 Task 10: Category-aware Sexism Detection Model with
    Self-training Strategy'
  tldr: 'This paper describes our system used in the SemEval-2023 \textbackslash{}textit\{Task
    10 Explainable Detection of Online Sexism (EDOS)\}. Specifically, we participated
    in subtask B: a 4-class sexism classification task, and subtask C: a more fine-grained
    (11-class) sexism classification task, where '
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper presents the winning system for the zero-shot Spanish framing
    detection task, which also achieves competitive places in eight additional languages.
    The challenge of the framing detection task lies in identifying a set of 14 frames
    when only a few or zero samples are available, i.e., a multilingual multi-label
    few- or zero-shot setting. Our developed solution employs a pre-training procedure
    based on multilingual Transformers using a label-aware contrastive loss function.
    In addition to describing the system, we perform an embedding space analysis and
    ablation study to demonstrate how our pre-training procedure supports framing
    detection to advance computational framing analysis.
  authors:
  - Markus Reiter-Haas
  - Alexander Ertl
  - Kevin Innerhofer
  - Elisabeth Lex
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_146
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 3: Detecting the Category, the Framing, and the Persuasion Techniques
    in Online News in a Multi-lingual Setup'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'mCPT at SemEval-2023 Task 3: Multilingual Label-Aware Contrastive Pre-Training
    of Transformers for Few- and Zero-shot Framing Detection'
  tldr: This paper presents the winning system for the zero-shot Spanish framing detection
    task, which also achieves competitive places in eight additional languages. The
    challenge of the framing detection task lies in identifying a set of 14 frames
    when only a few or zero samples are available, i.e., a mul
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The advancement in the healthcare sector assures improved diagnosis and
    supports appropriate decision making in medical domain. The medical domain data
    can be either radiology images or clinical data. The clinical data plays a major
    role in the healthcare sector by preventing and treating the health problem based
    on the evidence learned from the trials. This paper is related to multi-evidence
    natural language inference for clinical trial data analysis and its solution for
    the given subtasks (SemEval 2023 Task 7 - NLI4CT). In subtask 1 of NLI4CT, the
    inference relationship (entailment or contradiction) between the Clinical Trial
    Reports (CTRs) statement pairs with respect to the Clinical Trial Data (CTD) statement
    are determined. In subtask 2 of NLI4CT, predicted label (inference relationship)
    are defined and justified using set of supporting facts extracted from the premises.
    The objective of this work is to derive the conclusion from premises (CTRs statement
    pairs) and extracting the supporting premises using proposed Semantic Rule based
    Clinical Data Analysis (SRCDA) approach. From the results, the proposed model
    attained an highest F1-score of 0.667 and 0.716 for subtasks 1 and 2 respectively.
    The novelty of this proposed approach includes, creation of External Knowledge
    Base (EKB) along with its suitable semantic rules based on the input statements.
  authors:
  - Sheerin Sitara Noor Mohamed
  - Kavitha Srinivasan
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_147
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 7: Multi-Evidence Natural Language Inference for Clinical Trial
    Data'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'SSNSheerinKavitha at SemEval-2023 Task 7: Semantic Rule Based Label Prediction
    Using TF-IDF and BM25 Techniques'
  tldr: The advancement in the healthcare sector assures improved diagnosis and supports
    appropriate decision making in medical domain. The medical domain data can be
    either radiology images or clinical data. The clinical data plays a major role
    in the healthcare sector by preventing and treating the health
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes the method we submitted as the Janko team in the
    SemEval-2023 Task 2,Multilingual Complex Named Entity Recognition (MultiCoNER
    2). We only participated in the Chinese track. In this paper, we implement the
    BERT-BiLSTM-RDrop model. We use the fine-tuned BERT models, take the output of
    BERT as the input of the BiLSTM network, and finally use R-Drop technology to
    optimize the loss function. Our submission achieved a macro-averaged F1 score
    of 0.579 on the testset.
  authors:
  - Jiankuo Li
  - Zhengyi Guan
  - Haiyan Ding
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_148
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 2: MultiCoNER II Multilingual Complex Named Entity Recognition'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Janko at SemEval-2023 Task 2: Bidirectional LSTM Model Based on Pre-training
    for Chinese Named Entity Recognition'
  tldr: This paper describes the method we submitted as the Janko team in the SemEval-2023
    Task 2,Multilingual Complex Named Entity Recognition (MultiCoNER 2). We only participated
    in the Chinese track. In this paper, we implement the BERT-BiLSTM-RDrop model.
    We use the fine-tuned BERT models, take the outp
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper describes the methods and models applied by our team HHS in
    SubTask-A of SemEval-2023 Task 10 about sexism detection. In this task, we trained
    with the officially released data and analyzed the performance of five models,
    TextCNN, BERT, RoBERTa, XLNet, and Sup-SimCSE-RoBERTa. The experiments show that
    most of the models can achieve good results. Then, we tried data augmentation,
    model ensemble, dropout, and other operations on several of these models, and
    compared the results for analysis. In the end, the most effective approach that
    yielded the best results on the test set involved the following steps: enhancing
    the sexist data using dropout, feeding it as input to the Sup-SimCSE-RoBERTa model,
    and providing the raw data as input to the XLNet model. Then, combining the outputs
    of the two methods led to even better results. This method yielded a Macro-F1
    score of 0.823 in the final evaluation phase of the SubTask-A of the competition.'
  authors:
  - Yao Zhang
  - Liqing Wang
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_149
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 10: Towards Explainable Detection of Online Sexism'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'HHS at SemEval-2023 Task 10: A Comparative Analysis of Sexism Detection
    Based on the RoBERTa Model'
  tldr: This paper describes the methods and models applied by our team HHS in SubTask-A
    of SemEval-2023 Task 10 about sexism detection. In this task, we trained with
    the officially released data and analyzed the performance of five models, TextCNN,
    BERT, RoBERTa, XLNet, and Sup-SimCSE-RoBERTa. The experime
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes an approach to automat- ically close the knowledge
    gap of Clickbait- Posts via a transformer model trained for Question-Answering,
    augmented by a task- specific post-processing step. This was part of the SemEval
    2023 Clickbait shared task (Frbe et al., 2023a) - specifically task 2. We devised
    strategies to improve the existing model to fit the task better, e.g. with different
    special mod- els and a post-processor tailored to different inherent challenges
    of the task. Furthermore, we explored the possibility of expanding the original
    training data by using strategies from Heuristic Labeling and Semi-Supervised
    Learn- ing. With those adjustments, we were able to improve the baseline by 9.8
    percentage points to a BLEU-4 score of 48.0\%.
  authors:
  - Simon Birkenheuer
  - Jonathan Drechsel
  - Paul Justen
  - Jimmy Phlmann
  - Julius Gonsior
  - Anja Reusch
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_150
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 5: Clickbait Spoiling'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Sabrina Spellman at SemEval-2023 Task 5: Discover the Shocking Truth Behind
    this Composite Approach to Clickbait Spoiling!'
  tldr: This paper describes an approach to automat- ically close the knowledge gap
    of Clickbait- Posts via a transformer model trained for Question-Answering, augmented
    by a task- specific post-processing step. This was part of the SemEval 2023 Clickbait
    shared task (Frbe et al., 2023a) - specifically task
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Modeling the most likely label when an annotation task is perspective-dependent
    discards relevant sources of variation that come from the annotators themselves.
    We present three approaches to modeling the controversiality of a particular text.
    First, we explicitly represented annotators using annotator embeddings to predict
    the training signals of each annotator's selections in addition to a majority
    class label.This method leads to reduction in error relative to models without
    these features, allowing the overall result to influence the weights of each annotator
    on the final prediction. In a second set of experiments, annotators were not modeled
    individually but instead annotator judgments were combined in a pairwise fashion
    that allowed us to implicitly combine annotators. Overall, we found that aggregating
    and explicitly comparing annotators' responses to a static document representation
    produced high-quality predictions in all datasets, though some systems struggle
    to account for large or variable numbers of annotators.
  authors:
  - Michael Sullivan
  - Mohammed Yasin
  - Cassandra L. Jacobs
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_152
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 11: Learning with Disagreements (Le-Wi-Di)'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'University at Buffalo at SemEval-2023 Task 11: MASDA--Modelling Annotator
    Sensibilities through DisAggregation'
  tldr: Modeling the most likely label when an annotation task is perspective-dependent
    discards relevant sources of variation that come from the annotators themselves.
    We present three approaches to modeling the controversiality of a particular text.
    First, we explicitly represented annotators using annota
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes the participation of SINAI research team in the Explainable
    Detection of Online Sexism (EDOS) Shared Task at SemEval 2023. Specifically, we
    participate in subtask A (binary sexism detection), subtask B (category of sexism),
    and subtask C (fine-grained vector of sexism). For the three subtasks, we propose
    a system that integrates information related to emotions, sentiments, and irony
    in order to check whether these features help detect sexism content. Our team
    ranked 46th in subtask A, 37th in subtask B, and 29th in subtask C, achieving
    0.8245, 0.6043, and 0.4376 of macro f1-score, respectively, among the participants.
  authors:
  - "Mar\xEDa Estrella Vallecillo Rodrguez"
  - Flor Miriam Plaza Del Arco
  - "L. Alfonso Ure\xF1a L\xF3pez"
  - "M. Teresa Mart\xEDn Valdivia"
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_153
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 10: Towards Explainable Detection of Online Sexism'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'SINAI at SemEval-2023 Task 10: Leveraging Emotions, Sentiments, and Irony
    Knowledge for Explainable Detection of Online Sexism'
  tldr: 'This paper describes the participation of SINAI research team in the Explainable
    Detection of Online Sexism (EDOS) Shared Task at SemEval 2023. Specifically, we
    participate in subtask A (binary sexism detection), subtask B (category of sexism),
    and subtask C (fine-grained vector of sexism). For the '
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The goal of the NLI4CT task is to build a Natural Language Inference system
    for Clinical Trial Reports that will be used for evidence interpretation and retrieval.
    Large Language models have demonstrated state-of-the-art performance in various
    natural language processing tasks across multiple domains. We suggest using an
    instruction-finetuned Large Language Models (LLMs) to take on this particular
    task in light of these developments. We have evaluated the publicly available
    LLMs under zeroshot setting, and finetuned the best performing Flan-T5 model for
    this task. On the leaderboard, our system ranked second, with an F1 Score of 0.834
    on the official test set.
  authors:
  - Kamal Raj Kanakarajan
  - Malaikannan Sankarasubbu
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_154
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 7: Multi-Evidence Natural Language Inference for Clinical Trial
    Data'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Saama AI Research at SemEval-2023 Task 7: Exploring the Capabilities of
    Flan-T5 for Multi-evidence Natural Language Inference in Clinical Trial Data'
  tldr: The goal of the NLI4CT task is to build a Natural Language Inference system
    for Clinical Trial Reports that will be used for evidence interpretation and retrieval.
    Large Language models have demonstrated state-of-the-art performance in various
    natural language processing tasks across multiple domain
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper presents our submitted system to AfriSenti SemEval-2023 Task
    12: Sentiment Analysis for African Languages. The AfriSenti consists of three
    different tasks, covering monolingual, multilingual, and zero-shot sentiment analysis
    scenarios for African languages. To improve model generalization, we have explored
    the following steps: 1) further pre-training of the AfroXLM Pre-trained Language
    Model (PLM),  2) combining AfroXLM and MARBERT PLMs using a residual layer, and
    3) studying the impact of metric learning and two out-of-distribution generalization
    training objectives. The overall evaluation results show that our system has achieved
    promising results on several sub-tasks of Task A. For Tasks B and C, our system
    is ranked among the top six participating systems.'
  authors:
  - Abdelkader El Mahdaouy
  - Hamza Alami
  - Salima Lamsiyah
  - Ismail Berrada
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_155
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 12: AfriSenti-SemEval: Sentiment Analysis for Low-resource African
    Languages using Twitter Dataset'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'UM6P at SemEval-2023 Task 12: Out-Of-Distribution Generalization Method
    for African Languages Sentiment Analysis'
  tldr: 'This paper presents our submitted system to AfriSenti SemEval-2023 Task 12:
    Sentiment Analysis for African Languages. The AfriSenti consists of three different
    tasks, covering monolingual, multilingual, and zero-shot sentiment analysis scenarios
    for African languages. To improve model generalization'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes SemEval-2022's shared task "Explainable Detection
    of Online Sexism". The fine-grained classification of sexist content plays a major
    role in building explainable frameworks for online sexism detection. We hypothesize
    that by encoding dependency information using Graph Convolutional Networks (GCNs)
    we may capture more stylistic information about sexist contents. Online sexism
    has the potential to cause significant harm to women who are the targets of such
    behavior. It not only creates unwelcoming and inaccessible spaces for women online
    but also perpetuates social asymmetries and injustices. We believed improving
    the robustness and generalization ability of neural networks during training will
    allow models to capture different belief distributions for sexism categories.
    So we proposed adversarial training with GCNs for explainable detection of online
    sexism. In the end, our proposed method achieved very competitive results in all
    subtasks and shows that adversarial training of GCNs is a promising method for
    the explainable detection of online sexism.
  authors:
  - Ehsan Tavan
  - Maryam Najafi
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_156
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 10: Towards Explainable Detection of Online Sexism'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'MarSan at SemEval-2023 Task 10: Can Adversarial Training with help of a
    Graph Convolutional Network Detect Explainable Sexism?'
  tldr: This paper describes SemEval-2022's shared task "Explainable Detection of
    Online Sexism". The fine-grained classification of sexist content plays a major
    role in building explainable frameworks for online sexism detection. We hypothesize
    that by encoding dependency information using Graph Convolutio
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes the submission of UZH\_CLyp for the SemEval 2023
    Task 9 "Multilingual Tweet Intimacy Analysis. We achieved second-best results
    in all 10 languages according to the official Pearson's correlation regression
    evaluation measure. Our cross-lingual transfer learning approach explores the
    benefits of using a Head-First Fine-Tuning method (HeFiT) that first updates only
    the regression head parameters and then also updates the pre-trained transformer
    encoder parameters at a reduced learning rate. Additionally, we study the impact
    of using a small set of automatically generated examples (in our case, from ChatGPT)
    for low-resource settings where no human-labeled data is available. Our study
    shows that HeFiT stabilizes training and consistently improves results for pre-trained
    models that lack domain adaptation to tweets. Our study also shows a noticeable
    performance increase in cross-lingual learning when synthetic data is used, confirming
    the usefulness of current text generation systems to improve zeroshot baseline
    results. Finally, we examine how possible inconsistencies in the annotated data
    contribute to cross-lingual interference issues.
  authors:
  - Andrianos Michail
  - Stefanos Konstantinou
  - Simon Clematide
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_157
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 9: Multilingual Tweet Intimacy Analysis'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'UZH_CLyp at SemEval-2023 Task 9: Head-First Fine-Tuning and ChatGPT Data
    Generation for Cross-Lingual Learning in Tweet Intimacy Prediction'
  tldr: This paper describes the submission of UZH\_CLyp for the SemEval 2023 Task
    9 "Multilingual Tweet Intimacy Analysis. We achieved second-best results in all
    10 languages according to the official Pearson's correlation regression evaluation
    measure. Our cross-lingual transfer learning approach explores
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'In this system paper, we describe our submission for the 11th task of
    SemEval2023: Learning with Disagreements, or Le-Wi-Di for short. In the task,
    the assumption that there is a single gold label in NLP tasks such as hate speech
    or misogyny detection is challenged, and instead the opinions of multiple annotators
    are considered. The goal is instead to capture the agreements/disagreements of
    the annotators. For our system, we utilize the capabilities of modern large-language
    models as our backbone and investigate various techniques built on top, such as
    ensemble learning, multi-task learning, or Gaussian processes. Our final submission
    shows promising results and we achieve an upper-half finish.'
  authors:
  - "Dennis Gr\xF6tzinger"
  - Simon Heuschkel
  - Matthias Drews
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_158
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 11: Learning with Disagreements (Le-Wi-Di)'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'CICL_DMS at SemEval-2023 Task 11: Learning With Disagreements (Le-Wi-Di)'
  tldr: 'In this system paper, we describe our submission for the 11th task of SemEval2023:
    Learning with Disagreements, or Le-Wi-Di for short. In the task, the assumption
    that there is a single gold label in NLP tasks such as hate speech or misogyny
    detection is challenged, and instead the opinions of multi'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper presents our system for the SemEval-2023 Task 4, which aims
    to identify human values behind arguments by classifying whether or not an argument
    draws on a specific category. Our approach leverages a second-phase pre-training
    method to adapt a RoBERTa Language Model (LM) and tackles the problem using a
    One-Versus-All strategy. Final predictions are determined by a majority voting
    module that combines the outputs of an ensemble of three sets of per-label models.
    We conducted experiments to evaluate the impact of different pre-trained LMs on
    the task, comparing their performance in both pre-trained and task-adapted settings.
    Our findings show that fine-tuning the RoBERTa LM on the task-specific dataset
    improves its performance, outperforming the best-performing baseline BERT approach.
    Overall, our approach achieved a macro-F1 score of 0.47 on the official test set,
    demonstrating its potential in identifying human values behind arguments.
  authors:
  - Dimitrios Zaikis
  - Stefanos D. Stefanidis
  - Konstantinos Anagnostopoulos
  - Ioannis Vlahavas
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_159
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 4: ValueEval: Identification of Human Values behind Arguments'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Aristoxenus at SemEval-2023 Task 4: A Domain-Adapted Ensemble Approach to
    the Identification of Human Values behind Arguments'
  tldr: This paper presents our system for the SemEval-2023 Task 4, which aims to
    identify human values behind arguments by classifying whether or not an argument
    draws on a specific category. Our approach leverages a second-phase pre-training
    method to adapt a RoBERTa Language Model (LM) and tackles the pr
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'In this paper we present and discuss the results achieved by the "Augustine
    of Hippo" team at SemEval-2023 Task 4 about human value detection.In particular,
    we provide a quantitative and qualitative reviews of the results obtained by SuperASKE,
    discussing respectively performance metrics and classification errors.Finally,
    we present our main contribution: an explainable and unsupervised approach mapping
    arguments to concepts, followed by a supervised classification model mapping concepts
    to human values.'
  authors:
  - Alfio Ferrara
  - Sergio Picascia
  - Elisabetta Rocchetti
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_160
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 4: ValueEval: Identification of Human Values behind Arguments'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Augustine of Hippo at SemEval-2023 Task 4: An Explainable Knowledge Extraction
    Method to Identify Human Values in Arguments with SuperASKE'
  tldr: In this paper we present and discuss the results achieved by the "Augustine
    of Hippo" team at SemEval-2023 Task 4 about human value detection.In particular,
    we provide a quantitative and qualitative reviews of the results obtained by SuperASKE,
    discussing respectively performance metrics and classif
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Our contribution to the 2023 AfriSenti-SemEval shared task 12:  Sentiment
    Analysis for African Languages, provides insight into how a  multilingual large
    language model can be a resource for sentiment analysis in languages not seen
    during pretraining. The shared task provides datasets of a variety of African
    languages from different language families. The languages are to various degrees
    related to languages used during pretraining, and the language data contain various
    degrees of code-switching.  We experiment with both monolingual and multilingual
    datasets for the final fine-tuning, and find that with the provided datasets that
    contain samples in the thousands, monolingual fine-tuning yields the best results.'
  authors:
  - "Egil R\xF8nningstad"
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_161
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 12: AfriSenti-SemEval: Sentiment Analysis for Low-resource African
    Languages using Twitter Dataset'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'UIO at SemEval-2023 Task 12:  Multilingual fine-tuning for sentiment classification
    in low-resource Languages'
  tldr: 'Our contribution to the 2023 AfriSenti-SemEval shared task 12:  Sentiment
    Analysis for African Languages, provides insight into how a  multilingual large
    language model can be a resource for sentiment analysis in languages not seen
    during pretraining. The shared task provides datasets of a variety o'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes the participation of the UMUTeam in the Learning
    With Disagreements (Le-Wi-Di) shared task proposed at SemEval 2023, which objective
    is the development of supervised automatic classifiers that consider, during training,
    the agreements and disagreements among the annotators of the datasets. Specifically,
    this edition includes a multilingual dataset. Our proposal is grounded on the
    development of ensemble learning classifiers that combine the outputs of several
    Large Language Models. Our proposal ranked position 18 of a total of 30 participants.
    However, our proposal did not incorporate the information about the disagreements.
    In contrast, we compare the performance of building several classifiers for each
    dataset separately with a merged dataset.
  authors:
  - "Jos\xE9 Antonio Garc\xEDa-D\xEDaz"
  - Ronghao Pan
  - "Gema Alcar\xE1z-M\xE1rmol"
  - "Mar\xEDa Jos\xE9 Mar\xEDn-P\xE9rez"
  - "Rafael Valencia-Garc\xEDa"
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_162
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 11: Learning with Disagreements (Le-Wi-Di)'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'UMUTeam at SemEval-2023 Task 11: Ensemble Learning applied to Binary Supervised
    Classifiers with disagreements'
  tldr: This paper describes the participation of the UMUTeam in the Learning With
    Disagreements (Le-Wi-Di) shared task proposed at SemEval 2023, which objective
    is the development of supervised automatic classifiers that consider, during training,
    the agreements and disagreements among the annotators of th
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'The Clickbait Spoiling shared task aims at tackling two aspects of spoiling:
    classifying the spoiler type based on its length and generating the spoiler. This
    paper focuses on the task of classifying the spoiler type. Better classification
    of the spoiler type would eventually help in generating a better spoiler for the
    post. We use BERT-base (cased) to classify the clickbait posts. The model achieves
    a balanced accuracy of 0.63 as we give only the post content as the input to our
    model instead of the concatenation of the post title and post content to find
    out the differences that the post title might be bringing in.'
  authors:
  - Nukit Tailor
  - Radhika Mamidi
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_163
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 5: Clickbait Spoiling'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Matt Bai at SemEval-2023 Task 5: Clickbait spoiler classification via BERT'
  tldr: 'The Clickbait Spoiling shared task aims at tackling two aspects of spoiling:
    classifying the spoiler type based on its length and generating the spoiler. This
    paper focuses on the task of classifying the spoiler type. Better classification
    of the spoiler type would eventually help in generating a be'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: As social media platforms grow, so too does the volume of hate speech
    and negative sentiment expressed towards particular social groups. In this paper,
    we describe our approach to SemEval-2023 Task 10, involving the detection and
    classification of online sexism (abuse directed towards women), with fine-grained
    categorisations intended to facilitate the development of a more nuanced understanding
    of the ideologies and processes through which online sexism is expressed. We experiment
    with several approaches involving language model finetuning, class-specific adapters,
    and pseudo-labelling. Our best-performing models involve the training of adapters
    specific to each subtask category (combined via fusion layers) using a weighted
    loss function, in addition to performing naive pseudo-labelling on a large quantity
    of unlabelled data. We successfully outperform the baseline models on all 3 subtasks,
    placing 56th (of 84) on Task A, 43rd (of 69) on Task B,and 37th (of 63) on Task
    C.
  authors:
  - Thomas Pickard
  - Tyler Loakman
  - Mugdha Pandya
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_164
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 10: Towards Explainable Detection of Online Sexism'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'shefnlp at SemEval-2023 Task 10: Compute-Efficient Category Adapters'
  tldr: As social media platforms grow, so too does the volume of hate speech and
    negative sentiment expressed towards particular social groups. In this paper,
    we describe our approach to SemEval-2023 Task 10, involving the detection and
    classification of online sexism (abuse directed towards women), with f
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes the development of a system for SemEval-2023 Shared
    Task 11 on Learning with Disagreements (Le-Wi-Di). Labelled data plays a vital
    role in the development of machine learning systems. The human-annotated labels
    are usually considered the truth for training or validation. To obtain truth labels,
    a traditional way is to hire domain experts to perform an expensive annotation
    process. Crowd-sourcing labelling is comparably cheap, whereas it raises a question
    on the reliability of annotators. A common strategy in a mixed-annotator dataset
    with various sets of annotators for each instance is to aggregate the labels among
    multiple groups of annotators to obtain the truth labels.  However, these annotators
    might not reach an agreement, and there is no guarantee of the reliability of
    these labels either. With further problems caused by human label variation, subjective
    tasks usually suffer from the different opinions provided by the annotators. In
    this paper, we propose two simple heuristic functions to compute the annotator
    ranking scores, namely AnnoHard and AnnoSoft, based on the hard labels (i.e.,
    aggregative labels) and soft labels (i.e., cross-entropy values). By introducing
    these scores, we adjust the weights of the training instances to improve the learning
    with disagreements among the annotators.
  authors:
  - Xia Cui
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_165
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 11: Learning with Disagreements (Le-Wi-Di)'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'xiacui at SemEval-2023 Task 11: Learning a Model in Mixed-Annotator Datasets
    Using Annotator Ranking Scores as Training Weights'
  tldr: This paper describes the development of a system for SemEval-2023 Shared Task
    11 on Learning with Disagreements (Le-Wi-Di). Labelled data plays a vital role
    in the development of machine learning systems. The human-annotated labels are
    usually considered the truth for training or validation. To obta
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper presents a study on the effectiveness of various approaches
    for addressing the challenge of multilingual sentiment analysis in low-resource
    African languages. . The approaches evaluated in the study include Support Vector
    Machines (SVM), translation, and an ensemble of pre-trained multilingual sentimental
    models methods. The paper provides a detailed analysis of the performance of each
    approach based on experimental results. In our findings, we suggest that the ensemble
    method is the most effective with an F1-Score of 0.68 on the final testing. This
    system ranked 19 out of 33 participants in the competition.
  authors:
  - Alina Hancharova
  - John Wang
  - Mayank Kumar
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_166
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 12: AfriSenti-SemEval: Sentiment Analysis for Low-resource African
    Languages using Twitter Dataset'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Team ISCL_WINTER at SemEval-2023 Task 12:AfriSenti-SemEval: Sentiment Analysis
    for Low-resource African Languages using Twitter Dataset'
  tldr: This paper presents a study on the effectiveness of various approaches for
    addressing the challenge of multilingual sentiment analysis in low-resource African
    languages. . The approaches evaluated in the study include Support Vector Machines
    (SVM), translation, and an ensemble of pre-trained multili
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In this paper, we describe our approach to the clickbait spoiling task
    of SemEval 2023.The core idea behind our system is to leverage pre-trained models
    capable of Question Answering (QA) to extract the spoiler from article texts based
    on the clickbait title without any task-specific training.Since oftentimes, these
    titles are not phrased as questions, we automatically rephrase the clickbait titles
    as questions in order to better suit the pretraining task of the QA-capable models.Also,
    to fit as much relevant context into the model's limited input size as possible,
    we propose to reorder the sentences by their relevance using a semantic similarity
    model.Finally, we evaluate QA as well as text generation models (via prompting)
    to extract the spoiler from the text.Based on the validation data, our final model
    selects each of these components depending on the spoiler type and achieves satisfactory
    zero-shot results.The ideas described in this paper can easily be applied in fine-tuning
    settings.
  authors:
  - Dirk Wangsadirdja
  - Jan Pfister
  - Konstantin Kobs
  - Andreas Hotho
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_167
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 5: Clickbait Spoiling'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Jack-Ryder at SemEval-2023 Task 5: Zero-Shot Clickbait Spoiling by Rephrasing
    Titles as Questions'
  tldr: 'In this paper, we describe our approach to the clickbait spoiling task of
    SemEval 2023.The core idea behind our system is to leverage pre-trained models
    capable of Question Answering (QA) to extract the spoiler from article texts based
    on the clickbait title without any task-specific training.Since '
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: System Description Paper for Task 3 Subtask 1 and 2 of Semeval 2023. The
    paper describes our approach to handling the News Genre Categorisation and Framing
    Detection using RoBERTa and ALBERT models.
  authors:
  - Arjun Khanchandani
  - Nitansh Jain
  - Jatin Bedi
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_168
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 3: Detecting the Category, the Framing, and the Persuasion Techniques
    in Online News in a Multi-lingual Setup'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'MLModeler5 at SemEval-2023 Task 3: Detecting the Category and the Framing
    Techniques in Online News in a Multi-lingual Setup'
  tldr: System Description Paper for Task 3 Subtask 1 and 2 of Semeval 2023. The paper
    describes our approach to handling the News Genre Categorisation and Framing Detection
    using RoBERTa and ALBERT models.
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'In this paper, I describe the approach used in the SemEval 2023 - Task
    10 Explainable Detection of Online Sexism (EDOS) competition (Kirk et al., 2023).
    I use different transformermodels, including BERT and RoBERTa which were fine-tuned
    on the EDOS dataset to classify text into different categories of sexism. I participated
    in three subtasks: subtask A is to classify given text as either sexist or not,
    while subtask B is to identify the specific category of sexism, such as (1) threats,
    (2) derogation, (3) animosity, (4) prejudiced discussions. Finally, subtask C
    involves predicting a finegrained vector representation of sexism, which included
    information about the severity, target and type of sexism present in the text.
    The use of transformer models allows the system to learn from the input data and
    make predictions on unseen text. By fine-tuning the models on the EDOS dataset,
    the system can improve its performance on the specific task of detecting online
    sexism. I got the following macro F1 scores: subtask A:77.16, subtask B: 46.11,
    and subtask C: 30.2.'
  authors:
  - Madisetty Padmavathi
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_169
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 10: Towards Explainable Detection of Online Sexism'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'DS at SemEval-2023 Task 10: Explaining Online Sexism using Transformer based
    Approach'
  tldr: 'In this paper, I describe the approach used in the SemEval 2023 - Task 10
    Explainable Detection of Online Sexism (EDOS) competition (Kirk et al., 2023).
    I use different transformermodels, including BERT and RoBERTa which were fine-tuned
    on the EDOS dataset to classify text into different categories '
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This task focuses on identifying complex named entities (NEs) in several
    languages. In the context of SemEval-2023 competition, our team presents an exploration
    of a base transformer model''s capabilities regarding the task, focused more specifically
    on five languages (English, Spanish, Swedish, German, Italian). We take DistilBERT
    and BERT as two examples of basic transformer models, using DistilBERT as a baseline
    and BERT as the platform to create an improved model. The dataset that we are
    using, MultiCoNER II, is a large multilingual dataset used for NER, that covers
    domains like: Wiki sentences, questions and search queries across 12 languages.
    This dataset contains 26M tokens and it is assembled from public resources. MultiCoNER
    II defines a NER tag-set with 6 classes and 67 tags.We have managed to get moderate
    results in the English track (we ranked 17th out of 34), while our results in
    the other tracks could be further improved in the future (overall third to last).'
  authors:
  - Viorica-Camelia Lupancu
  - Alexandru-Gabriel Platica
  - Cristian-Mihai Rosu
  - Daniela Gifu
  - Diana Trandabat
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_170
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 2: MultiCoNER II Multilingual Complex Named Entity Recognition'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'FII_Better at SemEval-2023 Task 2: MultiCoNER II Multilingual Complex Named
    Entity Recognition'
  tldr: This task focuses on identifying complex named entities (NEs) in several languages.
    In the context of SemEval-2023 competition, our team presents an exploration of
    a base transformer model's capabilities regarding the task, focused more specifically
    on five languages (English, Spanish, Swedish, Germ
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Social media is the media through which people share their thoughts and
    opinions. This has both its pros and cons which depends on the type of information
    being conveyed. If any information conveyed over social media hurts or affects
    a person, such information can be removed as it may disturb their mental health
    and may decrease their self confidence. During the last decade, hateful and sexist
    content towards women in being increasingly spread on social networks. The exposure
    to sexist speech has serious consequences to women''s life and limits their freedom
    of speech. Sexism is expressed in very different forms: it includes subtle stereotypes
    and attitudes that, although frequently unnoticed, are extremely harmful for both
    women and society. Sexist comments have a major impact on women being subjected
    to it. We as a team participated in the shared task Explainable Detection of Online
    Sexism (EDOS) at SemEval 2023 and have proposed a model which identifies the sexist
    comments and its type from English social media posts using the data set shared
    for the task. Different transformer model like BERT , DistilBERT and RoBERT are
    used by the proposed model for implementing all the three tasks shared by EDOS.
    On using the BERT model, macro F1 score of 0.8073, 0.5876 and 0.3729 are achieved
    for Task A, Task B and Task C respectively.'
  authors:
  - C. Jerin Mahibha
  - C. M Swaathi
  - R. Jeevitha
  - R. Princy Martina
  - Durairaj Thenmozhi
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_171
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 10: Towards Explainable Detection of Online Sexism'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Brainstormers_msec at SemEval-2023 Task 10: Detection of sexism related
    comments in social media using deep learning'
  tldr: 'Social media is the media through which people share their thoughts and opinions.
    This has both its pros and cons which depends on the type of information being
    conveyed. If any information conveyed over social media hurts or affects a person,
    such information can be removed as it may disturb their '
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Rhetorical Roles (RR) prediction is to predict the label of each sentence
    in legal documents, which is regarded as an emergent task for legal document understanding.
    In this study, we present a novel method for the RR task by exploiting the long
    context representation. Specifically, legal documents are known as long texts,
    in which previous works have no ability to consider the inherent dependencies
    among sentences. In this paper, we propose GNNRR (Graph Neural Network for Rhetorical
    Roles Prediction), which is able to model the cross-information for long texts.
    Furthermore, we develop multitask learning by incorporating label shift prediction
    (LSP) for segmenting a legal document. The proposed model is evaluated on the
    SemEval 2023 Task 6 - Legal Eval Understanding Legal Texts for RR sub-task. Accordingly,
    our method achieves the top 4 in the public leaderboard of the sub-task. Our source
    code is available for further investigation\textbackslash{}footnote\{https://github.com/hiepnh137/SemEval2023-Task6-Rhetorical-Roles\}.
  authors:
  - Hiep Nguyen
  - Hoang Ngo
  - Nam Bui
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_172
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 6: LegalEval: Understanding Legal Texts'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: VTCC-NLP at SemEval-2023 Task 6:Long-Text Representation Based on Graph Neural
    Network for Rhetorical Roles Prediction
  tldr: 'Rhetorical Roles (RR) prediction is to predict the label of each sentence
    in legal documents, which is regarded as an emergent task for legal document understanding.
    In this study, we present a novel method for the RR task by exploiting the long
    context representation. Specifically, legal documents '
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Within the scope of the shared task MultiCoNER II our aim was to improve
    the recognition of named entities in English. We as team Minanto fine-tuned a
    cross-lingual model for Named Entity Recognition on English data and achieved
    an average F1 score of 51.47\textbackslash{}\% in the final submission. We found
    that a monolingual model works better on English data than a cross-lingual and
    that the input of external data from earlier Named Entity Recognition tasks provides
    only minor improvements. In this paper we present our system, discuss our results
    and analyze the impact of external data.
  authors:
  - "Antonia H\xF6fer"
  - Mina Mottahedin
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_174
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 2: MultiCoNER II Multilingual Complex Named Entity Recognition'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Minanto at SemEval-2023 Task 2: Fine-tuning XLM-RoBERTa for Named Entity
    Recognition on English Data'
  tldr: Within the scope of the shared task MultiCoNER II our aim was to improve the
    recognition of named entities in English. We as team Minanto fine-tuned a cross-lingual
    model for Named Entity Recognition on English data and achieved an average F1
    score of 51.47\textbackslash{}\% in the final submission.
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper describes the submission to SemEval-2023 Task 2: Multilingual
    Complex Named Entity Recognition (MultiCoNER II) by team SAB. This task aims to
    encourage growth in the field of Named Entity Recognition (NER) by focusing on
    complex and difficult categories of entities, in 12 different language tracks.
    The task of NER has historically shown the best results when a model incorporates
    an external knowledge base or gazetteer, however, less research has been applied
    to examining the effects of incorporating linguistic information into the model.
    In this task, we explored combining NER, part-of-speech (POS), and dependency
    relation labels into a multi-task model and report on the findings. We determine
    that the addition of POS and dependency relation information in this manner does
    not improve results.'
  authors:
  - Siena Biales
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_175
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 2: MultiCoNER II Multilingual Complex Named Entity Recognition'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'SAB at SemEval-2023 Task 2: Does Linguistic Information Aid in Named Entity
    Recognition?'
  tldr: 'This paper describes the submission to SemEval-2023 Task 2: Multilingual
    Complex Named Entity Recognition (MultiCoNER II) by team SAB. This task aims to
    encourage growth in the field of Named Entity Recognition (NER) by focusing on
    complex and difficult categories of entities, in 12 different langua'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'We present our submission to SemEval-2023 Task 10: Explainable Detection
    of Online Sexism (EDOS). We address all three tasks: Task A consists of identifying
    whether a post is sexist. If so, Task B attempts to assign it one of four categories:
    threats, derogation, animosity, and prejudiced discussions. Task C aims for an
    even more fine-grained classification, divided among 11 classes. Our team UniBoe''s
    experiments with fine-tuning of hate-tuned Transformer-based models and priming
    for generative models. In addition, we explore model-agnostic strategies, such
    as data augmentation techniques combined with active learning, as well as obfuscation
    of identity terms. Our official submissions obtain an F1\_score of 0.83 for Task
    A, 0.58 for Task B and 0.32 for Task C.'
  authors:
  - Arianna Muti
  - Francesco Fernicola
  - "Alberto Barr\xF3n-Cede\xF1o"
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_176
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 10: Towards Explainable Detection of Online Sexism'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'UniBoe''s at SemEval-2023 Task 10: Model-Agnostic Strategies for the Improvement
    of Hate-Tuned and Generative Models in the Classification of Sexist Posts'
  tldr: 'We present our submission to SemEval-2023 Task 10: Explainable Detection
    of Online Sexism (EDOS). We address all three tasks: Task A consists of identifying
    whether a post is sexist. If so, Task B attempts to assign it one of four categories:
    threats, derogation, animosity, and prejudiced discussion'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The MultiCoNER II shared task aims at detecting complex, ambiguous named
    entities with fine-grained types in a low context setting. Previous winning systems
    incorporated external knowledge bases to retrieve helpful contexts. In our submission
    we additionally propose splitting the NER task into two stages, a Span Extraction
    Step, and an Entity Classification step. Our results show that the former does
    not suffer from the low context setting comparably, and in so leading to a higher
    overall performance for an external KB-assisted system. We achieve 3rd place on
    the multilingual track and an average of 6th place overall.
  authors:
  - Mohab Elkaref
  - Nathan Herr
  - Shinnosuke Tanaka
  - Geeth De Mel
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_177
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 2: MultiCoNER II Multilingual Complex Named Entity Recognition'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'NLPeople at SemEval-2023 Task 2: A Staged Approach for Multilingual Named
    Entity Recognition'
  tldr: The MultiCoNER II shared task aims at detecting complex, ambiguous named entities
    with fine-grained types in a low context setting. Previous winning systems incorporated
    external knowledge bases to retrieve helpful contexts. In our submission we additionally
    propose splitting the NER task into two s
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'The ability to automatically recognise the rhetorical roles of sentences
    in a legal case judgement is a crucial challenge to tackle since it can be useful
    for a number of activities that come later, such as summarising legal judgements
    and doing legal searches. The task is exigent since legal case documents typically
    lack structure, and their rhetorical roles could be subjective. This paper describes
    SemEval-2023 Task 6: LegalEval: Understanding Legal Texts, Sub-task A: Rhetorical
    Roles Prediction (RR). We propose a system to automatically generate rhetorical
    roles of all the sentences in a legal case document using Hierarchical Bi-LSTM
    CRF model and RoBERTa transformer. We also showcase different techniques used
    to manipulate dataset to generate a set of varying embeddings and train the Hierarchical
    Bi-LSTM CRF model to achieve better performance. Among all, model trained with
    the sent2vec embeddings concatenated with the handcrafted features perform better
    with the micro f1-score of 0.74 on test data.'
  authors:
  - Patchipulusu Sindhu
  - Diya Gupta
  - Sanjeevi Meghana
  - Anand Kumar M
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_178
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 6: LegalEval: Understanding Legal Texts'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'NITK_LEGAL at SemEval-2023 Task 6: A Hierarchical based system for identification
    of Rhetorical Roles in legal judgements'
  tldr: The ability to automatically recognise the rhetorical roles of sentences in
    a legal case judgement is a crucial challenge to tackle since it can be useful
    for a number of activities that come later, such as summarising legal judgements
    and doing legal searches. The task is exigent since legal case d
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In this paper, we have performed sentiment analysis on three African languages
    (Hausa, Swahili, and Yoruba). We used various deep learning and traditional models
    paired with a vectorizer for classification and data -preprocessing. We have also
    used a few data oversampling methods to handle the imbalanced text data. Thus,
    we could analyze the performance of those models in all the languages by using
    weighted and macro F1 scores as evaluation metrics.
  authors:
  - Shashank Rathi
  - Siddhesh Pande
  - Harshwardhan Atkare
  - Rahul Tangsali
  - Aditya Vyawahare
  - Dipali Kadam
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_179
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 12: AfriSenti-SemEval: Sentiment Analysis for Low-resource African
    Languages using Twitter Dataset'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Trinity at SemEval-2023 Task 12: Sentiment Analysis for Low-resource African
    Languages using Twitter Dataset'
  tldr: In this paper, we have performed sentiment analysis on three African languages
    (Hausa, Swahili, and Yoruba). We used various deep learning and traditional models
    paired with a vectorizer for classification and data -preprocessing. We have also
    used a few data oversampling methods to handle the imbal
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper describes our approach for Subtask 1 of Task 3 at SemEval-2023.
    In this subtask, task participants were asked to classify multilingual news articles
    for one of three classes: Reporting, Opinion Piece or Satire. By training an AdapterFusion
    layer composing the task-adapters from different languages, we successfully combine
    the language-exclusive knowledge and show that this improves the results in nearly
    all cases, including in zero-shot scenarios.'
  authors:
  - Fabian Billert
  - Stefan Conrad
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_180
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 3: Detecting the Category, the Framing, and the Persuasion Techniques
    in Online News in a Multi-lingual Setup'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'HHU at SemEval-2023 Task 3: An Adapter-based Approach for News Genre Classification'
  tldr: 'This paper describes our approach for Subtask 1 of Task 3 at SemEval-2023.
    In this subtask, task participants were asked to classify multilingual news articles
    for one of three classes: Reporting, Opinion Piece or Satire. By training an AdapterFusion
    layer composing the task-adapters from different '
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This report describes GMU''s sentiment analysis system for the SemEval-2023
    shared task AfriSenti-SemEval. We participated in all three sub-tasks: Monolingual,
    Multilingual, and Zero-Shot. Our approach uses models initialized with AfroXLMR-large,
    a pre-trained multilingual language model trained on African languages and fine-tuned
    correspondingly. We also introduce augmented training data along with original
    training data. Alongside finetuning, we perform phylogeny-based adapter-tuning
    to create several models and ensemble the best models for the final submission.
    Our system achieves the best F1-score on track 5: Amharic, with 6.2 points higher
    F1-score than the second-best performing system on this track. Overall, our system
    ranks 5th among the 10 systems participating in all 15 tracks.'
  authors:
  - Md Mahfuz Ibn Alam
  - Ruoyu Xie
  - Fahim Faisal
  - Antonios Anastasopoulos
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_181
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 12: AfriSenti-SemEval: Sentiment Analysis for Low-resource African
    Languages using Twitter Dataset'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'GMNLP at SemEval-2023 Task 12: Sentiment Analysis with Phylogeny-Based Adapters'
  tldr: 'This report describes GMU''s sentiment analysis system for the SemEval-2023
    shared task AfriSenti-SemEval. We participated in all three sub-tasks: Monolingual,
    Multilingual, and Zero-Shot. Our approach uses models initialized with AfroXLMR-large,
    a pre-trained multilingual language model trained on A'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Our team silp\_nlp participated in SemEval2023 Task 2: MultiCoNER II.
    Our work made systems for 11 mono-lingual tracks. For leveraging the advantage
    of all track knowledge we chose transformer-based pretrained models, which have
    strong cross-lingual transferability. Hence our model trained in two stages, the
    first stage for multi-lingual learning from all tracks and the second for fine-tuning
    individual tracks. Our work highlights that the knowledge of all tracks can be
    transferred to an individual track if the baseline language model has crosslingual
    features. Our system positioned itself in the top 10 for 4 tracks by scoring 0.7432
    macro F1 score for the Hindi track ( 7th rank ) and 0.7322 macro F1 score for
    the Bangla track ( 9th rank ).'
  authors:
  - Sumit Singh
  - Uma Tiwary
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_182
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 2: MultiCoNER II Multilingual Complex Named Entity Recognition'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Silp_nlp at SemEval-2023 Task 2: Cross-lingual Knowledge Transfer for Mono-lingual
    Learning'
  tldr: 'Our team silp\_nlp participated in SemEval2023 Task 2: MultiCoNER II. Our
    work made systems for 11 mono-lingual tracks. For leveraging the advantage of
    all track knowledge we chose transformer-based pretrained models, which have strong
    cross-lingual transferability. Hence our model trained in two st'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper elaborates on our work in designing a system for SemEval 2023
    Task 12: AfriSentiSemEval, which involves sentiment analysis for low-resource
    African languages using the Twitter dataset. We utilised a pre-trained model to
    perform sentiment classification in Hausa language tweets. We used a multilingual
    version of the roBERTa model, which is pretrained on 100 languages, to classify
    sentiments in Hausa. To tokenize the text, we used the AfriBERTa model, which
    is specifically pretrained on African languages.'
  authors:
  - Nishaanth Ramanathan
  - Rajalakshmi Sivanaiah
  - Angel Deborah S
  - Mirnalinee Thanka Nadar Thanagathai
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_183
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 12: AfriSenti-SemEval: Sentiment Analysis for Low-resource African
    Languages using Twitter Dataset'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'TechSSN at SemEval-2023 Task 12: Monolingual Sentiment Classification in
    Hausa Tweets'
  tldr: 'This paper elaborates on our work in designing a system for SemEval 2023
    Task 12: AfriSentiSemEval, which involves sentiment analysis for low-resource
    African languages using the Twitter dataset. We utilised a pre-trained model to
    perform sentiment classification in Hausa language tweets. We used a '
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Using pre-trained language models to implement classifiers from small
    to modest amounts of training data is an area of active research. The ability
    of large language models to generalize from few-shot examples and to produce strong
    classifiers is extended using the engineering approach of parameter-efficient
    tuning. Using the Explainable Detection of Online Sexism (EDOS) training data
    and a small number of trainable weights to create a tuned prompt vector, a competitive
    model for this task was built, which was top-ranked in Subtask B.
  authors:
  - Jeffrey Sorensen
  - Katerina Korre
  - John Pavlopoulos
  - Katrin Tomanek
  - Nithum Thain
  - Lucas Dixon
  - "L\xE9o Laugier"
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_184
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 10: Towards Explainable Detection of Online Sexism'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'JUAGE at SemEval-2023 Task 10: Parameter Efficient Classification'
  tldr: Using pre-trained language models to implement classifiers from small to modest
    amounts of training data is an area of active research. The ability of large language
    models to generalize from few-shot examples and to produce strong classifiers
    is extended using the engineering approach of parameter-
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In this paper we present an analysis of our approaches for the 2023 SemEval-2023
    Clickbait Challenge. We only participated in the sub-task aiming at identifying
    different clikcbait spoiling types comparing several machine learning and deep
    learning approaches.Our analysis confirms previous results on this task and show
    that automatic methods are able to reach approximately 70\textbackslash{}\% accuracy
    at predicting what type of additional content is needed to mitigate sensationalistic
    posts on social media. Furthermore, we provide a qualitative analysis of the results,
    showing that the models may do better in practice than the metric indicates since
    the evaluate does not depend only on the predictor, but also on the typology we
    choose to define clickbait spoiling.
  authors:
  - Dragos-stefan Mihalcea
  - Sergiu Nisioi
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_185
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 5: Clickbait Spoiling'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Clark Kent at SemEval-2023 Task 5: SVMs, Transformers, and Pixels for Clickbait
    Spoiling'
  tldr: In this paper we present an analysis of our approaches for the 2023 SemEval-2023
    Clickbait Challenge. We only participated in the sub-task aiming at identifying
    different clikcbait spoiling types comparing several machine learning and deep
    learning approaches.Our analysis confirms previous results o
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The SemEval-2023 Task 3 competition offers participants a multi-lingual
    dataset with three schemes one for each subtask. The competition challenges participants
    to construct machine learning systems that can categorize news articles based
    on their nature and style of writing. We esperiment with many state-of-the-art
    transformer-based language models proposed in the natural language processing
    literature and report the results of the best ones. Our top performing model is
    based on a transformer called "Longformer" and has achieved an F1-Micro score
    of 0.256 on the English version of subtask-1 and F1-Macro of 0.442 on subtask-2
    on the test data. We also experiment with a number of state-of-the-art multi-lingual
    transformer-based models and report the results of the best performing ones.
  authors:
  - Ahmed Al-Qarqaz
  - Malak Abdullah
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_186
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Task-1 - Visual Word Sense Disambiguation (Visual-WSD)
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Team JUSTR00 at SemEval-2023 Task 3: Transformers for News Articles Classification'
  tldr: The SemEval-2023 Task 3 competition offers participants a multi-lingual dataset
    with three schemes one for each subtask. The competition challenges participants
    to construct machine learning systems that can categorize news articles based
    on their nature and style of writing. We esperiment with many
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper proposes an approach to classify andan approach to generate
    spoilers for clickbaitarticles and posts. For the spoiler classification,XLNET
    was trained to fine-tune a model. Withan accuracy of 0.66, 2 out of 3 spoilers
    arepredicted accurately. The spoiler generationapproach involves preprocessing
    the clickbaittext and post-processing the output to fit thespoiler type. The approach
    is evaluated on atest dataset of 1000 posts, with the best resultfor spoiler generation
    achieved by fine-tuninga RoBERTa Large model with a small learningrate and sample
    size, reaching a BLEU scoreof 0.311. The paper provides an overview ofthe models
    and techniques used and discussesthe experimental setup.
  authors:
  - "Pia St\xF6rmer"
  - Tobias Esser
  - Patrick Thomasius
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_187
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 5: Clickbait Spoiling'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Sam Miller at SemEval-2023 Task 5: Classification and Type-specific Spoiler
    Extraction Using XLNET and Other Transformer Models'
  tldr: This paper proposes an approach to classify andan approach to generate spoilers
    for clickbaitarticles and posts. For the spoiler classification,XLNET was trained
    to fine-tune a model. Withan accuracy of 0.66, 2 out of 3 spoilers arepredicted
    accurately. The spoiler generationapproach involves prepro
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This work presents the approach developed by the DUTH team for participating
    in the SemEval-2023 Task 9: Multilingual Tweet Intimacy Analysis. Our results
    show that pre-processing techniques do not affect the learning performance for
    the task of multilingual intimacy analysis. In addition, we show that fine-tuning
    a transformer-based model does not provide advantages over using the pre-trained
    model to generate text embeddings and using the resulting representations to train
    simpler and more efficient models such as MLP. Finally, we utilize an ensemble
    of classifiers, including three MLPs with different architectures and a CatBoost
    model, to improve the regression accuracy.'
  authors:
  - Giorgos Arampatzis
  - Vasileios Perifanis
  - Symeon Symeonidis
  - Avi Arampatzis
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_188
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 9: Multilingual Tweet Intimacy Analysis'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'DUTH at SemEval-2023 Task 9: An Ensemble Approach for Twitter Intimacy Analysis'
  tldr: 'This work presents the approach developed by the DUTH team for participating
    in the SemEval-2023 Task 9: Multilingual Tweet Intimacy Analysis. Our results
    show that pre-processing techniques do not affect the learning performance for
    the task of multilingual intimacy analysis. In addition, we show t'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes our submission to Task 10 at SemEval 2023-Explainable
    Detection of Online Sexism (EDOS), divided into three subtasks. The recent rise
    in social media platforms has seen an increase in disproportionate levels of sexism
    experienced by women on social media platforms. This has made detecting and explaining
    online sexist content more important than ever to make social media safer and
    more accessible for women. Our approach consists of experimenting and finetuning
    BERT-based models and using a Majority Voting ensemble model that outperforms
    individual baseline model scores. Our system achieves a macro F1 score of 0.8392
    for Task A, 0.6092 for Task B, and 0.4319 for Task C.
  authors:
  - Sriya Rallabandi
  - Sanchit Singhal
  - Pratinav Seth
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_189
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 10: Towards Explainable Detection of Online Sexism'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'SSS at SemEval-2023 Task 10: Explainable Detection of Online Sexism using
    Majority Voted Fine-Tuned Transformers'
  tldr: This paper describes our submission to Task 10 at SemEval 2023-Explainable
    Detection of Online Sexism (EDOS), divided into three subtasks. The recent rise
    in social media platforms has seen an increase in disproportionate levels of sexism
    experienced by women on social media platforms. This has made
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Misinformation spreading in mainstream and social media has been misleading
    users in different ways. Manual detection and verification efforts by journalists
    and fact-checkers can no longer cope with the great scale and quick spread of
    misleading information. This motivated research and industry efforts to develop
    systems for analyzing and verifying news spreading online. The SemEval-2023 Task
    3 is an attempt to address several subtasks under this overarching problem, targeting
    writing techniques used in news articles to affect readers' opinions. The task
    addressed three subtasks with six languages, in addition to three "surprise" test
    languages, resulting in 27 different test setups. This paper describes our participating
    system to this task. Our team is one of the 6 teams that successfully submitted
    runs for all setups. The official results show that our system is ranked among
    the top 3 systems for 10 out of the 27 setups.
  authors:
  - Maram Hasanain
  - Ahmed El-Shangiti
  - Rabindra Nath Nandi
  - Preslav Nakov
  - Firoj Alam
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_190
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 3: Detecting the Category, the Framing, and the Persuasion Techniques
    in Online News in a Multi-lingual Setup'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'QCRI at SemEval-2023 Task 3: News Genre, Framing and Persuasion Techniques
    Detection Using Multilingual Models'
  tldr: Misinformation spreading in mainstream and social media has been misleading
    users in different ways. Manual detection and verification efforts by journalists
    and fact-checkers can no longer cope with the great scale and quick spread of
    misleading information. This motivated research and industry eff
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper presents our work on LegalEval (understanding legal text),
    one of the tasks in SemEval-2023. It comprises of three sub-tasks namely Rhetorical
    Roles (RR), Legal Named Entity Recognition (L-NER), and Court Judge- ment Prediction
    with Explanation (CJPE). We developed different deep-learning models for each
    sub-tasks. For RR, we developed a multi- task learning model with contextual sequential
    sentence classification as the main task and non- contextual single sentence prediction
    as the sec- ondary task. Our model achieved an F1-score of 76.50\% on the unseen
    test set, and we at- tained the 14th position on the leaderboard. For the L-NER
    problem, we have designed a hybrid model, consisting of a multi-stage knowledge
    transfer learning framework and a rule-based system. This model achieved an F1-score
    of 91.20\% on the blind test set and attained the top position on the final leaderboard.
    Finally, for the CJPE task, we used a hierarchical ap- proach and could get around
    66.67\% F1-score on judgment prediction and 45.83\% F1-score on the explainability
    of the CJPE task, and we attained 8th position on the leaderboard for this sub-task.
  authors:
  - Dhanachandra Ningthoujam
  - Pinal Patel
  - Rajkamal Kareddula
  - Ramanand Vangipuram
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_191
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 6: LegalEval: Understanding Legal Texts'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'ResearchTeam_HCN at SemEval-2023 Task 6: A knowledge enhanced transformers
    based legal NLP system'
  tldr: This paper presents our work on LegalEval (understanding legal text), one
    of the tasks in SemEval-2023. It comprises of three sub-tasks namely Rhetorical
    Roles (RR), Legal Named Entity Recognition (L-NER), and Court Judge- ment Prediction
    with Explanation (CJPE). We developed different deep-learning
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This study introduces the system submitted to the SemEval 2022 Task 2:
    MultiCoNER II (Multilingual Complex Named Entity Recognition) by the LSJSP team.
    We propose FTBC, a FastText-based framework with pre-trained Bert for NER tasks
    with complex entities and over a noisy dataset.Our system achieves an average
    of 58.27\% F1 score (fine-grained) and 75.79\% F1 score (coarse-grained) across
    all languages. FTBC outperforms the baseline BERT-CRF model on all 12 monolingual
    tracks.'
  authors:
  - Shilpa Chatterjee
  - Leo Evenss
  - Pramit Bhattacharyya
  - Joydeep Mondal
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_192
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 2: MultiCoNER II Multilingual Complex Named Entity Recognition'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'LSJSP at SemEval-2023 Task 2: FTBC: A FastText based framework with pre-trained
    BERT for NER'
  tldr: 'This study introduces the system submitted to the SemEval 2022 Task 2: MultiCoNER
    II (Multilingual Complex Named Entity Recognition) by the LSJSP team. We propose
    FTBC, a FastText-based framework with pre-trained Bert for NER tasks with complex
    entities and over a noisy dataset.Our system achieves a'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The web contains an abundance of user- generated content. While this content
    is useful for many applications, it poses many challenges due to the presence
    of offensive, biased, and overall toxic language. In this work, we present a system
    that identifies and classifies sexist content at different levels of granularity.
    Using transformer-based models, we explore the value of data augmentation, use
    of ensemble methods, and leverage in-context learning using foundation models
    to tackle the task. We evaluate the different components of our system both quantitatively
    and qualitatively. Our best systems achieve an F1 score of 0.84 for the binary
    classification task  aiming to identify whether a given content is sexist or not  and
    0.64 and 0.47 for the two multi-class tasks that aim to identify the coarse and
    fine-grained types of sexism present in the given content respectively.
  authors:
  - Weston Feely
  - Prabhakar Gupta
  - Manas Ranjan Mohanty
  - Timothy Chon
  - Tuhin Kundu
  - Vijit Singh
  - Sandeep Atluri
  - Tanya Roosta
  - Viviane Ghaderi
  - Peter Schulam
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_193
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 10: Towards Explainable Detection of Online Sexism'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'QCon at SemEval-2023 Task 10: Data Augmentation and Model Ensembling for
    Detection of Online Sexism'
  tldr: The web contains an abundance of user- generated content. While this content
    is useful for many applications, it poses many challenges due to the presence
    of offensive, biased, and overall toxic language. In this work, we present a system
    that identifies and classifies sexist content at different le
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Semeval 2023 task 1: VWSD, In this paper, we propose an ensemble of two
    Neural network systems that ranks 10 images given a word and limited textual context.
    We have used openAI Clip based models for the English language and multilingual
    text-to-text translation models for Farsi-to-English and Italian-to-English. Additionally,
    we propose a system that learns from multilingual bert-base embeddings for text
    and resnet101 embeddings for the image. Considering all the three languages into
    account this system has achieved the fourth rank.'
  authors:
  - Rahul Patil
  - Pinal Patel
  - Charin Patel
  - Mangal Verma
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_194
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Task-1 - Visual Word Sense Disambiguation (Visual-WSD)
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Rahul Patil at SemEval-2023 Task 1: V-WSD: Visual Word Sense Disambiguation'
  tldr: 'Semeval 2023 task 1: VWSD, In this paper, we propose an ensemble of two Neural
    network systems that ranks 10 images given a word and limited textual context.
    We have used openAI Clip based models for the English language and multilingual
    text-to-text translation models for Farsi-to-English and Itali'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: To precisely identify the different forms of online sexism, we utilize
    several sentence transformer models such as ALBERT, BERT, RoBERTa, DistilBERT,
    and XLNet. By combining the predictions from these models, we can generate a more
    comprehensive and improved result. Each transformer model is trained after pre-processing
    the data from the training dataset, ensuring that the models are effective at
    detecting and classifying instances of online sexism. For Task A, the model had
    to classify the texts as sexist or not sexist. We implemented ALBERT, an NLP-based
    sentence transformer. For task B, we implemented BERT, RoBERTa, DistilBERT and
    XLNet and took the mode of predictions for each text as the final prediction for
    the given text. For task C, we implemented ALBERT, BERT, RoBERTa, DistilBERT and
    XLNet and took the mode of predictions as the final prediction for the given text.
  authors:
  - Shruti Sriram
  - Padma Pooja Chandran
  - Shrijith M R
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_195
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 10: Towards Explainable Detection of Online Sexism'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'PoSh at SemEval-2023 Task 10: Explainable Detection of Online Sexism'
  tldr: To precisely identify the different forms of online sexism, we utilize several
    sentence transformer models such as ALBERT, BERT, RoBERTa, DistilBERT, and XLNet.
    By combining the predictions from these models, we can generate a more comprehensive
    and improved result. Each transformer model is trained
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Named Entity Recognition (NER) is a subtask of Natural Language Processing
    (NLP) that involves identifying and categorizing named entities. The result annotation
    makes unstructured natural texts applicable for other NLP tasks, including information
    retrieval, question answering, and machine translation. NER is also essential
    in legal as an initial stage in extracting relevant entities. However, legal texts
    contain domain-specific named entities, such as applicants, defendants, courts,
    statutes, and articles. The latter makes standard named entity recognizers incompatible
    with legal documents. This paper proposes an approach combining multiple models'
    results via a voting mechanism for unique entity identification in legal texts.
    This endeavor focuses on extracting legal named entities, and the specific assignment
    (task B) is to create a legal NER system for unique entity annotation in legal
    documents. The results of our experiments and system implementation are published
    in https://github.com/SuperEDG/Legal\_Project.
  authors:
  - Junzhe Zhao
  - Yingxi Wang
  - Nicolay Rusnachenko
  - Huizhi Liang
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_196
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 6: LegalEval: Understanding Legal Texts'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Legal_try at SemEval-2023 Task 6: Voting Heterogeneous Models for Entities
    identification in Legal Documents'
  tldr: Named Entity Recognition (NER) is a subtask of Natural Language Processing
    (NLP) that involves identifying and categorizing named entities. The result annotation
    makes unstructured natural texts applicable for other NLP tasks, including information
    retrieval, question answering, and machine translat
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We present our entry to the Multi-evidence Natural Language Inference
    for Clinical Trial Datatask at SemEval 2023. We submitted entries forboth the
    evidence retrieval and textual entailment sub-tasks. For the evidence retrieval
    task,we fine-tuned the PubMedBERT transformermodel to extract relevant evidence
    from clinicaltrial data given a hypothesis concerning either asingle clinical
    trial or pair of clinical trials. Ourbest performing model achieved an F1 scoreof
    0.804. For the textual entailment task, inwhich systems had to predict whether
    a hypothesis about either a single clinical trial or pair ofclinical trials is
    true or false, we fine-tuned theBioLinkBERT transformer model. We passedour evidence
    retrieval model's output into ourtextual entailment model and submitted its output
    for the evaluation. Our best performingmodel achieved an F1 score of 0.695.
  authors:
  - Robert Bevan
  - Oisn Turbitt
  - Mouhamad Aboshokor
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_197
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 7: Multi-Evidence Natural Language Inference for Clinical Trial
    Data'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'MDC at SemEval-2023 Task 7: Fine-tuning Transformers for Textual Entailment
    Prediction and Evidence Retrieval in Clinical Trials'
  tldr: We present our entry to the Multi-evidence Natural Language Inference for
    Clinical Trial Datatask at SemEval 2023. We submitted entries forboth the evidence
    retrieval and textual entailment sub-tasks. For the evidence retrieval task,we
    fine-tuned the PubMedBERT transformermodel to extract relevant e
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper describes our submission to the SemEval-2023 for Task 6 on
    LegalEval: Understanding Legal Texts. Our submission concentrated on three subtasks:
    Legal Named Entity Recognition (L-NER) for Task-B, Legal Judgment Prediction (LJP)
    for Task-C1, and Court Judgment Prediction with Explanation (CJPE) for Task-C2.
    We conducted various experiments on these subtasks and presented the results in
    detail, including data statistics and methodology. It is worth noting that legal
    tasks, such as those tackled in this research, have been gaining importance due
    to the increasing need to automate legal analysis and support. Our team obtained
    competitive rankings of 15th, 11th, and 1st in Task-B, Task-C1, and Task-C2, respectively,
    as reported on the leaderboard.'
  authors:
  - Shubham Kumar Nigam
  - Aniket Deroy
  - Noel Shallum
  - Ayush Kumar Mishra
  - Anup Roy
  - Shubham Kumar Mishra
  - Arnab Bhattacharya
  - Saptarshi Ghosh
  - Kripabandhu Ghosh
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_198
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 6: LegalEval: Understanding Legal Texts'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Nonet at SemEval-2023 Task 6: Methodologies for Legal Evaluation'
  tldr: 'This paper describes our submission to the SemEval-2023 for Task 6 on LegalEval:
    Understanding Legal Texts. Our submission concentrated on three subtasks: Legal
    Named Entity Recognition (L-NER) for Task-B, Legal Judgment Prediction (LJP) for
    Task-C1, and Court Judgment Prediction with Explanation (C'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Intimacy estimation of a given text has recently gained importance due
    to the increase in direct interaction of NLP systems with humans. Intimacy is
    an important aspect of natural language and has a substantial impact on our everyday
    communication. Thus the level of intimacy can provide us with deeper insights
    and richer semantics of conversations. In this paper, we present our work on the
    SemEval shared task 9 on predicting the level of intimacy for the given text.
    The dataset consists of tweets in ten languages, out of which only six are available
    in the training dataset. We conduct several experiments and show that an ensemble
    of multilingual models along with a language-specific monolingual model has the
    best performance. We also evaluate other data augmentation methods such as translation
    and present the results. Lastly, we study the results thoroughly and present some
    noteworthy insights into this problem.
  authors:
  - Tanmay Chavan
  - Ved Patwardhan
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_199
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 9: Multilingual Tweet Intimacy Analysis'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'ChaPat at SemEval-2023 Task 9: Text Intimacy Analysis using Ensembles of
    Multilingual Transformers'
  tldr: Intimacy estimation of a given text has recently gained importance due to
    the increase in direct interaction of NLP systems with humans. Intimacy is an
    important aspect of natural language and has a substantial impact on our everyday
    communication. Thus the level of intimacy can provide us with deep
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Detecting harmful content on social media plat-forms is crucial in preventing
    the negative ef-fects these posts can have on social media users.This paper presents
    our methodology for tack-ling task 10 from SemEval23, which focuseson detecting
    and classifying online sexism insocial media posts. We constructed our solu-tion
    using an ensemble of transformer-basedmodels (that have been fine-tuned; BERTweet,RoBERTa,
    and DeBERTa). To alleviate the var-ious issues caused by the class imbalance inthe
    dataset provided and improve the general-ization of our model, our framework employsdata
    augmentation and semi-supervised learn-ing. Specifically, we use back-translation
    fordata augmentation in two scenarios: augment-ing the underrepresented class
    and augment-ing all classes. In this study, we analyze theimpact of these different
    strategies on the sys-tem''s overall performance and determine whichtechnique
    is the most effective. Extensive ex-periments demonstrate the efficacy of our
    ap-proach. For sub-task A, the system achievedan F1-score of 0.8613. The source
    code to re-produce the proposed solutions is available onGithub'
  authors:
  - Israel Abebe Azime
  - Sana Al-azzawi
  - Atnafu Lambebo Tonja
  - Iyanuoluwa Shode
  - Jesujoba Alabi
  - Ayodele Awokoya
  - Mardiyyah Oduwole
  - Tosin Adewumi
  - Samuel Fanijo
  - Awosan Oyinkansola
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_200
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 12: AfriSenti-SemEval: Sentiment Analysis for Low-resource African
    Languages using Twitter Dataset'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Masakhane-Afrisenti at SemEval-2023 Task 12: Sentiment Analysis using Afro-centric
    Language Models and Adapters for Low-resource African Languages'
  tldr: Detecting harmful content on social media plat-forms is crucial in preventing
    the negative ef-fects these posts can have on social media users.This paper presents
    our methodology for tack-ling task 10 from SemEval23, which focuseson detecting
    and classifying online sexism insocial media posts. We co
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'The paper describes a transformer-based system designed for SemEval-2023
    Task 9: Multilingual Tweet Intimacy Analysis. The purpose of the task was to predict
    the intimacy of tweets in a range from 1 (not intimate at all) to 5 (very intimate).
    The official training set for the competition consisted of tweets in six languages
    (English, Spanish, Italian, Portuguese, French, and Chinese). The test set included
    the given six languages as well as external data with four languages not presented
    in the training set (Hindi, Arabic, Dutch, and Korean). We presented a solution
    based on an ensemble of XLM-T, a multilingual RoBERTa model adapted to the Twitter
    domain. To improve the performance on unseen languages, each tweet was supplemented
    by its English translation. We explored the effectiveness of translated data for
    the languages seen in fine-tuning compared to unseen languages and estimated strategies
    for using translated data in transformer-based models. Our solution ranked 4th
    on the leaderboard while achieving an overall Pearson''s r of 0.5989 over the
    test set. The proposed system improves up to 0.088 Pearson''s r over a score averaged
    across all 45 submissions.'
  authors:
  - Anna Glazkova
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_201
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 9: Multilingual Tweet Intimacy Analysis'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'tmn at SemEval-2023 Task 9: Multilingual Tweet Intimacy Detection Using
    XLM-T, Google Translate, and Ensemble Learning'
  tldr: 'The paper describes a transformer-based system designed for SemEval-2023
    Task 9: Multilingual Tweet Intimacy Analysis. The purpose of the task was to predict
    the intimacy of tweets in a range from 1 (not intimate at all) to 5 (very intimate).
    The official training set for the competition consisted o'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The rise of the internet and social media platforms has brought about
    significant changes in how people interact with each another. For a lot of people,
    the internet have also become the only source of news and information about the
    world. Thus due to the increase in accessibility of information, online sexism
    has also increased. Efforts should be made to make the internet a safe space for
    everyone, irrespective of gender, both from a larger social norms perspective
    and legal or technical regulations to help alleviate online gender-based violence.
    As a part of this, this paper explores simple methods that can be easily deployed
    to automatically detect online sexism in textual statements.
  authors:
  - Judith Jeyafreeda Andrew
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_202
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 10: Towards Explainable Detection of Online Sexism'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'JudithJeyafreeda at SemEval-2023 Task 10: Machine Learning for Explainable
    Detection of Online Sexism'
  tldr: The rise of the internet and social media platforms has brought about significant
    changes in how people interact with each another. For a lot of people, the internet
    have also become the only source of news and information about the world. Thus
    due to the increase in accessibility of information, on
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We study the influence of different activation functions in the output
    layer of pre-trained transformer models for soft and hard label prediction in
    the learning with disagreement task. In this task, the goal is to quantify the
    amount of disagreement via predicting soft labels. To predict the soft labels,
    we use BERT-based preprocessors and encoders and vary the activation function
    used in the output layer, while keeping other parameters constant. The soft labels
    are then used for the hard label prediction. The activation functions considered
    are sigmoid as well as a step-function that is added to the model post-training
    and a sinusoidal activation function, which is introduced for the first time in
    this paper.
  authors:
  - Peyman Hosseini
  - Mehran Hosseini
  - Sana Al-azzawi
  - Marcus Liwicki
  - Ignacio Castro
  - Matthew Purver
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_203
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 11: Learning with Disagreements (Le-Wi-Di)'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: "Lon-e\xE5 at SemEval-2023 Task 11: A Comparison of Activation Functions\
    \ for Soft and Hard Label Prediction"
  tldr: We study the influence of different activation functions in the output layer
    of pre-trained transformer models for soft and hard label prediction in the learning
    with disagreement task. In this task, the goal is to quantify the amount of disagreement
    via predicting soft labels. To predict the soft l
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Named Entity Recognition (NER) is a core natural language processing
    task in which pre-trained language models have shown remarkable performance. However,
    standard benchmarks like CoNLL 2003 do not address many of the challenges that
    deployed NER systems face, such as having to classify emerging or complex entities
    in a fine-grained way. In this paper we present a novel NER cascade approach comprising
    three steps: first, identifying candidate entities in the input sentence; second,
    linking the each candidate to an existing knowledge base; third, predicting the
    fine-grained category for each entity candidate. We empirically demonstrate the
    significance of external knowledge bases in accurately classifying fine-grained
    and emerging entities. Our system exhibits robust performance in the MultiCoNER2
    shared task, even in the low-resource language setting where we leverage knowledge
    bases of high-resource languages.'
  authors:
  - "Iker Garc\xEDa-Ferrero"
  - Jon Ander Campos
  - Oscar Sainz
  - Ander Salaberria
  - Dan Roth
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_204
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 2: MultiCoNER II Multilingual Complex Named Entity Recognition'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'IXA/Cogcomp at SemEval-2023 Task 2: Context-enriched Multilingual Named
    Entity Recognition Using Knowledge Bases'
  tldr: 'Named Entity Recognition (NER) is a core natural language processing task
    in which pre-trained language models have shown remarkable performance. However,
    standard benchmarks like CoNLL 2003 do not address many of the challenges that
    deployed NER systems face, such as having to classify emerging or '
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper describes the system and experimental results of an ensemble-based
    approach tomultilingual framing detection for the submission of the ACCEPT team
    to the SemEval-2023 Task 3 on Framing Detection (Subtask 2). The approach is based
    on an ensemble that combines three different methods: a classifier based on large
    language models, a classifier based on static word embeddings, and an approach
    that uses external commonsense knowledge graphs, in particular, ConceptNet. The
    results of the three classification heads are aggregated into an overall prediction
    for each frame class.Our best submission yielded a micro F1-score of 50.69\% (rank
    10) and a macro F1-score of 50.20\% (rank 3) for English articles. Our experimental
    results show that static word embeddings and knowledge graphs are useful components
    for frame detection, while the ensemble of all three methods combines the strengths
    of our three proposed methods. Through system ablations, we show that the commonsenseguided
    knowledge graphs are the outperforming method for many languages.'
  authors:
  - Philipp Heinisch
  - Moritz Plenz
  - Anette Frank
  - Philipp Cimiano
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_205
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 3: Detecting the Category, the Framing, and the Persuasion Techniques
    in Online News in a Multi-lingual Setup'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'ACCEPT at SemEval-2023 Task 3: An Ensemble-based Approach to Multilingual
    Framing Detection'
  tldr: 'This paper describes the system and experimental results of an ensemble-based
    approach tomultilingual framing detection for the submission of the ACCEPT team
    to the SemEval-2023 Task 3 on Framing Detection (Subtask 2). The approach is based
    on an ensemble that combines three different methods: a cla'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper presents a hierarchical similarity-aware approach for the SemEval-2023
    task 4 human value detection behind arguments using SBERT. The approach takes
    similarity score as an additional source of information between the input arguments
    and the lower level of labels in a human value hierarchical dataset. Our similarity-aware
    model improved the similarity-agnostic baseline model, especially showing a significant
    increase in or the value categories with lowest scores by the baseline model.
  authors:
  - Sumire Honda
  - Sebastian Wilharm
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_206
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 4: ValueEval: Identification of Human Values behind Arguments'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Noam Chomsky at SemEval-2023 Task 4: Hierarchical Similarity-aware Model
    for Human Value Detection'
  tldr: This paper presents a hierarchical similarity-aware approach for the SemEval-2023
    task 4 human value detection behind arguments using SBERT. The approach takes
    similarity score as an additional source of information between the input arguments
    and the lower level of labels in a human value hierarchi
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The analysis of legal cases poses a considerable challenge for researchers,
    practitioners, and academicians due to the lengthy and intricate nature of these
    documents. Developing countries such as India are experiencing a significant increase
    in the number of pending legal cases, which are often unstructured and difficult
    to process using conventional methods. To address this issue, the authors have
    implemented a sequential sentence classification process, which categorizes legal
    documents into 13 segments, known as Rhetorical Roles. This approach enables the
    extraction of valuable insights from the various classes of the structured document.
    The performance of this approach was evaluated using the F1 score, which measures
    the model's precision and recall. The authors' approach achieved an F1 score of
    0.83, which surpasses the baseline score of 0.79 established by the task organizers.
    The authors have combined sequential sentence classification and the SetFit method
    in a hierarchical manner by combining similar classes to achieve this score.
  authors:
  - Harsh Kataria
  - Ambuje Gupta
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_207
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 6: LegalEval: Understanding Legal Texts'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'NLP-Titan at SemEval-2023 Task 6: Identification of Rhetorical Roles Using
    Sequential Sentence Classification'
  tldr: The analysis of legal cases poses a considerable challenge for researchers,
    practitioners, and academicians due to the lengthy and intricate nature of these
    documents. Developing countries such as India are experiencing a significant increase
    in the number of pending legal cases, which are often uns
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The Explainable Detection of Online Sexism task presents the problem of
    explainable sexism detection through fine-grained categorisation of sexist cases
    with three subtasks. Our team experimented with different ways to combat class
    imbalance throughout the tasks using data augmentation and loss alteration techniques.
    We tackled the challenge by utilising ensembles of Transformer models trained
    on different datasets, which are tested to find the balance between performance
    and interpretability. This solution ranked us in the top 40\% of teams for each
    of the tracks.
  authors:
  - Adam Rydelek
  - Daryna Dementieva
  - Georg Groh
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_208
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 10: Towards Explainable Detection of Online Sexism'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'AdamR at SemEval-2023 Task 10: Solving the Class Imbalance Problem in Sexism
    Detection with Ensemble Learning'
  tldr: The Explainable Detection of Online Sexism task presents the problem of explainable
    sexism detection through fine-grained categorisation of sexist cases with three
    subtasks. Our team experimented with different ways to combat class imbalance
    throughout the tasks using data augmentation and loss alte
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper presents the approaches proposedfor I2C Group to address the
    SemEval-2023Task 4: Identification of Human Values behindArguments (ValueEval)",
    whose goal is to classify 20 different categories of human valuesgiven a textual
    argument. The dataset of thistask consists of one argument per line, including
    its unique argument ID, conclusion, stanceof the premise towards the conclusion
    and thepremise text. To indicate whether the argumentdraws or not on that category
    a binary indication (1 or 0) is included. Participants can submit approaches that
    detect one, multiple, or allof these values in arguments. The task providesan
    opportunity for researchers to explore theuse of automated techniques to identify
    humanvalues in text and has potential applications invarious domains such as social
    science, politics,and marketing. To deal with the imbalancedclass distribution
    given, our approach undersamples the data. Additionally, the three components
    of the argument (conclusion, stanceand premise) are used for training. The systemoutperformed
    the BERT baseline according toofficial evaluation metrics, achieving a f1 scoreof
    0.46.'
  authors:
  - Nordin El Balima Cordero
  - "Jacinto Mata V\xE1zquez"
  - "Victoria Pach\xF3n \xC1lvarez"
  - Abel Pichardo Estevez
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_209
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 4: ValueEval: Identification of Human Values behind Arguments'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'I2C Huelva at SemEval-2023 Task 4: A Resampling and Transformers Approach
    to Identify Human Values behind Arguments'
  tldr: 'This paper presents the approaches proposedfor I2C Group to address the SemEval-2023Task
    4: Identification of Human Values behindArguments (ValueEval)", whose goal is
    to classify 20 different categories of human valuesgiven a textual argument. The
    dataset of thistask consists of one argument per lin'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Extracting of NERs from low-resource languages and recognizing their types
    is one of the important tasks in the entity extraction domain. Recently many studies
    have been conducted in this area of research. In our study, we introduce a system
    for identifying complex entities and recognizing their types from low-resource
    language Bangla, which was published in SemEval Task 2 MulitCoNER II 2023. For
    this sequence labeling task, we use a pre-trained language model built on a natural
    language processing framework. Our team name in this competition is \textbackslash{}textbf\{MLlab4CS\}.
    Our model \textbackslash{}textbf\{\textbackslash{}Muril\} produces a macro average
    F-score of 76.27\textbackslash{}\%, which is a comparable result for this competition.
  authors:
  - Shrimon Mukherjee
  - Madhusudan Ghosh
  - Girish .
  - Partha Basuchowdhuri
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_210
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 2: MultiCoNER II Multilingual Complex Named Entity Recognition'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'MLlab4CS at SemEval-2023 Task 2: Named Entity Recognition in Low-resource
    Language Bangla Using Multilingual Language Models'
  tldr: Extracting of NERs from low-resource languages and recognizing their types
    is one of the important tasks in the entity extraction domain. Recently many studies
    have been conducted in this area of research. In our study, we introduce a system
    for identifying complex entities and recognizing their typ
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper presents a solution for Semeval 2023 subtask3 of task 3: persuasion
    techniques in paragraphs detection. The aim of this task is to identify all persuasion
    techniques in each paragraph of a given news article. We use hierarchical multitask
    neural networks combined with transformers. Span detection is an auxiliary task
    that helps in the main task: identifying propaganda techniques. Our experiments
    show that if we change the index of BERT embedding from the first token of the
    whole input to the first token of the identified  span, it can improve performance.
    Span and label detection can be performed using one network, so we save data and,
    when data is limited, we can use more of it for training.'
  authors:
  - Katarzyna Baraniak
  - M Sydow
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_211
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 3: Detecting the Category, the Framing, and the Persuasion Techniques
    in Online News in a Multi-lingual Setup'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Kb at SemEval-2023 Task 3: On Multitask Hierarchical BERT Base Neural Network
    for Multi-label Persuasion Techniques Detection'
  tldr: 'This paper presents a solution for Semeval 2023 subtask3 of task 3: persuasion
    techniques in paragraphs detection. The aim of this task is to identify all persuasion
    techniques in each paragraph of a given news article. We use hierarchical multitask
    neural networks combined with transformers. Span d'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The use of Natural Language Processing techniques in the legal domain
    has become established for supporting attorneys and domain experts in content
    retrieval and decision-making. However, understanding the legal text poses relevant
    challenges in the recognition of domain-specific entities and the adaptation and
    explanation of predictive models. This paper addresses the Legal Entity Name Recognition
    (L-NER) and Court judgment Prediction (CPJ) and Explanation (CJPE) tasks. The
    L-NER solution explores the use of various transformer-based models, including
    an entity-aware method attending domain-specific entities. The CJPE proposed method
    relies on hierarchical BERT-based classifiers combined with local input attribution
    explainers. We propose a broad comparison of eXplainable AI methodologies along
    with a novel approach based on NER. For the L-NER task, the experimental results
    remark on the importance of domain-specific pre-training. For CJP our lightweight
    solution shows performance in line with existing approaches, and our NER-boosted
    explanations show promising CJPE results in terms of the conciseness of the prediction
    explanations.
  authors:
  - Irene Benedetto
  - Alkis Koudounas
  - Lorenzo Vaiani
  - Eliana Pastor
  - Elena Baralis
  - Luca Cagliero
  - Francesco Tarasconi
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_212
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 6: LegalEval: Understanding Legal Texts'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'PoliToHFI at SemEval-2023 Task 6: Leveraging Entity-Aware and Hierarchical
    Transformers For Legal Entity Recognition and Court Judgment Prediction'
  tldr: The use of Natural Language Processing techniques in the legal domain has
    become established for supporting attorneys and domain experts in content retrieval
    and decision-making. However, understanding the legal text poses relevant challenges
    in the recognition of domain-specific entities and the ad
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper presents the work produced by students of the University of
    Orlans Masters in Natural Language Processing program by way of participating
    in SemEval Task 6, LegalEval, which aims to enhance the capabilities of legal
    professionals through automated systems. Two out of the three sub-tasks available
    -- Rhetorical Role prediction (RR) and Legal Named Entity Recognition (L-NER)
    -- were tackled, with the express intent of developing lightweight and interpretable
    systems. For the L-NER sub-task, a CRF model was trained, augmented with post-processing
    rules for some named entity types. A macro F1 score of 0.74 was obtained on the
    DEV set, and 0.64 on the evaluation set. As for the RR sub-task, two sentence
    classification systems were built: one based on the Bag-of-Words technique with
    L-NER system output integrated, the other using a sentence-transformer approach.
    Rule-based post-processing then converted the results of the sentence classification
    systems into RR predictions. The better-performing Bag-of-Words system obtained
    a macro F1 score of 0.49 on the DEV set and 0.57 on the evaluation set.'
  authors:
  - "S\xE9bastien Bosch"
  - "Louis Est\xE8ve"
  - Joanne Loo
  - Anne-Lyse Minard
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_213
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 6: LegalEval: Understanding Legal Texts'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'UO-LouTAL at SemEval-2023 Task 6: Lightweight Systems for Legal Processing'
  tldr: This paper presents the work produced by students of the University of Orlans
    Masters in Natural Language Processing program by way of participating in SemEval
    Task 6, LegalEval, which aims to enhance the capabilities of legal professionals
    through automated systems. Two out of the three sub-tasks a
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In this paper, we propose a methodology fortask 10 of SemEval23, focusing
    on detectingand classifying online sexism in social me-dia posts. The task is
    tackling a serious is-sue, as detecting harmful content on socialmedia platforms
    is crucial for mitigating theharm of these posts on users. Our solutionfor this
    task is based on an ensemble of fine-tuned transformer-based models (BERTweet,RoBERTa,
    and DeBERTa). To alleviate prob-lems related to class imbalance, and to improvethe
    generalization capability of our model, wealso experiment with data augmentation
    andsemi-supervised learning. In particular, fordata augmentation, we use back-translation,
    ei-ther on all classes, or on the underrepresentedclasses only. We analyze the
    impact of thesestrategies on the overall performance of thepipeline through extensive
    experiments. whilefor semi-supervised learning, we found thatwith a substantial
    amount of unlabelled, in-domain data available, semi-supervised learn-ing can
    enhance the performance of certainmodels. Our proposed method (for which thesource
    code is available on Github12) attainsan F 1-score of 0.8613 for sub-taskA, whichranked
    us 10th in the competition.
  authors:
  - Sana Al-Azzawi
  - "Gy\xF6rgy Kov\xE1cs"
  - Filip Nilsson
  - Tosin Adewumi
  - Marcus Liwicki
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_214
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 10: Towards Explainable Detection of Online Sexism'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'NLP-LTU at SemEval-2023 Task 10: The Impact of Data Augmentation and Semi-Supervised
    Learning Techniques on Text Classification Performance on an Imbalanced Dataset'
  tldr: In this paper, we propose a methodology fortask 10 of SemEval23, focusing
    on detectingand classifying online sexism in social me-dia posts. The task is
    tackling a serious is-sue, as detecting harmful content on socialmedia platforms
    is crucial for mitigating theharm of these posts on users. Our solu
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper presents the system submissions of the John-Arthur team to
    the SemEval Task 4 ``ValueEval: Identification of Human Values behind Arguments''''.
    The best system of the team was ranked 3rd and the overall rank of the team was
    2nd (the first team had the two best systems). John-Arthur team models the ValueEval
    problem as a multi-class, multi-label text classification problem. The solutions
    leverage recently proposed large language models that are fine-tuned on the provided
    datasets. To boost the achieved performance we employ different best practises
    whose impact on the model performance we evaluate here. The code ispublicly available
    at github and the model onHuggingface hub.'
  authors:
  - Georgios Balikas
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_215
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 4: ValueEval: Identification of Human Values behind Arguments'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'John-Arthur at SemEval-2023 Task 4: Fine-Tuning Large Language Models for
    Arguments Classification'
  tldr: 'This paper presents the system submissions of the John-Arthur team to the
    SemEval Task 4 ``ValueEval: Identification of Human Values behind Arguments''''.
    The best system of the team was ranked 3rd and the overall rank of the team was
    2nd (the first team had the two best systems). John-Arthur team mod'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Persuasion techniques detection in news in a multi-lingual setup is non-trivial
    and comes with challenges, including little training data. Our system successfully
    leverages (back-)translation as data augmentation strategies with multi-lingual
    transformer models for the task of detecting persuasion techniques. The automatic
    and human evaluation of our augmented data allows us to explore whether (back-)translation
    aid or hinder performance. Our in-depth analyses indicate that both data augmentation
    strategies boost performance; however, balancing human-produced and machine-generated
    data seems to be crucial.
  authors:
  - Neele Falk
  - Annerose Eichel
  - Prisca Piccirilli
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_216
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 3: Detecting the Category, the Framing, and the Persuasion Techniques
    in Online News in a Multi-lingual Setup'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'NAP at SemEval-2023 Task 3: Is Less Really More? (Back-)Translation as Data
    Augmentation Strategies for Detecting Persuasion Techniques'
  tldr: Persuasion techniques detection in news in a multi-lingual setup is non-trivial
    and comes with challenges, including little training data. Our system successfully
    leverages (back-)translation as data augmentation strategies with multi-lingual
    transformer models for the task of detecting persuasion t
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Visual-Word Sense Disambiguation (V-WSD) entails resolving the linguistic
    ambiguity in a text by selecting a clarifying image from a set of (potentially
    misleading) candidates. In this paper, we address V-WSD using a state-of-the-art
    Image-Text Retrieval system, namely CLIP. We propose to alleviate the linguistic
    ambiguity across multiple domains and languages via text and image augmentation.
    To augment the textual content we rely on back-translation with the aid of a variety
    of auxiliary languages. The approach based on finetuning CLIP on the full phrases
    is effective in accurately disambiguating words and incorporating back-translation
    enhance the system's robustness and performance on the test samples written in
    Indo-European languages.
  authors:
  - Lorenzo Vaiani
  - Luca Cagliero
  - Paolo Garza
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_217
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Task-1 - Visual Word Sense Disambiguation (Visual-WSD)
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'PoliTo at SemEval-2023 Task 1: CLIP-based Visual-Word Sense Disambiguation
    Based on Back-Translation'
  tldr: 'Visual-Word Sense Disambiguation (V-WSD) entails resolving the linguistic
    ambiguity in a text by selecting a clarifying image from a set of (potentially
    misleading) candidates. In this paper, we address V-WSD using a state-of-the-art
    Image-Text Retrieval system, namely CLIP. We propose to alleviate '
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The paper presents an approach for solving SemEval 2023 Task 7 - identifying
    the inference relation in a clinical trials dataset. The system has two levels
    for retrieving relevant clinical trial evidence for a statement and then classifying
    the inference relation based on the relevant sentences. In the first level, the
    system classifies the evidence-statement pairs as relevant or not using a BERT-based
    classifier and contextual data augmentation (subtask 2).  Using the relevant parts
    of the clinical trial from the first level, the system uses an additional BERT-based
    classifier to determine whether the relation is entailment or contradiction (subtask
    1). In both levels, the contextual data augmentation is showing a significant
    improvement in the F1 score on the test set of 3.7\% for subtask 2 and 7.6\% for
    subtask 1, achieving final F1 scores of 82.7\% for subtask 2 and 64.4\% for subtask
    1.
  authors:
  - Sylvia Vassileva
  - Georgi Grazhdanski
  - Svetla Boytcheva
  - Ivan Koychev
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_218
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 7: Multi-Evidence Natural Language Inference for Clinical Trial
    Data'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'FMI-SU at SemEval-2023 Task 7: Two-level Entailment Classification of Clinical
    Trials Enhanced by Contextual Data Augmentation'
  tldr: 'The paper presents an approach for solving SemEval 2023 Task 7 - identifying
    the inference relation in a clinical trials dataset. The system has two levels
    for retrieving relevant clinical trial evidence for a statement and then classifying
    the inference relation based on the relevant sentences. In '
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Successful word sense disambiguation (WSD)is a fundamental element of
    natural languageunderstanding. As part of SemEval-2023 Task1, we investigate WSD
    in a multimodal setting,where ambiguous words are to be matched withcandidate
    images representing word senses. Wecompare multiple systems based on pre-trainedCLIP
    models. In our experiments, we findCLIP to have solid zero-shot performance onmonolingual
    and multilingual data. By em-ploying different fine-tuning techniques, we areable
    to further enhance performance. However,transferring knowledge between data distribu-tions
    proves to be more challenging.
  authors:
  - Clifton Poth
  - Martin Hentschel
  - Tobias Werner
  - Hannah Sterz
  - Leonard Bongard
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_219
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Task-1 - Visual Word Sense Disambiguation (Visual-WSD)
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'ML Mob at SemEval-2023 Task 1: Probing CLIP on Visual Word-Sense Disambiguation'
  tldr: Successful word sense disambiguation (WSD)is a fundamental element of natural
    languageunderstanding. As part of SemEval-2023 Task1, we investigate WSD in a
    multimodal setting,where ambiguous words are to be matched withcandidate images
    representing word senses. Wecompare multiple systems based on pr
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Clickbait posts are a common problem on social media platforms, as they
    often deceive users by providing misleading or sensational headlines that do not
    match the content of the linked web page. The aim of this study is to create a
    technique for identifying the specific type of suitable spoiler - be it a phrase,
    a passage, or a multipart spoiler - needed to neutralize clickbait posts. This
    is achieved by developing a machine learning classifier analyzing both the clickbait
    post and the linked web page.Modern approaches for constructing a text classifier
    usually rely on fine-tuning a transformer-based model pre-trained on large unsupervised
    corpora. However, recent advances in the development of large-scale language models
    have led to the emergence of a new transfer learning paradigm based on prompt
    engineering.In this work, we study these two transfer learning techniques and
    compare their effectiveness for clickbait spoiler-type detection task.Our experimental
    results show that for this task, using the standard fine-tuning method gives better
    results than using prompting. The best model can achieve a similar performance
    to that presented by Hagen et al. (2022).
  authors:
  - "Mateusz Wo\u017Any"
  - Mateusz Lango
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_220
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 5: Clickbait Spoiling'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Alexander Knox at SemEval-2023 Task 5: The comparison of prompting and standard
    fine-tuning techniques for selecting the type of spoiler needed to neutralize
    a clickbait'
  tldr: Clickbait posts are a common problem on social media platforms, as they often
    deceive users by providing misleading or sensational headlines that do not match
    the content of the linked web page. The aim of this study is to create a technique
    for identifying the specific type of suitable spoiler - be
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In this paper, we describe SemEval-2023 Task 10, a shared task on detecting
    and predicting sexist language. The dataset consists of labeled sexist and non-sexist
    data targeted towards women acquired from both Reddit and Gab. We present and
    compare several approaches we experimented with and our final submitted model.
    Additional error analysis is given to recognize challenges we dealt with in our
    process. A total of 84 teams participated. Our model ranks 55th    overall in
    Subtask A of the shared task.
  authors:
  - Wiebke Petersen
  - Diem-Ly Tran
  - Marion Wroblewitz
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_221
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 10: Towards Explainable Detection of Online Sexism'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'hhuEDOS at SemEval-2023 Task 10: Explainable Detection of Online Sexism
    (EDOS)  Binary Sexism Detection (Subtask A)'
  tldr: In this paper, we describe SemEval-2023 Task 10, a shared task on detecting
    and predicting sexist language. The dataset consists of labeled sexist and non-sexist
    data targeted towards women acquired from both Reddit and Gab. We present and
    compare several approaches we experimented with and our fina
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper describes our system used in SemEval-2023 Task-1: Visual Word
    Sense Disambiguation (VWSD). The VWSD task is to identify the correct image that
    corresponds to an ambiguous target word given limited textual context. To reduce
    word ambiguity and enhance image selection, we proposed several text augmentation
    techniques, such as prompting, WordNet synonyms, and text generation. We experimented
    with different vision-language pre-trained models to capture the joint features
    of the augmented text and image. Our approach achieved the best performance using
    a combination of GPT-3 text generation and the CLIP model. On the multilingual
    test sets, our system achieved an average hit rate (at top-1) of 51.11 and a mean
    reciprocal rank of 65.69.'
  authors:
  - Keyi Li
  - Sen Yang
  - Chenyang Gao
  - Ivan Marsic
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_222
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Task-1 - Visual Word Sense Disambiguation (Visual-WSD)
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Rutgers Multimedia Image Processing Lab at SemEval-2023 Task-1: Text-Augmentation-based
    Approach for Visual Word Sense Disambiguation'
  tldr: 'This paper describes our system used in SemEval-2023 Task-1: Visual Word
    Sense Disambiguation (VWSD). The VWSD task is to identify the correct image that
    corresponds to an ambiguous target word given limited textual context. To reduce
    word ambiguity and enhance image selection, we proposed several t'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: While sentiment classification has been considered a practically solved
    task for high-resource languages such as English, the scarcity of data for many
    languages still makes it a challenging task. The AfriSenti-SemEval shared task
    aims to classify sentiment on Twitter data for 14 low-resource African languages.
    In our participation, we focus on Nigerian Pidgin as the target language. We have
    investigated the effect of English monolingual and multilingual pre-trained models
    on the sentiment classification task for Nigerian Pidgin. Our setup includes zero-shot
    models (using English, Igbo and Hausa data) and a Nigerian Pidgin fine-tuned model.
    Our results show that English fine-tuned models perform slightly better than models
    fine-tuned on other Nigerian languages, which could be explained by the lexical
    and structural closeness between Nigerian Pidgin and English. The best results
    were reported on the monolingual Nigerian Pidgin data. The model pre-trained on
    English and fine-tuned on Nigerian Pidgin was submitted to Task A Track 4 of the
    AfriSenti-SemEval Shared Task 12, and scored 25 out of 32 in the ranking.
  authors:
  - Annika Kniele
  - Meriem Beloucif
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_223
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 12: AfriSenti-SemEval: Sentiment Analysis for Low-resource African
    Languages using Twitter Dataset'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Uppsala University at SemEval-2023 Task12: Zero-shot Sentiment Classification
    for Nigerian Pidgin Tweets'
  tldr: While sentiment classification has been considered a practically solved task
    for high-resource languages such as English, the scarcity of data for many languages
    still makes it a challenging task. The AfriSenti-SemEval shared task aims to classify
    sentiment on Twitter data for 14 low-resource Africa
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper introduces our system for the SemEval 2023 Task 2: Multilingual
    Complex Named Entity Recognition (MultiCoNER II) competition. Our team focused
    on the sub-task of Named Entity Recognition (NER) for the language of English
    in the challenge and reported our results. To achieve our goal, we utilized transfer
    learning by fine-tuning pre-trained language models (PLMs) on the competition
    dataset. Our approach involved combining a BERT-based PLM with external knowledge
    to provide additional context to the model. In this report, we present our findings
    and results.'
  authors:
  - Caleb Martin
  - Huichen Yang
  - William Hsu
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_226
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 2: MultiCoNER II Multilingual Complex Named Entity Recognition'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'KDDIE at SemEval-2023 Task 2: External Knowledge Injection for Named Entity
    Recognition'
  tldr: 'This paper introduces our system for the SemEval 2023 Task 2: Multilingual
    Complex Named Entity Recognition (MultiCoNER II) competition. Our team focused
    on the sub-task of Named Entity Recognition (NER) for the language of English
    in the challenge and reported our results. To achieve our goal, we u'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Sentiment Analysis is an aspect of natural languageprocessing (NLP) that
    has been a topicof research. While most studies focus on highresourcelanguages
    with an extensive amountof available data, the study on low-resource languageswith
    insufficient data needs attention.To address this issue, we propose a transformerbasedmethod
    for sentiment analysis for lowresourcesAfrican languages, Nigerian Pidginand Yoruba.
    To evaluate the effectiveness ofour multilingual language models for monolingualsentiment
    classification, we participated inthe AfriSenti SemEval shared task 2023 competition.On
    the official e valuation s et, ourgroup (named as Bhattacharya\_Lab) ranked1 out
    of 33 participating groups in the MonolingualSentiment Classification task (i.e.,
    TaskA) for Nigerian Pidgin (i.e., Track 4), and inthe Top 5 among 33 participating
    groups inthe Monolingual Sentiment Classification taskfor Yoruba (i.e., Track
    2) respectively, demonstratingthe potential for our transformer-basedlanguage
    models to improve sentiment analysisin low-resource languages. Overall, ourstudy
    highlights the importance of exploringthe potential of NLP in low-resource languagesand
    the impact of transformer-based multilinguallanguage models in sentiment analysis
    forthe low-resource African languages, NigerianPidgin and Yoruba.
  authors:
  - Nathaniel Hughes
  - Kevan Baker
  - Aditya Singh
  - Aryavardhan Singh
  - Tharalillah Dauda
  - Sutanu Bhattacharya
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_227
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 12: AfriSenti-SemEval: Sentiment Analysis for Low-resource African
    Languages using Twitter Dataset'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Bhattacharya_Lab at SemEval-2023 Task 12: A Transformer-based Language Model
    for Sentiment Classification for Low Resource African Languages: Nigerian Pidgin
    and Yoruba'
  tldr: Sentiment Analysis is an aspect of natural languageprocessing (NLP) that has
    been a topicof research. While most studies focus on highresourcelanguages with
    an extensive amountof available data, the study on low-resource languageswith
    insufficient data needs attention.To address this issue, we propo
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: One of the most extensively researched applications in natural language
    processing (NLP) is sentiment analysis. While the majority of the study focuses
    on high-resource languages (e.g., English), this research will focus on low-resource
    African languages namely Igbo and Hausa. The annotated tweets of both languages
    have a significant number of code-mixed tweets. The curated datasets necessary
    to build complex AI applications are not available for the majority of African
    languages. To optimize the use of such datasets, research is needed to determine
    the viability of present NLP procedures as well as the development of novel techniques.
    This paper outlines our efforts to develop a sentiment analysis (for positive
    and negative as well as neutral) system for tweets from the Hausa, and Igbo languages.
    Sentiment analysis can computationally analyze and discover sentiments in a text
    or document. We worked on the first thorough compilation of AfriSenti-SemEval
    2023 Shared Task 12 Twitter datasets that are human-annotated for the most widely
    spoken languages in Nigeria, such as Hausa and Igbo. Here we trained the modern
    pre-trained language model AfriBERTa large on the AfriSenti-SemEval Shared Task
    12 Twitter dataset to create sentiment classification. In particular, the results
    demonstrate that our model trained on AfriSenti-SemEval Shared Task 12 datasets
    and produced with an F1 score of 80.85\% for Hausa and 80.82\% for Igbo languages
    on the sentiment analysis test. In AfriSenti-SemEval 2023 shared task 12 (Task
    A), we consistently ranked top 10 by achieving a mean F1 score of more than 80\%
    for both the Hausa and Igbo languages.
  authors:
  - Nilanjana Raychawdhary
  - Amit Das
  - Gerry Dozier
  - Cheryl D. Seals
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_228
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 12: AfriSenti-SemEval: Sentiment Analysis for Low-resource African
    Languages using Twitter Dataset'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Seals_Lab at SemEval-2023 Task 12:  Sentiment Analysis for Low-resource
    African Languages, Hausa and Igbo'
  tldr: One of the most extensively researched applications in natural language processing
    (NLP) is sentiment analysis. While the majority of the study focuses on high-resource
    languages (e.g., English), this research will focus on low-resource African languages
    namely Igbo and Hausa. The annotated tweets o
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper presents our proposed method for SemEval-2023 Task 12, which
    focuses on sentiment analysis for low-resource African languages. Our method utilizes
    a language-centric domain adaptation approach which is based on adversarial training,
    where a small version of Afro-XLM-Roberta serves as a generator model and a feed-forward
    network as a discriminator. We participated in all three subtasks: monolingual
    (12 tracks), multilingual (1 track), and zero-shot (2 tracks). Our results show
    an improvement in weighted F1 for 13 out of 15 tracks with a maximum increase
    of 4.3 points for Moroccan Arabic compared to the baseline. We observed that using
    language family-based labels along with sequence-level input representations for
    the discriminator model improves the quality of the cross-lingual sentiment analysis
    for the languages unseen during the training. Additionally, our experimental results
    suggest that training the system on languages that are close in a language families
    tree enhances the quality of sentiment analysis for low-resource languages. Lastly,
    the computational complexity of the prediction step was kept at the same level
    which makes the approach to be interesting from a practical perspective. The code
    of the approach can be found in our repository.'
  authors:
  - Maksim Aparovich
  - Santosh Kesiraju
  - Aneta Dufkova
  - Pavel Smrz
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_229
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 12: AfriSenti-SemEval: Sentiment Analysis for Low-resource African
    Languages using Twitter Dataset'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'FIT BUT at SemEval-2023 Task 12: Sentiment Without Borders - Multilingual
    Domain Adaptation for Low-Resource Sentiment Classification'
  tldr: This paper presents our proposed method for SemEval-2023 Task 12, which focuses
    on sentiment analysis for low-resource African languages. Our method utilizes
    a language-centric domain adaptation approach which is based on adversarial training,
    where a small version of Afro-XLM-Roberta serves as a ge
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper describes a system for the SemEval 2023 Task 9: Multilingual
    Tweet Intimacy Analysis. This system consists of a pretrained multilingual masked
    language model as a text encoder and a neural network as a regression model. Data
    augmentation based on neural machine translation models is adopted to improve
    model performance under the low-resource scenario. This system is further improved
    through the ensemble of multiple models with the best performance in each language.
    This system ranks 4th in languages unseen in the training data and 16th in languages
    seen in the training data. The code and data can be found in this link: https://github.com/Cloudy0219/Multilingual.'
  authors:
  - Qinyuan Zheng
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_230
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 9: Multilingual Tweet Intimacy Analysis'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'WKU_NLP at SemEval-2023 Task 9: Translation Augmented Multilingual Tweet
    Intimacy Analysis'
  tldr: 'This paper describes a system for the SemEval 2023 Task 9: Multilingual Tweet
    Intimacy Analysis. This system consists of a pretrained multilingual masked language
    model as a text encoder and a neural network as a regression model. Data augmentation
    based on neural machine translation models is adopt'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The rapid growth of online communication using social media platforms
    has led to an increase in the presence of hate speech, especially in terms of
    sexist language online. The proliferation of such hate speech has a significant
    impact on the mental health and well-being of the users and hence the need for
    automated systems to detect and filter such texts. In this study, we explore the
    effectiveness of conventional machine learning techniques for detecting sexist
    text. We explore five conventional classifiers, namely, Logistic Regression, Decision
    Tree, XGBoost, Support Vector Machines, and Random Forest. The results show that
    different classifiers perform differently on each task due to their different
    inherent architectures which may be suited to a certain problem more. These models
    are trained on the shared task dataset, which includes both sexist and non-sexist
    texts.All in all, this study explores the potential of conventional machine learning
    techniques in detecting online sexist content. The results of this study highlight
    the strengths and weaknesses of all classifiers with respect to all subtasks.
    The results of this study will be useful for researchers and practitioners interested
    in developing systems for detecting or filtering online hate speech.
  authors:
  - Jayant Panwar
  - Radhika Mamidi
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_231
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 10: Towards Explainable Detection of Online Sexism'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'PanwarJayant at SemEval-2023 Task 10: Exploring the Effectiveness of Conventional
    Machine Learning Techniques for Online Sexism Detection'
  tldr: The rapid growth of online communication using social media platforms has
    led to an increase in the presence of hate speech, especially in terms of sexist
    language online. The proliferation of such hate speech has a significant impact
    on the mental health and well-being of the users and hence the ne
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In our work, a model is implemented that solves the task, based on multilingual
    pre-trained models. We also consider various methods of data preprocessing
  authors:
  - Daniil Homskiy
  - Narek Maloyan
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_232
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 12: AfriSenti-SemEval: Sentiment Analysis for Low-resource African
    Languages using Twitter Dataset'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'DN at SemEval-2023 Task 12: Low-Resource Language Text Classification via
    Multilingual Pretrained Language Model Fine-tuning'
  tldr: In our work, a model is implemented that solves the task, based on multilingual
    pre-trained models. We also consider various methods of data preprocessing
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In this paper, we describe the implementations of our systems for the
    SemEval-2023 Task 5 'Clickbait Spoiling', which involves the classification of
    clickbait posts in sub-task 1 and the spoiler generation and question answering
    of clickbait posts in sub-task 2, ultimately achieving a balanced accuracy of
    0.593 and a BLEU score of 0.322 on the test datasets in sub-task 1 and sub-task
    2 respectively.For this, we propose the usage of RoBERTa transformer models and
    modify them for each specific downstream task.In sub-task 1, we use the pre-trained
    RoBERTa model and use it in conjunction with NER, a spoiler-title ratio, a regex
    check for enumerations and lists and the use of input reformulation.In sub-task
    2, we propose the usage of the RoBERTa-SQuAD2.0 model for extractive question
    answering in combination with a contextual rule-based approach for multi-type
    spoilers in order to generate spoiler answers.
  authors:
  - Andreas Kruff
  - Anh Huy Tran
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_233
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 5: Clickbait Spoiling'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Billie-Newman at SemEval-2023 Task 5: Clickbait Classification and Question
    Answering with Pre-Trained Language Models, Named Entity Recognition and Rule-Based
    Approaches'
  tldr: In this paper, we describe the implementations of our systems for the SemEval-2023
    Task 5 'Clickbait Spoiling', which involves the classification of clickbait posts
    in sub-task 1 and the spoiler generation and question answering of clickbait posts
    in sub-task 2, ultimately achieving a balanced accur
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Nowadays, persuasive messages are more and more frequent in social networks,
    which generates great concern in several communities, given that persuasion seeks
    to guide others towards the adoption of ideas, attitudes or actions that they
    consider to be beneficial to themselves. The efficient detection of news genre
    categories, detection of framing and detection of persuasion techniques requires
    several scientific disciplines, such as computational linguistics and sociology.  Here
    we illustrate how we use lexical features given a news article, determine whether
    it is an opinion piece, aims to report factual news, or is satire. This paper
    presents a novel strategy for news based on Lexical Weirdness. The results are
    part of our participation in subtasks 1 and 2 in SemEval 2023 Task 3.
  authors:
  - Juan Cuadrado
  - Elizabeth Martinez
  - Anderson Morillo
  - "Daniel Pe\xF1a"
  - Kevin Sossa
  - Juan Martinez-Santos
  - Edwin Puertas
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_234
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 3: Detecting the Category, the Framing, and the Persuasion Techniques
    in Online News in a Multi-lingual Setup'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'UTB-NLP at SemEval-2023 Task 3: Weirdness, Lexical Features for Detecting
    Categorical Framings, and Persuasion in Online News'
  tldr: Nowadays, persuasive messages are more and more frequent in social networks,
    which generates great concern in several communities, given that persuasion seeks
    to guide others towards the adoption of ideas, attitudes or actions that they
    consider to be beneficial to themselves. The efficient detectio
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper summarizes the CLaC submission for the MultiCoNER 2 task which
    concerns the recognition of complex, fine-grained named entities. We compare two
    popular approaches for NER, namely SequenceLabeling and Span Prediction. We find
    that our best Span Prediction system performs slightly better than our best  Sequence
    Labeling system on test data. Moreover, we find that using the larger version
    of XLM RoBERTa significantly improves performance. Post-competition experiments
    show that Span Prediction and Sequence Labeling approaches improve when they use
    special input tokens ([s] and [/s]) of XLM-RoBERTa. The code for training all
    models, preprocessing, and post-processing is available at https://github.com/harshshredding/semeval2023-multiconer-paper.
  authors:
  - Harsh Verma
  - Sabine Bergler
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_235
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 2: MultiCoNER II Multilingual Complex Named Entity Recognition'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'CLaC at SemEval-2023 Task 2: Comparing Span-Prediction and Sequence-Labeling
    Approaches for NER'
  tldr: This paper summarizes the CLaC submission for the MultiCoNER 2 task which
    concerns the recognition of complex, fine-grained named entities. We compare two
    popular approaches for NER, namely SequenceLabeling and Span Prediction. We find
    that our best Span Prediction system performs slightly better th
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The widespread popularity of social media has led to an increase in hateful,
    abusive, and sexist language, motivating methods for the automatic detection of
    such phenomena. The goal of the SemEval shared task Towards Explainable Detection
    of Online Sexism (EDOS 2023) is to detect sexism in English social media posts
    (subtask A), and to categorize such posts into four coarse-grained sexism categories
    (subtask B), and eleven fine-grained subcategories (subtask C). In this paper,
    we present our submitted systems for all three subtasks, based on a multi-task
    model that has been fine-tuned on a range of related tasks and datasets before
    being fine-tuned on the specific EDOS subtasks. We implement multi-task learning
    by formulating each task as binary pairwise text classification, where the dataset
    and label descriptions are given along with the input text. The results show clear
    improvements over a fine-tuned DeBERTa-V3 serving as a baseline leading to F1-scores
    of 85.9\% in subtask A (rank 13/84), 64.8\% in subtask B (rank 19/69), and 44.9\%
    in subtask C (26/63).
  authors:
  - Janis Goldzycher
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_236
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 10: Towards Explainable Detection of Online Sexism'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'CL-UZH at SemEval-2023 Task 10: Sexism Detection through Incremental Fine-Tuning
    and Multi-Task Learning with Label Descriptions'
  tldr: The widespread popularity of social media has led to an increase in hateful,
    abusive, and sexist language, motivating methods for the automatic detection of
    such phenomena. The goal of the SemEval shared task Towards Explainable Detection
    of Online Sexism (EDOS 2023) is to detect sexism in English s
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Misogyny and sexism are growing problems in social media. Advances have
    been made in online sexism detection but the systems are often uninterpretable.
    SemEval-2023 Task 10 on Explainable Detection of Online Sexism aims at increasing
    explainability of the sexism detection, and our team participated in all the proposed
    subtasks. Our system is based on further domain-adaptive pre-training. Building
    on the Transformer-based models with the domain adaptation, we compare fine-tuning
    with multi-task learning and show that each subtask requires a different system
    configuration. In our experiments, multi-task learning performs on par with standard
    fine-tuning for sexism detection and noticeably better for coarse-grained sexism
    classification, while fine-tuning is preferable for fine-grained classification.
  authors:
  - Konstantin Chernyshev
  - Ekaterina Garanina
  - Duygu Bayram
  - Qiankun Zheng
  - Lukas Edman
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_237
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 10: Towards Explainable Detection of Online Sexism'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'LCT-1 at SemEval-2023 Task 10: Pre-training and Multi-task Learning for
    Sexism Detection and Classification'
  tldr: 'Misogyny and sexism are growing problems in social media. Advances have been
    made in online sexism detection but the systems are often uninterpretable. SemEval-2023
    Task 10 on Explainable Detection of Online Sexism aims at increasing explainability
    of the sexism detection, and our team participated '
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In our article, we present the systems developed for SemEval-2023 Task
    3, which aimed to evaluate the ability of Natural Language Processing (NLP) systems
    to detect genres and persuasion techniques in multiple languages. We experimented
    with several data augmentation techniques, including machine translation (MT)
    and text generation. For genre detection, synthetic texts for each class were
    created using the OpenAI GPT-3 Davinci language model. In contrast, to detect
    persuasion techniques, we relied on augmenting the dataset through text translation
    using the DeepL translator. Fine-tuning the models using augmented data resulted
    in a top-ten ranking across all languages, indicating the effectiveness of the
    approach. The models for genre detection demonstrated excellent performance, securing
    the first, second, and third positions in Spanish, German, and Italian, respectively.
    Moreover, one of the models for persuasion techniques' detection secured the third
    position in Polish. Our contribution constitutes the system architecture that
    utilizes DeepL and GPT-3 for data augmentation for the purpose of detecting both
    genre and persuasion techniques.
  authors:
  - Arkadiusz Modzelewski
  - Witold Sosnowski
  - Magdalena Wilczynska
  - Adam Wierzbicki
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_238
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 3: Detecting the Category, the Framing, and the Persuasion Techniques
    in Online News in a Multi-lingual Setup'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'DSHacker at SemEval-2023 Task 3: Genres and Persuasion Techniques Detection
    with Multilingual Data Augmentation through Machine Translation and Text Generation'
  tldr: In our article, we present the systems developed for SemEval-2023 Task 3,
    which aimed to evaluate the ability of Natural Language Processing (NLP) systems
    to detect genres and persuasion techniques in multiple languages. We experimented
    with several data augmentation techniques, including machine tr
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Given a word in context, the task of VisualWord Sense Disambiguation consists
    of select-ing the correct image among a set of candidates.To select the correct
    image, we propose a so-lution blending text augmentation and multi-modal models.
    Text augmentation leverages thefine-grained semantic annotation from Word-Net
    to get a better representation of the tex-tual component. We then compare this
    sense-augmented text to the set of image using pre-trained multimodal models CLIP
    and ViLT. Oursystem has been ranked 16th for the Englishlanguage, achieving 68.5
    points for hit rate and79.2 for mean reciprocal rank.
  authors:
  - Shibingfeng Zhang
  - Shantanu Nath
  - Davide Mazzaccara
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_239
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Task-1 - Visual Word Sense Disambiguation (Visual-WSD)
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'GPL at SemEval-2023 Task 1: WordNet and CLIP to Disambiguate Images'
  tldr: Given a word in context, the task of VisualWord Sense Disambiguation consists
    of select-ing the correct image among a set of candidates.To select the correct
    image, we propose a so-lution blending text augmentation and multi-modal models.
    Text augmentation leverages thefine-grained semantic annotati
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper presents our system descriptions for SemEval 2023-Task 7:
    Multi-evidence Natural Language Inference for Clinical Trial Data sub-tasks one
    and two. Provided with a collection of Clinical Trial Reports (CTRs) and corresponding
    expert-annotated claim statements, sub-task one involves determining an inferential
    relationship between the statement and CTR premise: contradiction or entailment.
    Sub-task two involves retrieving evidence from the CTR which is necessary to determine
    the entailment in sub-task one. For sub-task two we employ a recent transformer-based
    language model pretrained on biomedical literature, which we domain-adapt on a
    set of clinical trial reports. For sub-task one, we take an ensemble approach
    in which we leverage the evidence retrieval model from sub-task two to extract
    relevant sections, which are then passed to a second model of equivalent architecture
    to determine entailment. Our system achieves a ranking of seventh on sub-task
    one with an F1-score of 0.705 and sixth on sub-task two with an F1-score of 0.806.
    In addition, we find that the high rate of success of language models on this
    dataset may be partially attributable to the existence of annotation artifacts.'
  authors:
  - Ahamed Alameldin
  - Ashton Williamson
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_240
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 7: Multi-Evidence Natural Language Inference for Clinical Trial
    Data'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Clemson NLP at SemEval-2023 Task 7: Applying GatorTron to Multi-Evidence
    Clinical NLI'
  tldr: 'This paper presents our system descriptions for SemEval 2023-Task 7: Multi-evidence
    Natural Language Inference for Clinical Trial Data sub-tasks one and two. Provided
    with a collection of Clinical Trial Reports (CTRs) and corresponding expert-annotated
    claim statements, sub-task one involves determi'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In this paper, we describe the multi strategy system for SemEval-2022
    Task 7, This task aims to determine whether a given statement is supported by
    one or two Clinical Trial reports, and to identify evidence that supports the
    statement. This is a task that requires high natural language inference capabilities.
    In Subtask 1, we compare our strategy based on prompt learning and ChatGPT with
    a baseline constructed using BERT in zero-shot setting, and validate the effectiveness
    of our strategy. In Subtask 2, we fine-tune DeBERTaV3 for classification without
    relying on the results from Subtask 1, and we observe that early stopping can
    effectively prevent model overfitting, which performs well in Subtask 2. In addition,
    we did not use any ensemble strategies. Ultimately, we achieved the 10th place
    in Subtask 1 and the 2nd place in Subtask 2.
  authors:
  - Xiaofeng Zhao
  - Min Zhang
  - Miaomiao Ma
  - Chang Su
  - Yilun Liu
  - Minghan Wang
  - Xiaosong Qiao
  - Jiaxin Guo
  - Yinglu Li
  - Wenbing Ma
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_241
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 7: Multi-Evidence Natural Language Inference for Clinical Trial
    Data'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'HW-TSC at SemEval-2023 Task 7: Exploring the Natural Language Inference
    Capabilities of ChatGPT and Pre-trained Language Model for Clinical Trial'
  tldr: In this paper, we describe the multi strategy system for SemEval-2022 Task
    7, This task aims to determine whether a given statement is supported by one or
    two Clinical Trial reports, and to identify evidence that supports the statement.
    This is a task that requires high natural language inference ca
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In this paper, we initially discuss about the ValueEval task and the challenges
    involved in multi-label classification tasks. We tried to approach this task using
    Natural Language Inference and proposed a Grouped-BERT architecture which leverages
    commonality between the classes for a multi-label classification tasks.
  authors:
  - Ajay Narasimha Mopidevi
  - Hemanth Chenna
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_242
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 4: ValueEval: Identification of Human Values behind Arguments'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Quintilian at SemEval-2023 Task 4: Grouped BERT for Multi-Label Classification'
  tldr: In this paper, we initially discuss about the ValueEval task and the challenges
    involved in multi-label classification tasks. We tried to approach this task using
    Natural Language Inference and proposed a Grouped-BERT architecture which leverages
    commonality between the classes for a multi-label cla
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper presents our approach to the SemEval-2023 Task 3 to detect
    online persuasion techniques in a multilingual setup.  Our classification system
    is based on the RoBERTa-base model trained predominantly on English to label the
    persuasion techniques  across 9 different languages. Our system was able to significantly
    surpass the baseline performance in 3 of the 9 languages: English, Georgian and
    Greek. However, our wrong assumption that a single classification system trained
    predominantly on English could generalize well to other languages, negatively
    impacted our scores on the other 6 languages. In this paper, we provide a description
    of the reasoning behind the development of our final model and what conclusions
    may be drawn from its performance for future work.'
  authors:
  - Nelson Filipe Costa
  - Bryce Hamilton
  - Leila Kosseim
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_244
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 3: Detecting the Category, the Framing, and the Persuasion Techniques
    in Online News in a Multi-lingual Setup'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'CLaC at SemEval-2023 Task 3: Language Potluck RoBERTa Detects Online Persuasion
    Techniques in a Multilingual Setup'
  tldr: 'This paper presents our approach to the SemEval-2023 Task 3 to detect online
    persuasion techniques in a multilingual setup.  Our classification system is based
    on the RoBERTa-base model trained predominantly on English to label the persuasion
    techniques  across 9 different languages. Our system was '
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper introduces our method in the system for SemEval 2023 Task
    2: MultiCoNER II Multilingual Complex Named Entity Recognition, Track 9-Chinese.
    This task focuses on detecting fine-grained named entities whose data set has
    a fine-grained taxonomy of 36 NE classes, representing a realistic challenge for
    NER. In this task, we need to identify entity boundaries and category labels for
    the six identified categories. We use BERT embedding to represent each character
    in the original sentence and train CRF-Rdrop to predict named entity categories
    using the data set provided by the organizer. Our best submission, with a macro
    average F1 score of 0.5657, ranked 15th out of 22 teams.'
  authors:
  - Jing Li
  - Xiaobing Zhou
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_245
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 2: MultiCoNER II Multilingual Complex Named Entity Recognition'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'YNUNLP at SemEval-2023 Task 2: The Pseudo Twin Tower Pre-training Model
    for Chinese Named Entity Recognition'
  tldr: 'This paper introduces our method in the system for SemEval 2023 Task 2: MultiCoNER
    II Multilingual Complex Named Entity Recognition, Track 9-Chinese. This task focuses
    on detecting fine-grained named entities whose data set has a fine-grained taxonomy
    of 36 NE classes, representing a realistic chall'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper presents a model for clickbait spoiling,which aims at generating
    short texts that satisfy thecuriosity induced by a clickbait post. The modelis
    split into two tasks: identifying the clickbaittype and spoiling the clickbait.
    The first task isto classify the spoiler type that the clickbait postwarrants,
    and the second task is to generate thespoiler for the clickbait post. The model
    utilizesthe Distilbert-base-uncased model for the first taskand the Bert-base-uncased
    model for the secondtask. The trained model is optimized through trialand error
    on different model selections, and hyper-parameters and results are presented
    in a confusionmatrix. The main reason we utilized Distilbert-base-uncased is that
    it analyzes words in the con-text of what''s around it. The objective of this
    modelis to save readers time and spoil the clickbait of dif-ferent articles they
    may see on different platformslike Twitter and Reddit'
  authors:
  - Vineet Saravanan
  - Steven Wilson
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_246
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 5: Clickbait Spoiling'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Mr-wallace at SemEval-2023 Task 5: Novel Clickbait Spoiling Algorithm Using
    Natural Language Processing'
  tldr: 'This paper presents a model for clickbait spoiling,which aims at generating
    short texts that satisfy thecuriosity induced by a clickbait post. The modelis
    split into two tasks: identifying the clickbaittype and spoiling the clickbait.
    The first task isto classify the spoiler type that the clickbait '
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'In this paper, we describe our system for SemEval-2023 Task 7: Multi-evidence
    Natural Language Inference for Clinical Trial Data. Given a CTR premise, and a
    statement, this task involves 2 sub-tasks (i) identifying the inference relation
    between CTR - statement pairs (Task 1: Textual Entailment), and (ii) extracting
    a set of supporting facts, from the premise, to justify the label predicted in
    Task 1 (Task 2: Evidence Retrieval). We adopt an explanations driven NLI approach
    to tackle the tasks.  Given a statement to verify, the idea is to first identify
    relevant evidence from the target CTR(s), perform evidence level inferences and
    then ensemble them to arrive at the final inference. We have experimented with
    various BERT based models and T5 models. Our final model uses T5 base that achieved
    better performance compared to BERT models. In summary, our system achieves F1
    score of 70.1\% for Task 1 and 80.2\% for Task 2. We ranked 8th respectively under
    both the tasks. Moreover, ours was one of the  5 systems that ranked within the
    Top 10 under both tasks.'
  authors:
  - Saravanan Rajamanickam
  - Kanagasabai Rajaraman
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_247
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 7: Multi-Evidence Natural Language Inference for Clinical Trial
    Data'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'I2R at SemEval-2023 Task 7:  Explanations-driven Ensemble Approach for Natural
    Language Inference over Clinical Trial Data'
  tldr: 'In this paper, we describe our system for SemEval-2023 Task 7: Multi-evidence
    Natural Language Inference for Clinical Trial Data. Given a CTR premise, and a
    statement, this task involves 2 sub-tasks (i) identifying the inference relation
    between CTR - statement pairs (Task 1: Textual Entailment), an'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We describe our submission to SemEval 2023 Task 3, specifically the subtask
    on persuasion technique detection. In this work, our team NLUBot101 tackled a
    novel task of classifying persuasion techniques in online news articles at a paragraph
    level. The low-resource multilingual datasets, along with the imbalanced label
    distribution, make this task challenging. Our team presented a cross-lingual data
    augmentation approach and leveraged a recently proposed multilingual natural language
    inference model to address these challenges. Our solution achieves the highest
    macro-F1 score for the English task, and top 5 micro-F1 scores on both the English
    and Russian leaderboards.
  authors:
  - Genglin Liu
  - Yi Fung
  - Heng Ji
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_248
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 3: Detecting the Category, the Framing, and the Persuasion Techniques
    in Online News in a Multi-lingual Setup'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'NLUBot101 at SemEval-2023 Task 3: An Augmented Multilingual NLI Approach
    Towards Online News Persuasion Techniques Detection'
  tldr: We describe our submission to SemEval 2023 Task 3, specifically the subtask
    on persuasion technique detection. In this work, our team NLUBot101 tackled a
    novel task of classifying persuasion techniques in online news articles at a paragraph
    level. The low-resource multilingual datasets, along with t
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This study presents an ensemble approach for detecting sexist text in
    the context of the Semeval-2023 task 10. Our approach leverages 18 models, including
    DeBERTa-v3-base models with different input sequence lengths, a BERT-based model
    trained on identifying hate speech, and three more models pre-trained on the task's
    unlabeled data with varying input lengths. The results of our framework on the
    development set show an f1-score of 84.92\% and on the testing set 84.55\%, effectively
    demonstrating the strength of the ensemble approach in getting accurate results.
  authors:
  - Mutaz Younes
  - Ali Kharabsheh
  - Mohammad Bani Younes
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_249
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Task-1 - Visual Word Sense Disambiguation (Visual-WSD)
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Alexa at SemEval-2023 Task 10: Ensemble Modeling of DeBERTa and BERT Variations
    for Identifying Sexist Text'
  tldr: This study presents an ensemble approach for detecting sexist text in the
    context of the Semeval-2023 task 10. Our approach leverages 18 models, including
    DeBERTa-v3-base models with different input sequence lengths, a BERT-based model
    trained on identifying hate speech, and three more models pre-tr
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper presents the systems and approaches of the Gallagher team
    for the SemEval-2023 Task 5: Clickbait Spoiling. We propose a method to classify
    the type of spoiler (phrase, passage, multi) and a question-answering method to
    generate spoilers that satisfy the curiosity caused by clickbait posts. We experiment
    with the state-of-the-art Seq2Seq model T5. To identify the spoiler types we used
    a fine-tuned T5 classifier (Subtask 1). A mixture of T5 and Flan-T5 was used to
    generate the spoilers for clickbait posts (Subtask 2). Our system officially ranks
    first in generating phrase type spoilers in Subtask 2, and achieves the highest
    precision score for passage type spoilers in Subtask 1.'
  authors:
  - Tugay Bilgis
  - Nimet Beyza Bozdag
  - Steven Bethard
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_250
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 5: Clickbait Spoiling'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Gallagher at SemEval-2023 Task 5: Tackling Clickbait with Seq2Seq Models'
  tldr: 'This paper presents the systems and approaches of the Gallagher team for
    the SemEval-2023 Task 5: Clickbait Spoiling. We propose a method to classify the
    type of spoiler (phrase, passage, multi) and a question-answering method to generate
    spoilers that satisfy the curiosity caused by clickbait posts'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper presents the systems and approaches of the Arizonans team
    for the SemEval 2023 Task 9: Multilingual Tweet Intimacy Analysis. We finetune
    the Multilingual RoBERTa model trained with about 200M tweets, XLM-T. Our final
    model ranked 9th out of 45 overall, 13th in seen languages, and 8th in unseen
    languages.'
  authors:
  - Nimet Beyza Bozdag
  - Tugay Bilgis
  - Steven Bethard
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_251
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 9: Multilingual Tweet Intimacy Analysis'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Arizonans at SemEval-2023 Task 9: Multilingual Tweet Intimacy Analysis with
    XLM-T'
  tldr: 'This paper presents the systems and approaches of the Arizonans team for
    the SemEval 2023 Task 9: Multilingual Tweet Intimacy Analysis. We finetune the
    Multilingual RoBERTa model trained with about 200M tweets, XLM-T. Our final model
    ranked 9th out of 45 overall, 13th in seen languages, and 8th in u'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'There are two competing approaches for modelling annotator disagreement:
    distributional soft-labelling approaches (which aim to capture the level of disagreement)
    or modelling perspectives of individual annotators or groups thereof. We adapt
    a multi-task architecture which has previously shown success in modelling perspectives
    to evaluate its performance on the SEMEVAL Task 11. We do so by combining both
    approaches, i.e. predicting individual annotator perspectives as an interim step
    towards predicting annotator disagreement. Despite its previous success, we found
    that a multi-task approach performed poorly on datasets which contained distinct
    annotator opinions, suggesting that this approach may not always be suitable when
    modelling perspectives. Furthermore, our results explain that while strongly perspectivist
    approaches might not achieve state-of-the-art performance according to evaluation
    metrics used by distributional approaches, our approach allows for a more nuanced
    understanding of individual perspectives present in the data. We argue that perspectivist
    approaches are preferable because they enable decision makers to amplify minority
    views, and that it is important to re-evaluate metrics to reflect this goal.'
  authors:
  - Nikolas Vitsakis
  - Amit Parekh
  - Tanvi Dinkar
  - Gavin Abercrombie
  - Ioannis Konstas
  - Verena Rieser
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_252
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 11: Learning with Disagreements (Le-Wi-Di)'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'iLab at SemEval-2023 Task 11 Le-Wi-Di: Modelling Disagreement or Modelling
    Perspectives?'
  tldr: 'There are two competing approaches for modelling annotator disagreement:
    distributional soft-labelling approaches (which aim to capture the level of disagreement)
    or modelling perspectives of individual annotators or groups thereof. We adapt
    a multi-task architecture which has previously shown succe'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Sexism is one of the most concerning problems in the internet society.
    By detecting sexist expressions, we can reduce the offense toward females and
    provide useful information to understand how sexism occurs. Our work focuses on
    a newly-published dataset, EDOS, which annotates English sexist expressions from
    Reddit and categorizes their specific types. Our method is to train a DeBERTaV3
    classifier with all three kinds of labels provided by the dataset, including sexist,
    category, and granular vectors. Our classifier predicts the probability distribution
    on vector labels and further applies it to represent category and sexist distributions.
    Our classifier uses its label and finer-grained labels for each classification
    to calculate the hierarchical loss for optimization. Our experiments and analyses
    show that using a combination of loss with finer-grained labels generally achieves
    better performance on sexism detection and categorization. Codes for our implementation
    can be found at https://github.com/KomeijiForce/SemEval2023\_Task10.
  authors:
  - Letian Peng
  - Bosung Kim
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_255
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 10: Towards Explainable Detection of Online Sexism'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Chride at SemEval-2023 Task 10: Fine-tuned Deberta-V3 on Detection of Online
    Sexism with Hierarchical Loss'
  tldr: Sexism is one of the most concerning problems in the internet society. By
    detecting sexist expressions, we can reduce the offense toward females and provide
    useful information to understand how sexism occurs. Our work focuses on a newly-published
    dataset, EDOS, which annotates English sexist express
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We experiment with XLM-Twitter and XLM-RoBERTa models to predict the intimacy
    scores in Tweets i.e. the extent to which a Tweet contains intimate content. We  propose
    a Transformer-TabNet based multimodal architecture using text data and statistical
    features from the text, which performs better than the vanilla Transformer based
    model. We further experiment with Adversarial Weight Perturbation to make our
    models generalized and robust. The ensemble of four of our best models achieve
    an over-all Pearson Coefficient of 0.5893 on the test dataset.
  authors:
  - Priyanshu Kumar
  - Amit Kumar
  - Jiban Prakash
  - Prabhat Lamba
  - Irfan Abdul
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_256
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 9: Multilingual Tweet Intimacy Analysis'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'ODA_SRIB at SemEval-2023 Task 9: A Multimodal Approach for Improved Intimacy
    Analysis'
  tldr: We experiment with XLM-Twitter and XLM-RoBERTa models to predict the intimacy
    scores in Tweets i.e. the extent to which a Tweet contains intimate content. We  propose
    a Transformer-TabNet based multimodal architecture using text data and statistical
    features from the text, which performs better than
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The NLI4CT task aims to entail hypotheses based on Clinical Trial Reports
    (CTRs) and retrieve the corresponding evidence supporting the justification. This
    task poses a significant challenge, as verifying hypotheses in the NLI4CT task
    requires the integration of multiple pieces of evidence from one or two CTR(s)
    and the application of diverse levels of reasoning, including textual and numerical.
    To address these problems, we present a multi-granularity system for CTR-based
    textual entailment and evidence retrieval in this paper. Specifically, we construct
    a Multi-granularity Inference Network (MGNet) that exploits sentence-level and
    token-level encoding to handle both textual entailment and evidence retrieval
    tasks. Moreover, we enhance the numerical inference capability of the system by
    leveraging a T5-based model, SciFive, which is pre-trained on the medical corpus.
    Model ensembling and a joint inference method are further utilized in the system
    to increase the stability and consistency of inference. The system achieves f1-scores
    of 0.856 and 0.853 on textual entailment and evidence retrieval tasks, resulting
    in the best performance on both subtasks. The experimental results corroborate
    the effectiveness of our proposed method.
  authors:
  - Yuxuan Zhou
  - Ziyu Jin
  - Meiwei Li
  - Miao Li
  - Xien Liu
  - Xinxin You
  - Ji Wu
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_257
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 7: Multi-Evidence Natural Language Inference for Clinical Trial
    Data'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'THiFLY Research at SemEval-2023 Task 7: A Multi-granularity System for CTR-based
    Textual Entailment and Evidence Retrieval'
  tldr: 'The NLI4CT task aims to entail hypotheses based on Clinical Trial Reports
    (CTRs) and retrieve the corresponding evidence supporting the justification. This
    task poses a significant challenge, as verifying hypotheses in the NLI4CT task
    requires the integration of multiple pieces of evidence from one '
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper describes our approach for SemEval-2023 Task 10: Explainable
    Detection of Online Sexism (EDOS). The task deals with identification and categorization
    of sexist content into fine-grained categories for explainability in sexism classification.
    The explainable categorization is proposed through a set of three hierarchical
    tasks that constitute a taxonomy of sexist content, each task being more granular
    than the former for categorization of the content. Our team (iREL) participated
    in all three hierarchical subtasks. Considering the inter-connected task structure,
    we study multilevel training to study the transfer learning from coarser to finer
    tasks. Our experiments based on pretrained transformer architectures also make
    use of additional strategies such as domain-adaptive pretraining to adapt our
    models to the nature of the content dealt with, and use of the focal loss objective
    for handling class imbalances. Our best-performing systems on the three tasks
    achieve macro-F1 scores of 85.93, 69.96 and 54.62 on their respective validation
    sets.'
  authors:
  - Nirmal Manoj
  - Sagar Joshi
  - Ankita Maity
  - Vasudeva Varma
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_258
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 10: Towards Explainable Detection of Online Sexism'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'iREL at SemEval-2023 Task 10: Multi-level Training for Explainable Detection
    of Online Sexism'
  tldr: 'This paper describes our approach for SemEval-2023 Task 10: Explainable Detection
    of Online Sexism (EDOS). The task deals with identification and categorization
    of sexist content into fine-grained categories for explainability in sexism classification.
    The explainable categorization is proposed thro'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper describes the DuluthNLP system that participated in Task 12
    of SemEval-2023 on AfriSenti-SemEval: Sentiment Analysis for Low-resource African
    Languages using Twitter Dataset. Given a set of tweets, the task requires participating
    systems to classify each tweet as negative, positive or neutral. We evaluate a
    range of monolingual and multilingual pretrained models on the Twi language dataset,
    one among the 14 African languages included in the SemEval task. We introduce
    TwiBERT, a new pretrained model trained from scratch. We show that TwiBERT, along
    with mBERT, generally perform best when trained on the Twi dataset, achieving
    an F1 score of 64.29\% on the official evaluation test data, which ranks 14 out
    of 30 of the total submissions for Track 10. The TwiBERT model is released at
    https://huggingface.co/sakrah/TwiBERT'
  authors:
  - Samuel Akrah
  - Ted Pedersen
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_260
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 12: AfriSenti-SemEval: Sentiment Analysis for Low-resource African
    Languages using Twitter Dataset'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'DuluthNLP at SemEval-2023 Task 12: AfriSenti-SemEval: Sentiment Analysis
    for Low-resource African Languages using Twitter Dataset'
  tldr: 'This paper describes the DuluthNLP system that participated in Task 12 of
    SemEval-2023 on AfriSenti-SemEval: Sentiment Analysis for Low-resource African
    Languages using Twitter Dataset. Given a set of tweets, the task requires participating
    systems to classify each tweet as negative, positive or neu'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper explains the participation of team Hitachi to SemEval-2023
    Task 3 "Detecting the genre, the framing, and the persuasion techniques in online
    news in a multi-lingual setup.'' Based on the multilingual, multi-task nature
    of the task and the low-resource setting, we investigated different cross-lingual
    and multi-task strategies for training the pretrained language models. Through
    extensive experiments, we found that (a) cross-lingual/multi-task training, and
    (b) collecting an external balanced dataset, can benefit the genre and framing
    detection. We constructed ensemble models from the results and achieved the highest
    macro-averaged F1 scores in Italian and Russian genre categorization subtasks.
  authors:
  - Yuta Koreeda
  - Ken-ichi Yokote
  - Hiroaki Ozaki
  - Atsuki Yamaguchi
  - Masaya Tsunokake
  - Yasuhiro Sogawa
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_261
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 3: Detecting the Category, the Framing, and the Persuasion Techniques
    in Online News in a Multi-lingual Setup'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Hitachi at SemEval-2023 Task 3: Exploring Cross-lingual Multi-task Strategies
    for Genre and Framing Detection in Online News'
  tldr: This paper explains the participation of team Hitachi to SemEval-2023 Task
    3 "Detecting the genre, the framing, and the persuasion techniques in online news
    in a multi-lingual setup.'' Based on the multilingual, multi-task nature of the
    task and the low-resource setting, we investigated different cr
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Clickbait spoiling and spoiler type classification in the setting of the
    SemEval2023 shared task five was used to explore transformer based text classification
    in comparison to conventional, shallow learned classifying models. Additionally,
    an initial model for spoiler creation was explored. The task was to classify or
    create spoilers for clickbait social media posts. The classification task was
    addressed by comparing different classifiers trained on hand crafted features
    to pre-trained and fine-tuned RoBERTa transformer models. The spoiler generation
    task was formulated as a question answering task, using the clickbait posts as
    questions and the articles as foundation to retrieve the answer from.The results
    show that even of the shelve transformer models outperform shallow learned models
    in the classification task. The spoiler generation task is more complex and needs
    an advanced system.
  authors:
  - "J\xFCri Keller"
  - Nicolas Rehbach
  - Ibrahim Zafar
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_262
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 5: Clickbait Spoiling'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'nancy-hicks-gribble at SemEval-2023 Task 5: Classifying and generating clickbait
    spoilers with RoBERTa'
  tldr: Clickbait spoiling and spoiler type classification in the setting of the SemEval2023
    shared task five was used to explore transformer based text classification in
    comparison to conventional, shallow learned classifying models. Additionally,
    an initial model for spoiler creation was explored. The tas
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We demonstrate a simple yet effective approach to augmenting training
    data for multilingual named entity recognition using translations. The named entity
    spans from the original sentences are transferred to translations via word alignment
    and then filtered with the baseline recognizer. The proposed approach outperforms
    the baseline XLM-Roberta on the multilingual dataset.
  authors:
  - Alberto Poncelas
  - Maksim Tkachenko
  - Ohnmar Htun
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_263
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 2: MultiCoNER II Multilingual Complex Named Entity Recognition'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Sakura at SemEval-2023 Task 2: Data Augmentation via Translation'
  tldr: 'We demonstrate a simple yet effective approach to augmenting training data
    for multilingual named entity recognition using translations. The named entity
    spans from the original sentences are transferred to translations via word alignment
    and then filtered with the baseline recognizer. The proposed '
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper describes our participation in SemEval-2023 Task 4, ValueEval:
    Identification of Human Values behind Arguments. The aim of this task is to identify
    whether or not an input text supports each of the 20 pre-defined human values.
    Previous work on human value detection has shown the effectiveness of a sequence
    classification approach using BERT. However, little is known about what type of
    task formulation is suitable for the task. To this end, this paper explores various
    task formulations, including sequence classification, question answering, and
    question answering with chain-of-thought prompting and evaluates their performances
    on the shared task dataset. Experiments show that a zero-shot approach is not
    as effective as other methods, and there is no one approach that is optimal in
    every scenario. Our analysis also reveals that utilizing the descriptions of human
    values can help to improve performance.'
  authors:
  - Masaya Tsunokake
  - Atsuki Yamaguchi
  - Yuta Koreeda
  - Hiroaki Ozaki
  - Yasuhiro Sogawa
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_264
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 4: ValueEval: Identification of Human Values behind Arguments'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Hitachi at SemEval-2023 Task 4: Exploring Various Task Formulations Reveals
    the Importance of Description Texts on Human Values'
  tldr: 'This paper describes our participation in SemEval-2023 Task 4, ValueEval:
    Identification of Human Values behind Arguments. The aim of this task is to identify
    whether or not an input text supports each of the 20 pre-defined human values.
    Previous work on human value detection has shown the effective'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We conduct a comparison of pre-trained encoder-only and decoder-only language
    models with and without continued pre-training, to detect online sexism. Our fine-tuning-based
    classifier system achieved the 16th rank in the SemEval 2023 Shared Task 10 Subtask
    A that asks to distinguish sexist and non-sexist texts. Additionally, we conduct
    experiments aimed at enhancing the interpretability of systems designed to detect
    online sexism. Our findings provide insights into the features and decision-making
    processes underlying our classifier system, thereby contributing to a broader
    effort to develop explainable AI models to detect online sexism.
  authors:
  - Kanishk Verma
  - Kolawole Adebayo
  - Joachim Wagner
  - Brian Davis
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_265
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 10: Towards Explainable Detection of Online Sexism'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'DCU at SemEval-2023 Task 10: A Comparative Analysis of Encoder-only and
    Decoder-only Language Models with Insights into Interpretability'
  tldr: We conduct a comparison of pre-trained encoder-only and decoder-only language
    models with and without continued pre-training, to detect online sexism. Our fine-tuning-based
    classifier system achieved the 16th rank in the SemEval 2023 Shared Task 10 Subtask
    A that asks to distinguish sexist and non-s
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Visual Word Sense Disambiguation (VWSD) task aims to find the most related
    image among 10 images to an ambiguous word in some limited textual context. In
    this work, we use AltCLIP features and a 3-layer standard transformer encoder
    to compare the cosine similarity between the given phrase and different images.
    Also, we improve our model's generalization by using a subset of LAION-5B. The
    best official baseline achieves 37.20\% and 54.39\% macro-averaged hit rate and
    MRR (Mean Reciprocal Rank) respectively. Our best configuration reaches 39.61\%
    and 56.78\% macro-averaged hit rate and MRR respectively. The code will be made
    publicly available on GitHub.
  authors:
  - Mohammad Javad Pirhadi
  - Motahhare Mirzaei
  - Mohammad Reza Mohammadi
  - Sauleh Eetemadi
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_266
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Task-1 - Visual Word Sense Disambiguation (Visual-WSD)
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'PMCoders at SemEval-2023 Task 1: RAltCLIP: Use Relative AltCLIP Features
    to Rank'
  tldr: Visual Word Sense Disambiguation (VWSD) task aims to find the most related
    image among 10 images to an ambiguous word in some limited textual context. In
    this work, we use AltCLIP features and a 3-layer standard transformer encoder
    to compare the cosine similarity between the given phrase and differ
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper describes our system submitted to SemEval-2023 Task 5: Clickbait
    Spoiling. We work on spoiler generation of the subtask 2 and develop a system
    which comprises two parts: 1) simple seq2seq spoiler generation and 2) post-hoc
    model ensembling. Using this simple method, we address the challenge of generating
    multipart spoiler. In the test set, our submitted system outperformed the baseline
    by a large margin (approximately 10 points above on the BLEU score) for mixed
    types of spoilers. We also found that our system successfully handled the challenge
    of the multipart spoiler, confirming the effectiveness of our approach.'
  authors:
  - Hiroto Kurita
  - Ikumi Ito
  - Hiroaki Funayama
  - Shota Sasaki
  - Shoji Moriya
  - Ye Mengyu
  - Kazuma Kokuta
  - Ryujin Hatakeyama
  - Shusaku Sone
  - Kentaro Inui
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_267
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 5: Clickbait Spoiling'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'TohokuNLP at SemEval-2023 Task 5: Clickbait Spoiling via Simple Seq2Seq
    Generation and Ensembling'
  tldr: 'This paper describes our system submitted to SemEval-2023 Task 5: Clickbait
    Spoiling. We work on spoiler generation of the subtask 2 and develop a system
    which comprises two parts: 1) simple seq2seq spoiler generation and 2) post-hoc
    model ensembling. Using this simple method, we address the challen'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes the performance of a system which uses stance as
    an output instead of taking it as an input to identify 20 human values behind
    given arguments, based on two datasets for SemEval-2023 Task 4. The rationale
    was to draw a conclusion on whether predicting stance would help predict the given
    human values better. For this setup---predicting 21 labels---a pre-trained language
    model, RoBERTa-Large was used. The system had an F\$\_1\$-score of 0.50 for predicting
    these human values for the main test set while this score was 0.35 on the secondary
    test set, and through further analysis, this paper aims to give insight into the
    problem of human value identification.
  authors:
  - Fidan Can
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_268
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 4: ValueEval: Identification of Human Values behind Arguments'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: "T\xFCbingen at SemEval-2023 Task 4: What Can Stance Tell? A Computational\
    \ Study on Detecting Human Values behind Arguments"
  tldr: This paper describes the performance of a system which uses stance as an output
    instead of taking it as an input to identify 20 human values behind given arguments,
    based on two datasets for SemEval-2023 Task 4. The rationale was to draw a conclusion
    on whether predicting stance would help predict t
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'We present a system for natural language inference in breast cancer clinical
    trial reports, as framed by SemEval 2023 Task 7: Multi-evidence Natural Language
    Inference for  Clinical Trial Data. In particular, we propose a suite of techniques
    for two related inference subtasks: entailment and evidence retrieval. The purpose
    of the textual entailment identification subtask is to determine the inference
    relation (either entailment or contradiction) between given statement pairs, while
    the goal of the evidence retrieval task is to identify a set of sentences that
    support this inference relation. To this end, we propose fine-tuning  Bio+Clinical
    BERT, a BERT-based model pre-trained on clinical data. Along with presenting our
    system, we analyze our architectural decisions in the context of our model''s
    accuracy and conduct an error analysis. Overall, our system ranked 20 / 30 on
    the entailment subtask.'
  authors:
  - Conner Takehana
  - Dylan Lim
  - Emirhan Kurtulus
  - Ramya Iyer
  - Ellie Tanimura
  - Pankhuri Aggarwal
  - Molly Cantillon
  - Alfred Yu
  - Sarosh Khan
  - Nathan Chi
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_269
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 7: Multi-Evidence Natural Language Inference for Clinical Trial
    Data'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Stanford MLab at SemEval 2023 Task 7: Neural Methods for Clinical Trial
    Report NLI'
  tldr: 'We present a system for natural language inference in breast cancer clinical
    trial reports, as framed by SemEval 2023 Task 7: Multi-evidence Natural Language
    Inference for  Clinical Trial Data. In particular, we propose a suite of techniques
    for two related inference subtasks: entailment and evidenc'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper describes the HEVS-TUW team submission to the SemEval-2023
    Task 8: Causal Claims. We participated in two subtasks: (1) causal claims detection
    and (2)  PIO identification. For subtask 1, we experimented with an ensemble of
    weakly supervised question detection and fine-tuned Transformer-based models.
    For subtask 2 of PIO frame extraction, we used a combination of deep representation
    learning and a rule-based approach. Our best model for subtask 1 ranks fourth
    with an F1-score of 65.77\%. It shows moderate benefit from ensembling models
    pre-trained on independent categories. The results for subtask 2 warrant further
    investigation for improvement.'
  authors:
  - Anjani Dhrangadhariya
  - Wojciech Kusa
  - "Henning M\xFCller"
  - Allan Hanbury
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_270
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 8: Causal medical claim identification and related PICO frame
    extraction from social media posts'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'HEVS-TUW at SemEval-2023 Task 8: Ensemble of Language Models and Rule-based
    Classifiers for Claims Identification and PICO Extraction'
  tldr: 'This paper describes the HEVS-TUW team submission to the SemEval-2023 Task
    8: Causal Claims. We participated in two subtasks: (1) causal claims detection
    and (2)  PIO identification. For subtask 1, we experimented with an ensemble of
    weakly supervised question detection and fine-tuned Transformer-ba'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In this work, we present a Named Entity Recognition (NER) system that
    was trained using a Frustratingly Easy Domain Adaptation (FEDA) over multiple
    legal corpora. The goal was to create a NER capable of detecting 14 types of legal
    named entities in Indian judgments. Besides the FEDA architecture, we explored
    a method based on overlapping context and averaging tensors to process long input
    texts, which can be beneficial when processing legal documents. The proposed NER
    reached an F1-score of 0.9007 in the sub-task B of Semeval-2023 Task 6, Understanding
    Legal Texts.
  authors:
  - "Luis Adri\xE1n Cabrera-Diego"
  - Akshita Gheewala
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_271
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 6: LegalEval: Understanding Legal Texts'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Jus Mundi at SemEval-2023 Task 6: Using a Frustratingly Easy Domain Adaption
    for a Legal Named Entity Recognition System'
  tldr: In this work, we present a Named Entity Recognition (NER) system that was
    trained using a Frustratingly Easy Domain Adaptation (FEDA) over multiple legal
    corpora. The goal was to create a NER capable of detecting 14 types of legal named
    entities in Indian judgments. Besides the FEDA architecture, we
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'In this paper, we discuss the methods we applied at SemEval-2023 Task
    10: Towards the Explainable Detection of Online Sexism. Given an input text, we
    perform three classification tasks to predict whether the text is sexist and classify
    the sexist text into subcategories in order to provide an additional explanation
    as to why the text is sexist. We explored many different types of models, including
    GloVe embeddings as the baseline approach, transformer-based deep learning models
    like BERT, RoBERTa, and DeBERTa, ensemble models, and model blending. We explored
    various data cleaning and augmentation methods to improve model performance. Pre-training
    transformer models yielded significant improvements in performance, and ensembles
    and blending slightly improved robustness in the F1 score.'
  authors:
  - Aaron Wan
  - Hong Meng Yam
  - Swetha Yogeswaran
  - Beining Zhou
  - Hee Jung Choi
  - Trevor Chow
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_272
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 10: Towards Explainable Detection of Online Sexism'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Stanford MLab at SemEval-2023 Task 10: Exploring GloVe- and Transformer-Based
    Methods for the Explainable Detection of Online Sexism'
  tldr: 'In this paper, we discuss the methods we applied at SemEval-2023 Task 10:
    Towards the Explainable Detection of Online Sexism. Given an input text, we perform
    three classification tasks to predict whether the text is sexist and classify
    the sexist text into subcategories in order to provide an additi'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'In the article, we present the CodeNLP submission to the SemEval-2023
    Task 2: MultiCoNER II Multilingual Complex Named Entity Recognition. Our approach
    is based on data augmentation by combining various strategies of sequence generation
    for training. We show that the extended procedure of fine-tuning a pre-trained
    language model can bring improvements compared to any single strategy. On the
    development subsets, the improvements were 1.7 pp and 3.1 pp of F-measure, for
    English and multilingual datasets, respectively. On the test subsets our models
    achieved 63.51\% and 73.22\% of Macro F1, respectively.'
  authors:
  - "Micha Marci\u0144czuk"
  - Wiktor Walentynowicz
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_273
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 2: MultiCoNER II Multilingual Complex Named Entity Recognition'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'CodeNLP at SemEval-2023 Task 2: Data Augmentation for Named Entity Recognition
    by Combination of Sequence Generation Strategies'
  tldr: 'In the article, we present the CodeNLP submission to the SemEval-2023 Task
    2: MultiCoNER II Multilingual Complex Named Entity Recognition. Our approach is
    based on data augmentation by combining various strategies of sequence generation
    for training. We show that the extended procedure of fine-tunin'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Sexism has been prevalent online. In this paper, we explored the effect
    of explicit linguistic features and continuous pretraining on the performance
    of pretrained language models in sexism detection. While adding linguistic features
    did not improve the performance of the model, continuous pretraining did slightly
    boost the performance of the model in Task B from a mean macro-F1 score of 0.6156
    to 0.6246. The best mean macro-F1 score in Task A was achieved by a finetuned
    HateBERT model using regular pretraining (0.8331). We observed that the linguistic
    features did not improve the model's performance. At the same time, continuous
    pretraining proved beneficial only for nuanced downstream tasks like Task-B.
  authors:
  - Murali Manohar Kondragunta
  - Amber Chen
  - Karlo Slot
  - Sanne Weering
  - Tommaso Caselli
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_274
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 10: Towards Explainable Detection of Online Sexism'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'SKAM at SemEval-2023 Task 10: Linguistic Feature Integration and Continuous
    Pretraining for Online Sexism Detection and Classification'
  tldr: Sexism has been prevalent online. In this paper, we explored the effect of
    explicit linguistic features and continuous pretraining on the performance of
    pretrained language models in sexism detection. While adding linguistic features
    did not improve the performance of the model, continuous pretraini
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Online articles using striking headlines that promise intriguing information
    are often used to attract readers. Most of the time, the information provided
    in the text is disappointing to the reader after the headline promised exciting
    news. As part of the SemEval-2023 challenge, we propose a system to generate a
    spoiler for these headlines. The spoiler provides the information promised by
    the headline and eliminates the need to read the full article. We consider Multi-Task
    Learning and generating more data using a distillation approach in our system.
    With this, we achieve an F1 score up to 51.48\% on extracting the spoiler from
    the articles.
  authors:
  - Hannah Sterz
  - Leonard Bongard
  - Tobias Werner
  - Clifton Poth
  - Martin Hentschel
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_275
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 5: Clickbait Spoiling'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'ML Mob at SemEval-2023 Task 5: "Breaking News: Our Semi-Supervised and Multi-Task
    Learning Approach Spoils Clickbait"'
  tldr: 'Online articles using striking headlines that promise intriguing information
    are often used to attract readers. Most of the time, the information provided
    in the text is disappointing to the reader after the headline promised exciting
    news. As part of the SemEval-2023 challenge, we propose a system '
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The SemEval 2023 shared task 10 ``Explainable Detection of Online Sexism"
    focuses on detecting and identifying comments and tweets containing sexist expressions
    and also explaining why it is sexist.This paper describes our system that we used
    to participate in this shared task. Our model is an ensemble of different variants
    of fine tuned DeBERTa models that employs a k-fold cross-validation. We have participated
    in the three tasks A, B and C. Our model ranked  2 nd position in tasks A, 7 th
    in task B and 4 th in task C.
  authors:
  - Fadi Hassan
  - Abdessalam Bouchekif
  - Walid Aransa
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_276
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 10: Towards Explainable Detection of Online Sexism'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'FiRC at SemEval-2023 Task 10: Fine-grained Classification of Online Sexism
    Content Using DeBERTa'
  tldr: The SemEval 2023 shared task 10 ``Explainable Detection of Online Sexism"
    focuses on detecting and identifying comments and tweets containing sexist expressions
    and also explaining why it is sexist.This paper describes our system that we used
    to participate in this shared task. Our model is an ensem
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We describe our systems participated in the SemEval-2023 shared task for
    Named Entity Recognition (NER) in English and Bangla. In order to address the
    challenges of the task, where a large number of fine-grained named entity types
    need to be detected with only a small amount of training data, we use a method
    to augment the training data based on BabelNet conceptsand Wikipedia redirections
    to automatically annotate named entities from Wikipedia articles. We build our
    NER systems based on the powerful mDeBERTa pretrained language model and trained
    on the augmented data. Our approach significantly enhances the performance of
    the fine-grained NER task in both English and Bangla subtracks, outperforming
    the baseline models. Specifically, our augmented systems achieve macro-f1 scores
    of 52.64\% and 64.31\%, representing improvements of 2.38\% and 11.33\% over the
    English and Bangla baselines, respectively.
  authors:
  - Phu Gia Hoang
  - Le Thanh
  - Hai-Long Trieu
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_277
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 2: MultiCoNER II Multilingual Complex Named Entity Recognition'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'VBD_NLP at SemEval-2023 Task 2: Named Entity Recognition Systems Enhanced
    by BabelNet and Wikipedia'
  tldr: We describe our systems participated in the SemEval-2023 shared task for Named
    Entity Recognition (NER) in English and Bangla. In order to address the challenges
    of the task, where a large number of fine-grained named entity types need to be
    detected with only a small amount of training data, we use
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'For SemEval-2023 Task 5, we have submitted three DeBERTaV3[LARGE] models
    to tackle the first subtask, classifying spoiler types (passage, phrase, multi)
    of clickbait web articles. The choice of basic parameters like sequence length
    with BERT[BASE] uncased and further approaches were then tested with DeBERTaV3[BASE]
    only moving the most promising ones to DeBERTaV3[LARGE]. Our research showed that
    information-placement on webpages is often optimized regarding e.g. ad-placement
    Those informations are usually described within the webpages markup which is why
    we conducted an approach that takes this into account. Overall we could not manage
    to beat the baseline, which we lead down to three reasons: First we only crawled
    markup for Huffington Post articles, extracting only \textless{}p\textgreater{}-
    and \textless{}a\textgreater{}-tags which will not cover enough aspects of a webpages
    design. Second Huffington Post articles are overrepresented in the given dataset,
    which, third, shows an imbalance towards the spoiler tags.We highly suggest re-annotating
    the given dataset to use markup-optimized models like MarkupLM or TIE and to clear
    it from embedded articles like "Yahoo" or archives like "archive.is" or "web.archive"
    to avoid noise. Also, the imbalance should be tackled by adding articles from
    sources other than Huffington Post, considering that also multi-tagged entries
    should be balanced towards passage- and phrase-tagged ones.\textbackslash{}end\{abstract\}'
  authors:
  - Sabrina Spreitzer
  - Hoai Nam Tran
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_278
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 5: Clickbait Spoiling'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Stephen Colbert at SemEval-2023 Task 5: Using Markup for Classifying Clickbait'
  tldr: For SemEval-2023 Task 5, we have submitted three DeBERTaV3[LARGE] models to
    tackle the first subtask, classifying spoiler types (passage, phrase, multi) of
    clickbait web articles. The choice of basic parameters like sequence length with
    BERT[BASE] uncased and further approaches were then tested with
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper describes our system designed for SemEval-2023 Task 12: Sentiment
    analysis for African languages. The challenge faced by this task is the scarcity
    of labeled data and linguistic resources in low-resource settings. To alleviate
    these, we propose a generalized multilingual system SACL-XLMR for sentiment analysis
    on low-resource languages. Specifically, we design a lexicon-based multilingual
    BERT to facilitate language adaptation and sentiment-aware representation learning.
    Besides, we apply a supervised adversarial contrastive learning technique to learn
    sentiment-spread structured representations and enhance model generalization.
    Our system achieved competitive results, largely outperforming baselines on both
    multilingual and zero-shot sentiment classification subtasks. Notably, the system
    obtained the 1st rank on the zero-shot classification subtask in the official
    ranking. Extensive experiments demonstrate the effectiveness of our system.'
  authors:
  - Dou Hu
  - Lingwei Wei
  - Yaxin Liu
  - Wei Zhou
  - Songlin Hu
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_279
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 12: AfriSenti-SemEval: Sentiment Analysis for Low-resource African
    Languages using Twitter Dataset'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'UCAS-IIE-NLP at SemEval-2023 Task 12: Enhancing Generalization of Multilingual
    BERT for Low-resource Sentiment Analysis'
  tldr: 'This paper describes our system designed for SemEval-2023 Task 12: Sentiment
    analysis for African languages. The challenge faced by this task is the scarcity
    of labeled data and linguistic resources in low-resource settings. To alleviate
    these, we propose a generalized multilingual system SACL-XLMR '
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: A legal document is usually long and dense requiring human effort to parse
    it. It also contains significant amounts of jargon which make deriving insights
    from it using existing models a poor approach. This paper presents the approaches
    undertaken to perform the task of rhetorical role labelling on Indian Court Judgements.
    We experiment with graph based approaches like Graph Convolutional Networks and
    Label Propagation Algorithm, and transformer-based approaches including variants
    of BERT to improve accuracy scores on text classification of complex legal documents.
  authors:
  - Anshika Gupta
  - Shaz Furniturewala
  - Vijay Kumari
  - Yashvardhan Sharma
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_281
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 6: LegalEval: Understanding Legal Texts'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Steno AI at SemEval-2023 Task 6: Rhetorical Role Labelling of Legal Documents
    using Transformers and Graph Neural Networks'
  tldr: 'A legal document is usually long and dense requiring human effort to parse
    it. It also contains significant amounts of jargon which make deriving insights
    from it using existing models a poor approach. This paper presents the approaches
    undertaken to perform the task of rhetorical role labelling on '
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'With the increasing number of clinical trial reports generated every
    day, it is becoming hard to keep up with novel discoveries that inform evidence-based
    healthcare recommendations. To help automate this process and assist medical experts,
    NLP solutions are being developed. This motivated the SemEval-2023 Task 7, where
    the goal was to develop an NLP system for two tasks: evidence retrieval and natural
    language inference from clinical trial data. In this paper, we describe our two
    developed systems. The first one is a pipeline system that models the two tasks
    separately, while the second one is a joint system that learns the two tasks simultaneously
    with a shared representation and a multi-task learning approach. The final system
    combines their outputs in an ensemble system. We formalize the models, present
    their characteristics and challenges, and provide an analysis of achieved results.
    Our system ranked 3rd out of 40 participants with a final submission.'
  authors:
  - Juraj Vladika
  - Florian Matthes
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_282
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 7: Multi-Evidence Natural Language Inference for Clinical Trial
    Data'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Sebis at SemEval-2023 Task 7: A Joint System for Natural Language Inference
    and Evidence Retrieval from Clinical Trial Reports'
  tldr: With the increasing number of clinical trial reports generated every day,
    it is becoming hard to keep up with novel discoveries that inform evidence-based
    healthcare recommendations. To help automate this process and assist medical experts,
    NLP solutions are being developed. This motivated the SemEv
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In this paper, we describe our approach to Task 4 in SemEval 2023. Our
    pipeline tries to solve the problem of multi-label text classification of human
    values in English-written arguments. We propose a label-aware system where we
    reframe the multi-label task into a binary task resembling an NLI task. We propose
    to include the semantic description of the human values by comparing each description
    to each argument and ask whether there is entailment or not.
  authors:
  - Ignacio Talavera Cepeda
  - Amalie Pauli
  - Ira Assent
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_284
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 4: ValueEval: Identification of Human Values behind Arguments'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Sren Kierkegaard at SemEval-2023 Task 4: Label-aware text classification
    using Natural Language Inference'
  tldr: In this paper, we describe our approach to Task 4 in SemEval 2023. Our pipeline
    tries to solve the problem of multi-label text classification of human values
    in English-written arguments. We propose a label-aware system where we reframe
    the multi-label task into a binary task resembling an NLI task.
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The Clickbait Challenge targets spoiling the clickbaits using short pieces
    of information known as spoilers to satisfy the curiosity induced by a clickbait
    post.The large context of the article associated with the clickbait and differences
    in the spoiler forms, make the task challenging.Hence, to tackle the large context,
    we propose an Information Condensation-based approach, which prunes down the unnecessary
    context.Given an article, our filtering module optimised with a contrastive learning
    objective first selects the parapraphs that are the most relevant to the corresponding
    clickbait.The resulting condensed article is then fed to the two downstream tasks
    of spoiler type classification and spoiler generation.We demonstrate and analyze
    the gains from this approach on both the tasks.Overall, we win the task of spoiler
    type classification and achieve competitive results on spoiler generation.
  authors:
  - Anubhav Sharma
  - Sagar Joshi
  - Tushar Abhishek
  - Radhika Mamidi
  - Vasudeva Varma
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_285
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 5: Clickbait Spoiling'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Billy-Batson at SemEval-2023 Task 5: An Information Condensation based System
    for Clickbait Spoiling'
  tldr: The Clickbait Challenge targets spoiling the clickbaits using short pieces
    of information known as spoilers to satisfy the curiosity induced by a clickbait
    post.The large context of the article associated with the clickbait and differences
    in the spoiler forms, make the task challenging.Hence, to ta
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Clickbait is the text or a thumbnail image that entices the user to click
    the accompanying link. Clickbaits employ strategies while deliberately hiding
    the critical elements of the article and revealing partial information in the
    title, which arouses sufficient curiosity and motivates the user to click the
    link. In this work, we identify the kind of spoiler given a clickbait title. We
    formulate this as a text classification problem. We finetune pretrained transformer
    models on the title of the post and build models for theclickbait-spoiler classification.
    We achieve a balanced accuracy of 0.70 which is close to the baseline.
  authors:
  - Vijayasaradhi Indurthi
  - Vasudeva Varma
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_286
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 5: Clickbait Spoiling'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Francis Wilde at SemEval-2023 Task 5: Clickbait Spoiler Type Identification
    with Transformers'
  tldr: Clickbait is the text or a thumbnail image that entices the user to click
    the accompanying link. Clickbaits employ strategies while deliberately hiding
    the critical elements of the article and revealing partial information in the
    title, which arouses sufficient curiosity and motivates the user to cl
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper presents the submissions of the DH-FBK team for the three
    tasks of Task 10 at SemEval 2023. The Explainable Detection of Online Sexism (EDOS)
    task aims at detecting sexism in English text in an accurate and explainable way,
    thanks to a fine-grained annotation that follows a three-level schema: sexist
    or not (Task A), category of sexism (Task B) and vector of sexism (Task C) exhibited.We
    use a multi-task learning approach in which models share representations from
    all three tasks, allowing for knowledge to be shared across them. Notably, with
    our approach a single model can solve all three tasks.In addition, motivated by
    the subjective nature of the task, we incorporate inter-annotator agreement information
    in our multi-task architecture. Although disaggregated annotations are not available,
    we artificially estimate them using a 5-classifier ensemble, and show that ensemble
    agreement can be a good approximation of crowd agreement.  Our approach achieves
    competitive results, ranking 32nd out of 84, 24th out of 69 and 11th out of 63
    for Tasks A, B and C respectively. We finally show that low inter-annotator agreement
    levels are associated with more challenging examples for models, making agreement
    information use ful for this kind of task.'
  authors:
  - Elisa Leonardelli
  - Camilla Casula
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_287
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 10: Towards Explainable Detection of Online Sexism'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'DH-FBK at SemEval-2023 Task 10: Multi-Task Learning with Classifier Ensemble
    Agreement for Sexism Detection'
  tldr: This paper presents the submissions of the DH-FBK team for the three tasks
    of Task 10 at SemEval 2023. The Explainable Detection of Online Sexism (EDOS)
    task aims at detecting sexism in English text in an accurate and explainable way,
    thanks to a fine-grained annotation that follows a three-level sc
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The rise of social media has exponentially witnessed the use of clickbait
    posts that grab users' attention. Although work has been done to detect clickbait
    posts, this is the first task focused on generating appropriate spoilers for these
    potential clickbaits. This paper presents our approach in this direction. We use
    different encoding techniques that capture the context of the post text and the
    target paragraph. We propose hierarchical encoding with count and document length
    feature-based model for spoiler type classification which uses Recurrence over
    Pretrained Encoding. We also propose combining multiple ranking with reciprocal
    rank fusion for passage spoiler retrieval and question-answering approach for
    phrase spoiler retrieval. For multipart spoiler retrieval, we combine the above
    two spoiler retrieval methods. Experimental results over the benchmark suggest
    that our proposed spoiler retrieval methods are able to retrieve spoilers that
    are semantically very close to the ground truth spoilers.
  authors:
  - Sujit Kumar
  - Aditya Sinha
  - Soumyadeep Jana
  - Rahul Mishra
  - Sanasam Ranbir Singh
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_288
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 5: Clickbait Spoiling'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Jack-flood at SemEval-2023 Task 5:Hierarchical Encoding and Reciprocal Rank
    Fusion-Based System for Spoiler Classification and Generation
  tldr: The rise of social media has exponentially witnessed the use of clickbait
    posts that grab users' attention. Although work has been done to detect clickbait
    posts, this is the first task focused on generating appropriate spoilers for these
    potential clickbaits. This paper presents our approach in thi
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Online social platforms are now propagating sexist content endangering
    the involvement and inclusion of women on these platforms. Sexism refers to hostility,
    bigotry, or discrimination based on gender, typically against women. The proliferation
    of such notions deters women from engaging in social media spontaneously. Hence,
    detecting sexist content is critical to ensure a safe online platform where women
    can participate without the fear of being a target of sexism. This paper describes
    our participation in subtask A of SemEval-2023 Task 10: Explainable Detection
    of Online Sexism (EDOS). This subtask requires classifying textual content as
    sexist or not sexist. We incorporate a RoBERTa-based architecture and further
    finetune the hyperparameters to entail better performance. The procured results
    depict the competitive performance of our approach among the other participants.'
  authors:
  - Fareen Tasneem
  - Tashin Hossain
  - Jannatun Naim
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_289
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 10: Towards Explainable Detection of Online Sexism'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'KingsmanTrio at SemEval-2023 Task 10: Analyzing the Effectiveness of Transfer
    Learning Models for Explainable Online Sexism Detection'
  tldr: Online social platforms are now propagating sexist content endangering the
    involvement and inclusion of women on these platforms. Sexism refers to hostility,
    bigotry, or discrimination based on gender, typically against women. The proliferation
    of such notions deters women from engaging in social me
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Based on recent progress in image-text retrieval techniques, this paper
    presents a fine-tuned model for the Visual Word Sense Disambiguation (VWSD) task.
    The proposed system fine-tunes a pre-trained model using ITC and ITM losses and
    employs a candidate selection approach for faster inference. The system was trained
    on the VWSD task dataset and evaluated on a separate test set using Mean Reciprocal
    Rank (MRR) metric. Additionally, the system was tested on the provided test set
    which contained Persian and Italian languages, and the results were evaluated
    on each language separately. Our proposed system demonstrates the potential of
    fine-tuning pre-trained models for complex language tasks and provides insights
    for further research in the field of image text retrieval.
  authors:
  - Mohammadreza Molavi
  - Hossein Zeinali
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_290
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Task-1 - Visual Word Sense Disambiguation (Visual-WSD)
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'SLT at SemEval-2023 Task 1: Enhancing Visual Word Sense Disambiguation through
    Image Text Retrieval using BLIP'
  tldr: Based on recent progress in image-text retrieval techniques, this paper presents
    a fine-tuned model for the Visual Word Sense Disambiguation (VWSD) task. The proposed
    system fine-tunes a pre-trained model using ITC and ITM losses and employs a candidate
    selection approach for faster inference. The s
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes the NER system designed by the CAIR-NLP team for
    submission to Multilingual Complex Named Entity Recognition (MultiCoNER II) shared
    task, which presents a novel challenge of recognizing complex, ambiguous, and
    fine-grained entities in low-context, multi-lingual, multi-domain dataset and
    evaluation on the noisy subset. We propose a Multi-Objective Joint Learning System
    (MOJLS) for NER, which aims to enhance the representation of entities and improve
    label predictions through the joint implementation of a set of learning objectives.
    Our official submission MOJLS implements four objectives. These include the representation
    of the named entities should be close to its entity type definition, low-context
    inputs should have representation close to their augmented context, and also minimization
    of two label prediction errors, one based on CRF and another biaffine-based predictions,
    where both are producing similar output label distributions. The official results
    ranked our system 2nd in five tracks (Multilingual, Spanish, Swedish, Ukrainian,
    and Farsi) and 3 rd in three (French, Italian, and Portuguese) out of 13 tracks.
    Also evaluation of the noisy subset, our model achieved relatively better ranks.
    Official results indicate the effectiveness of the proposed MOJLS in dealing with
    the contemporary challenges of NER.
  authors:
  - Sangeeth N
  - Biswajit Paul
  - Chandramani Chaudhary
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_291
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 2: MultiCoNER II Multilingual Complex Named Entity Recognition'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'CAIR-NLP at SemEval-2023 Task 2: A Multi-Objective Joint Learning System
    for Named Entity Recognition'
  tldr: This paper describes the NER system designed by the CAIR-NLP team for submission
    to Multilingual Complex Named Entity Recognition (MultiCoNER II) shared task,
    which presents a novel challenge of recognizing complex, ambiguous, and fine-grained
    entities in low-context, multi-lingual, multi-domain dat
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Many nations and organizations have begun collecting and storing clinical
    trial records for storage and analytical purposes so that medical and clinical
    practitioners can refer to them on a centralized database over the internet and
    stay updated with the current clinical information. The amount of clinical trial
    records have gone through the roof, making it difficult for many medical and clinical
    practitioners to stay updated with the latest information. To help and support
    medical and clinical practitioners, there is a need to build intelligent systems
    that can update them with the latest information in a byte-sized condensed format
    and, at the same time, leverage their understanding capabilities to help them
    make decisions. This paper describes our contribution to SemEval 2023 Task 7:
    Multi-evidence Natural Language Inference for Clinical Trial Data (NLI4CT). Our
    results show that there is still a need to build domain-specific models as smaller
    transformer-based models can be finetuned on that data and outperform foundational
    large language models like GPT-3.5. We also demonstrate how the performance of
    GPT-3.5 can be increased using few-shot prompting by leveraging the semantic similarity
    of the text samples and the few-shot train snippets. We will also release our
    code and our models on open source hosting platforms, GitHub and HuggingFace.'
  authors:
  - Bhavish Pahwa
  - Bhavika Pahwa
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_292
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 7: Multi-Evidence Natural Language Inference for Clinical Trial
    Data'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'BpHigh at SemEval-2023 Task 7: Can Fine-tuned Cross-encoders Outperform
    GPT-3.5 in NLI Tasks on Clinical Trial Data?'
  tldr: Many nations and organizations have begun collecting and storing clinical
    trial records for storage and analytical purposes so that medical and clinical
    practitioners can refer to them on a centralized database over the internet and
    stay updated with the current clinical information. The amount of c
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Intimacy is an essential element of human relationships and language is
    a crucial means of conveying it. Textual intimacy analysis can reveal social norms
    in different contexts and serve as a benchmark for testing computational models'
    ability to understand social information. In this paper, we propose a novel weak-labeling
    strategy for data augmentation in text regression tasks called WADER. WADER uses
    data augmentation to address the problems of data imbalance and data scarcity
    and provides a method for data augmentation in cross-lingual, zero-shot tasks.
    We benchmark the performance of State-of-the-Art pre-trained multilingual language
    models using WADER and analyze the use of sampling techniques to mitigate bias
    in data and optimally select augmentation candidates. Our results show that WADER
    outperforms the baseline model and provides a direction for mitigating data imbalance
    and scarcity in text regression tasks.
  authors:
  - Manan Suri
  - Aaryak Garg
  - Divya Chaudhary
  - Ian Gorton
  - Bijendra Kumar
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_293
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 9: Multilingual Tweet Intimacy Analysis'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'WADER at SemEval-2023 Task 9: A Weak-labelling framework for Data augmentation
    in tExt Regression Tasks'
  tldr: Intimacy is an essential element of human relationships and language is a
    crucial means of conveying it. Textual intimacy analysis can reveal social norms
    in different contexts and serve as a benchmark for testing computational models'
    ability to understand social information. In this paper, we prop
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The computational identification of human values is a novel and challenging
    research that holds the potential to offer valuable insights into the nature of
    human behavior and cognition. This paper presents the methodology adopted by the
    Arthur-Caplan research team for the SemEval-2023 Task 4, which entailed the detection
    of human values behind arguments. The proposed system integrates  BERT, ERNIE2.0,
    RoBERTA and XLNet models with fine tuning. Experimental results show that the
    macro F1 score of our system achieved 0.512, which overperformed baseline methods
    by 9.2\%  on the test set.
  authors:
  - Xianxian Song
  - Jinhui Zhao
  - Ruiqi Cao
  - Linchi Sui
  - Binyang Li
  - Tingyue Guan
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_294
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 4: ValueEval: Identification of Human Values behind Arguments'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Arthur Caplan at SemEval-2023 Task 4: Enhancing Human Value Detection through
    Fine-tuned Pre-trained Models'
  tldr: 'The computational identification of human values is a novel and challenging
    research that holds the potential to offer valuable insights into the nature of
    human behavior and cognition. This paper presents the methodology adopted by the
    Arthur-Caplan research team for the SemEval-2023 Task 4, which '
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper presents an approach to tackle the task of Visual Word Sense
    Disambiguation (Visual-WSD), which involves determining the most appropriate image
    to represent a given polysemous word in one of its particular senses. The proposed
    approach leverages the CLIP model, prompt engineering, and text-to-image models
    such as GLIDE and DALL-E 2 for both image retrieval and generation. To evaluate
    our approach, we participated in the SemEval 2023 shared task on ``Visual Word
    Sense Disambiguation (Visual-WSD)'' using a zero-shot learning setting, where
    we compared the accuracy of different combinations of tools, including ``Simple
    prompt-based'' methods and ``Generated prompt-based'' methods for prompt engineering
    using completion models, and text-to-image models for changing input modality
    from text to image. Moreover, we explored the benefits of cross-modality evaluation
    between text and candidate images using CLIP. Our experimental results demonstrate
    that the proposed approach reaches better results than cross-modality approaches,
    highlighting the potential of prompt engineering and text-to-image models to improve
    accuracy in Visual-WSD tasks. We assessed our approach in a zero-shot learning
    scenario and attained an accuracy of 68.75\textbackslash{}\% in our best attempt.
  authors:
  - Zeinab Taghavi
  - Parsa Haghighi Naeini
  - Mohammad Ali Sadraei Javaheri
  - Soroush Gooran
  - Ehsaneddin Asgari
  - Hamid Reza Rabiee
  - Hossein Sameti
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_295
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Task-1 - Visual Word Sense Disambiguation (Visual-WSD)
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Ebhaam at SemEval-2023 Task 1: A CLIP-Based Approach for Comparing Cross-modality
    and Unimodality in Visual Word Sense Disambiguation'
  tldr: This paper presents an approach to tackle the task of Visual Word Sense Disambiguation
    (Visual-WSD), which involves determining the most appropriate image to represent
    a given polysemous word in one of its particular senses. The proposed approach
    leverages the CLIP model, prompt engineering, and tex
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In this paper, we introduce our submission in the task of visual word
    sense disambiguation (vWSD). Our proposed solution operates by deriving quasi-symbolic
    semantic categories from the hidden representations of multi-modal text-image
    encoders. Our results are mixed, as we manage to achieve a substantial boost in
    performance when evaluating on a validation set, however, we experienced detrimental
    effects during evaluation on the actual test set. Our positive results on the
    validation set confirms the validity of the quasi-symbolic features, whereas our
    results on the test set revealed that the proposed technique was not able to cope
    with the sufficiently different distribution of the test data.
  authors:
  - "G\xE1bor Berend"
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_296
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Task-1 - Visual Word Sense Disambiguation (Visual-WSD)
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'SzegedAI at SemEval-2023 Task 1: Applying Quasi-Symbolic Representations
    in Visual Word Sense Disambiguation'
  tldr: In this paper, we introduce our submission in the task of visual word sense
    disambiguation (vWSD). Our proposed solution operates by deriving quasi-symbolic
    semantic categories from the hidden representations of multi-modal text-image
    encoders. Our results are mixed, as we manage to achieve a substa
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In this paper, we have worked on explainability and understanding of the
    decisions made by models in the form of classification tasks. The task is divided
    into 3 subtasks. The first task consists of determining Binary Sexism Detection.
    The second task describes the Category of Sexism. The third task describes a more
    Fine-grained Category of Sexism. Our work explores solving these tasks as a classification
    problem by fine-tuning transformer-based architecture. We have performed several
    experiments with our architecture, including combining multiple transformers,
    using domain adaptive pretraining on the unlabelled dataset provided by Reddit
    and Gab, Joint learning, and taking different layers of transformers as input
    to a classification head. Our system (with the team name Attention') was able
    to achieve a macro F1 score of 0.839 for task A, 0.5835 macro F1 score for task
    B and 0.3356 macro F1 score for task C at the Codalab SemEval Competition. Later
    we improved the accuracy of Task B to 0.6228 and Task C to 0.3693 in the test
    set.
  authors:
  - Debashish Roy
  - Manish Shrivastava
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_297
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 10: Towards Explainable Detection of Online Sexism'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Attention at SemEval-2023 Task 10: Explainable Detection of Online Sexism
    (EDOS)'
  tldr: In this paper, we have worked on explainability and understanding of the decisions
    made by models in the form of classification tasks. The task is divided into 3
    subtasks. The first task consists of determining Binary Sexism Detection. The
    second task describes the Category of Sexism. The third task
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This study investigates learning with disagreement in NLP tasks and evaluates
    its performance on four datasets. The results suggest that the model performs
    best on the experimental dataset and faces challenges in minority languages. Furthermore,
    the analysis indicates that annotator demographics play a significant role in
    the interpretation of such tasks. This study suggests the need for greater consideration
    of demographic differences in annotators and more comprehensive evaluation metrics
    for NLP models.
  authors:
  - Ruyuan Wan
  - Karla Badillo-Urquiola
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_298
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 11: Learning with Disagreements (Le-Wi-Di)'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Dragonfly_captain at SemEval-2023 Task 11: Unpacking Disagreement with Investigation
    of Annotator Demographics and Task Difficulty'
  tldr: This study investigates learning with disagreement in NLP tasks and evaluates
    its performance on four datasets. The results suggest that the model performs
    best on the experimental dataset and faces challenges in minority languages. Furthermore,
    the analysis indicates that annotator demographics pla
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'We present the findings of our participation in the SemEval-2023 Task
    10: Explainable Detection of Online Sexism (EDOS) task, a shared task on offensive
    language (sexism) detection on English Gab and Reddit dataset. We investigated
    the effects of transferring two language models: XLM-T (sentiment classification)
    and HateBERT (same domain - Reddit) for multilevel classification into Sexist
    or not Sexist, and other subsequent sub-classifications of the sexist data. We
    also use synthetic classification of unlabelled dataset and intermediary class
    information to maximize the performance of our models. We submitted a system in
    Task A, and it ranked 49th with F1-score of 0.82. This result showed to be competitive
    as it only under-performed the best system by 0.052\%F1-score.'
  authors:
  - Saminu Mohammad Aliyu
  - Idris Abdulmumin
  - Shamsuddeen Hassan Muhammad
  - Ibrahim Said Ahmad
  - Saheed Abdullahi Salahudeen
  - Aliyu Yusuf
  - Falalu Ibrahim Lawan
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_299
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 10: Towards Explainable Detection of Online Sexism'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'HausaNLP at SemEval-2023 Task 10: Transfer Learning, Synthetic Data and
    Side-information for Multi-level Sexism Classification'
  tldr: 'We present the findings of our participation in the SemEval-2023 Task 10:
    Explainable Detection of Online Sexism (EDOS) task, a shared task on offensive
    language (sexism) detection on English Gab and Reddit dataset. We investigated
    the effects of transferring two language models: XLM-T (sentiment cl'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Human values identification from a set of argument is becoming a prominent
    area of research in argument mining. Among some options, values convey what may
    be the most desirable and widely accepted answer. The diversity of human beliefs,
    random texture and implicit meaning within the arguments makes it more difficult
    to identify human values from the arguments. To address these challenges, SemEval-2023
    Task 4 introduced a shared task ValueEval focusing on identifying human values
    categories based on given arguments. This paper presents our participation in
    this task where we propose a finetuned DeBERTa transformers-based classification
    approach to identify the desire human value category. We utilize different training
    strategy with the finetuned DeBERTa model to enhance contextual representation
    on this downstream task. Our proposed method achieved competitive performance
    among the participants' methods.
  authors:
  - Abdul Aziz
  - Md. Akram Hossain
  - Abu Nowshed Chy
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_300
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 4: ValueEval: Identification of Human Values behind Arguments'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'CSECU-DSG at SemEval-2023 Task 4: Fine-tuning DeBERTa Transformer Model
    with Cross-fold Training and Multi-sample Dropout for Human Values Identification'
  tldr: 'Human values identification from a set of argument is becoming a prominent
    area of research in argument mining. Among some options, values convey what may
    be the most desirable and widely accepted answer. The diversity of human beliefs,
    random texture and implicit meaning within the arguments makes '
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper describes our approach for SemEval- 2023 Task 3: Detecting
    the category, the fram- ing, and the persuasion techniques in online news in a
    multilingual setup. For Subtask 1 (News Genre), we propose an ensemble of fully
    trained and adapter mBERT models which was ranked joint-first for German, and
    had the high- est mean rank of multi-language teams. For Subtask 2 (Framing),
    we achieved first place in 3 languages, and the best average rank across all the
    languages, by using two separate ensem- bles: a monolingual RoBERTa-MUPPETLARGE
    and an ensemble of XLM-RoBERTaLARGE with adapters and task adaptive pretraining.
    For Sub- task 3 (Persuasion Techniques), we trained a monolingual RoBERTa-Base
    model for English and a multilingual mBERT model for the re- maining languages,
    which achieved top 10 for all languages, including 2nd for English. For each subtask,
    we compared monolingual and multilingual approaches, and considered class imbalance
    techniques.'
  authors:
  - Ben Wu
  - Olesya Razuvayevskaya
  - Freddy Heppell
  - "Jo\xE3o A. Leite"
  - Carolina Scarton
  - Kalina Bontcheva
  - Xingyi Song
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_301
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 3: Detecting the Category, the Framing, and the Persuasion Techniques
    in Online News in a Multi-lingual Setup'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'SheffieldVeraAI at SemEval-2023 Task 3: Mono and Multilingual Approaches
    for News Genre, Topic and Persuasion Technique Classification'
  tldr: 'This paper describes our approach for SemEval- 2023 Task 3: Detecting the
    category, the fram- ing, and the persuasion techniques in online news in a multilingual
    setup. For Subtask 1 (News Genre), we propose an ensemble of fully trained and
    adapter mBERT models which was ranked joint-first for Germa'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The SemEval 2023 Task 9 Multilingual Tweet Intimacy Analysis, is a shared
    task for analysing the intimacy in the tweets posted on Twitter. The dataset was
    provided by Pei and Jurgens, who are part of the task organisers, for this task
    consists of tweets in various languages, such as Chinese, English, French, Italian,
    Portuguese, and Spanish. The testing dataset also had unseen languages such as
    Hindi, Arabic, Dutch and Korean. The tweets may or may not be related to intimacy.
    The task of our team was to score the intimacy in tweets and place it in the range
    of 05 based on the level of intimacy in the tweet using the dataset provided which
    consisted of tweets along with its scores. The intimacy score is used to indicate
    whether a tweet is intimate or not. Our team participated in the task and proposed
    the ROBERTa model to analyse the intimacy of the tweets.
  authors:
  - Harish B
  - Naveen D
  - Prem Balasubramanian
  - Aarthi S
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_302
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 9: Multilingual Tweet Intimacy Analysis'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'CKingCoder at SemEval-2023 Task 9: Multilingual Tweet Intimacy Analysis'
  tldr: The SemEval 2023 Task 9 Multilingual Tweet Intimacy Analysis, is a shared
    task for analysing the intimacy in the tweets posted on Twitter. The dataset was
    provided by Pei and Jurgens, who are part of the task organisers, for this task
    consists of tweets in various languages, such as Chinese, English
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The MultiCoNER \textbackslash{}RNum\{2\} shared task aims to tackle multilingual
    named entity recognition (NER) in fine-grained and noisy scenarios, and it inherits
    the semantic ambiguity and low-context setting of the MultiCoNER \textbackslash{}RNum\{1\}
    task. To cope with these problems, the previous top systems in the MultiCoNER
    \textbackslash{}RNum\{1\} either incorporate the knowledge bases or gazetteers.
    However, they still suffer from insufficient knowledge, limited context length,
    single retrieval strategy. In this paper, our team \textbackslash{}textbf\{DAMO-NLP\}
    proposes a unified retrieval-augmented system (U-RaNER) for fine-grained multilingual
    NER. We perform error analysis on the previous top systems and reveal that their
    performance bottleneck lies in insufficient knowledge. Also, we discover that
    the limited context length causes the retrieval knowledge to be invisible to the
    model. To enhance the retrieval context, we incorporate the entity-centric Wikidata
    knowledge base, while utilizing the infusion approach to broaden the contextual
    scope of the model. Also, we explore various search strategies and refine the
    quality of retrieval knowledge. Our system\textbackslash{}footnote\{We will release
    the dataset, code, and scripts of our system at \{\textbackslash{}small \textbackslash{}url\{https://github.com/modelscope/AdaSeq/tree/master/examples/U-RaNER\}\}.\}
    wins 9 out of 13 tracks in the MultiCoNER \textbackslash{}RNum\{2\} shared task.
    Additionally, we compared our system with ChatGPT, one of the large language models
    which have unlocked strong capabilities on many tasks. The results show that there
    is still much room for improvement for ChatGPT on the extraction task.
  authors:
  - Zeqi Tan
  - Shen Huang
  - Zixia Jia
  - Jiong Cai
  - Yinghui Li
  - Weiming Lu
  - Yueting Zhuang
  - Kewei Tu
  - Pengjun Xie
  - Fei Huang
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_303
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 2: MultiCoNER II Multilingual Complex Named Entity Recognition'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'DAMO-NLP at SemEval-2023 Task 2: A Unified Retrieval-augmented System for
    Multilingual Named Entity Recognition'
  tldr: The MultiCoNER \textbackslash{}RNum\{2\} shared task aims to tackle multilingual
    named entity recognition (NER) in fine-grained and noisy scenarios, and it inherits
    the semantic ambiguity and low-context setting of the MultiCoNER \textbackslash{}RNum\{1\}
    task. To cope with these problems, the previ
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We build a model using large multilingual pretrained language model XLM-T
    for regression task and fine-tune it on the MINT (Multilingual INTmacy) analysis
    dataset which covers 6 languages for training and 4 languages for testing zero-shot
    performance of the model. The dataset was annotated and the annotations are intimacy
    scores. We experiment with several deep learning architectures to predict intimacy
    score. To achieve optimal performance we modify several model settings including
    loss function, number and type of layers. In total, we ran 16 end-to-end experiments.
    Our best system achieved a Pearson Correlation score of 0.52.
  authors:
  - Mohammadmostafa Rostamkhani
  - Ghazal Zamaninejad
  - Sauleh Eetemadi
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_305
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 9: Multilingual Tweet Intimacy Analysis'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'ROZAM at SemEval 2023 Task 9: Multilingual Tweet Intimacy Analysis'
  tldr: We build a model using large multilingual pretrained language model XLM-T
    for regression task and fine-tune it on the MINT (Multilingual INTmacy) analysis
    dataset which covers 6 languages for training and 4 languages for testing zero-shot
    performance of the model. The dataset was annotated and the a
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper introduces a data augmentation technique for the task of detecting
    human values. Our approach involves generating additional examples using metadata
    that describes the labels in the datasets. We evaluated the effectiveness of our
    method by fine-tuning BERT and RoBERTa models on our augmented dataset and comparing
    their F1 -scores to those of the non-augmented dataset. We obtained competitive
    results on both the Main test set and the Nahj al-Balagha test set, ranking 14th
    and 7th respectively among the participants. We also demonstrate that by incorporating
    our augmentation technique, the classification performance of BERT and RoBERTa
    is improved, resulting in an increase of up to 10.1\% in their F1-score.
  authors:
  - Erfan Moosavi Monazzah
  - Sauleh Eetemadi
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_306
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 4: ValueEval: Identification of Human Values behind Arguments'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Prodicus at SemEval-2023 Task 4: Enhancing Human Value Detection with Data
    Augmentation and Fine-Tuned Language Models'
  tldr: This paper introduces a data augmentation technique for the task of detecting
    human values. Our approach involves generating additional examples using metadata
    that describes the labels in the datasets. We evaluated the effectiveness of our
    method by fine-tuning BERT and RoBERTa models on our augmen
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In this paper, we discuss our efforts on SemEval-2023 Task4, a task to
    classify the human value categoriesthat an argument draws on. Arguments consist
    of a premise, conclusion,and the premise's stance on the conclusion. Our team
    experimented with GloVe embeddings and fine-tuning BERT. We found that an ensembling
    of BERT and GloVe with RidgeRegression worked the best.
  authors:
  - Kenan Hasanaliyev
  - Kevin Li
  - Saanvi Chawla
  - Michael Nath
  - Rohan Sanda
  - Justin Wu
  - William Huang
  - Daniel Yang
  - Shane Mion
  - Kiran Bhat
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_307
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 4: ValueEval: Identification of Human Values behind Arguments'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Francis Bacon at SemEval-2023 Task 4: Ensembling BERT and GloVe for Value
    Identification in Arguments'
  tldr: 'In this paper, we discuss our efforts on SemEval-2023 Task4, a task to classify
    the human value categoriesthat an argument draws on. Arguments consist of a premise,
    conclusion,and the premise''s stance on the conclusion. Our team experimented
    with GloVe embeddings and fine-tuning BERT. We found that '
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We describe the systems of the University of Alberta team for the SemEval-2023
    Visual Word Sense Disambiguation (V-WSD) Task. We present a novel algorithm that
    leverages glosses retrieved from BabelNet, in combination with text and image
    encoders. Furthermore, we compare language-specific encoders against the application
    of English encoders to translated texts. As the contexts given in the task datasets
    are extremely short, we also experiment with augmenting these contexts with descriptions
    generated by a language model. This yields substantial improvements in accuracy.
    We describe and evaluate additional V-WSD methods which use image generation and
    text-conditioned image segmentation. Some of our experimental results exceed those
    of our official submissions on the test set. Our code is publicly available at
    https://github.com/UAlberta-NLP/v-wsd.
  authors:
  - Michael Ogezi
  - Bradley Hauer
  - Talgat Omarov
  - Ning Shi
  - Grzegorz Kondrak
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_308
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Task-1 - Visual Word Sense Disambiguation (Visual-WSD)
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'UAlberta at SemEval-2023 Task 1: Context Augmentation and Translation for
    Multilingual Visual Word Sense Disambiguation'
  tldr: We describe the systems of the University of Alberta team for the SemEval-2023
    Visual Word Sense Disambiguation (V-WSD) Task. We present a novel algorithm that
    leverages glosses retrieved from BabelNet, in combination with text and image
    encoders. Furthermore, we compare language-specific encoders a
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes our system (iREL) for Tweet intimacy analysis sharedtask
    of the SemEval 2023 workshop at ACL 2023. Oursystem achieved an overall Pearson's
    r score of 0.5924  and ranked 10th on the overall leaderboard. For the unseen
    languages, we ranked third on the leaderboard and achieved a Pearson's r score
    of 0.485.  We used a single multilingual model for all languages, as discussed
    in this paper. We provide a detailed description of our pipeline along with multiple
    ablation experiments to further analyse each component of the pipeline.  We demonstrate
    how translation-based augmentation, domain-specific features, and domain-adapted
    pre-trained models improve the understanding of intimacy in tweets. The codecan
    be found at \textbackslash{}href\{https://github.com/bhavyajeet/Multilingual-tweet-intimacy\}\{https://github.com/bhavyajeet/Multilingual-tweet-intimacy\}
  authors:
  - Bhavyajeet Singh
  - Ankita Maity
  - Pavan Kandru
  - Aditya Hari
  - Vasudeva Varma
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_309
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 9: Multilingual Tweet Intimacy Analysis'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'iREL at SemEval-2023 Task 9: Improving understanding of multilingual Tweets
    using Translation-Based Augmentation and Domain Adapted Pre-Trained Models'
  tldr: This paper describes our system (iREL) for Tweet intimacy analysis sharedtask
    of the SemEval 2023 workshop at ACL 2023. Oursystem achieved an overall Pearson's
    r score of 0.5924  and ranked 10th on the overall leaderboard. For the unseen
    languages, we ranked third on the leaderboard and achieved a P
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We describe our system for SemEval-2022 Task 3 subtask 2 which on detecting
    the frames used in a news article in a multi-lingual setup. We propose a multi-lingual
    approach based on machine translation of the input, followed by an English prediction
    model. Our system demonstrated good zero-shot transfer capability, achieving micro-F1
    scores of 53\% for Greek (4th on the leaderboard) and 56.1\%  for Georgian (3rd
    on the leaderboard), without any prior training on translated data for these languages.
    Moreover, our system achieved comparable performance on seven other languages,
    including German, English, French, Russian, Italian, Polish, and Spanish. Our
    results demonstrate the feasibility of creating a language-agnostic model for
    automatic framing detection in online news.
  authors:
  - Osama Mohammed Afzal
  - Preslav Nakov
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_310
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 3: Detecting the Category, the Framing, and the Persuasion Techniques
    in Online News in a Multi-lingual Setup'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Team TheSyllogist at SemEval-2023 Task 3: Language-Agnostic Framing Detection
    in Multi-Lingual Online News: A Zero-Shot Transfer Approach'
  tldr: We describe our system for SemEval-2022 Task 3 subtask 2 which on detecting
    the frames used in a news article in a multi-lingual setup. We propose a multi-lingual
    approach based on machine translation of the input, followed by an English prediction
    model. Our system demonstrated good zero-shot trans
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Identifying human values behind arguments isa complex task which requires
    understandingof premise, stance and conclusion together. Wepropose a method that
    uses a pre-trained lan-guage model, DeBERTa, to tokenize and con-catenate the
    text before feeding it into a fullyconnected neural network. We also show thatleveraging
    the hierarchy in values improves theperformance by .14 F1 score.
  authors:
  - Pavan Kandru
  - Bhavyajeet Singh
  - Ankita Maity
  - Kancharla Aditya Hari
  - Vasudeva Varma
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_311
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 4: ValueEval: Identification of Human Values behind Arguments'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Tenzin-Gyatso at SemEval-2023 Task 4: Identifying Human Values behind Arguments
    Using DeBERTa'
  tldr: Identifying human values behind arguments isa complex task which requires
    understandingof premise, stance and conclusion together. Wepropose a method that
    uses a pre-trained lan-guage model, DeBERTa, to tokenize and con-catenate the
    text before feeding it into a fullyconnected neural network. We als
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We present the system proposed by the MilaNLP team for the Explainable
    Detection of Online Sexism (EDOS) shared task.We propose an ensemble modeling
    approach to combine different classifiers trained with domain adaptation objectives
    and standard fine-tuning.Our results show that the ensemble is more robust than
    individual models and that regularized models generate more ``conservative'' predictions,
    mitigating the effects of lexical overfitting.However, our error analysis also
    finds that many of the misclassified instances are debatable, raising questions
    about the objective annotatability of hate speech data.
  authors:
  - Amanda Cercas Curry
  - Giuseppe Attanasio
  - Debora Nozza
  - Dirk Hovy
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_312
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 10: Towards Explainable Detection of Online Sexism'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'MilaNLP at SemEval-2023 Task 10: Ensembling Domain-Adapted and Regularized
    Pretrained Language Models for Robust Sexism Detection'
  tldr: We present the system proposed by the MilaNLP team for the Explainable Detection
    of Online Sexism (EDOS) shared task.We propose an ensemble modeling approach to
    combine different classifiers trained with domain adaptation objectives and standard
    fine-tuning.Our results show that the ensemble is more
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'To understand a legal document for real-world applications, SemEval-2023
    Task 6 proposes a shared Subtask A, rhetorical roles (RRs) prediction, which requires
    a system to automatically assign a RR label for each semantical segment in a legal
    text. In this paper, we propose a LEGAL-BERT based hierarchical BiLSTM model with
    conditional random field (CRF) for RR prediction, which primarily consists of
    two parts: word-level and sentence-level encoders. The word-level encoder first
    adopts a legal-domain pre-trained language model, LEGAL-BERT, initially word-embedding
    words in each sentence in a document and a word-level BiLSTM further encoding
    such sentence representation. The sentence-level encoder then uses an attentive
    pooling method for sentence embedding and a sentence-level BiLSTM for document
    modeling. Finally, a CRF is utilized to predict RRs for each sentence. The officially
    released results show that our method outperformed the baseline systems. Our team
    won 7th rank out of 27 participants in Subtask A.'
  authors:
  - Yu Chen
  - You Zhang
  - Jin Wang
  - Xuejie Zhang
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_313
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 6: LegalEval: Understanding Legal Texts'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'YNU-HPCC at SemEval-2023 Task 6: LEGAL-BERT Based Hierarchical BiLSTM with
    CRF for Rhetorical Roles Prediction'
  tldr: To understand a legal document for real-world applications, SemEval-2023 Task
    6 proposes a shared Subtask A, rhetorical roles (RRs) prediction, which requires
    a system to automatically assign a RR label for each semantical segment in a legal
    text. In this paper, we propose a LEGAL-BERT based hierarc
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Under the umbrella of anonymous social networks, many women have suffered
    from abuse, discrimination, and other sexist expressions online. However, exsiting
    methods based on keyword filtering and matching performed poorly on online sexism
    detection, which lacked the capability to identify implicit stereotypes and discrimination.
    Therefore, this paper proposes a System of Ensembling Fine-tuning Models (SEFM)
    at SemEval-2023 Task 10: Explainable Detection of Online Sexism. We firstly use
    four task-adaptive pre-trained language models to flag all texts. Secondly, we
    alleviate the data imbalance from two perspectives: over-sampling the labelled
    data and adjusting the loss function. Thirdly, we add indicators and feedback
    modules to enhance the overall performance. Our system attained macro F1 scores
    of 0.8538, 0.6619, and 0.4641 for Subtask A, B, and C, respectively. Our system
    exhibited strong performance across multiple tasks, with particularly noteworthy
    performance in Subtask B. Comparison experiments and ablation studies demonstrate
    the effectiveness of our system.'
  authors:
  - Tianyun Zhong
  - Runhui Song
  - Xunyuan Liu
  - Juelin Wang
  - Boya Wang
  - Binyang Li
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_314
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 10: Towards Explainable Detection of Online Sexism'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'UIRISC at SemEval-2023 Task 10: Explainable Detection of Online Sexism by
    Ensembling Fine-tuning Language Models'
  tldr: Under the umbrella of anonymous social networks, many women have suffered
    from abuse, discrimination, and other sexist expressions online. However, exsiting
    methods based on keyword filtering and matching performed poorly on online sexism
    detection, which lacked the capability to identify implicit s
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Sexism is a harmful phenomenon that provokes gender inequalities and social
    imbalances. The expanding application of sexist content on social media platforms
    creates an unwelcoming and discomforting environment for many users. The implication
    of sexism is a multi-faceted subject as it can be integrated with other categories
    of discrimination. Binary classification tools are frequently employed to identify
    sexist content, but most of them provide extensive, generic categories with no
    further insights. SemEval-2023 introduced the Explainable Detection of Online
    Sexism (EDOS) task that emphasizes detecting and explaining the category of sexist
    content. The content of this paper details our involvement in this task where
    we present a neural network architecture employing document embeddings from a
    fine-tuned transformer-based model into stacked long short-term memory (LSTM)
    and a fully connected linear (FCL) layer . Our proposed methodology obtained an
    F1 score of 0.8218 (ranked 51st) in Task A. It achieved an F1 score of 0.5986
    (ranked 40th) and 0.4419 (ranked 28th) in Tasks B and C, respectively.
  authors:
  - Afrin Sultana
  - Radiathun Tasnia
  - Nabila Ayman
  - Abu Nowshed Chy
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_315
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 10: Towards Explainable Detection of Online Sexism'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'CSECU-DSG at SemEval-2023 Task 10: Exploiting Transformers with Stacked
    LSTM for the Explainable Detection of Online Sexism'
  tldr: Sexism is a harmful phenomenon that provokes gender inequalities and social
    imbalances. The expanding application of sexist content on social media platforms
    creates an unwelcoming and discomforting environment for many users. The implication
    of sexism is a multi-faceted subject as it can be integra
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Clickbait spoiling is a task of generating or retrieving a fairly short
    text with a purpose to satisfy curiosity of a content consumer without their addressing
    to the document linked to a clickbait post or headline. In this paper we introduce
    an ensemble approach to clickbait spoiling task at SemEval-2023. The tasks consists
    of spoiler classification and retrieval on Webis-Clickbait-22 dataset. We show
    that such an ensemble solution is quite successful at classification, whereas
    it might perform poorly at retrieval with no additional features. In conclusion
    we outline our thoughts on possible directions to improving the approach and shape
    a set of suggestions to the said features.
  authors:
  - Maksim Shmalts
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_316
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 5: Clickbait Spoiling'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'John Boy Walton at SemEval-2023 Task 5: An Ensemble Approach to Spoiler
    Classification and Retrieval for Clickbait Spoiling'
  tldr: Clickbait spoiling is a task of generating or retrieving a fairly short text
    with a purpose to satisfy curiosity of a content consumer without their addressing
    to the document linked to a clickbait post or headline. In this paper we introduce
    an ensemble approach to clickbait spoiling task at SemEva
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes the system we used to participate in the shared task,
    as well as additional experiments beyond the scope of the shared task, but using
    its data.Our primary goal is to compare the effectiveness of transformers model
    compared to low-resource dictionaries. Secondly, we compare the difference in
    performance of a learned dictionary and of a dictionary designed by experts in
    the field of values.Our findings surprisingly show that transformers perform on
    par with a dictionary containing less than 1k words, when evaluated with 19 fine-grained
    categories, and only outperform a dictionary-based approach in a coarse setting
    with 10 categories.Interestingly, the expert dictionary has a precision on par
    with the learned one, while its recall is clearly lower, potentially an indication
    of overfitting of topics to values in the shared task's dataset.Our findings should
    be of interest to both the NLP and  Value scientific communities on the use of
    automated approaches for value classification
  authors:
  - Nicolas Stefanovitch
  - Bertrand De Longueville
  - Mario Scharfbillig
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_317
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 4: ValueEval: Identification of Human Values behind Arguments'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'TeamEC at SemEval-2023 Task 4: Transformers vs. Low-Resource Dictionaries,
    Expert Dictionary vs. Learned Dictionary'
  tldr: 'This paper describes the system we used to participate in the shared task,
    as well as additional experiments beyond the scope of the shared task, but using
    its data.Our primary goal is to compare the effectiveness of transformers model
    compared to low-resource dictionaries. Secondly, we compare the '
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Automated processing of legal documents is essential to manage the enormous
    volume of legal corpus and to make it easily accessible to a broad spectrum of
    people. But due to the amorphous and variable nature of legal documents, it is
    very challenging to directly proceed with complicated processes such as summarization,
    analysis, and query. Segmenting the documents as per the rhetorical roles can
    aid and accelerate such procedures. This paper describes our participation in
    SemEval-2023 task 6: Sub-task A: Rhetorical Roles Prediction. We utilize a finetuned
    Legal-BERT to address this task.  We also conduct an error analysis to illustrate
    the shortcomings of our deployed approach.'
  authors:
  - Fareen Tasneem
  - Tashin Hossain
  - Jannatun Naim
  - Abu Nowshed Chy
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_318
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 6: LegalEval: Understanding Legal Texts'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'CSECU-DSG at SemEval-2023 Task 6: Segmenting Legal Documents into Rhetorical
    Roles via Fine-tuned Transformer Architecture'
  tldr: Automated processing of legal documents is essential to manage the enormous
    volume of legal corpus and to make it easily accessible to a broad spectrum of
    people. But due to the amorphous and variable nature of legal documents, it is
    very challenging to directly proceed with complicated processes su
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Class imbalance problem can cause machine learning models to produce an
    undesirable performance on the minority class as well as the whole dataset. Using
    data augmentation techniques to increase the number of samples is one way to tackle
    this problem. We introduce a novel counterfactual data augmentation by verb replacement
    for the identification of medical claims. In addition, we investigate the impact
    of this method and compare it with 3 other data augmentation techniques, showing
    that the proposed method can result in significant (relative) improvement on the
    minority class.
  authors:
  - Akbar Karimi
  - Lucie Flek
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_319
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 8: Causal medical claim identification and related PICO frame
    extraction from social media posts'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'CAISA at SemEval-2023 Task 8: Counterfactual Data Augmentation for Mitigating
    Class Imbalance in Causal Claim Identification'
  tldr: Class imbalance problem can cause machine learning models to produce an undesirable
    performance on the minority class as well as the whole dataset. Using data augmentation
    techniques to increase the number of samples is one way to tackle this problem.
    We introduce a novel counterfactual data augment
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes a multilingual persuasion detection system that incorporates
    persuasion technique attributes for a multi-label classification task.The proposed
    method has two advantages. First, it combines persuasion features with a sequence
    classification transformer to classify persuasion techniques. Second, it is a
    language agnostic approach that supports a total of 100 languages, guaranteed
    by the multilingual transformer module and the Google translator interface. We
    found that our persuasion system outperformed the SemEval baseline in all languages
    except zero shot prediction languages, which did not constitute the main focus
    of our research. With the highest F1-Micro score of 0.45, Italian achieved the
    eighth position on the leaderboard.
  authors:
  - Fatima Zahra Qachfar
  - Rakesh Verma
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_320
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 3: Detecting the Category, the Framing, and the Persuasion Techniques
    in Online News in a Multi-lingual Setup'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'ReDASPersuasion at SemEval-2023 Task 3: Persuasion Detection using Multilingual
    Transformers and Language Agnostic Features'
  tldr: This paper describes a multilingual persuasion detection system that incorporates
    persuasion technique attributes for a multi-label classification task.The proposed
    method has two advantages. First, it combines persuasion features with a sequence
    classification transformer to classify persuasion tec
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes our system used in the SemEval-2023 Task 11 Learning
    With Disagreements (Le-Wi-Di). This is a subjective task since it deals with detecting
    hate speech, misogyny and offensive language. Thus, disagreement among annotators
    is expected. We experiment with different settings like loss functions specific
    for subjective tasks and include anonymized annotator-specific information to
    help us understand the level of disagreement. We perform an in-depth analysis
    of the performance discrepancy of these different modelling choices. Our system
    achieves a cross-entropy of 0.58, 4.01 and 3.70 on the test sets of HS-Brexit,
    ArMIS and MD-Agreement, respectively. Our code implementation is publicly available.
  authors:
  - Ankita Maity
  - Pavan Kandru
  - Bhavyajeet Singh
  - Kancharla Aditya Hari
  - Vasudeva Varma
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_321
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 11: Learning with Disagreements (Le-Wi-Di)'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'IREL at SemEval-2023 Task 11: User Conditioned Modelling for Toxicity Detection
    in Subjective Tasks'
  tldr: This paper describes our system used in the SemEval-2023 Task 11 Learning
    With Disagreements (Le-Wi-Di). This is a subjective task since it deals with detecting
    hate speech, misogyny and offensive language. Thus, disagreement among annotators
    is expected. We experiment with different settings like l
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We describe SemEval-2023 Task 11 on behavioral segregation of annotations
    to find the similarities and contextual thinking of a group of annotators. We
    have utilized a behavioral segmentation analysis on the annotators to model them
    independently and combine the results to yield soft and hard scores. Our team
    focused on experimenting with hierarchical clustering with various distance metrics
    for similarity, dissimilarity, and reliability. We modeled the clusters and assigned
    weightage to find the soft and hard scores. Our team was able to find out hidden
    behavioral patterns among the judgments of annotators after rigorous experiments.
    The proposed system is made available.
  authors:
  - Guneet Singh Kohli
  - Vinayak Tiwari
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_322
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 11: Learning with Disagreements (Le-Wi-Di)'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Arguably at SemEval-2023 Task 11: Learning the disagreements using unsupervised
    behavioral clustering and language models'
  tldr: We describe SemEval-2023 Task 11 on behavioral segregation of annotations
    to find the similarities and contextual thinking of a group of annotators. We
    have utilized a behavioral segmentation analysis on the annotators to model them
    independently and combine the results to yield soft and hard scores
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In online forums like Reddit, users share their experiences with medical
    conditions and treatments, including making claims, asking questions, and discussing
    the effects of treatments on their health. Building systems to understand this
    information can effectively monitor the spread of misinformation and verify user
    claims. The Task-8 of the 2023 International Workshop on Semantic Evaluation focused
    on medical applications, specifically extracting patient experience- and medical
    condition-related entities from user posts on social media. The Reddit Health
    Online Talk (RedHot) corpus contains posts from medical condition-related subreddits
    with annotations characterizing the patient experience and medical conditions.
    In Subtask-1, patient experience is characterized by personal experience, questions,
    and claims. In Subtask-2, medical conditions are characterized by population,
    intervention, and outcome. For the automatic extraction of patient experiences
    and medical condition information, as a part of the challenge, we proposed language-model-based
    extraction systems that ranked \$3\^{}\{rd\}\$ on both subtasks' leaderboards.
    In this work, we describe our approach and, in addition, explore the automatic
    extraction of this information using domain-specific language models and the inclusion
    of external knowledge.
  authors:
  - Giridhar Kaushik Ramachandran
  - Haritha Gangavarapu
  - Kevin Lybarger
  - Ozlem Uzuner
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_323
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 8: Causal medical claim identification and related PICO frame
    extraction from social media posts'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'MasonNLP+ at SemEval-2023 Task 8: Extracting Medical Questions, Experiences
    and Claims from Social Media using Knowledge-Augmented Pre-trained Language Models'
  tldr: In online forums like Reddit, users share their experiences with medical conditions
    and treatments, including making claims, asking questions, and discussing the
    effects of treatments on their health. Building systems to understand this information
    can effectively monitor the spread of misinformatio
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'The recent release of the AfriSenti-SemEval shared Task 12 has made available
    14 new datasets annotated for sentiment analysis on African Languages. We proposed
    and evaluated two approaches to this task, Delta TF-IDF, and a proposed Language-Specific
    Model Fusion Algorithm using Language Identification, both of which produced comparable
    or better classification performance than the current state-of-art models on this
    task: AfriBERTa, AfroXLMR, and AfroLM.'
  authors:
  - Saurav Aryal
  - Howard Prioleau
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_325
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 12: AfriSenti-SemEval: Sentiment Analysis for Low-resource African
    Languages using Twitter Dataset'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Howard University Computer Science at SemEval-2023 Task 12: A 2-Step System
    Design for Multilingual Sentiment Classification with Language Identification'
  tldr: The recent release of the AfriSenti-SemEval shared Task 12 has made available
    14 new datasets annotated for sentiment analysis on African Languages. We proposed
    and evaluated two approaches to this task, Delta TF-IDF, and a proposed Language-Specific
    Model Fusion Algorithm using Language Identificat
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Visual Word Sense Disambiguation (V-WSD) identifies the correct visual
    sense of a multi-sense word in a specific context. This can be challenging as
    images may need to provide additional context and words may have multiple senses.
    A proper V-WSD system can benefit applications like image retrieval and captioning.
    This paper proposes a Prompt Generation approach to solve this challenge. This
    approach improves the robustness of language-image models like CLIP to contextual
    ambiguities and helps them better correlate between textual and visual contexts
    of different senses of words.
  authors:
  - Omid Ghahroodi
  - Seyed Arshan Dalili
  - Sahel Mesforoush
  - Ehsaneddin Asgari
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_327
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Task-1 - Visual Word Sense Disambiguation (Visual-WSD)
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'SUT at SemEval-2023 Task 1: Prompt Generation for Visual Word Sense Disambiguation'
  tldr: Visual Word Sense Disambiguation (V-WSD) identifies the correct visual sense
    of a multi-sense word in a specific context. This can be challenging as images
    may need to provide additional context and words may have multiple senses. A proper
    V-WSD system can benefit applications like image retrieval a
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The human values expressed in argumentative texts can provide valuable
    insights into the culture of a society. They can be helpful in various applications
    such as value-based profiling and ethical analysis. However, one of the first
    steps in achieving this goal is to detect the category of human value from an
    argument accurately. This task is challenging due to the lack of data and the
    need for philosophical inference. It also can be challenging for humans to classify
    arguments according to their underlying human values. This paper elaborates on
    our model for the SemEval 2023 Task 4 on human value detection. We propose a class-token
    attention-based model and evaluate it against baseline models, including finetuned
    BERT language model and a keyword-based approach.
  authors:
  - Omid Ghahroodi
  - Mohammad Ali Sadraei Javaheri
  - Doratossadat Dastgheib
  - Mahdieh Soleymani Baghshah
  - Mohammad Hossein Rohban
  - Hamid Rabiee
  - Ehsaneddin Asgari
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_328
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 4: ValueEval: Identification of Human Values behind Arguments'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Sina at SemEval-2023 Task 4: A Class-Token Attention-based Model for Human
    Value Detection'
  tldr: The human values expressed in argumentative texts can provide valuable insights
    into the culture of a society. They can be helpful in various applications such
    as value-based profiling and ethical analysis. However, one of the first steps
    in achieving this goal is to detect the category of human val
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper describes SinaAI''s participation in SemEval-2023 Task 3,
    which involves detecting propaganda in news articles across multiple languages.
    The task comprises three sub-tasks: (i) genre detection, (ii) news framing,and
    (iii) persuasion technique identification.The employed dataset includes news articles
    in nine languages and domains, including English, French, Italian, German, Polish,
    Russian, Georgian, Greek, and Spanish, with labeled instances of news framing,
    genre, and persuasion techniques. Our approach combines fine-tuning  multilingual
    language models such as XLM, LaBSE, and mBERT with data augmentation techniques.
    Our experimental results show that XLM outperforms other models in terms of F1-Micro
    in and F1-Macro, and the ensemble of XLM and LaBSE achieved the best performance.
    Our study highlights the effectiveness of multilingual sentence embedding models
    in multilingual propaganda detection. Our models achieved highest score for two
    languages (greek and italy) in sub-task 1 and one language (Russian) for sub-task
    2.'
  authors:
  - Aryan Sadeghi
  - Reza Alipour
  - Kamyar Taeb
  - Parimehr Morassafar
  - Nima Salemahim
  - Ehsaneddin Asgari
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_330
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 3: Detecting the Category, the Framing, and the Persuasion Techniques
    in Online News in a Multi-lingual Setup'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'SinaAI at SemEval-2023 Task 3: A Multilingual Transformer Language Model-based
    Approach for the Detection of News Genre, Framing and Persuasion Techniques'
  tldr: 'This paper describes SinaAI''s participation in SemEval-2023 Task 3, which
    involves detecting propaganda in news articles across multiple languages. The
    task comprises three sub-tasks: (i) genre detection, (ii) news framing,and (iii)
    persuasion technique identification.The employed dataset includes n'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes the participation of the RCLN team at the Visual
    Word Sense Disambiguation task at SemEval 2023. The participation was focused
    on the use of CLIP as a base model for the matching between text and images with
    additional information coming from captions generated from images and the generation
    of images from the prompt text using Stable Diffusion. The results we obtained
    are not particularly good, but interestingly enough, we were able to improve over
    the CLIP baseline in Italian by recurring simply to the generated images.
  authors:
  - Antonina Mijatovic
  - Davide Buscaldi
  - Ekaterina Borisova
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_331
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Task-1 - Visual Word Sense Disambiguation (Visual-WSD)
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'RCLN at SemEval-2023 Task 1: Leveraging Stable Diffusion and Image Captions
    for Visual WSD'
  tldr: This paper describes the participation of the RCLN team at the Visual Word
    Sense Disambiguation task at SemEval 2023. The participation was focused on the
    use of CLIP as a base model for the matching between text and images with additional
    information coming from captions generated from images and t
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Literature permeates through almost every facet of our lives, whether
    through books, magazines, or internet articles. Moreover, every piece of written
    work contains ideas and opinions that we tend to relate to, accept or disregard,
    debate over, or enlighten ourselves with. However, the existence of subtle themes
    that are difficult to discern had inspired us to utilize four machine learning
    algorithms: Decision Trees, Random Forest, Logistic Regression, and Support Vec-
    tor Classifier (SVC) to aid in their detection. Trained on the ValueEval data
    set as a multi- label classification problem, the supervised ma- chine learning
    models did not perform as well as expected, with F1 metrics hovering from 0.0
    to 0.04 for each value. Noting this, the lim- itations and weaknesses of our approach
    are discussed in our paper.'
  authors:
  - Sruthi Sundharram
  - Abdul Mohammed
  - Sanidhya Sharma
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_332
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 4: ValueEval: Identification of Human Values behind Arguments'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Friedrich Nietzsche at SemEval-2023 Task 4: Detection of Human Values from
    Text Using Machine Learning'
  tldr: 'Literature permeates through almost every facet of our lives, whether through
    books, magazines, or internet articles. Moreover, every piece of written work
    contains ideas and opinions that we tend to relate to, accept or disregard, debate
    over, or enlighten ourselves with. However, the existence of '
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In this article, which was prepared for the sameval2023 competition (task
    number 2), information about the implementation techniques of the transformer
    model and the use of the pre-trained BERT model in order to identify the named
    entity (NER) in the English language, has been collected and also the implementation
    method is explained.Finally, it led to an F1 score of about 57\% for Fine-grained
    and 72\% for Coarse-grained in the dev data.In the final test data, F1 score reached
    50\%.
  authors:
  - Reza Ahmadi
  - Shiva Arefi
  - Mohammad Jafarabad
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_333
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 2: MultiCoNER II Multilingual Complex Named Entity Recognition'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'azaad@BND at SemEval-2023 Task 2: How to Go from a Simple Transformer Model
    to a Better Model to Get Better Results in Natural Language Processing'
  tldr: In this article, which was prepared for the sameval2023 competition (task
    number 2), information about the implementation techniques of the transformer
    model and the use of the pre-trained BERT model in order to identify the named
    entity (NER) in the English language, has been collected and also the
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper describes our system used in the SemEval-2023 Task 10: Towards
    ExplainableDetection of Online Sexism (Kirk et al., 2023). The harmful effects
    of sexism on the internet have impacted both men and women, yet current research
    lacks a fine-grained classification of sexist content. The task involves three
    hierarchical sub-tasks, which we addressed by employing a multitask-learning framework.
    To further enhance our system''s performance, we pre-trained the roberta-large
    (Liu et al., 2019b) and deberta-v3-large (He et al., 2021) models on two million
    unlabeled data, resulting in significant improvements on sub-tasks A and C. In
    addition, the multitask-learning approach boosted the performance of our models
    on subtasks A and B. Our system exhibits promising results in achieving explainable
    detection of online sexism, attaining a test f1-score of 0.8746 on sub-task A
    (ranking 1st on the leaderboard), and ranking 5th on sub-tasks B and C.'
  authors:
  - Mengyuan Zhou
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_334
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 10: Towards Explainable Detection of Online Sexism'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'PingAnLifeInsurance at SemEval-2023 Task 10: Using Multi-Task Learning to
    Better Detect Online Sexism'
  tldr: 'This paper describes our system used in the SemEval-2023 Task 10: Towards
    ExplainableDetection of Online Sexism (Kirk et al., 2023). The harmful effects
    of sexism on the internet have impacted both men and women, yet current research
    lacks a fine-grained classification of sexist content. The task in'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Online sexism is a widespread and harmful phenomenon. Automated tools
    can assist the detection of sexism at scale. Binary detection, however, disregards
    the diversity of sexist content, and fails to provide clear explanations for why
    something is sexist. To address this issue, we introduce SemEval Task 10 on the
    Explainable Detection of Online Sexism (EDOS). We make three main contributions:
    i) a novel hierarchical taxonomy of sexist content, which includes granular vectors
    of sexism to aid explainability; ii) a new dataset of 20,000 social media comments
    with fine-grained labels, along with larger unlabelled datasets for model adaptation;
    and iii) baseline models as well as an analysis of the methods, results and errors
    for participant submissions to our task.'
  authors:
  - Hannah Kirk
  - Wenjie Yin
  - Bertie Vidgen
  - Paul R\"{o}ttger
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_335
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Task Overview Papers
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'SemEval-2023 Task 10: Explainable Detection of Online Sexism'
  tldr: Online sexism is a widespread and harmful phenomenon. Automated tools can
    assist the detection of sexism at scale. Binary detection, however, disregards
    the diversity of sexist content, and fails to provide clear explanations for why
    something is sexist. To address this issue, we introduce SemEval T
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Transformer language models are now a solid baseline for Named Entity
    Recognition and can be significantly improved by leveraging complementary resources,
    either by integrating external knowledge or by annotating additional data. In
    a preliminary step, this work presents experiments on fine-tuning transformer
    models. Then, a set of experiments has been conducted with a Wikipedia-based reclassification
    system. Additionally, we conducted a small annotation campaign on the Farsi language
    to evaluate the impact of additional data. These two methods with complementary
    resources showed improvements compared to fine-tuning only.
  authors:
  - Kevin Deturck
  - Pierre Magistry
  - "B\xE9n\xE9dicte Diot-Parvaz Ahmad"
  - Ilaine Wang
  - Damien Nouvel
  - Hugo Lafayette
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_336
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 2: MultiCoNER II Multilingual Complex Named Entity Recognition'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Ertim at SemEval-2023 Task 2: Fine-tuning of Transformer Language Models
    and External Knowledge Leveraging for NER in Farsi, English, French and Chinese'
  tldr: Transformer language models are now a solid baseline for Named Entity Recognition
    and can be significantly improved by leveraging complementary resources, either
    by integrating external knowledge or by annotating additional data. In a preliminary
    step, this work presents experiments on fine-tuning t
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes the results of SemEval 2023 task 7 -- Multi-Evidence
    Natural Language Inference for Clinical Trial Data (NLI4CT) -- consisting of 2
    tasks, a Natural Language Inference (NLI) task, and an evidence selection task
    on clinical trial data. The proposed challenges require multi-hop biomedical and
    numerical reasoning, which are of significant importance to the development of
    systems capable of large-scale interpretation and retrieval of medical evidence,
    to provide personalized evidence-based care.Task 1, the entailment task, received
    643 submissions from 40 participants, and Task 2, the evidence selection task,
    received 364 submissions from 23 participants. The tasks are challenging, with
    the majority of submitted systems failing to significantly outperform the majority
    class baseline on the entailment task, and we observe significantly better performance
    on the evidence selection task than on the entailment task. Increasing the number
    of model parameters leads to a direct increase in performance, far more significant
    than the effect of biomedical pre-training. Future works could explore the limitations
    of large models for generalization and numerical inference, and investigate methods
    to augment clinical datasets to allow for more rigorous testing and to facilitate
    fine-tuning.We envisage that the dataset, models, and results of this task will
    be useful to the biomedical NLI and evidence retrieval communities. The dataset,
    competition leaderboard, and website are publicly available.
  authors:
  - Ma\"{e}l Jullien
  - Marco Valentino
  - Hannah Frost
  - Paul O'regan
  - Donal Landers
  - Andr\'{e} Freitas
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_338
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Task Overview Papers
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'SemEval-2023 Task 7: Multi-Evidence Natural Language Inference for Clinical
    Trial Data'
  tldr: This paper describes the results of SemEval 2023 task 7 -- Multi-Evidence
    Natural Language Inference for Clinical Trial Data (NLI4CT) -- consisting of 2
    tasks, a Natural Language Inference (NLI) task, and an evidence selection task
    on clinical trial data. The proposed challenges require multi-hop bi
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'This paper presents the Visual Word Sense Disambiguation (Visual-WSD)
    task.The objective of Visual-WSD is to identify among a set of ten images the
    one that corresponds to the intended meaning of a given ambiguous word which is
    accompanied with minimal context. The task provides datasets for three different
    languages: English, Italian, and Farsi.We received a total of 96 different submissions.
    Out of these, 40 systems outperformed a strong zero-shot CLIP-based baseline.Participating
    systems proposed different zero- and few-shot approaches, often involving generative
    models and data augmentation.More information can be found on the task''s website:
    \textbackslash{}url\{https://raganato.github.io/vwsd/\}.'
  authors:
  - Alessandro Raganato
  - Iacer Calixto
  - Asahi Ushio
  - Jose Camacho-Collados
  - Mohammad Taher Pilehvar
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_339
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Task Overview Papers
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'SemEval-2023 Task 1: Visual Word Sense Disambiguation'
  tldr: This paper presents the Visual Word Sense Disambiguation (Visual-WSD) task.The
    objective of Visual-WSD is to identify among a set of ten images the one that
    corresponds to the intended meaning of a given ambiguous word which is accompanied
    with minimal context. The task provides datasets for three d
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Intimacy is an important social aspect of language. Computational modeling
    of intimacy in language could help many downstream applications like dialogue
    systems and offensiveness detection. Despite its importance, resources and approaches
    on modeling textual intimacy remain rare. To address this gap, we introduce MINT,
    a new Multilingual intimacy analysis dataset covering 13,372 tweets in 10 languages
    including English, French, Spanish, Italian, Portuguese, Korean, Dutch, Chinese,
    Hindi, and Arabic along with SemEval 2023 Task 9: Multilingual Tweet Intimacy
    Analysis. Our task attracted 45 participants from around the world. While the
    participants are able to achieve overall good performance on languages in the
    training set, zero-shot prediction of intimacy in unseen languages remains challenging.
    Here we provide an overview of the task, summaries of the common approaches, and
    potential future directions on modeling intimacy across languages. All the relevant
    resources are available at https: //sites.google.com/umich.edu/ semeval-2023-tweet-intimacy.'
  authors:
  - Jiaxin Pei
  - V\'{i}tor Silva
  - Maarten Bos
  - Yozen Liu
  - Leonardo Neves
  - David Jurgens
  - Francesco Barbieri
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_340
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Task Overview Papers
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'SemEval-2023 Task 9: Multilingual Tweet Intimacy Analysis'
  tldr: Intimacy is an important social aspect of language. Computational modeling
    of intimacy in language could help many downstream applications like dialogue
    systems and offensiveness detection. Despite its importance, resources and approaches
    on modeling textual intimacy remain rare. To address this gap
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We present the findings of SemEval-2023 Task 2 on Fine-grained Multilingual
    Named Entity Recognition (MultiCoNER 2). Divided into 13 tracks, the task focused
    on methods to identify complex fine-grained named entities (like WRITTENWORK,
    VEHICLE, MUSICALGRP) across 12 languages, in both monolingual and multilingual
    scenarios, as well as noisy settings. The task used the MultiCoNER V2 dataset,
    composed of 2.2 million instances in Bangla, Chinese, English, Farsi, French,
    German, Hindi, Italian., Portuguese, Spanish, Swedish, and Ukrainian. MultiCoNER
    2 was one of the most popular tasks of SemEval-2023. It attracted 842 submissions
    from 47 teams, and 34 teams submitted system papers. Results showed that complex
    entity types such as media titles and product names were the most challenging.
    Methods fusing external knowledge into transformer models achieved the best performance,
    and the largest gains were on the Creative Work and Group classes, which are still
    challenging even with external knowledge. Some fine-grained classes proved to
    be more challenging than others, such as SCIENTIST, ARTWORK, and PRIVATECORP.
    We also observed that noisy data has a significant impact on model performance,
    with an average drop of 10\% on the noisy subset. The task highlights the need
    for future research on improving NER robustness on noisy data containing complex
    entities.
  authors:
  - Besnik Fetahu
  - Sudipta Kar
  - Zhiyu Chen
  - Oleg Rokhlenko
  - Shervin Malmasi
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_341
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Task Overview Papers
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'SemEval-2023 Task 2: Fine-grained Multilingual Named Entity Recognition
    (MultiCoNER 2)'
  tldr: We present the findings of SemEval-2023 Task 2 on Fine-grained Multilingual
    Named Entity Recognition (MultiCoNER 2). Divided into 13 tracks, the task focused
    on methods to identify complex fine-grained named entities (like WRITTENWORK,
    VEHICLE, MUSICALGRP) across 12 languages, in both monolingual an
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Identification of medical claims from user-generated text data is an
    onerous but essential step for various tasks including content moderation, and
    hypothesis generation. SemEval-2023 Task 8 is an effort towards building those
    capabilities and motivating further research in this direction. This paper summarizes
    the details and results of shared task 8 at SemEval-2023 which involved identifying
    causal medical claims and extracting related Populations, Interventions, and Outcomes
    ("PIO'''') frames from social media (Reddit) text. This shared task comprised
    two subtasks: (1) Causal claim identification;  and (2) PIO frame extraction.
    In total, seven teams participated in the task. Of the seven, six provided system
    descriptions which we summarize here. For the first subtask, the best approach
    yielded a macro-averaged F-1 score of 78.40, and for the second subtask, the best
    approach achieved token-level F-1 scores of 40.55 for Populations, 49.71 for Interventions,
    and 30.08 for Outcome frames.'
  authors:
  - Vivek Khetan
  - Somin Wadhwa
  - Byron Wallace
  - Silvio Amir
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_342
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Task Overview Papers
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'SemEval-2023 Task 8: Causal Medical Claim Identification and Related PIO
    Frame Extraction from Social Media Posts'
  tldr: Identification of medical claims from user-generated text data is an onerous
    but essential step for various tasks including content moderation, and hypothesis
    generation. SemEval-2023 Task 8 is an effort towards building those capabilities
    and motivating further research in this direction. This pape
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'In this overview paper, we report on the second PAN\textasciitilde{}Clickbait
    Challenge hosted as Task\textasciitilde{}5 at SemEval\textasciitilde{}2023. The
    challenge''s focus is to better support social media users by automatically generating
    short spoilers that close the curiosity gap induced by a clickbait post. We organized
    two subtasks: (1) spoiler type classification to assess what kind of spoiler a
    clickbait post warrants (e.g., a phrase), and (2) spoiler generation to generate
    an actual spoiler for a clickbait post.'
  authors:
  - Maik Fr\"{o}be
  - Benno Stein
  - Tim Gollub
  - Matthias Hagen
  - Martin Potthast
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_343
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Task Overview Papers
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'SemEval-2023 Task 5: Clickbait Spoiling'
  tldr: In this overview paper, we report on the second PAN\textasciitilde{}Clickbait
    Challenge hosted as Task\textasciitilde{}5 at SemEval\textasciitilde{}2023. The
    challenge's focus is to better support social media users by automatically generating
    short spoilers that close the curiosity gap induced by a
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Argumentation is ubiquitous in natural language communication, from politics
    and media to everyday work and private life. Many arguments derive their persuasive
    power from human values, such as self-directed thought or tolerance, albeit often
    implicitly. These values are key to understanding the semantics of arguments,
    as they are generally accepted as justifications for why a particular option is
    ethically desirable. Can automated systems uncover the values on which an argument
    draws? To answer this question, 39 teams submitted runs to ValueEval'23. Using
    a multi-sourced dataset of over 9K arguments, the systems achieved F1-scores up
    to 0.87 (nature) and over 0.70 for three more of 20 universal value categories.
    However, many challenges remain, as evidenced by the low peak F1-score of 0.39
    for stimulation, hedonism, face, and humility.
  authors:
  - Johannes Kiesel
  - Milad Alshomary
  - Nailia Mirzakhmedova
  - Maximilian Heinrich
  - Nicolas Handke
  - Henning Wachsmuth
  - Benno Stein
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_344
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Task Overview Papers
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'SemEval-2023 Task 4: ValueEval: Identification of Human Values Behind Arguments'
  tldr: Argumentation is ubiquitous in natural language communication, from politics
    and media to everyday work and private life. Many arguments derive their persuasive
    power from human values, such as self-directed thought or tolerance, albeit often
    implicitly. These values are key to understanding the sem
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'NLP datasets annotated with human judgments are rife with disagreements
    between the judges. This is especially true for tasks depending on subjective
    judgments such as sentiment analysis or offensive language detection. Particularly
    in these latter cases, the NLP community has come to realize that the common approach
    of reconciling'' these different subjective interpretations risks misrepresenting
    the evidence. Many NLP researchers have therefore concluded that rather than eliminating
    disagreements from annotated corpora, we should preserve themindeed, some argue
    that corpora should aim to preserve all interpretations produced by annotators.
    But this approach to corpus creation for NLP has not yet been widely accepted.
    The objective of the Le-Wi-Di series of shared tasks is to promote this approach
    to developing NLP models by providing a unified framework for training and evaluating
    with such datasets. We report on the second such shared task, which differs from
    the first edition in three crucial respects: (i) it focuses entirely on NLP, instead
    of both NLP and computer vision tasks in its first edition; (ii) it focuses on
    subjective tasks, instead of covering different types of disagreements as training
    with aggregated labels for subjective NLP tasks is in effect a misrepresentation
    of the data; and (iii) for the evaluation, we concentrated on soft approaches
    to evaluation. This second edition of Le-Wi-Di attracted a wide array of partici-
    pants resulting in 13 shared task submission papers.'
  authors:
  - Elisa Leonardelli
  - Gavin Abercrombie
  - Dina Almanea
  - Valerio Basile
  - Tommaso Fornaciari
  - Barbara Plank
  - Verena Rieser
  - Alexandra Uma
  - Massimo Poesio
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_345
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Task Overview Papers
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'SemEval-2023 Task 11: Learning with Disagreements (LeWiDi)'
  tldr: NLP datasets annotated with human judgments are rife with disagreements between
    the judges. This is especially true for tasks depending on subjective judgments
    such as sentiment analysis or offensive language detection. Particularly in these
    latter cases, the NLP community has come to realize that t
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'We present the first Africentric SemEval Shared task, Sentiment Analysis
    for African Languages (AfriSenti-SemEval) - The dataset is available at https://github.com/afrisenti-semeval/afrisent-semeval-2023.
    AfriSenti-SemEval is a sentiment classification challenge in 14 African languages:
    Amharic, Algerian Arabic, Hausa, Igbo, Kinyarwanda, Moroccan Arabic, Mozambican
    Portuguese, Nigerian Pidgin, Oromo, Swahili, Tigrinya, Twi, Xitsonga, and Yorb
    (Muhammad et al., 2023), using data labeled with 3 sentiment classes. We present
    three subtasks: (1) Task A: monolingual classification, which received 44 submissions;
    (2) Task B: multilingual classification, which received 32 submissions; and (3)
    Task C: zero-shot classification, which received 34 submissions. The best performance
    for tasks A and B was achieved by NLNDE team with 71.31 and 75.06 weighted F1,
    respectively. UCAS-IIE-NLP achieved the best average score for task C with 58.15
    weighted F1. We describe the various approaches adopted by the top 10 systems
    and their approaches.'
  authors:
  - Shamsuddeen Hassan Muhammad
  - Idris Abdulmumin
  - Seid Muhie Yimam
  - David Ifeoluwa Adelani
  - Ibrahim Said Ahmad
  - Nedjma Ousidhoum
  - Abinew Ali Ayele
  - Saif Mohammad
  - Meriem Beloucif
  - Sebastian Ruder
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_346
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Task Overview Papers
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'SemEval-2023 Task 12: Sentiment Analysis for African Languages (AfriSenti-SemEval)'
  tldr: 'We present the first Africentric SemEval Shared task, Sentiment Analysis
    for African Languages (AfriSenti-SemEval) - The dataset is available at https://github.com/afrisenti-semeval/afrisent-semeval-2023.
    AfriSenti-SemEval is a sentiment classification challenge in 14 African languages:
    Amharic, Alg'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: This paper describes the submissions of the Natural Language Processing
    (NLP) team from the Australian Research Council Industrial Transformation Training
    Centre (ITTC) for Cognitive Computing in Medical Technologies to the SemEval 2023
    Task 7, i.e., multi-evidence natural language inference for clinical trial data
    (NLI4CT). More specifically, we were working on subtask 2 whose objective is to
    identify the relevant parts of the premise from clinical trial report that justify
    the truth of information in the statement. We approach the evidence retrieval
    problem as a document retrieval and sentence similarity task. Our results show
    that the task poses some challenges which involve dealing with complex sentences
    and implicit evidences.
  authors:
  - Rahmad Mahendra
  - Damiano Spina
  - Karin Verspoor
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_347
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: 'Task 7: Multi-Evidence Natural Language Inference for Clinical Trial
    Data'
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'ITTC at SemEval 2023-Task 7: Document Retrieval and Sentence Similarity
    for Evidence Retrieval in Clinical Trial Data'
  tldr: This paper describes the submissions of the Natural Language Processing (NLP)
    team from the Australian Research Council Industrial Transformation Training Centre
    (ITTC) for Cognitive Computing in Medical Technologies to the SemEval 2023 Task
    7, i.e., multi-evidence natural language inference for cli
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'We describe SemEval-2023 task 3 on Detecting the Category, the Framing,
    and the Persuasion Techniques in Online News in a Multilingual Setup: the dataset,
    the task organization process, the evaluation setup, the results, and the participating
    systems. The task focused on news articles in nine languages (six known to the
    participants upfront: English, French, German, Italian, Polish, and Russian),
    and three additional ones revealed to the participants at the testing phase: Spanish,
    Greek, and Georgian). The task featured three subtasks: (1) determining the genre
    of the article (opinion, reporting, or satire), (2) identifying one or more frames
    used in an article from a pool of 14 generic frames, and (3) identify the persuasion
    techniques used in each paragraph of the article, using a taxonomy of 23 persuasion
    techniques. This was a very popular task: a total of 181 teams registered to participate,
    and 41 eventually made an official submission on the test set.'
  authors:
  - Jakub Piskorski
  - Nicolas Stefanovitch
  - Giovanni Da San Martino
  - Preslav Nakov
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_348
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Task Overview Papers
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'SemEval-2023 Task 3: Detecting the Category, the Framing, and the Persuasion
    Techniques in Online News in a Multi-lingual Setup'
  tldr: 'We describe SemEval-2023 task 3 on Detecting the Category, the Framing, and
    the Persuasion Techniques in Online News in a Multilingual Setup: the dataset,
    the task organization process, the evaluation setup, the results, and the participating
    systems. The task focused on news articles in nine langua'
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'In populous countries, pending legal cases have been growing exponentially.
    There is a need for developing NLP-based techniques for processing and automatically
    understanding legal documents. To promote research in the area of Legal NLP we
    organized the shared task LegalEval - Understanding Legal Texts  at SemEval 2023.
    LegalEval task has three sub-tasks: Task-A (Rhetorical Roles Labeling) is about
    automatically structuring legal documents into semantically coherent units, Task-B
    (Legal Named Entity Recognition) deals with identifying relevant entities in a
    legal document and Task-C (Court Judgement Prediction with Explanation) explores
    the possibility of automatically predicting the outcome of a legal case along
    with providing an explanation for the prediction. In total 26 teams (approx. 100
    participants spread across the world) submitted systems paper. In each of the
    sub-tasks, the proposed systems outperformed the baselines; however, there is
    a lot of scope for improvement. This paper describes the tasks, and analyzes techniques
    proposed by various teams.'
  authors:
  - Ashutosh Modi
  - Prathamesh Kalamkar
  - Saurabh Karn
  - Aman Tiwari
  - Abhinav Joshi
  - Sai Kiran Tanikella
  - Shouvik Kumar Guha
  - Sachin Malhan
  - Vivek Raghavan
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - SemEval
  forum: ''
  id: SemEval_349
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Task Overview Papers
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'SemEval-2023 Task 6: LegalEval - Understanding Legal Texts'
  tldr: In populous countries, pending legal cases have been growing exponentially.
    There is a need for developing NLP-based techniques for processing and automatically
    understanding legal documents. To promote research in the area of Legal NLP we
    organized the shared task LegalEval - Understanding Legal Te
  track: The 17th International Workshop on Semantic Evaluation (SemEval-2023)
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Scientific publications follow conventionalized rhetorical structures.
    Classifying the Argumentative Zone (AZ), e.g., identifying whether a sentence
    states a Motivation, a Result or Background information, has been proposed to
    improve processing of scholarly documents. In this work, we adapt and extend this
    idea to the domain of materials science research. We present and release a new
    dataset of 50 manually annotated research articles. The dataset spans seven sub-topics
    and is annotated with a materials-science focused multi-label annotation scheme
    for AZ. We detail corpus statistics and demonstrate high inter-annotator agreement.
    Our computational experiments show that using domain-specific pre-trained transformer-based
    text encoders is key to high classification performance. We also find that AZ
    categories from existing datasets in other domains are transferable to varying
    degrees.
  authors:
  - Timo Schrader
  - Teresa B{\"u}rkle
  - Sophie Henning
  - Sherry Tan
  - Matteo Finco
  - Stefan Gr{\"u}newald
  - Maira Indrikova
  - Felix Hildebrand
  - Annemarie Friedrich
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - CODI
  forum: ''
  id: CODI_1
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Regular long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'MuLMS-AZ: An Argumentative Zoning Dataset for the Materials Science Domain'
  tldr: Scientific publications follow conventionalized rhetorical structures. Classifying
    the Argumentative Zone (AZ), e.g., identifying whether a sentence states a Motivation,
    a Result or Background information, has been proposed to improve processing of
    scholarly documents. In this work, we adapt and ext
  track: 4th Workshop on Computational Approaches to Discourse
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Though discourse parsing can help multiple NLP fields, there has been
    no wide language model search done on implicit discourse relation classification.
    This hinders researchers from fully utilizing public-available models in discourse
    analysis. This work is a straightforward, fine-tuned discourse performance comparison
    of 7 pre-trained language models. We use PDTB-3, a popular discourse relation
    annotated dataset. Through our model search, we raise SOTA to 0.671 ACC and obtain
    novel observations. Some are contrary to what has been reported before (Shi and
    Demberg, 2019b), that sentence-level pre-training objectives (NSP, SBO, SOP) generally
    fail to produce the best-performing model for implicit discourse relation classification.
    Counterintuitively, similar-sized PLMs with MLM and full attention led to better
    performance. Our code is publicly released.
  authors:
  - Bruce W. Lee
  - Bongseok Yang
  - Jason Lee
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - CODI
  forum: ''
  id: CODI_4
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Regular short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: A Side-by-side Comparison of Transformers for Implicit Discourse Relation
    Classification
  tldr: Though discourse parsing can help multiple NLP fields, there has been no wide
    language model search done on implicit discourse relation classification. This
    hinders researchers from fully utilizing public-available models in discourse
    analysis. This work is a straightforward, fine-tuned discourse pe
  track: 4th Workshop on Computational Approaches to Discourse
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Entity coreference resolution is an important research problem with many
    applications, including information extraction and question answering. Coreference
    resolution for English has been studied extensively. However, there is relatively
    little work for other languages. A problem that frequently occurs when working
    with a non-English language is the scarcity of annotated training data. To overcome
    this challenge, we design a simple but effective ensemble-based framework that
    combines various transfer learning (TL) techniques. We first train several models
    using different TL methods. Then, during inference, we compute the unweighted
    average scores of the models'' predictions to extract the final set of predicted
    clusters. Furthermore, we also propose a low-cost TL method that bootstraps coreference
    resolution models by utilizing Wikipedia anchor texts. Leveraging the idea that
    the coreferential links naturally exist between anchor texts pointing to the same
    article, our method builds a sizeable distantly-supervised dataset for the target
    language that consists of tens of thousands of documents. We can pre-train a model
    on the pseudo-labeled dataset before finetuning it on the final target dataset.
    Experimental results on two benchmark datasets, OntoNotes and SemEval, confirm
    the effectiveness of our methods. Our best ensembles consistently outperform the
    baseline approach of simple training by up to 7.68\% in the F1 score. These ensembles
    also achieve new state-of-the-art results for three languages: Arabic, Dutch,
    and Spanish.'
  authors:
  - Tuan Lai
  - Heng Ji
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - CODI
  forum: ''
  id: CODI_5
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Regular long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Ensemble Transfer Learning for Multilingual Coreference Resolution
  tldr: Entity coreference resolution is an important research problem with many applications,
    including information extraction and question answering. Coreference resolution
    for English has been studied extensively. However, there is relatively little
    work for other languages. A problem that frequently occ
  track: 4th Workshop on Computational Approaches to Discourse
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The extended structural context has made scientific paper summarization
    a challenging task. This paper proposes CHANGES, a contrastive hierarchical graph
    neural network for extractive scientific paper summarization. CHANGES represents
    a scientific paper with a hierarchical discourse graph and learns effective sentence
    representations with dedicated designed hierarchical graph information aggregation.
    We also propose a graph contrastive learning module to learn global theme-aware
    sentence representations. Extensive experiments on the PubMed and arXiv benchmark
    datasets prove the effectiveness of CHANGES and the importance of capturing hierarchical
    structure information in modeling scientific papers.
  authors:
  - Haopeng Zhang
  - Xiao Liu
  - Jiawei Zhang
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - CODI
  forum: ''
  id: CODI_6
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Regular long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Contrastive Hierarchical Discourse Graph for Scientific Document Summarization
  tldr: The extended structural context has made scientific paper summarization a
    challenging task. This paper proposes CHANGES, a contrastive hierarchical graph
    neural network for extractive scientific paper summarization. CHANGES represents
    a scientific paper with a hierarchical discourse graph and learns
  track: 4th Workshop on Computational Approaches to Discourse
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We directly embed easily extractable discourse structure information (subsection,
    paragraph and text type) in a transformer-based Dutch event coreference resolution
    model in order to more explicitly provide it with structural information that
    is known to be important in coreferential relationships. Results show that integrating
    this type of knowledge leads to a significant improvement in CONLL F1 for within-document
    settings (+ 8.6\textbackslash{}\%) and a minor improvement for cross-document
    settings (+ 1.1\textbackslash{}\%).
  authors:
  - Loic De Langhe
  - Orphee De Clercq
  - Veronique Hoste
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - CODI
  forum: ''
  id: CODI_10
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Regular short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Leveraging Structural Discourse Information for Event Coreference Resolution
    in Dutch
  tldr: 'We directly embed easily extractable discourse structure information (subsection,
    paragraph and text type) in a transformer-based Dutch event coreference resolution
    model in order to more explicitly provide it with structural information that
    is known to be important in coreferential relationships. '
  track: 4th Workshop on Computational Approaches to Discourse
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Biomedical argument mining (BAM) aims at automatically identifying the
    argumentative structure in biomedical texts. However, identifying and classifying
    argumentative relations (AR) between argumentative components (AC) is challenging
    since it not only needs to understand the semantics of ACs but also need to capture
    the interactions between them. We argue that entities can serve as bridges that
    connect different ACs since entities and their mentions convey significant semantic
    information in biomedical argumentation. For example, it is common that related
    AC pairs share a common entity. Capturing such entity information can be beneficial
    for the Relation Identification (RI) task. In order to incorporate this entity
    information into BAM, we propose an Entity Coreference and Co-occurrence aware
    Argument Mining (ECCAM) framework based on an edge-oriented graph model for BAM.
    We evaluate our model on a benchmark dataset and from the experimental results
    we find that our method improves upon state-of-the-art methods.
  authors:
  - Boyang Liu
  - Viktor Schlegel
  - Riza Batista-navarro
  - Sophia Ananiadou
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - CODI
  forum: ''
  id: CODI_12
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Regular short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Entity Coreference and Co-occurrence Aware Argument Mining from Biomedical
    Literature
  tldr: Biomedical argument mining (BAM) aims at automatically identifying the argumentative
    structure in biomedical texts. However, identifying and classifying argumentative
    relations (AR) between argumentative components (AC) is challenging since it not
    only needs to understand the semantics of ACs but al
  track: 4th Workshop on Computational Approaches to Discourse
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Machine-readable inventories of discourse connectives that provide information
    on multiple levels are valuable resources for automated discourse analysis, e.g.
    discourse parsing, machine translation, text summarization and argumentation mining.
    While there are already several connective lexicons available for certain languages
    (such as German, English, French, Czech, Portuguese, Hebrew, and Spanish), currently,
    there is no such resource available for Chinese, despite it being one of the most
    widely spoken languages in the world. To address this gap, we developed the Chinese-DimLex,
    a discourse lexicon for Chinese (Mandarin). It features 137 Chinese connectives
    () and is augmented with five layers of information, specifically morphological
    variations, syntactic categories (part-of-speech), semantic relations (PDTB3.0
    sense inventory), usage examples, and English translations. Chinese-DimLex is
    publicly accessible in both XML format and through an easy-to-use web-interface,
    which enables browsing and searching of the lexicon, as well as comparison of
    discourse connectives across different languages based on their syntactic and
    semantic properties. In this extended abstract, we provide an overview of the
    data and the workflow used to populate the lexicon, followed by discussion of
    several Chinese-specific considerations and issues that arose during the process.
    By submitting this abstract, we aim to a) contribute to discourse research and
    b) receive feedback to promote and expand the lexicon for future work.
  authors:
  - Shujun Wan
  - Peter Bourgonje
  - Hongling Xiao
  - Clara Wan Ching Ho
  - Manfred Stede
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - CODI
  forum: ''
  id: CODI_13
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Extended abstract
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Chinese-DiMLex: A Lexicon of Chinese Discourse Connectives'
  tldr: Machine-readable inventories of discourse connectives that provide information
    on multiple levels are valuable resources for automated discourse analysis, e.g.
    discourse parsing, machine translation, text summarization and argumentation mining.
    While there are already several connective lexicons ava
  track: 4th Workshop on Computational Approaches to Discourse
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Recently, the identification of free connective phrases as signals for
    discourse relations has received new attention with the introduction of statistical
    models for their automatic extraction. The limited amount of annotations makes
    it still challenging to develop well-performing models. In our work, we want to
    overcome this limitation with semi-supervised learning from unlabeled news texts.
    We implement a self-supervised sequence labeling approach and filter its predictions
    by a second model trained to disambiguate signal candidates. With our novel model
    design, we report state-of-the-art results and in addition, achieve an average
    improvement of about 5\% for both exactly and partially matched alternativelylexicalized
    discourse signals due to weak supervision.
  authors:
  - Ren{\'e} Knaebel
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - CODI
  forum: ''
  id: CODI_14
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Regular short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: A Weakly-Supervised Learning Approach to the Identification of "Alternative
    Lexicalizations" in Shallow Discourse Parsing
  tldr: Recently, the identification of free connective phrases as signals for discourse
    relations has received new attention with the introduction of statistical models
    for their automatic extraction. The limited amount of annotations makes it still
    challenging to develop well-performing models. In our wor
  track: 4th Workshop on Computational Approaches to Discourse
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Discourse-aware techniques, including entity-aware approaches, play a
    crucial role in summarization. In this paper, we propose an entity-based SpanCopy
    mechanism to tackle the entity-level factual inconsistency problem in abstractive
    summarization, i.e. reducing the mismatched entities between the generated summaries
    and the source documents. Complemented by a Global Relevance component to identify
    summary-worthy entities, our approach demonstrates improved factual consistency
    while preserving saliency on four summarization datasets, contributing to the
    effective application of discourse-aware methods summarization tasks.
  authors:
  - Wen Xiao
  - Giuseppe Carenini
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - CODI
  forum: ''
  id: CODI_16
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Regular long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Entity-based SpanCopy for Abstractive Summarization to Improve the Factual
    Consistency
  tldr: Discourse-aware techniques, including entity-aware approaches, play a crucial
    role in summarization. In this paper, we propose an entity-based SpanCopy mechanism
    to tackle the entity-level factual inconsistency problem in abstractive summarization,
    i.e. reducing the mismatched entities between the g
  track: 4th Workshop on Computational Approaches to Discourse
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In this study, we examine the benefits of incorporating discourse information
    into document-level temporal dependency parsing. Specifically, we evaluate the
    effectiveness of integrating both high-level discourse profiling information,
    which describes the discourse function of sentences, and surface-level sentence
    position information into temporal dependency graph (TDG) parsing. Unexpectedly,
    our results suggest that simple sentence position information, particularly when
    encoded using our novel sentence-position embedding method, performs the best,
    perhaps because it does not rely on noisy model-generated feature inputs. Our
    proposed system surpasses the current state-of-the-art TDG parsing systems in
    performance.Furthermore, we aim to broaden the discussion on the relationship
    between temporal dependency parsing and discourse analysis, given the substantial
    similarities shared between the two tasks. We argue that discourse analysis results
    should not be merely regarded as an additional input feature for temporal dependency
    parsing. Instead, adopting advanced discourse analysis techniques and research
    insights can lead to more effective and comprehensive approaches to temporal information
    extraction tasks.
  authors:
  - Jingcheng Niu
  - Victoria Ng
  - Erin Rees
  - Simon De Montigny
  - Gerald Penn
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - CODI
  forum: ''
  id: CODI_17
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Regular short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Discourse Information for Document-Level Temporal Dependency Parsing
  tldr: In this study, we examine the benefits of incorporating discourse information
    into document-level temporal dependency parsing. Specifically, we evaluate the
    effectiveness of integrating both high-level discourse profiling information,
    which describes the discourse function of sentences, and surface-
  track: 4th Workshop on Computational Approaches to Discourse
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We present a quantitative and qualitative comparison of the discourse
    trees defined by the Rhetorical Structure Theory and Questions under Discussion
    models. Based on an empirical analysis of parallel annotations for 28 texts (blog
    posts and podcast transcripts), we conclude that both discourse frameworks capture
    similar structural information. The qualitative analysis shows that while complex
    discourse units often match between analyses, QUD structures do not indicate the
    centrality of segments.
  authors:
  - Sara Shahmohammadi
  - Hannah Seemann
  - Manfred Stede
  - Tatjana Scheffler
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - CODI
  forum: ''
  id: CODI_18
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Regular long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Encoding Discourse Structure: Comparison of RST and QUD'
  tldr: We present a quantitative and qualitative comparison of the discourse trees
    defined by the Rhetorical Structure Theory and Questions under Discussion models.
    Based on an empirical analysis of parallel annotations for 28 texts (blog posts
    and podcast transcripts), we conclude that both discourse fram
  track: 4th Workshop on Computational Approaches to Discourse
  underline_id: null
  underline_url: null
  video_url: null
- abstract: '[Finding paper]Discourse processing suffers from data sparsity, especially
    for dialogues. As a result, we explore approaches to build discourse structures
    for dialogues, based on attention matrices from Pre-trained Language Models (PLMs).
    We investigate multiple tasks for fine-tuning and show that the dialogue-tailored
    Sentence Ordering task performs best. To locate and exploit discourse information
    in PLMs, we propose an unsupervised and a semi-supervised method. Our proposals
    thereby achieve encouraging results on the STAC corpus, with F1 scores of 57.2
    and 59.3 for the unsupervised and semi-supervised methods, respectively. When
    restricted to projective trees, our scores improved to 63.3 and 68.1.'
  authors:
  - Chuyuan Li
  - Patrick Huber
  - Wen Xiao
  - Maxime Amblard
  - Chlo{\'e} Braud
  - Giuseppe Carenini
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - CODI
  forum: ''
  id: CODI_19
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Extended abstract
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Discourse Structure Extraction from Pre-Trained and Fine-Tuned Language Models
    in Dialogues
  tldr: '[Finding paper]Discourse processing suffers from data sparsity, especially
    for dialogues. As a result, we explore approaches to build discourse structures
    for dialogues, based on attention matrices from Pre-trained Language Models (PLMs).
    We investigate multiple tasks for fine-tuning and show that t'
  track: 4th Workshop on Computational Approaches to Discourse
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In discourse relation recognition, the classification labels are typically
    represented as one-hot vectors. However, the categories are in fact not all independent
    of one another  on the contrary, there are several frameworks that describe the
    labels' similarities (by e.g. sorting them into a hierarchy or describing them
    interms of features (Sanders et al., 2021)). Recently, several methods for representing
    the similarities between labels have been proposed (Zhang et al., 2018; Wang et
    al., 2018; Xiong et al., 2021). We here explore and extend the Label Confusion
    Model (Guo et al., 2021) for learning a representation for discourse relation
    labels. We explore alternative ways of informing the model about the similarities
    between relations, by representing relations in terms of their names (and parent
    category), their typical markers, or in terms of CCR features that describe the
    relations. Experimental results show that exploiting label similarity improves
    classification results.
  authors:
  - Nobel Varghese
  - Frances Yung
  - Kaveri Anuranjana
  - Vera Demberg
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - CODI
  forum: ''
  id: CODI_20
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Regular short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Exploiting Knowledge about Discourse Relations for Implicit Discourse Relation
    Classification
  tldr: In discourse relation recognition, the classification labels are typically
    represented as one-hot vectors. However, the categories are in fact not all independent
    of one another  on the contrary, there are several frameworks that describe the
    labels' similarities (by e.g. sorting them into a hierarc
  track: 4th Workshop on Computational Approaches to Discourse
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Incorporating external knowledge, such as pre-trained language models
    (PLMs), into neural topic modeling has achieved great success in recent years.
    However, employing PLMs for topic modeling generally ignores the maximum sequence
    length of PLMs and the interaction between external knowledge and bag-of-words
    (BOW). To this end, we propose a sentence-aware encoder for neural topic modeling,
    which adopts fine-grained sentence embeddings as external knowledge to entirely
    utilize the semantic information of input documents. We introduce sentence-aware
    attention for document representation, where BOW enables the model to attend on
    topical sentences that convey topic-related cues. Experiments on three benchmark
    datasets show that our framework outperforms other state-of-the-art neural topic
    models in topic coherence. Further, we demonstrate that the proposed approach
    can yield better latent document-topic features through improvement on the document
    classification.
  authors:
  - Hao Liu
  - Jingsheng Gao
  - Suncheng Xiang
  - Ting Liu
  - Yuzhuo Fu
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - CODI
  forum: ''
  id: CODI_22
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Regular short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'SAE-NTM: Sentence-Aware Encoder for Neural Topic Modeling'
  tldr: Incorporating external knowledge, such as pre-trained language models (PLMs),
    into neural topic modeling has achieved great success in recent years. However,
    employing PLMs for topic modeling generally ignores the maximum sequence length
    of PLMs and the interaction between external knowledge and bag
  track: 4th Workshop on Computational Approaches to Discourse
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Document-level context for neural machine translation (NMT) is crucial
    to improve the translation consistency and cohesion, the translation of ambiguous
    inputs, as well as several other linguistic phenomena.Many works have been published
    on the topic of document-level NMT, but most restrict the system to only local
    context, typically including just the one or two preceding sentences as additional
    information.This might be enough to resolve some ambiguous inputs, but it is probably
    not sufficient to capture some document-level information like the topic or style
    of a conversation.When increasing the context size beyond just the local context,
    there are two challenges: (i) the memory usage increases exponentially (ii) the
    translation performance starts to degrade.We argue that the widely-used attention
    mechanism is responsible for both issues.Therefore, we propose a constrained attention
    variant that focuses the attention on the most relevant parts of the sequence,
    while simultaneously reducing the memory consumption.For evaluation, we utilize
    targeted test sets in combination with novel evaluation techniques to analyze
    the translations in regards to specific discourse-related phenomena.We find that
    our approach is a good compromise between sentence-level NMT vs attending to the
    full context, especially in low resource scenarios.'
  authors:
  - Christian Herold
  - Hermann Ney
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - CODI
  forum: ''
  id: CODI_23
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Regular long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Improving Long Context Document-Level Machine Translation
  tldr: Document-level context for neural machine translation (NMT) is crucial to
    improve the translation consistency and cohesion, the translation of ambiguous
    inputs, as well as several other linguistic phenomena.Many works have been published
    on the topic of document-level NMT, but most restrict the syst
  track: 4th Workshop on Computational Approaches to Discourse
  underline_id: null
  underline_url: null
  video_url: null
- abstract: In this paper, we present principles of constructing and resolving ambiguity
    in implicit discourse relations. Following these principles, we created a dataset
    in both English and Egyptian Arabic that controls for semantic disambiguation,
    enabling the investigation of prosodic features in future work. In these datasets,
    examples are two-part sentences with an implicit discourse relation that can be
    ambiguously read as either causal or concessive, paired with two different preceding
    context sentences forcing either the causal or the concessive reading. We also
    validated both datasets by humans and language models (LMs) to study whether context
    can help humans or LMs resolve ambiguities of implicit relations and identify
    the intended relation. As a result, this task posed no difficulty for humans,
    but proved challenging for BERT/CamelBERT and ELECTRA/AraELECTRA models.
  authors:
  - Ahmed Ruby
  - Sara Stymne
  - Christian Hardmeier
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - CODI
  forum: ''
  id: CODI_24
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Regular long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Unpacking Ambiguous Structure: A Dataset for Ambiguous Implicit Discourse
    Relations for English and Egyptian Arabic'
  tldr: In this paper, we present principles of constructing and resolving ambiguity
    in implicit discourse relations. Following these principles, we created a dataset
    in both English and Egyptian Arabic that controls for semantic disambiguation,
    enabling the investigation of prosodic features in future work
  track: 4th Workshop on Computational Approaches to Discourse
  underline_id: null
  underline_url: null
  video_url: null
- abstract: '[***NOTE: This is an ACL Findings paper***]Automatic discourse processing
    is bottlenecked by data: current discourse formalisms pose highly demanding annotation
    tasks involving large taxonomies of discourse relations, making them inaccessible
    to lay annotators. This work instead adopts the linguistic framework of Questions
    Under Discussion (QUD) for discourse analysis and seeks to derive QUD structures
    automatically. QUD views each sentence as an answer to a question triggered in
    prior context; thus, we characterize relationships between sentences as free-form
    questions, in contrast to exhaustive fine-grained taxonomies. We develop the first-of-its-kind
    QUD parser that derives a dependency structure of questions over full documents,
    trained using a large, crowdsourced question-answering dataset DCQA (Ko et al.,
    2022). Strong human evaluation results show that QUD dependency parsing is highly
    feasible under this crowdsourced, generalizable annotation scheme. We illustrate
    how our QUD structure is distinct from RST trees, and demonstrate the utility
    of QUD analysis in the context of document simplification. Our findings show that
    QUD parsing is an appealing alternative for automatic discourse processing.'
  authors:
  - Wei-jen Ko
  - Yating Wu
  - Cutter Dalton
  - Dananjay Srinivas
  - Greg Durrett
  - Junyi Jessy Li
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - CODI
  forum: ''
  id: CODI_25
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Regular long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Discourse Analysis via Questions and Answers: Parsing Dependency Structures
    of Questions Under Discussion'
  tldr: '[***NOTE: This is an ACL Findings paper***]Automatic discourse processing
    is bottlenecked by data: current discourse formalisms pose highly demanding annotation
    tasks involving large taxonomies of discourse relations, making them inaccessible
    to lay annotators. This work instead adopts the linguisti'
  track: 4th Workshop on Computational Approaches to Discourse
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Text simplification research has mostly focused on sentence-level simplification,
    even though many desirable edits --- such as adding relevant background information
    or reordering content --- may require document-level context.Prior work has also
    predominantly framed simplification as a single-step, input-to-output task, only
    implicitly modeling the fine-grained, span-level edits that elucidate the simplification
    process.To address both gaps, we introduce the SWiPE dataset, which reconstructs
    the document-level editing process from English Wikipedia (EW) articles to paired
    Simple Wikipedia (SEW) articles. In contrast to prior work, SWiPE leverages the
    entire revision history when pairing pages in order to better identify simplification
    edits. We work with Wikipedia editors to annotate 5,000 EW-SEW document pairs,
    labeling more than 40,000 edits with proposed 19 categories.To scale our efforts,
    we propose several models to automatically label edits, achieving an F-1 score
    of up to 70.6, indicating that this is a tractable but challenging NLU task. Finally,
    we categorize the edits produced by several simplification models and find that
    SWiPE-trained models generate more complex edits while reducing unwanted edits.
  authors:
  - Philippe Laban
  - Jesse Vig
  - Wojciech Kryscinski
  - Shafiq Joty
  - Caiming Xiong
  - Chien-sheng Wu
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - CODI
  forum: ''
  id: CODI_26
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Regular long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'SWiPE: A Dataset for Document-Level Simplification of Wikipedia Pages'
  tldr: Text simplification research has mostly focused on sentence-level simplification,
    even though many desirable edits --- such as adding relevant background information
    or reordering content --- may require document-level context.Prior work has also
    predominantly framed simplification as a single-step,
  track: 4th Workshop on Computational Approaches to Discourse
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'While a large body of literature suggests that large language models
    (LLMs) acquire rich linguistic representations, little is known about whether
    they adapt to linguistic biases in a human-like way. The present study probes
    this question by comparing InstructGPT''s performance on learning referential
    biases with results from real psycholinguistic experiments. Recent psycholinguistic
    studies suggest that humans adapt their referential biases with exposure to referential
    patterns; closely replicating three relevant psycholinguistic experiments from
    Johnson and Arnold (2022) in an in-context learning (ICL) framework, we found
    that InstructGPT adapts its pronominal interpretations in response to the frequency
    of referential patterns in the local discourse, though in a limited fashion: adaptation
    was only observed relative to syntactic but not semantic biases. Our results provide
    further evidence that contemporary LLMs discourse representations are sensitive
    to syntactic patterns in the local context but less so to semantic patterns.'
  authors:
  - Suet-ying Lam
  - Qingcheng Zeng
  - Kexun Zhang
  - Chenyu You
  - Rob Voigt
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - CODI
  forum: ''
  id: CODI_27
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Regular long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'Replicate and Compare with Humans: LLMs Represent Partial Semantic Knowledge
    in Pronoun Interpretation'
  tldr: While a large body of literature suggests that large language models (LLMs)
    acquire rich linguistic representations, little is known about whether they adapt
    to linguistic biases in a human-like way. The present study probes this question
    by comparing InstructGPT's performance on learning referentia
  track: 4th Workshop on Computational Approaches to Discourse
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Transforming narrative structure to implicit discourse relations in long-form
    text has recently seen a mindset shift toward assessing generation consistency.
    To this extent, summarization of lengthy biographical discourse is of practical
    benefit to readers, as it helps them decide whether immersing for days or weeks
    in a bulky book turns a rewarding experience. Machine-generated summaries can
    reduce the cognitive load and the time spent by authors to write the summary.
    Nevertheless, summarization faces significant challenges of factual inconsistencies
    with respect to the inputs. In this paper, we explored a two-step summary generation
    aimed to retain source-summary faithfulness. Our method uses a graph representation
    to rank sentence saliency in each of the novel chapters, leading to distributing
    summary segments in distinct regions of the chapter. Basing on the previously
    extracted sentences we produced an abstractive summary in a manner more computationally
    tractable for detecting inconsistent information. We conducted a series of quantitative
    analyses on a test set of four long biographical novels and showed to improve
    summarization quality in automatic evaluation over both single-tier settings and
    external baselines.
  authors:
  - Avi Bleiweiss
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - CODI
  forum: ''
  id: CODI_28
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Regular long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Two-step Text Summarization for Long-form Biographical Narrative Genre
  tldr: 'Transforming narrative structure to implicit discourse relations in long-form
    text has recently seen a mindset shift toward assessing generation consistency.
    To this extent, summarization of lengthy biographical discourse is of practical
    benefit to readers, as it helps them decide whether immersing '
  track: 4th Workshop on Computational Approaches to Discourse
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Time pressure and topic negotiation may impose constraints on how people
    leverage discourse relations (DRs) in spontaneous conversational contexts. In
    this work, we adapt a system of DRs for written language to spontaneous dialogue
    using crowdsourced annotations from novice annotators. We then test whether discourse
    relations are used differently across several types of multi-utterance contexts.
    We compare the patterns of DR annotation within and across speakers and within
    and across turns. Ultimately, we find that different discourse contexts produce
    distinct distributions of discourse relations, with single-turn annotations creating
    the most uncertainty for annotators. Additionally, we find that the discourse
    relation annotations are of sufficient quality to predict from embeddings of discourse
    units.
  authors:
  - S. Magal\'{i} L\'{o}pez Cortez
  - Cassandra L. Jacobs
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - CODI
  forum: ''
  id: CODI_29
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Regular short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: The distribution of discourse relations within and across turns in spontaneous
    conversation
  tldr: 'Time pressure and topic negotiation may impose constraints on how people
    leverage discourse relations (DRs) in spontaneous conversational contexts. In
    this work, we adapt a system of DRs for written language to spontaneous dialogue
    using crowdsourced annotations from novice annotators. We then test '
  track: 4th Workshop on Computational Approaches to Discourse
  underline_id: null
  underline_url: null
  video_url: null
- abstract: Our paper investigates the use of discourse embedding techniques to develop
    a community recommendation system that focuses on mental health support groups
    on social media. Social media platforms provide a means for users to anonymously
    connect with communities that cater to their specific interests. However, with
    the vast number of online communities available, users may face difficulties in
    identifying relevant groups to address their mental health concerns. To address
    this challenge, we explore the integration of discourse information from various
    subreddit communities using embedding techniques to develop an effective recommendation
    system. Our approach involves the use of content-based and collaborative filtering
    techniques to enhance the performance of the recommendation system. Our findings
    indicate that the proposed approach outperforms the use of each technique separately
    and provides interpretability in the recommendation process.
  authors:
  - Hy Dang
  - Bang Nguyen
  - Noah Ziems
  - Meng Jiang
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - CODI
  forum: ''
  id: CODI_30
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Regular long
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Embedding Mental Health Discourse for Community Recommendation
  tldr: Our paper investigates the use of discourse embedding techniques to develop
    a community recommendation system that focuses on mental health support groups
    on social media. Social media platforms provide a means for users to anonymously
    connect with communities that cater to their specific interests.
  track: 4th Workshop on Computational Approaches to Discourse
  underline_id: null
  underline_url: null
  video_url: null
- abstract: We present a corpus of parallel German-language simplified newspaper articles.
    The articles have been aligned at sentence level and annotated according to the
    Rhetorical Structure Theory (RST) framework. These RST annotated texts could shed
    light on structural aspects of text complexity and how simplifications work on
    a text-level.
  authors:
  - Freya Hewett
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - CODI
  forum: ''
  id: CODI_31
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Regular short
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'APA-RST: A Text Simplification Corpus with RST Annotations'
  tldr: We present a corpus of parallel German-language simplified newspaper articles.
    The articles have been aligned at sentence level and annotated according to the
    Rhetorical Structure Theory (RST) framework. These RST annotated texts could shed
    light on structural aspects of text complexity and how simp
  track: 4th Workshop on Computational Approaches to Discourse
  underline_id: null
  underline_url: null
  video_url: null
- abstract: '[Findings Paper]To date, most work on text simplification has focused
    on sentence-level inputs. Early attempts at document simplification merely applied
    these approaches iteratively over the sentences of a document. However, this fails
    to coherently preserve the discourse structure, leading to suboptimal output quality.
    Recently, strategies from controllable simplification have been leveraged to achieve
    state-of-the-art results on document simplification by first generating a document-level
    plan (a sequence of sentence-level simplification operations) and using this plan
    to guide sentence-level simplification downstream. However, this is still limited
    in that the simplification model has no direct access to the local inter-sentence
    document context, likely having a negative impact on surface realisation. We explore
    various systems that use document context within the simplification process itself,
    either by iterating over larger text units or by extending the system architecture
    to attend over a high-level representation of document context. In doing so, we
    achieve state-of-the-art performance on the document simplification task, even
    when not relying on plan-guidance. Further, we investigate the performance and
    efficiency tradeoffs of system variants and make suggestions of when each should
    be preferred.'
  authors:
  - Liam Cripwell
  - Jol Legrand
  - Claire Gardent
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - CODI
  forum: ''
  id: CODI_33
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Extended abstract
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Context-Aware Document Simplification
  tldr: '[Findings Paper]To date, most work on text simplification has focused on
    sentence-level inputs. Early attempts at document simplification merely applied
    these approaches iteratively over the sentences of a document. However, this fails
    to coherently preserve the discourse structure, leading to subop'
  track: 4th Workshop on Computational Approaches to Discourse
  underline_id: null
  underline_url: null
  video_url: null
- abstract: (This is a findings paper).Collaboration increasingly happens online.This
    is especially true for large groups working on global tasks, with collaborators
    all around the world. The size and distributed nature of such groups makes decision-making
    challenging. This paper proposes a set of dialog acts for the study of decision-making
    mechanisms in such groups, and provides a new annotated dataset based on real-world
    data from the public mail-archives of one such organization  the Internet Engineering
    Task Force (IETF). Weprovide an initial data analysis showing that this dataset
    can be used to better understanddecision-making in such organizations. Finally,
    we experiment with a preliminary transformerbased dialog act tagging model.
  authors:
  - Mladen Karan
  - Prashant Khare
  - Ravi Shekhar
  - Stephen Mcquistin
  - Colin Perkins
  - Ignacio Castro
  - Gareth Tyson
  - Patrick Healey
  - Matthew Purver
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - CODI
  forum: ''
  id: CODI_35
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Extended abstract
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: An Email Dataset for Analyzing Large-Group Decision-Making
  tldr: (This is a findings paper).Collaboration increasingly happens online.This
    is especially true for large groups working on global tasks, with collaborators
    all around the world. The size and distributed nature of such groups makes decision-making
    challenging. This paper proposes a set of dialog acts f
  track: 4th Workshop on Computational Approaches to Discourse
  underline_id: null
  underline_url: null
  video_url: null
- abstract: The submitted paper (camera-ready version) has been accepted to the Findings
    of ACL 2023. We are also submitting it to the LAW-XVII workshop.
  authors:
  - Yang Janet Liu
  - Amir Zeldes
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - CODI
  forum: ''
  id: CODI_37
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Extended abstract
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: 'GUMSum: Multi-Genre Data and Evaluation for English Abstractive Summarization'
  tldr: The submitted paper (camera-ready version) has been accepted to the Findings
    of ACL 2023. We are also submitting it to the LAW-XVII workshop.
  track: 4th Workshop on Computational Approaches to Discourse
  underline_id: null
  underline_url: null
  video_url: null
- abstract: '(Accepted to the Findings of ACL 2023) One crucial aspect of democracy
    is fair information sharing. While it is hard to prevent biases in news, they
    should be identified for better transparency. We propose an approach to automatically
    characterize biases that takes into account structural differences and that is
    efficient for long texts. This yields new ways to provide explanations for a textual
    classifier, going beyond mere lexical cues. We show that: (i) the use of discourse-based
    structure-aware document representations compare well to local, computationally
    heavy, or domain-specific models on classification tasks that deal with textual
    bias (ii) our approach based on different levels of granularity allows for the
    generation of better explanations of model decisions, both at the lexical and
    structural level, while addressing the challenge posed by long texts.'
  authors:
  - Nicolas Devatine
  - Philippe Muller
  - Chlo{\'e} Braud
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - CODI
  forum: ''
  id: CODI_38
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Extended abstract
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: An Integrated Approach for Political Bias Prediction and Explanation Based
    on Discursive Structure
  tldr: (Accepted to the Findings of ACL 2023) One crucial aspect of democracy is
    fair information sharing. While it is hard to prevent biases in news, they should
    be identified for better transparency. We propose an approach to automatically
    characterize biases that takes into account structural difference
  track: 4th Workshop on Computational Approaches to Discourse
  underline_id: null
  underline_url: null
  video_url: null
- abstract: 'Event Coreference Resolution (ECR) is the task of linking mentions of
    the same event either within or across documents. Most mention pairs are not coreferent,
    yet many that are coreferent can be identified through simple techniques such
    as lemma matching of the event triggers or the sentences in which they appear.
    Existing methods for training coreference systems sample from a largely skewed
    distribution, making it difficult for the algorithm to learn coreference beyond
    surface matching. Additionally, these methods are intractable because of the quadratic
    operations needed. To address these challenges, we break the problem of ECR into
    two parts: a) a heuristic to efficiently filter out a large number of non-coreferent
    pairs, and b) a training approach on a balanced set of coreferent and non-coreferent
    mention pairs. By following this approach, we show that we get comparable results
    to the state of the art on two popular ECR datasets while significantly reducing
    compute requirements. We also analyze the mention pairs that are "hard" to accurately
    classify as coreferent or non-coreferent. This is an accepted Findings paper at
    ACL 2023'
  authors:
  - Shafiuddin Rehan Ahmed
  - Abhijnan Nath
  - James H. Martin
  - Nikhil Krishnaswamy
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - CODI
  forum: ''
  id: CODI_39
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Extended abstract
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: '$2*n$ is better than $n^2$: Decomposing Event Coreference Resolution into
    Two Tractable Problems'
  tldr: Event Coreference Resolution (ECR) is the task of linking mentions of the
    same event either within or across documents. Most mention pairs are not coreferent,
    yet many that are coreferent can be identified through simple techniques such
    as lemma matching of the event triggers or the sentences in whi
  track: 4th Workshop on Computational Approaches to Discourse
  underline_id: null
  underline_url: null
  video_url: null
- abstract: (ACL Findings paper)Inspired by the curvature of space-time, we introduce
    Curved Contrastive Learning (CCL), a novel representation learning technique for
    learning the relative turn distance between utterance pairs in multi-turn dialogues.
    The resulting bi-encoder models can guide transformers as a response ranking model
    towards a goal in a zero-shot fashion by projecting the goal utterance and the
    corresponding reply candidates into a latent space. Here the cosine similarity
    indicates the distance/reachability of a candidate utterance toward the corresponding
    goal. Furthermore, we explore how these forward-entailing language representations
    can be utilized for assessing the likelihood of sequences by the entailment strength
    i.e. through the cosine similarity of its individual members (encoded separately)
    as an emergent property in the curved space. These non-local properties allow
    us to imagine the likelihood of future patterns in dialogues, specifically by
    ordering/identifying future goal utterances that are multiple turns away, given
    a dialogue context. As part of our analysis, we investigate characteristics that
    make conversations (un)plannable and find strong evidence of planning capability
    over multiple turns (in 61.56% over 3 turns) in conversations from the DailyDialog
    dataset. Finally, we show how we achieve higher efficiency in sequence modeling
    tasks compared to previous work thanks to our relativistic approach, where only
    the last utterance needs to be encoded and computed during inference.
  authors:
  - Justus-jonas Erker
  - Stefan Schaffer
  - Gerasimos Spanakis
  card_image_path: ''
  category: Workshop
  demo_url: null
  event_ids:
  - CODI
  forum: ''
  id: CODI_40
  is_paper: true
  keywords: []
  material: null
  paper_pdf: null
  paper_type: Extended abstract
  poster_pdf: null
  preview_image: null
  program: Workshop
  similar_paper_ids: []
  slides_pdf: null
  title: Imagination is All You Need! Curved Contrastive Learning for Abstract Sequence
    Modeling Utilized on Long Short-Term Dialogue Planning
  tldr: '(ACL Findings paper)Inspired by the curvature of space-time, we introduce
    Curved Contrastive Learning (CCL), a novel representation learning technique for
    learning the relative turn distance between utterance pairs in multi-turn dialogues.
    The resulting bi-encoder models can guide transformers as a '
  track: 4th Workshop on Computational Approaches to Discourse
  underline_id: null
  underline_url: null
  video_url: null
